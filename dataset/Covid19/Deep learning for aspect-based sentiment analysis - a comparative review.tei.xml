<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2020-05-07T04:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Elsevier BV</publisher>
				<availability status="unknown"><p>Copyright Elsevier BV</p>
				</availability>
				<date type="published" when="2019">2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><forename type="middle">Ha</forename><surname>Do</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pwc</forename><surname>Prasad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelika</forename><surname>Maag</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abeer</forename><surname>Alsadoon</surname></persName>
						</author>
						<title level="a" type="main">Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Expert Systems with Applications</title>
						<title level="j" type="abbrev">Expert Systems with Applications</title>
						<idno type="ISSN">0957-4174</idno>
						<imprint>
							<publisher>Elsevier BV</publisher>
							<biblScope unit="volume">118</biblScope>
							<biblScope unit="page" from="272" to="299"/>
							<date type="published" when="2019">2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.eswa.2018.10.003</idno>
					<note type="submission">Article history: Received 3 April 2018 Revised 13 August 2018 Accepted 2 October 2018</note>
					<note>Contents lists available at ScienceDirect Expert Systems With Applications journal homepage: www.elsevier.com/locate/eswa Review a r t i c l e i n f o a b s t r a c t</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The increasing volume of user-generated content on the web has made sentiment analysis an important tool for the extraction of information about the human emotional state. A current research focus for sentiment analysis is the improvement of granularity at aspect level, representing two distinct aims: aspect extraction and sentiment classification of product reviews and sentiment classification of target-dependent tweets. Deep learning approaches have emerged as a prospect for achieving these aims with their ability to capture both syntactic and semantic features of text without requirements for high-level feature engineering , as is the case in earlier methods. In this article, we aim to provide a comparative review of deep learning for aspect-based sentiment analysis to place different approaches in context.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The evolution of web technologies has enabled new means of communication through user-generated content, in the form of blogs, social networks, forums, website reviews, e-commerce websites, etc. <ref type="bibr" target="#b85">( Rana &amp; Cheah, 2016 )</ref>. Following this exponential growth, there has been strong interest from individuals and organisations in data mining technologies to exploit this source of subjective information. One of the most prolific research areas in computer sciences is sentiment analysis, which aims to identify and extract user opinions <ref type="bibr" target="#b12">( Cambria, Poria, Gelbukh, &amp; Thelwall, 2017 )</ref>.</p><p>In their seminal work on Aspect-based Sentiment Analysis (ABSA), <ref type="bibr" target="#b45">Hu et al. (2004)</ref> argued that the study of sentiment analysis is possible at three levels -document, sentence and entity or aspect. A focus on the document or sentence level presumes that only one topic is expressed in the document or sentence, which is not the case in many situations. A more thorough analysis, therefore, requires investigation at entity and aspect level to identify entities and related aspects and classify sentiments associated with these entities and aspects. Examples of entities include products, services, topics, issues, persons, organizations or events, which usually have several aspects ( Jim¨¦nez-Zafra, Mart¨ªn-Valdivia, <ref type="bibr" target="#b49">Mart¨ªnez-C¨¢mara, &amp; Ure?a-L¨®pez, 2016 )</ref>. For example, a laptop consists of a CPU, screen and keyboard; each also represents an aspect. Furthermore, as an entity is the hierarchy of all aspects, * Correspondence author. Penatiyana Withanage Chandana Prasad Charles Sturt University, Sydney Campus, Sydney, Australia.</p><p>E-mail addresses: cwithana@studygroup.com (P. Prasad), aalsadoon@studygroup.com (A. Maag), amaag@studygroup.com <ref type="bibr">(A. Alsadoon).</ref> it is also a general aspect. For the purpose of this paper, ABSA signifies sentiment analysis at entity or aspect level.</p><p>This kind of fine-grained analysis has generally relied on machine learning techniques, which, although effective, require large, domain specific datasets and manual training data ( <ref type="bibr" target="#b45">Hu &amp; Liu, 2004</ref> ). Furthermore, an aspect may be represented by different words requiring more than one classification algorithm <ref type="bibr" target="#b91">( Schouten &amp; Frasincar, 2016 )</ref>. More recently, experimental work with machine learning methods has shown promise, with Poria, <ref type="bibr" target="#b82">Cambria and Gelbukh (2016)</ref> reporting higher accuracy using deep convolutional neural networks, a feature of deep learning (DL), named for its 'deep' multilayer processing technique that uses successive module layers to build on prior output using a backpropagation algorithm <ref type="bibr" target="#b56">( Lecun, Bengio &amp; Hinton, 2015 )</ref>. In each layer, input is converted to numerical representations, which are subsequently classified. Thus, an increasingly higher level of abstraction is achieved <ref type="bibr" target="#b39">( Goodfellow, Bengio &amp; Courville, 2016 )</ref>. A range of algorithms (i.e. deep neural networks (DNN), recurrent neural networks (RNN), convolutional neural networks (CNN), recursive neural networks (RecNN), etc.) facilitate analysis in different fields with deep neural networks particularly suited to fine-grained work due to the significant number of layers of connected processors, activated either by sensors from the environment or by the weighted computations from preceding neurons <ref type="bibr" target="#b90">( Schmidhuber, 2015</ref> ). An increase in the level of depth leads to higher capability for selective and invariant representation (i.e. extricating different objects) ( <ref type="bibr" target="#b56">Lecun et al., 2015 )</ref>. Applied to Natural Language Processing (NLP) tasks, the advantage of DL lies in its independence from expert knowledge and linguistic resources <ref type="bibr" target="#b86">( Rojas-Barahona, 2016</ref> ) as well as in its superior performance, demonstrated in the areas of 'name-entity recognition' <ref type="bibr" target="#b20">( Chiu &amp; Nichols, 2015;</ref><ref type="bibr">Lample, Ballesteros, Subrama- nian, Kawakami, &amp; Dyer, 2016 ;</ref><ref type="bibr" target="#b64">X. Ma &amp; Hovy, 2016;</ref><ref type="bibr" target="#b92">Shen, Yun, Lipton, Kronrod, &amp; Anandkumar, 2017;</ref><ref type="bibr" target="#b94">Strubell, Verga, Belanger, &amp; McCallum, 2017;</ref><ref type="bibr" target="#b125">Yang, Salakhutdinov, &amp; Cohen, 2016 )</ref>, 'semantic role labelling' <ref type="bibr" target="#b27">( Do, Bethard, &amp; Moens, 2017;</ref><ref type="bibr" target="#b69">Marcheggiani, Frolov, &amp; Titov, 2017 )</ref> and 'Parts-Of-Speech (POS) tagging' ( <ref type="bibr" target="#b64">Ma et al, 2016;</ref><ref type="bibr" target="#b125">Yang et al., 2016 )</ref>.</p><p>Early approaches to DL investigated linguistic features, grammatical relations, machine learning classifiers and topic modelling to identify aspects and polarities <ref type="bibr" target="#b91">( Schouten &amp; Frasincar, 2016 )</ref>. More recently, DL methods have been successfully applied to NLP, which makes it interesting to investigate how DL has performed when set fine-grained tasks such as ABSA.</p><p>To the best of our knowledge, this work is the first of its type to investigate application of DL methods to ABSA tasks. Recent surveys on <ref type="bibr">DL</ref> have not yet covered the areas of ABSA in-depth, even in the work of <ref type="bibr" target="#b99">Tang, Qin, &amp; Liu, (2015)</ref>, <ref type="bibr" target="#b86">Rojas-Barahona, (2016)</ref>, Young, Hazarika, Poria, <ref type="bibr" target="#b12">Cambria et al., (2017)</ref> , and L. Zhang,  . Deep learning methods are also absent from surveys on ABSA methods, evident from the work of <ref type="bibr" target="#b85">Rana et al. (2016)</ref> , and <ref type="bibr" target="#b91">Schouten et al. (2016)</ref> . This paper, rather than repeating established findings from previous surveys, aims to present and compare more recent developments in DL approaches in general and for ABSA in particular. This review is specifically designed for students and researchers in the field of natural language processing, who would like to investigate deep neural networks as well as recent trends in research in ABSA.</p><p>The remainder of the paper is organized as follows: Section 2 defines the tasks of ABSA and evaluation measures; Section 3 and 4 analyse DL models for ABSA, investigating in particular how DL affects the interpretation, architecture and performance of ABSA tasks; Section 5 discusses challenges in terms of ABSA and sentiment analysis; the conclusion in section 6 summarises the current landscapes of ABSA and deep learning methods. The sentence has two opinion targets: sushi &amp; service. The category of "sushi" is "Food", with the attribute being "Quality" and polarity "Positive". The category is "Service", with an attribute of "General" and polarity of "Positive". focuses only on explicit targets while ACD is concerned with both explicit and implicit aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Domain and benchmark datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Aspect-Based Sentiment Analysis (ABSA)</head><p>2.1. The three tasks of ABSA <ref type="bibr" target="#b79">Pontiki et al. (2016)</ref> have assigned three important subtasks to ABSA: (i) Opinion target extraction (OTE), (ii) Aspect category detection (ACD) and (iii) Sentiment Polarity (SP), whereby OTE is concerned with the extraction of aspect terms (I.e. entity or attribute), ACD with identification of associated entities and attributes and (iii) and SP with the clarification of the sentiment polarity of the aspects. <ref type="figure" target="#fig_0">Fig. 1</ref> represents the three tasks of ABSA: The aim of OTE is to extract the opinion target (also referred as "aspect term" 1 _bookmark0) from sentences -in this case "sushi", or service. For ACD, given the predefined categories, the task is to identify the entity -the aspect of "sushi" as "food" and an attribute denoting "quality". SP identifies the sentiment of a target aspect -"positive" or "negative". It should be noted that the two latter tasks correlate strongly with each other as only through the combination of "great" and "sushi", can both, aspect category and polarity be recognised.</p><p>In terms of aspects, <ref type="bibr" target="#b45">Hu et al. (2004)</ref> identified two types, explicit and implicit, depending on whether or not the aspect words were explicitly stated. <ref type="figure" target="#fig_1">Fig. 2</ref> provides an example of an implicit opinion target in the statement "My HP is very heavy". It is clear that polarity and aspect can still be inferred. This implies that OTE ABSA is mainly applied to customer reviews from websites and e-commerce platforms such as Amazon, Yelp, Taobao and others. These are likely to be product or service reviews and it may be assumed that in each of these only one entity is mentioned but one or more aspects <ref type="bibr" target="#b88">( Saeidi, Bouchard, Liakata &amp; Riedel, 2016 )</ref>. In recent years, systems have been developed for domains such as electronic product reviews (laptop, camera, and phone) and hospitality reviews (restaurant, hotels). A number of benchmark datasets have been made available, including the customer review dataset by <ref type="bibr" target="#b45">Hu et al. (2004)</ref> and a number of datasets released by 'International Workshop on Semantic Evaluation' <ref type="bibr" target="#b11">(SemEval -2016</ref> on laptop, camera, restaurant and hotel reviews ( <ref type="bibr" target="#b81">Pontiki et al., 2014</ref><ref type="bibr" target="#b80">Pontiki et al., , 2015</ref><ref type="bibr" target="#b79">Pontiki et al., , 2016</ref>.</p><p>Another line of research for ABSA is targeted (or targetdependent) sentiment analysis <ref type="bibr" target="#b112">( Vo &amp; Zhang, 2015 )</ref>, which classifies opinion polarities of a certain target entity mentioned in sentences under scrutiny (normally a tweet). A number of benchmark datasets have been developed for this type such as the Twitter dataset by Dong, Wei, Tan, Tang, Zhou, and <ref type="bibr" target="#b28">Xu (2014)</ref> . <ref type="table" target="#tab_0">Table 1</ref> below provides a list of publicly available data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Previous approaches to ABSA tasks</head><p>Earlier approaches to identification of OTE and ACD (for example, <ref type="bibr" target="#b45">Hu et al., 2004</ref> ) were based on frequency of nouns and noun phrases in the text, with the assumption that aspect words were more likely to be repeated. The limitation of this approach is the dependency on the frequency of certain word categories (nouns/noun phrases), which may work well if the text contains high-frequency terms, but may fail if terms are infrequent ( <ref type="bibr" target="#b85">Rana et al., 2016</ref>) . XML tag, in which two attributes ("from and "to") that indicate its start and end offset in the text &lt; sentence id = "81" &gt; &lt; text &gt; Lightweight and the screen is beautiful! &lt; /text &gt; &lt; aspectTerms &gt; &lt; aspectTerm term = "screen" polarity = "positive" from = "20" to = "26"/ &gt; &lt; /aspectTerms &gt; &lt; /sentence &gt; XML tag of {E#A, polarity} &lt; sentence id = "1004293:0"&gt; &lt; text &gt; Judging from previous posts this used to be a good place, but not any longer. &lt; /text &gt; &lt; Opinions &gt; &lt; Opinion target = "place" category = "RESTAURANT#GENERAL" polarity = "negative" from = "51" to = "56"/ &gt; &lt; /Opinions XML tag of {E#A, polarity} &lt; sentence id = "1661043:4"&gt; &lt; text &gt; Decor is charming. &lt; /text &gt; &lt; Opinions &gt; &lt; Opinion target = "Decor" category = "AMBIENCE#GENERAL" polarity = "positive" from = "0" to = "5"/ &gt; &lt; /Opinions &gt; &lt; /sentence &gt;  MMAX format &lt; markables xmlns = "www.eml.org/NameSpaces/OpinionExpression"&gt; &lt; markable id = "markable_38" span = "word_118..word_119" referent = "empty" annotation_type = "holder" mmax_level = "opinionexpression" isreference = "true" / &gt; &lt; markable id = "markable_40" span = "word_123" annotation_type = "opinionexpression" opinionholder = "markable_38" mmax_level = "opinionexpression" opiniontarget = "markable_39" strength = "average" polarity = "positive" opinionmodifier = "empty" / &gt; &lt; markable id = "markable_37" span = "word_126" annotation_type = "target" mmax_level = "opinionexpression" isreference = "false" / &gt; &lt; markable id = "markable_39" span = "word_124" referent = "markable_37" annotation_type = "target" mmax_level = "opinionexpression" isreference = "true" / &gt; &lt; /markables &gt;  Financial news headlines: 529 samples; financial microblogs: 774 annotated posts JSON nodes with sentiment score ranged from -1 to 1, "target" indicates opinion target, and "aspect" indicates aspect categories according to different level "1": { "sentence": "Royal Mail chairman Donald Brydon set to step down", "info": [ { "snippets": "['set to step down']", "target": "Royal Mail", "sentiment_score": "-0.374", "aspects": " <ref type="bibr">[</ref> Others extracted OTE and ACD by exploiting opinion and target relations. Poria, <ref type="bibr" target="#b84">Chaturvedi, Cambria, and Bisio (2016)</ref> and <ref type="bibr" target="#b77">Piryani, Gupta, and Singh (2017)</ref> focused on rule-based linguistic patterns, including stop words and negation, etc. The assumption here was that it is easier to detect sentiment than aspect words. The authors proposed a set of opinion rules to first identify a sentiment word, and then use grammatical relations to build the syntactic structure of sentences and to detect the aspect. The final step consists of refinement where infrequent words are added and irrelevant aspects are removed. The lexical relation between sentiment words and aspects is the key element in this method, which is able to identify low-frequency aspects ( <ref type="bibr" target="#b91">Schouten et al., 2016)</ref> . However, a drawback is reliance on grammatical accuracy of the sentence and the requirement for manipulation .</p><formula xml:id="formula_0">&lt; classMention id = "StructuralSentiment_Instance_40,033"&gt; &lt; mentionClass id = "Mention.Person"&gt; Mention.Person &lt; /mentionClass &gt; &lt; hasSlotMention id = "StructuralSentiment_Instance_40,395" / &gt; &lt; /classMention</formula><p>Topic modelling has been widely used to perform ACD tasks, with the most popular model being Latent Dirichlet Allocation (LDA) as implemented in <ref type="bibr" target="#b83">Poria et al. (2015)</ref>, Alam, Ryu, and <ref type="bibr" target="#b5">Lee (2016)</ref>, Garc¨ªa-Pablos, Cuadros, and <ref type="bibr" target="#b35">Rigau (2018)</ref> , and Weichselbraun, Gindl, Fischer, <ref type="bibr" target="#b120">Vakulenko and Scharl, (2017)</ref> . The basis of LDA is the introduction of a latent variable "topic" between the variables "document" and "word", whereby each document contains a random mix of topics, and each topic is constructed through relevant words. While this approach is appropriate to detect aspects at the document level, these may be too broad to capture fine-grained aspects ( <ref type="bibr" target="#b91">Schouten et al., 2016)</ref> . Furthermore, it was also observed that in the majority of studies, such as that by <ref type="bibr" target="#b84">Poria, Chaturvedi, et al. (2016)</ref> , the topics are unlabelled and require manual evaluation.</p><p>For all three tasks, supervised learning approaches, characterised by the use of classifiers built from linguistic resources, predominated ( . A substantial number of studies in SemEval 2014-2016 chose classifiers such as Maximum Entropy (ME), Conditional Random Field (CRF) and Support Vector Machine (SVM) for aspect detection and polarity. Top performers include CRF models in <ref type="bibr" target="#b19">Chernyshevich (2014)</ref>, <ref type="bibr" target="#b106">Toh and Su, (2016)</ref> , and Brun, <ref type="bibr" target="#b11">Perez, and Roux (2016)</ref> . SVM models were found in <ref type="bibr" target="#b113">Wagner et al. (2014)</ref> and <ref type="bibr" target="#b111">Vicente, Saralegi, and Agerri (2017)</ref> and ME in <ref type="bibr" target="#b89">Saias (2015)</ref> . Supervised machine learning approaches were also used at aspect level in sentiment analysis of movie reviews <ref type="bibr">(SVM classifier in Manek, Shenoy, Mohan, and Venugopal (2017)</ref> , and Naive Bayes in <ref type="bibr" target="#b73">Parkhe and Biswas (2016)</ref> ). A recent study by Akhtar, <ref type="bibr" target="#b2">Gupta, Ekbal, and Bhattacharyya (2017)</ref> presented a cascaded framework based on two steps: first base learning algorithms as classifiers ME, CRF, SVM followed by an ensemble of feature selection and classifier using particle swarm optimization. While the machine learning is simple and quite efficient, it shows certain weaknesses, including the requirement for large datasets, reliance on manual training data, and non-replicable results for other domains . Furthermore, aspects can be represented by different words, which means one classification algorithm is insufficient ( <ref type="bibr" target="#b91">Schouten et al., 2016</ref>) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Evaluation measures of ABSA tasks</head><p>International Workshops on Semantic Evaluation are promoting the development of aspect-level sentiment analysis ( <ref type="bibr" target="#b81">Pontiki et al., 2014</ref><ref type="bibr" target="#b79">Pontiki et al., , 2016</ref><ref type="bibr" target="#b80">Pontiki et al., , 2015</ref> ) providing controlled evaluation methodology and shared datasets for all participants. For the measurement of the efficiency of a classifying model, four main measurements were proposed: Precision (P), Recall (R), F-score (F1) and Accuracy (Acc).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P = T P T P + F P</head><p>(1)</p><formula xml:id="formula_1">R = T P T P + F N (2) F 1 = 2 P R P + R (3) Acc = T P + T N T P + T N + F P + F N<label>(4)</label></formula><p>TP (true positives) and TN (true negatives) are the respective labels and non-labels that are assigned by the system (rather than by humans); FP (false positives) are those labels assigned by the system but not by human annotators, FN (false negatives) are those labels that human annotators assigned and which were not detected by the system.</p><p>Precision measures the percentage of labels correctly assigned by the system. Recall measures the percentage of labels found by the system. Accuracy and F-score represent true results (TF and TN).</p><p>For OTE and ACD tasks, the F-score is frequently used as the tasks are similar to information retrieval and, to evaluate SP, accuracy is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Learning for ABSA</head><p>Deep learning (DL) is a machine learning method based on learning data representation through algorithms -artificial neural and belief networks -based on multiple layers of modules where input is analysed and classified, with output from one layer fed into the next layer as input. This process is known as backpropagation ( <ref type="bibr" target="#b56">Lecun et al., 2015 )</ref>, whereby activation initiates the backward computation of the gradient of an objective function <ref type="bibr" target="#b90">( Schmidhuber, 2015 )</ref>. Irrelevant of the type of input (i.e. sound, image or text), it is converted to numerical vectors, then clustered into meaningful classification. As each successive layer is corresponding to an increased level of abstraction, DL can be said to represent "nested hierarchies" of simpler concepts <ref type="bibr" target="#b39">( Goodfellow et al., 2016 )</ref>. Another feature is that its depth level can be seen as similar to multi-stage programming, in which each layer is a computer's memory state after executing a set of instructions ( <ref type="bibr" target="#b39">Goodfellow et al., 2016 )</ref>. By increasing the depth level, the system capacity to selectively and invariantly represent is enhanced <ref type="bibr" target="#b56">( Lecun, Bengio, &amp; Hinton, 2015 )</ref>.</p><p>Deep neural networks (DNNs) are good examples of DL and are the focus of this paper. Deep neural networks are types of artificial neural networks (algorithms) which include a significant number of layers of "neurons" or connected processors, activated either by sensors from the environment or by the weighted computations from previous neurons <ref type="bibr" target="#b90">( Schmidhuber, 2015 )</ref>. For DL, as for machine learning approaches in general, datasets are often divided into three components: training, validation and test datasets, conforming to general machine learning principles. <ref type="bibr" target="#b56">Lecun et al. (2015)</ref> mapped out the training process as a conversion of input into vector scores, regardless of type of input (i.e. images). Initially, an error score appears which needs to be reduced by training the algorithm to more closely conform to the set parameters (weights) for the target word (or image). Adjustments are subsequently made by the machine to reduce the error. The error-adjustment trigger is a 'gradient vector', which responds to the manipulation of the parameters, which needs to counter-balance the error. <ref type="table">Table 2</ref> An example sentence with labels in IOB format with the opinion target/aspect term as "onion rings".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Words:</head><p>The </p><formula xml:id="formula_2">o = s ( a ) = s w i x i + b (5) i</formula><p>The activation function is a non-linear function, either the sigmoid function ( Eq. 6 ), the hyperbolic tangent function ( Eq. 7 ), or the rectified linear function ( Eq. 8 ).</p><formula xml:id="formula_3">sigmoid ( a ) = 1 1 + e ?a (6) tanh ( a ) = e 2 a ? 1 e 2 a + 1 (7) ReLU ( a ) = max ( 0 , a )<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multiple layers of DNN</head><p>For layers of neurons, given layer l with m neurons, each with n -dimensional input vector x ¡Ê R n with n -dimensional associated weight matrix W ¡Ê R m ¡Án and a bias scalar b ¡Ê R n and the activation function s (either sigmoid, tanh or ReLU ), the computation of l can be written as:</p><p>Deep neural network (DNN) approaches to NLP are distinguished by (i) dense word embeddings; (ii) multiple hidden layers between the input and output; and (iii) output units <ref type="figure" target="#fig_2">( Fig. 3 )</ref>.</p><p>Word embeddings are d-dimensional space representations of words, encoded as dense numerical vectors <ref type="bibr" target="#b86">( Rojas-Barahona, 2016</ref> ). These vectors, <ref type="bibr">Levy and Goldberg (2014)</ref> argue, establish the likelihood of a word appearing within a specific word matrix (i.e. with associated words). One of the first word embedding models was that of <ref type="bibr" target="#b10">Bengio et al. (2006)</ref> who proposed a neural probabilistic language model with shared lookup table. Thus, given a word and its preceding words, the model looks up its continuous vector, and then feeds the information into a feed-forward neural network to predict the probable function of the next word. In an attempt to reduce feature engineering, many DNN based studies have used word embeddings as the only feature (such as <ref type="bibr" target="#b60">Liu, Joty, &amp; Meng, 2015 )</ref>. In recent DNN models, word embeddings are typically pre-trained but not task-specific data so that the learning word vectors can capture general syntactical and semantic information ( Chen, Xu, He, &amp; Wang, 2017 ; P. <ref type="bibr" target="#b60">Liu et al., 2015;</ref><ref type="bibr">Po- ria, Cambria, et al., 2016 )</ref>. There are different models for word embeddings, such as Word2Vec <ref type="bibr" target="#b70">( Mikolov, Corrado, Chen, &amp; Dean, 2013</ref> ) that encode contextual information using continuous BagOf-Words (CBOW) and skip-gram models. Word embeddings are discussed further in Section 3.3.1.</p><p>The second feature -hidden layers -can be constructed in different forms and architectures, i.e. feed-forward networks and recurrent or recursive networks <ref type="bibr" target="#b37">( Goldberg, 2016 )</ref>. Each hidden layer is composed of multiple neurons, stacked together to compute non-linear outputs ( <ref type="bibr" target="#b56">Lecun et al., 2015 )</ref>. Generally, the higher layers evolve through training to exploit the complex compositional nonlinear functions of the lower layers and, hence, capture more abstract representations than the lower layers <ref type="bibr" target="#b37">( Goldberg, 2016 )</ref>.</p><p>The computation of hidden features starts with neurons, which take n input to produce a single output. Considering the inputs x 1 , x 2 , ¡¤ ¡¤ ¡¤ ¡Ê R with n associated parameters (or weights) w 1 , w 2 , ¡¤ ¡¤ ¡¤ ¡Ê R and a bias scalar b ¡Ê R , the activation of the neu-</p><formula xml:id="formula_4">l = s ( W x + b ) (9)</formula><p>The third feature -output units -represents the distributed probability over all labels or classes. Supposing the last layer is z and there are K labels/classes, the probability for the label i can be obtained using the softmax function as set out below:</p><formula xml:id="formula_5">z i y i = so f tmax ( z ) i = e K k =1 e z k (10)</formula><p>For OTE, categories can be represented as similar to sequence tagging of IBO labels ("B" is the start of the aspect term, "I" is the continuation of the aspect term and "O" is not an aspect term) ( <ref type="table">Table 2</ref> ).</p><p>For ACD, for a given category or attribute, the label can be represented as a binary T = {category, non-category}. For sentiment polarity, categories might be the set of 4 way polarities as in SemEval tasks T = {positive, negative, neutral, conflict} or simple binary polarities, such as T = {positive, negative}. For all three tasks, the output units of the DNN model can return the probability to assign a given label to each input, whereby the label with the highest probability represents the result of the prediction.</p><p>To summarise the above discussion, <ref type="bibr" target="#b56">Lecun et al. (2015)</ref> suggested that DNNs with distributed representation are able to generalise new combinations of learnt features beyond what has been learned in the training phase. Therefore, in contrast to standard machine learning, DNN models attempt to automatically learn good features or representations ( Rojas-Barahona, 2016 ). Unlike traditional methods, DNN models also do not require much feature engineering, and if the right model is chosen, it has more robust extraction and representation capacities ( Araque, Corcuera-Platas, <ref type="bibr" target="#b8">S¨¢nchez-Rada, &amp; Iglesias, 2017 )</ref>.</p><p>In the sections below, major DNN models applied to ABSA tasks will be reviewed, including convolutional neural networks (CNN), recurrent neural networks (RNN), recursive neural networks (RecNN), and hybrid models in Section 3.2 -3.5. In each section, a review of architecture will be provided, following by the application of models to ABSA tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Initialization of input vectors</head><p>where R ( ¦È ) is the regularisation term, and g ( w t , w t ? 1 , , w t ? n + 1 ; ¦È ) can be estimated by the softmax function as p ( w t | w t ? 1 , , Before reaching the first hidden layer of the DNN, the input layer is encoded with a distributed representation, or word embeddings, which represent each word as a low-dimensional, realvalued and continuous vector to encode its semantic and syntactic properties ( Tang, Wei, et al., 2016 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Word embeddings vectors</head><p>One of the first word embedding models is by Bengio, Schwenk, Sen¨¦cal, Morin, &amp; Gauvain (2006) who proposed a neural probabilistic language model with a shared lookup table. Given a word and its previous words, the model can look up its continuous vector, feed the vector to a feed-forward neural network and predict the probability function for its next word. Assuming a sequence of T words w 1 , w 2 , , w T with n previous words fed into the model, the model can predict the probability p of the words u t based on w t ? n + 1 ). <ref type="bibr" target="#b70">Mikolov et al. (2013)</ref> developed the word2vec with two different neural network models for creating word embeddings for training on large corpora: a bag-of-words based model (CBOW) that obtains word context from sequential word context; and the skip-gram model that predicts the word embeddings from neighbouring words <ref type="figure" target="#fig_3">( Fig. 4 )</ref>.</p><p>The CBOW for the target as the word w t at time step t , the model receives a window of n words around w t , and the loss function J can be written as:</p><formula xml:id="formula_6">T J = 1 T log p ( w t | w t?n , ¡¤ ¡¤ ¡¤ , w t?1 , w t+1 , ¡¤ ¡¤ ¡¤ , w t+ n ) (12) t=1</formula><p>In contrast, the skip-gram model uses the centre word w t to predict the neighbouring words w t + j . In this case, the objective function is: T finding the model parameter ¦È that maximises the objective funclog p ( w t+ j | w t ) <ref type="bibr">(13)</ref> tion J :</p><formula xml:id="formula_7">J = 1 T t=1 ?n ¡Ü j¡Ün, =0 T J = 1 T log g( w t , w t?1 , ¡¤ ¡¤ ¡¤ , w t?n +1 ; ¦È ) + R ( ¦È )<label>(11)</label></formula><p>In addition to word2vec framework, a number of software have been developed for training word embeddings such as GloVe developed by Stanford University or fastText developed by Facebook.  Pre-trained word vectors have also been developed such as SENNA (based on the Wikipedia corpus); Google (based on the Google News corpus); Amazon (based on the Amazon corpus); GloVe (based on Wikipedia and Twitter); SSWE (based on Twitter with inclusion of emoticons) <ref type="table" target="#tab_9">( Table 3 )</ref> There are a number of ways to initialize word embeddings, including random initialization (i.e setting the embedding vectors to random values), or pre-trained (i.e. tuning the vectors so that similar words will obtain similar vectors) <ref type="bibr" target="#b37">( Goldberg, 2016 )</ref>. The recent high performing models typically opt for pre-trained word embeddings and fine-tune them to better initialize the model. As discussed in <ref type="bibr" target="#b60">Liu et al. (2015)</ref> , the random approach can lead to stochastic gradient descent in the local minima, and if the pretrained word beddings are employed from readily available resource without tuning, this may not exploit automatic feature learning capacity of DNNs. Experiments from studies such as <ref type="bibr" target="#b60">Liu et al. (2015)</ref>, , <ref type="bibr" target="#b47">Jebbara and Cimiano (2016)</ref> show that the model will be beneficial with the initialization of pre-trained work embeddings and fine-tune them in training, for example, only using pre-trained word embeddings contributed a gain of 6-9% in aspect term extraction <ref type="bibr">( Poria, Cam- bria, et al., 2016</ref> ) or 2% in sentiment polarity ( Wu, Gu, Sun, &amp; Gu, 2016 ).    <ref type="formula" target="#formula_11">(2016)</ref> , for the task of customer review with less formal texts than Wikipedia and Google news corpus, a word embeddings scheme that contains more opinion specific words such as Amazon have better performance. Meanwhile, in Twitter target ABSA,  showed that a sentiment-specific word embeddings (SSWE) have better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Featuring vectors</head><p>As mentioned above, in contrast to previous approaches, deep learning rarely relies on feature engineering, parser, or positional information, but solely on language input ( <ref type="bibr" target="#b128">Young et al., 2017</ref> ). However, in order to generate more salient performance, a number of feature vectors are fed into the DNN, together with the word embeddings. The most common features are summarised below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Part-Of-Speech (POS) and chunk tags</head><p>One observation from <ref type="bibr" target="#b45">Hu et al. (2004)</ref> , there is high probability that the aspect terms are nouns or noun chunks, which reveals the importance of POS features in OTE. The number of classifications for POS tagging varies (i.e. 6 tags according to Stanford <ref type="table">Tag- ger</ref>  Generally, k tags, representing k parts of speech, can be encoded as k -dimensional binary vectors and then concatenated with the word embeddings vectors before being fed to the neural network models. Experiments have shown that POS tagging and word chunks complement word embeddings play a major role in aspect extraction, contributing from 1% ( <ref type="bibr" target="#b60">Liu et al., 2015;</ref><ref type="bibr" target="#b126">Ye et al., 2017</ref> ) to 4% gain <ref type="bibr" target="#b34">( Feng, Cai, &amp; Ma, 2018 )</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Commonsense knowledge</head><p>Another feature suggested by <ref type="bibr" target="#b48">Jebbara and Cimiano (2017)</ref> and <ref type="bibr" target="#b65">Ma, Peng, and Cambria (2018)</ref> to improve both aspect extraction and sentiment classification is common-sense knowledge through SenticNet. This base consists of over 50,0 0 0 concepts with associated affective properties ( Y. <ref type="bibr" target="#b65">Ma et al., 2018 )</ref> which are represented by real-value scores consisting of 5 sentics: pleasantness, attention, sensitivity, aptitude, and polarity, which can imply semantic links to aspect and sentiment <ref type="bibr" target="#b48">(Jebbara et al., 2017</ref>) . An example given by Y. <ref type="bibr" target="#b65">Ma et al. (2018)</ref> is the concept "cupcake" has the property "KindOf-food" that can be related to 'restaurant' or 'food quality', but also emotions, e.g., "Arise-joy" that supports sentiment classification.</p><p>By including them as 5 feature vectors for each concept, those studies have shown improvement. <ref type="bibr" target="#b65">Ma et al. (2018)</ref> suggested the Sentic LSTM significantly outperformed a baseline LSTM. <ref type="bibr" target="#b48">Jebbara and Cimiano (2017)</ref> observed that while sentics did not contribute to aspect term extraction, the usage of sentic vectors contributed to 4% gain in the model for sentiment analysis and considerably reduced the training time.</p><p>the final layer to obtain optimum performance ( <ref type="bibr" target="#b132">Zhang et al., 2018 )</ref>. The most extensively used classifiers in recent years include Support Vector Machine (SVM) and Conditional Random Fields (CRF) classifiers, with examples in ABSA tasks as CRF in , Xu, Lin, Wang, <ref type="bibr" target="#b122">Yin and Wang (2017)</ref>, <ref type="bibr" target="#b66">Mai &amp; Le (2018)</ref> , or SMV in Akhtar, <ref type="bibr" target="#b3">Kumar, Ekbal and Bhattacharyya (2016)</ref> , and <ref type="bibr" target="#b28">Dong et al. (2014)</ref> . All these are discriminative models, which learn the most useful features of the input to predict the output, and are trained with different loss functions ( <ref type="bibr" target="#b39">Goodfellow et al., 2016</ref> ).</p><p>The SVM model is a classifier that outputs the identity of different classes based on a linear function. The incorporation of the SVM with the neural network model can be implemented selecting the label with the highest score of y expressed as:</p><formula xml:id="formula_8">k = argmin y i (17) i</formula><p>Thus, for the highest scoring label k = argmin i y i and the correct labe? k = arg min i ? y i , the SVM loss function is:</p><formula xml:id="formula_9">L y, ? y = max 0 , 1 ? y ? k ? y k<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training process of DNNs</head><p>A neural network is trained through a backpropagation process in which the gradients of all parameters are computed backward and updated with stochastic gradient descent <ref type="bibr" target="#b37">( Goldberg, 2016</ref><ref type="bibr" target="#b38">( Goldberg, , 2017</ref> .</p><p>Let x = x 1 , x 2 , , x n be the input, and y = y 1 , y 2 , , y n be the output from the machine learning algorithm with the actual labels b¨º y = ? y 1 , ? y 2 , ¡¤ ¡¤ ¡¤ , ? y n , the goal of the algorithm is to estimate a function y = f ( x ) that matches the inputs with their correct label. The loss function is employed during the training phrase to calculate a numerical score L that is loss when predicting output y with respect t? y . In this sense, the parameters of the function (weight matrix W ¡Ê R m ¡Án and a bias scalar b ) are to be set to minimise the loss L . The loss function for the whole sample is calculated</p><p>Comparing the softmax layer with SVM, one advantage of the latter is that it is useful under conditions of hard decision rule (i.e. when it is not necessary to estimate the probability of each label) <ref type="bibr" target="#b38">( Goldberg, 2017 )</ref>. Nevertheless, applied to classification task such as sentiment analysis, its performance maybe hindered by its "sparse" and "discrete" features, which makes it difficult to transfer information regarding relationships and coherence of chunks or sentences <ref type="bibr" target="#b99">( Tang, Qin, &amp; Liu, 2015 b)</ref>.</p><p>CRF assumes that the output y is connected by undirected edges in an undirected graph <ref type="figure" target="#fig_7">( Fig. 5 )</ref>. In this sense, the CRF represents the score of a given label sequence (or the clique potential) as a conditional probability that is proportional to the input sequence ( T.  as:</p><formula xml:id="formula_10">scor e CRF ( x, y ) = p ( y | x ) = 1 Z x ¦Õ s ( y s , x s ) (19) s ¡Ê S ( y,x )</formula><p>where Z x is the normalization, S ( y, x ) the set of cliques of the undirected graph where the outputs are connected and ? s ( y s , x s ) is the clique potential. The loss function is calculated as:</p><formula xml:id="formula_11">L y, ? y = ? log scor e CRF ( x, y )<label>(20)</label></formula><p>with respect to the parameter ¦È as the average loss:</p><formula xml:id="formula_12">n L ( ¦È ) = 1 n L ( f ( x i ; ¦È ) , ? y i )<label>(14)</label></formula><p>i =1</p><p>In addition, while minimizing the cost, the model maybe overfitting. Thus, the algorithm combines another function R ( ¦È ) to measure the complexity. Therefore, the goal of the function then is For the CRF model, the labels of each consecutive point can influence others <ref type="bibr" target="#b37">( Goldberg, 2016 )</ref>, which overcomes the disadvantage of softmax which features independent labels <ref type="bibr" target="#b110">( Tutubalina &amp; Nikolenko, 2017 )</ref>. Thus, it can be inferred that CRFs can take advantage of the entire sentence sequence to estimate probability for the sentence labelling making CRF a frequent final classification layer of bidirectional RNNs ( T. <ref type="bibr" target="#b46">Irsoy &amp; Cardie, 2014;</ref><ref type="bibr" target="#b55">Lample et al., 2016</ref> ; P. <ref type="bibr" target="#b60">Liu et al., 2015 )</ref>.</p><p>to set the ¦È to minimize the loss value while keeping a low complexity R ( ¦È ):</p><formula xml:id="formula_13">3.4. Convolutional Neural Network Model (CNN) n ? ¦È = argmin ¦È L ( ¦È ) = argmin ¦È 1 n L ( f ( x i ; ¦È ) , ? y i ) + ¦ËR ( ¦È ) i =1<label>(15)</label></formula><p>Following the softmax function as above, the categorical crossentropy loss is used as the loss function:</p><formula xml:id="formula_14">L y, ? y = ? ? y i log ( y i ) (16) i</formula><p>Because the scores y i are not negative with the sum of one, the cross-entropy loss produces not only the label prediction but also the distribution.</p><p>In some cases, after the features have been obtained from the neural network models, non-neural classifiers are incorporated as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Architecture</head><p>CNN has become a popular DL model amongst NLP researchers, since the pioneering works of <ref type="bibr" target="#b23">Collobert et al. (2011)</ref> and <ref type="bibr" target="#b51">Kim (2014)</ref> who advocated the success of CNN in a number of NLP tasks, including sentiment analysis. The main strength of the CNN is its ability to extract the most important n-gram features from the input to create an "informative latent semantic representation" for undertaking further classification tasks <ref type="bibr" target="#b86">( Rojas-Barahona, 2016;</ref><ref type="bibr" target="#b128">Young et al., 2017 ;</ref><ref type="bibr" target="#b132">L. Zhang et al., 2018 )</ref>.</p><p>The basic single layer CNN for sentence modelling may consist of 4 layers as <ref type="figure" target="#fig_8">Fig. 6</ref> below, according to <ref type="bibr" target="#b51">Kim (2014)</ref> .</p><p>The representation of each layer is: Input layer : representing the sentence of length n as  where x i ¡Ê R d be the d -dimensional word vector corresponding to the i-th word in the sentence and is the concatenation operator. Convolutional layer : this layer generates the new feature c i with the filter w ¡Ê R hk , using the window of h words from i to i + h ? 1 as to capture the most important feature (n-gram) for each feature map; and second, the max-pooling layer can produce a fixedlength output regardless of the size of the filter window h .</p><formula xml:id="formula_15">x 1: n = x 1 x 2 .. x n<label>(21)</label></formula><formula xml:id="formula_16">c i = s ( w. x i : i + h ?1 + b ) (22)</formula><p>where b ¡Ê R is the bias term and s is a non-linear activation function, such as sigmoid , hyperbolic tangent ( tanh ), or rectified linear ( ReLU ) functions.</p><p>So for the sentence, as the possible windows are { x 1: h ; x 2: h + 1 ¡­; x n ? h + 1: n }, leading to the feature map as: </p><formula xml:id="formula_17">c = [ c 1 , c 2 , . . . c n ?h +1 ] ¡Ê R n ?h +1</formula><formula xml:id="formula_18">y j = softmax ( W z + b )<label>(24)</label></formula><p>So the softmax layer outputs a probability distribution overall output labels or classes.</p><p>The CNN has two important implications from the convolutional and max-pooling layer: first, the convolutional layer is able</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">Application in consumer review domain</head><p>The motivation for utilising the CNN model in ABSA tasks is the assumption that key words may contain the aspect term, and indicate a category or determine polarity, regardless of their position. The CNN is capable of learning to find those features with its architecture <ref type="figure" target="#fig_10">( Fig. 7 )</ref> and is, thus, able to extract local patterns from data regardless of their location. This is very useful for identifying fixed-length phrases <ref type="bibr" target="#b38">( Goldberg, 2017 )</ref>. Another advantage is that the CNN is a non-linear model which is expected to better fit the data than linear models such as the CRF and does not require extensive hand-crafted features such as fixed language rules  ).</p><p>The CNN model has been applied to all tasks of ABSA, mainly in consumer review domain <ref type="table" target="#tab_10">( Table 4</ref> ).</p><p>For the OTE task, a prime example of successful studies is Poria,  who adapted CNN architecture from sentence representation to word-based prediction. With the assumption that the tag of each word is dependent on each neighbouring words, they formed a local feature window of five words around each word in a sentence. A deep CNN of seven layers, in- Note: PM indicates that the dataset was primarily collected by authors. 3-way represents the three polarities of positive, negative, neutral. cluding one input layer, two convolution layers, two max-pool layers, and a fully connected layer with softmax output, is then applied to each window of words with the prediction to the centre of the window. Experiments show that the deep CNN model, even without any feature engineering or linguistic patterns, still outperformed state-of-the-art models. For other ABSA tasks, CNN is also a promising approach. Toh and Su (2016) achieved the best performance in SemEval 2016 in ACD with the assembling of two different machine learning systems. As they considered ACD as a multi-class classification problem, they followed a binary relevance approach. Particularly, they used multiple binary classifiers trained on a single layer feedforward neural network then combined the probabilities output from a deep CNN to predict if the text consists of an aspect category. Compared with other features, CNN features contributed the most to performance.   responding aspect. If that is the case, the sentiment classifier predicts sentiment polarity as positive or negative. Apart from the advantages of reduced feature engineering compared to traditional ML methods, the cascaded model also showed that the CNN presented a remarkable reduction of elapsed time, compared to SVM.</p><p>Oppositely, an example of multitasking CNN is the work of <ref type="bibr" target="#b87">Ruder et al. (2016)</ref> that proposed a CNN approach to undertake both ACD and SP. Similar to <ref type="bibr" target="#b106">Toh and Su (2016)</ref> , they considered aspect extraction as a multi-label classification problem but approached this through a probability distribution threshold . Assuming a sentence S contains K aspect categories, the probability for the with W being the weighted matrix function between x t and h t ? 1 , and s is a non-linear activation function, such as tanh or ReLU . Therefore, the output can be computed as:</p><formula xml:id="formula_19">y t = W h ( y ?1 ) h t<label>(26)</label></formula><p>sentence to contain an aspect category k is defined as</p><formula xml:id="formula_20">p( k | S ) = 1 K , otherwise, p ( k | S ) = 0.</formula><p>The threshold ¦Ó is selected to maximise the F1 score, and the aspect category is selected to satisfy p ( k | S ) ¡Ý ¦Ó .</p><p>To determine the sentiment towards an aspect, they concatenated an aspect vector with every word embedding and applied a CNN over it. The model also has demonstrated convincing results in the multilingual settings of Spanish, Dutch, and Turkish, showing the strength of DNN as language and domain independence. Another work, <ref type="bibr" target="#b121">Wu et al. (2016)</ref> , proposed a multitask CNN, which contains aspect mappers and a sentiment classifier sharing word embedding layer whereas other parameters are kept specific in each task. Although this is a promising approach, the experiment showed that multitask CNN performed just slightly better than cascaded CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Recurrent Neural Network Models (RNN)</head><p>Recurrent Neural Networks have become popular in sentiment analysis tasks. The basic of RNN models is that a fixed-size vector represents one sequence (i.e. sentence or document) by feeding each token into a recurrent unit, so it can capture the inherent sequential nature of language (i.e. one word develop its semantic meaning thanks to its previous word) <ref type="bibr" target="#b37">( Goldberg, 2016;</ref><ref type="bibr" target="#b39">Goodfellow et al., 2016</ref> ). Compared to the CNN models, RNN models have flexible computation steps that the output from RNN is dependent on the previous computations, making it capable of capturing context dependencies in language as well as capable to model various text lengths  ).</p><p>The RNN model has two important features compared to the feed-forward neural network. First, unlike the CNN has different parameters at each layer, the parameters in RNN are the same in each steps, which then reduces the number of parameters needed to learn ( L. <ref type="bibr" target="#b132">Zhang et al., 2018 )</ref>. Second, as the output of one state depends on the previous state, RNN can be said to have the memory of previous computations, making it more superior in processing sequential information compared to the CNN.</p><p>However, the simple RNN has a major weakness in terms of the vanishing gradient problems (the gradient comes close to zero) or exploding gradient (the gradient is extremely high) <ref type="bibr" target="#b37">( Goldberg, 2016 )</ref>. As discussed earlier, because the basic role of the gradient is to tune the parameters to improve the gradient, extremes make it difficult to decide in which direction to tweak the parameters, while an exploding gradient causes an unstable learning process ( <ref type="bibr" target="#b39">Goodfellow et al., 2016 )</ref>.</p><p>However, the simple RNN has limitations caused by the gradient. It may vanish (coming close to zero) or explode (being extremely high). This occurs during the backpropagation process, making it difficult to train and fine-tune the parameters <ref type="bibr" target="#b37">( Goldberg, 2016 )</ref>. This limitation has been improved with the introduction of networks such as long short-term memory (LSTM) <ref type="bibr" target="#b44">( Hochreiter &amp; Schmidhuber, 1997;</ref><ref type="bibr" target="#b131">Zaremba &amp; Sutskever, 2014</ref> ) and gated recurrent units (GRU) ( <ref type="bibr" target="#b21">Cho et al., 2014</ref> ). <ref type="table">Table 5</ref> below compares the demonstration and computation of hidden networks between LSTM and GRU. The basis of LSTM is a memory cell that controls the read, write and reset operations of its internal state through output, input and forget gates. At one time t , with the current input x t and output from the previous state h t ? 1 , the forget gate will decide which information to keep and which to offload, subsequently updating the memory cell. GRU consists of two gates -the reset and the update gate and handles the flow of information, similar to LSTM without the memory unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1.">Computation of RNN models</head><p>The simple RNN model is based on the Elman network <ref type="bibr" target="#b32">( Elman, 1991;</ref><ref type="bibr" target="#b39">Goodfellow et al., 2016</ref> ) with direct cycles in their hidden connection <ref type="bibr" target="#b37">( Goldberg, 2016;</ref><ref type="bibr" target="#b86">Rojas-Barahona, 2016</ref> ). This model proposes that the hidden state is dependent on the input and past hidden state, with the same function and the same set of parameters being used at every time step. <ref type="figure" target="#fig_12">Fig. 8</ref> shows a basic RNN with a three-layer network of input, hidden state and output. At time t, given x t as the input to the network, the hidden state h t is calculated as;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2.">Bidirectional RNN</head><p>The three models presented above focus on using past words to predict the next word. In practice, many studies would like to make predictions based on the future words, and thus, the bidirectional RNN models are proposed, with the incorporation a forward and a backward layer in order to learn information from preceding and following tokens <ref type="bibr" target="#b41">( Graves, 2008;</ref><ref type="bibr" target="#b33">Fan, Qian, Xie, &amp; Soong, 2014</ref> ). As shown in <ref type="figure" target="#fig_10">Fig. 7</ref>  </p><p>based on the future hidden state h t+1 and current input x t . The <ref type="table">Table 5</ref> Comparison of LSTM and GRU. Equations and figures from Chung, Gulcehre,  .</p><p>¡û forward and backward context representations h t and h t are then concatenated into a long vector at the timestep t as:</p><formula xml:id="formula_22">? ¡û h t = h t ; h t (37)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3.">Attention mechanism and memory networks</head><p>In ABSA, as the aim is to classify sentiment with respect to target aspect terms in the text, it is important for the method to model the interaction between the aspects and the whole sentence. The traditional encoder-decoder framework such as RNN has a potential problem in that the encoder may encode irrelevant information, especially when the input is very informationrich ( P. Chen, Sun, Bing, &amp; Yang, 2017 ; Y. Wang, Huang, Zhao, &amp; Zhu, 2016 ). One possible solution is to employ an attention mechanism, which allows the model to learn which part of the text to focus on. The general idea of the attention mechanism is to compute an attention weight from each lower level then aggregate the weighted vectors for higher level representation <ref type="bibr" target="#b62">( D. Ma, Li, Zhang, &amp; Wang, 2017 )</ref>. <ref type="figure" target="#fig_0">Fig. 10</ref> below shows the global attention model on a bidirectional LSTM, following the decoder-encoder model by Bahdanau, <ref type="bibr" target="#b21">Cho and Bengio (2014)</ref> in neural machine translation.</p><p>In this model, given the input sentence S = { x 1 , x 2 , x T }, at time step t , the output y t is dependent on the decoder state s t and the </p><formula xml:id="formula_23">s t = f ( s t?1 , y t?1 , c t ) (38)</formula><p>where c t is the context vector and c t is dependent on the set of H t = { h 1 , h 2 , h T }. Given the attention weights denoted as ¦Á t = { ¦Á t 1 , ¦Á t 2 , , ¦Á tT }, the context vector is computed as:</p><formula xml:id="formula_24">T c t = ¦Á ti h i (39) i =1</formula><p>To compute the attention weight, the model utilises an alignment process, which first computes the attention energies e ti from s t ? 1 and h i using a feed-forward neural network a as:</p><formula xml:id="formula_25">e ti = a ( s t?1 , h i )<label>(40)</label></formula><p>Variants of attention mechanisms can be computed according to a different function a , such as the additive attention ( Bahdanau et al., 2014 ; Y.  as:</p><formula xml:id="formula_26">e ti = v a tanh ( W h i + U s t )<label>(41)</label></formula><p>where W, U are weighted matrix and v a is the weight vector (or aspect embedding vector).</p><p>After that, the weight ¦Á ti can be computed using the softmax</p><formula xml:id="formula_27">function ¦Á ti = so f tmax ( e ti ) = exp ( e ti ) T j=1 exp e t j<label>(42)</label></formula><p>In the case of ABSA, this also implies that during the decoding period the decoder is conditioned on a "context" vector. This mechanism is most suitable to be applied for the task of sentiment classification, given the aspect terms or aspect categories. It is expected that the models with attention mechanism can focus on the important parts of the sentence in terms of aspects.</p><p>Another mechanism that can be applied to resolve the issue of irrelevant information is using external memory such as the Memory Networks model (MemNet) <ref type="bibr">( Sukhbaatar, Szlam, Weston, &amp; Fer- gus, 2015 )</ref>. <ref type="figure" target="#fig_0">Fig. 10</ref> shows an extension of the attention mechanism with external memory in a MemNet by  .</p><p>Initially, the sentence is modelled as the composition of n words {w 1 , w 2 , , w i , , w n } with the aspect word as w i . Simultaneously , the context word vectors {c 1 , c 2 , , c i ? 1 , c i + 1 , , c n } are stacked as the external memory slices { m 1 , m 2 , , m i ? 1 , m i + 1 , , m n }. Then, in the first computation layer (hop 1), the aspect vector v a is selected from the external memory. The output is a continuous vector v computed from the weighted sum of the memory slides as</p><formula xml:id="formula_28">n ?1 v = ¦Á i m i (43) i =1</formula><p>where ¦Á i is the weight of m i to be calculated and i w mi = 1. The scoring function aims at measuring the semantic similarity of each memory slice m i to the aspect vector v a as:</p><formula xml:id="formula_29">e i = tanh ( W a [ m i ; v a ] + b a ) (44)</formula><p>where W a is the weighted matrix and b a is the bias. This is followed by computation of the weight of m i using the softmax function as:</p><formula xml:id="formula_30">¦Á i = so f tmax ( e i ) = exp ( e i ) n ?1 j=1 exp e j<label>(45)</label></formula><p>After the first hop, the attention layer and the linear transformation of the aspect vector are totalled. The sum is stored in the external memory for further information retrieval. This means that the decoder can encode context embeddings from context words and aspect embeddings from aspect words. Thus, the application of attention and memory network mechanisms is useful for sentiment classification of the whole sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.4.">Application in the consumer review domain</head><p>Compared to other neural network models, RNNs and bidirectional RNNs have had a huge presence in the word-level and sentence-level classification in consumer review domain <ref type="table" target="#tab_12">( Table 6</ref> ).</p><p>Many RNN-based models took advantage of the bidirectional RNN to recode past and future contexts. An approach from <ref type="bibr" target="#b14">Chaudhuri &amp; Ghosh (2016)</ref> used hierarchical bidirectional RNN in ACD in highly skewed data of hotel review domain and obtained superior result over LSTM. The model is composed of six layers including four layers of bidirectional RNNs , one fully connected layer and one softmax layer. Each layer constitutes a hierarchy of classifier. They proposed a mini-batch approach whereby the input dataset is separated randomly into a few words to compute seed classifications; then the remaining words are placed into seed classes to find the highest similarity on average. With the similar task of ACD, <ref type="bibr" target="#b26">Ding et al. (2017b)</ref> also used an RNN to model the context of each word as well as the background context. Using continuous vectors to calculate the probabilities of generating different words, they offered an alternative solution for topic models, which was more effective. Jebbara and Cimiano (2016) employed bidirectional GRU for OTE and aspect-specific sentiment extraction. As a first step, a bidirectional GRU is used to extract aspects from a text as a sequence labelling of IOB. In a second step, a bidirectional GRU extracted aspect regarding its context and predicted its sentiment label. Other features include pre-trained semantic word embedding, semantic knowledge extracted from Word-Net and SenticNet.</p><p>One of the most successful attempts is to combine RNN with the CRF classification layer, so that the model not only captures the long-term dependency of the entire sentences, but also utilises the dependency of each label on each other. <ref type="bibr" target="#b60">Liu et al. (2015)</ref> proposed an application of recurrent neural network (RNN) in OTE with linguistic features of POS, word chucks, which showed better performance than a feature-rich CRF-based system. Inspired by the system of NER by <ref type="bibr" target="#b55">Lample et al. (2016)</ref> , T.  proposed a bidirectional LSTM-CRF in classifying numbers of targets in the sentence, but the model also achieved state-of-the-art performance in OTE. Overcoming the limitation of a fixed window size in CNN model, this network captured long-term dependencies of context information. The result of the bidirectional LSTM is two fixed-size vectors, which were then concatenated at the fully connected layer. For the IOB tagging, the authors use a CRF layer at last <ref type="figure" target="#fig_0">( Fig. 12 )</ref>. A similar model by <ref type="bibr" target="#b66">Mai and Le (2018)</ref> also showed its effectiveness in Vietnamese.</p><p>Another promising direction is to utilise attention and memory networks. Approaching the OTE task, <ref type="bibr">Li &amp; Lam (2017)</ref> proposed an extended memory framework for LSTM while <ref type="bibr">Ying et al. (2017)</ref> proposed a LSTM model in cross-domain aspect term extraction, with the combination of rule-based methods that generated auxiliary label sequence for each sentence. Another study by <ref type="bibr" target="#b57">Li et al. (2018)</ref> also incorporated attention in tasks of OTE and ACD with their Truncated History-Attention (THA) and Selective Transformation Network (STN) built on two LSTMs.</p><p>Meanwhile, W.  proposed a Coupled MultiLayer Attention Model (CMLA) based on GRU for co-extracting of aspect and opinion terms. Therefore, learning can be done by encoding/decoding the dual propagations of aspects and opinion terms, and not restricted to grammatical relations <ref type="figure" target="#fig_0">( Fig. 11 )</ref>. This framework reduces engineering features compared to the CRF and the co-extraction is a worth-noting feature.</p><p>Y.  and Y. <ref type="bibr" target="#b65">Ma et al. (2018)</ref> proposed a solution with attention weight, in which aspect embeddings are used to decide attention weights for sentiment classification, in addition with sentence representation. Therefore, the model can have different concentration on different parts when different aspects are given (for example in <ref type="figure" target="#fig_0">Fig. 12 )</ref>. Another work by Cheng et al.  Note: PM indicates that the dataset was primarily collected by the authors. 3-way refers to the three polarities of positive, negative, neutral.</p><p>(2017) applied attention with bidirectional GRU model to attend the aspect information for one given aspect and extract sentiment for that given aspect. Their work achieved state-of-the-art performance on benchmark datasets. Also in a similar task, <ref type="bibr" target="#b75">Peng et al. (2018)</ref> proposed an aspect target sequence model (ATSM) to incorporate adaptive embeddings at word, character and radical level in dealing with multiple-word aspect issues in Chinese.</p><p>On the other hand, Tang, Qin, et al. (2016) adopted a memory network (MemNet) solution, which is based on multiple-hop attention. They included a multiple-attention computation layer on the memory network, which improved lookup for most informational regions. Memory networks also feature in R.  who uses compositing strategies to represent context and features for each word. In deep hops, their model outperforms state-of-the-art approaches of SVM with less feature engineering. <ref type="bibr" target="#b104">Tay, Tuan, et al. (2017)</ref> proposed Dyadic Memory Networks (DyMemNN), which incorporates composition techniques that model the dyadic interactions between aspect and words in a document. Their model also achieved competitive performance in OTE and ACD.</p><p>An interesting work by <ref type="bibr" target="#b99">Tang, Qin, Feng, et al. (2015)</ref> proposed adding attention layers in their bidirectional LSTM. They proposed two models to achieve target-specific sentiment classification: Target Dependent L STM (TDL STM) directly uses the hidden outputs of a bidirectional LSTM sentence encoder in panning the target mentions, while Target Connection L STM (TCL STM) extends TDL STM by concatenating each input word vector with a target vector <ref type="figure" target="#fig_0">( Fig. 13 )</ref>. However, they failed to achieve competitive results possibly due to the small training corpus. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Recursive Neural Network Model (RecNN)</head><p>3.5.5. Application in targeted sentiment analysis RNN models are also applied in Twitter domain and new comments <ref type="table" target="#tab_14">( Table 7 )</ref> rather than CNN model, as explained above on the limitation of CNN model in capturing long-term dependencies. Unlike the customer review domain, the Twitter domain is challenged with the limited length, informal contexts and the use of emoticons ( <ref type="bibr" target="#b36">Giachanou &amp; Crestani, 2016 )</ref>. However, studies using RNN models have showed competitive performance in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1.">Architecture</head><p>Recursive neural network (RecNN) models are linguistically motivated in that they explore tree structures (e.g., syntactic structures) and aim to learn elegantly compositional semantics. Arguably, natural language demonstrates a natural recursive structure, placing words and phrases in a hierarchical manner. Thus, tree-structured models can better make use of such syntactic interpretations of sentence structure <ref type="bibr" target="#b93">( Socher, Perelygin, &amp; Wu, 2013;</ref><ref type="bibr" target="#b97">Tai, Socher, &amp; Manning, 2015 )</ref>. Generally, in a recursive neural network, the vector representation of each node in the tree structure is calculated from the representation of all its children us-    <ref type="figure" target="#fig_0">Fig. 13</ref>. Grammatical relations determine aspects and opinions: 'fish burger' and 'tastes' are obvious aspect terms, with the respective opinions of 'best' and 'fresh'. Considering tastes as an aspect term, fresh can be extracted as an opinion term through a direct relation. Considering 'fish burger' as an aspect term, 'tastes' can be extracted as another aspect term through the indirection relation. Taken from W.  .   ing a weight matrix W which is shared across the whole network ( <ref type="bibr" target="#b93">Socher et al., 2013 )</ref>. For example, giving c 1 and c 2 as ndimensional vector representation of nodes, their parent will also be an n -dimensional vector calculated using a non-linear function such as tanh:</p><formula xml:id="formula_31">p 1 , 2 = tanh (W [ c 1 ; c 2 ])<label>(46)</label></formula><p>So in general, a hidden vector for any node n associated with a word vector x n can be computed as:</p><formula xml:id="formula_32">h n = tanh W v . x n + b + W r nk h k (47) k ¡Ê K t</formula><p>where K n denotes the set of children of node n, r nk denotes the dependency relation between node n and its child node k , and hk is the hidden vector of the child node k .</p><p>The tree structures used for RNNs include constituency tree and dependency tree. In a constituency tree, the words is represented at leaf nodes, a phrase is represented at internal nodes the root node represents the whole sentence ( <ref type="bibr" target="#b93">Socher et al., 2013 )</ref>. Meanwhile, in a dependency tree, each node including represents a word, connecting with other nodes with dependency connections <ref type="bibr" target="#b93">( Socher et al., 2013</ref> ). Demonstration of the constituency tree and dependency tree is presented in <ref type="figure" target="#fig_0">Fig. 16</ref> . ral network frameworks can resolve this issue, they have not overcome the limitation of RecNN caused by the requirement for a predefined tree structure to encode sentences, which limits the scope of its application ( Rojas-Barahona, 2016 ).</p><p>Approaching the task of target-dependent Twitter sentiment analysis, <ref type="bibr" target="#b28">Dong et al. (2014)</ref> proposed Adaptive Recursive Neural Network that propagates the sentiments of words to target depending on the context and syntactic relationship. This work can be considered as similar to sentiment polarity of aspect term as their annotated dataset contains only one target per tweet.</p><p>Nevertheless, contrary to previous studies,  argued that simply averaging the attention vector in RNN models might not solve the issues of multiple targets within a text. Their proposed model of using dependency trees overcomes this issue and achieves competitive performance in Twitter target sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Hybrid models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2.">Application</head><p>Despite the popularity of Recursive Neural Networks (RecNN) in various NLP tasks, its application to ABSA is rather limited <ref type="table" target="#tab_15">( Table 8 )</ref>.</p><p>Regarding to consumer domain review, a study by <ref type="bibr" target="#b54">Lakkaraju et al. (2014)</ref> proposed recursive tensor neural networks in extracting both target and sentiment that is more robust than two single models, and also allows for the representation of multiple aspects within the text.</p><p>Two more recent works involving RecNN in the customer review domain include those of <ref type="bibr" target="#b71">Nguyen and Shirai (2015)</ref> and  , both aimed at exploiting the aspects through the dependency and constituent trees of the sentence. While <ref type="bibr" target="#b71">Nguyen &amp; Shirai (2015)</ref> just focus on OTE using dependency and constituent trees, W.  expanded the recursive neural network models by <ref type="bibr" target="#b54">Lakkaraju et al. (2014)</ref> with a novel framework of RecNN &amp; CRF to co-extract the aspect and opinion terms. This framework consists of a dependency-tree RecNN sentence representation, which feeds input to the CRF for target and opinion coextraction.</p><p>RecNN models developed by <ref type="bibr" target="#b54">Lakkaraju et al. (2014)</ref> and  aimed to improve the error-prone two-step approaches ( <ref type="bibr" target="#b2">Akhtar et al., 2017;</ref><ref type="bibr" target="#b42">Gu et al., 2017 ;</ref>, whereby error 1 leads to error 2. However, while recursive neuCoping with both advantages and disadvantages of the previously discussed models, many studies attempted to apply hybrid solutions in customer review domains, such as Xue, Zhou, , <ref type="bibr" target="#b126">Ye et al. (2017)</ref> , Chen, Xu, Yang, and  . <ref type="bibr" target="#b124">Xue et al. (2017)</ref> noted that the aspect terms and aspect category are closely related, so they proposed a multi-task framework of BiLSTM for OTE and CNN for ACD. The main benefits of this framework is the mutual information sharing of two tasks, in which the CNN can also utilize extra information learned in the BiLSTM to improve its informative features, while the predicted tag from the BiLSTM can also receive the most salient n-gram features via convolutional operations.</p><p>Similarly, <ref type="bibr" target="#b16">Chen et al. (2016)</ref> also combined LSTM and CNN together for sentiment classification but used LST for generating context embedding and CNN for detecting features. <ref type="bibr" target="#b126">Ye et al. (2017)</ref> proposed a dependency-tree based convolutional stacked neural network (DTBCSNN) for aspect term extraction, in which the convolution is included in the sentence's dependency parse trees to capture syntactic and semantic features. This can overcome the practical limitations of sequential models (RNNs) which cannot capture the tree-based dependency information. The proposed model does not need any handcraft features and flexible to include extra linguistic patterns. <ref type="table" target="#tab_16">Table 9</ref> below provides an overview of hybrid solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Comparison of performance on benchmark datasets</head><p>The above discussion has provided insights into the different approaches chosen by researchers for NLP tasks. It is evident that outcomes depend not only on model choice but on the seman-   <ref type="bibr" target="#b51">( Kim, 2014;</ref><ref type="bibr">Po- ria, Cambria, et al., 2016;</ref><ref type="bibr" target="#b121">Wu et al., 2016</ref> ) due to their ability to extract local patterns (i.e. the most important n-gram) of a sentence to produce fixed size input. However, this is true only as long as classification of key phrases of limited length is required <ref type="bibr" target="#b38">( Goldberg, 2017 )</ref>. Furthermore, CNN models demand large sets of training data and require a significant amount of fine-tuned parameters <ref type="bibr" target="#b127">( Yin, Kann, Yu, &amp; Sch¨¹tze, 2017;</ref><ref type="bibr" target="#b128">Young et al., 2017 )</ref>. Further issues arise from the fixed size of the hidden layer which prompts manipulation of input sentence length (CNN models add padding to short sentences and reduce long sentences), making capture of broader contextual information and sentence dependen-  <ref type="bibr" target="#b134">Zhao et al., 2017 )</ref>. Although this limitation can to some extent be overcome by a text window approach, whereby local feature windows of neighbouring words form around each word such as demonstrated by  , no information can be captured outside the window. As pointed out by Tu, Lu, Liu, Liu, and  , this has important implications for the application of CNN to languages with morphologically-rich texts such as Russian and Mandarin. In such cases, a model capable of recognising long-term dependencies such as Recurrent Neural Networks (RNN) or Recursive Neural Networks (RecNN) is called for.</p><p>RNNs are powerful because they combine two properties: (i) Distributed hidden states that allows them to efficiently store information from past computations; and (ii) Non-linear dynamics that better fit the non-linear nature of data <ref type="bibr" target="#b99">( Tang, Qin, Feng, et al., 2015 ;</ref> ). Significant research suggests that RNN is superior to CNN, citing the example of the LSTM model which does not require large training datasets <ref type="bibr">( Plank, S?gaard, &amp; Gold- berg, 2016</ref> ) and can achieve comparative performance to CNNs with fewer parameters <ref type="bibr" target="#b43">( Hassan &amp; Mahmood, 2017 )</ref>. Therefore, in terms of ABSA tasks, RNNs may perform better than CNNs if the classification is dependent on the semantic relationship of whole sentences.</p><p>In the case of RecNNs, a simple architecture and the ability to learn tree-structures of sentences and new words are distinct advantages <ref type="bibr" target="#b93">( Socher et al., 2013;</ref><ref type="bibr" target="#b97">Tai et al., 2015 )</ref>. However, they are heavily dependent on parsers <ref type="bibr" target="#b93">( Socher et al., 2013 ) and</ref> have not yet shown consistent performance in sentence classification <ref type="bibr" target="#b38">( Goldberg, 2017 )</ref>. Further research is clearly required.</p><p>Thus, these different models were designed with different objectives in terms of sentence modelling, particularly when analysing CNNs and RNNs. While CNNs try to extract the most important n-grams, RNNs try to create a composition with unbounded context ( <ref type="bibr" target="#b128">Young et al., 2017 ;</ref><ref type="bibr" target="#b132">L. Zhang et al., 2018 )</ref>. <ref type="table" target="#tab_0">Table 10</ref> presents the summary of model comparison.</p><p>To provide insights into the large number of proposed methods for ABSA, the below session will classify all methods according to three ABSA tasks: aspect term (or opinionated target) extraction (OTE), methods focusing on aspect category detection (ACD), and methods focusing on aspect-specific sentiment polarity (SP).</p><p>For each task, the comparison is presented with a table outlining the attempted DNN methods together with the bestperforming methods from SemEval ABSA. Each table contains the method, its domain, the performance as reported by the studies. The performance is reported in the form of Precision, Recall, F1, and Accuracy. It should be noted that: (i) some papers did not provide all the measures; (ii) when multiple models are proposed, the best model will be reported; (iii) due to the difference in experimental settings, the methods should not be compared using the scores; (iv) some researchers aimed to resolve two or three tasks and so will appear more than once in the tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Opinion target extraction</head><p>As the majority of studies in this task used data from SemEval 2014 with two domains in English, namely laptop and restaurant, <ref type="table" target="#tab_0">Table 10</ref> evaluates performance for a range of models with respect to this dataset. It is apparent that the majority of approaches were implemented according to RNN and its variants as LSTM or GRU, with high performance in both domains -F1 of over 75 in the laptop domain and over 80 in the restaurant domain. The current best model appears to be the CNN system by  for both domains, showing that a window-approach in CNN can extract relevant opinion targets, and can overcome the issue of long-term dependency. It is also interesting that the attention mechanism can boost performance of RNN-based systems (for examples <ref type="bibr" target="#b57">Li et al., 2018 ;</ref>. Fewer attempts were made to apply RecNN with lower performance, suggesting that processing words sequentially may be more informative than a tree structure. <ref type="table" target="#tab_0">Table 12</ref> compares the performance of models across different languages within the SemEval 2016 dataset for the restaurant domain. It is apparent that the performance of models in English is better than for other languages, followed by French and Spanish. It is also interesting to see that the LSTM models <ref type="bibr" target="#b57">Li et al., 2018</ref> ) and hybrid models ( <ref type="bibr" target="#b124">Xue et al., 2017 )</ref> show higher performance than the best models in the SemEval competition. There is, therefore, some evidence that the LSTM-based models are more effective in multilingual environments, due to their ability to record past and future contexts of words.</p><p>It should be emphasised that in this dataset most of the sentences consist only of one target term, and most target terms are expressed by a single word. Therefore, the CNNs can extract the target efficiently. However, the comparison also shows that when RNNs are incorporated with other components such as attention and MemNet, they have comparable power. Such combinations can overcome the weakness of RNNs in capturing key phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Aspect category detection</head><p>Similar to the OTE task, the DNN model has outperformed the best performing supervised machine learning models <ref type="table" target="#tab_0">( Table 13 )</ref>, with their performance in ACD reaching F1 60-70 in English, and over 50 for other languages. In this task, more datasets and more <ref type="table" target="#tab_0">Table 11</ref> Performance in opinion target extraction using the SemEval 2014 dataset (restaurant and laptop domains). The best outcomes are highlighted in blue.</p><p>languages have been used, from which can be inferred that the performance in English is much higher than for other languages. In term of the model, CNN seems to have the best performance for this task, with winning models in SemEval ABSA 2016 by <ref type="bibr" target="#b106">Toh et al. (2016)</ref> and <ref type="bibr" target="#b87">Ruder et al. (2016)</ref> and an outperforming model by <ref type="bibr" target="#b124">Xue et al. (2017)</ref> . Nevertheless, because of the limited neural network studies in this ACD task, it is difficult to conclude which model achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Sentiment polarity of aspect-based consumer reviews</head><p>The task of aspect level sentiment polarity is more challenging than general task of sentiment analysis because the model needs to incorporate the impacts of context words towards the target or aspect. A general approach for using DNN in this task is through representing context, generating a target representation, and then identifying the important sentiment words for the target. In polarity classification, although many deep learning techniques have been proposed, there has not yet been an attempt that uses the RecNN model <ref type="table" target="#tab_0">( Table 14 )</ref>. Similar to the task of OTE, the RNNs have <ref type="table" target="#tab_0">Table 12</ref> Performance in opinion target extraction with SemEval 2016 dataset in restaurant domain. Shaded cells highlight the best models in the SemEval competition. demonstrated their competitive performance, in terms of capturing long-term dependency in sentences and general semantic classification. Furthermore, the best performers are the RNNs that incorporate attention or memory networks. This shows that with an attention weight aggregated from a lower level, the models can learn how to concentrate on different parts of the sentence to classify target and opinion words and the link between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Sentiment polarity of targeted text</head><p>ABSA tasks with CNN models but achieved lower outcomes than with the SVM approach. <ref type="bibr" target="#b129">Yuan et al. (2017)</ref> found that a purely window-based neural network produces outcomes that are comparable to an LSTM-RNN approach, and concluded that local context rather than long-term dependencies were important for aspect extraction. A study by <ref type="bibr" target="#b4">Al-Smadi et al. (2017)</ref> for Arabic hotel reviews demonstrated that the SVM approach outperforms other deep RNN approaches for all ABSA tasks. All this illustrates that there are still significant challenges in terms of the application of DL methods to sentiment analysis in general and to ABSA in particular. <ref type="table" target="#tab_0">Table 15</ref> shows various performance indicators of models based on a Twitter dataset by <ref type="bibr" target="#b28">Dong et al. (2014)</ref> . Compared to the performance indicators reported in Section 4.3 , it is interesting to observe that the accuracy of this domain is lower than in the customer review domain, which is largely due to the characteristics of tweets -short, highly expressive, high use of sarcasms, and less grammatical correctness than review texts <ref type="bibr" target="#b36">( Giachanou &amp; Crestani, 2016 )</ref>. It also shows that the CNN model has not yet been utilized, possibly because of its weaknesses in processing this type of data. Overall, the performance of RNN and RecNN are similar, with accuracy ranging from 69 to 72. While the CNN and RNN may work better in a grammatically correct context, overall this indicates that for the identification of sentiment polarity of targeted text, the tree structure and parser represent a promising approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Domain adaptation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Challenges</head><p>From the above discussion, it is clear that DL methods are still in their infancy. There are cases where the performance of DL methods is not as effective as expected. An example comes from Xu, Liu, <ref type="bibr" target="#b123">Wang, &amp; Yin, (2018)</ref> who attempted to approach three One major challenge for ABSA is the current focus on consumer reviews, which raises the issue of domain adaptation, namely whether the trained parameters in one domain can be applied to another domain. It is apparent that the sentiment of a word can only be determined given its domain or context. For example, "small" contains a positive sentiment in the electronics domain in "the phone is small and convenient" but it has negative sentiment in a restaurant review when it states, "the portion is small". Given numerous domains, domain adaptation is clearly important to exploit the knowledge from one domain and increase the effectiveness of the analysis <ref type="bibr" target="#b29">( Dragoni &amp; Petrucci, 2017 )</ref>.</p><p>With respect to ABSA tasks, it is clear from the above findings that while one method may perform well in one domain, there is no guarantee of similar performance others. For example, <ref type="bibr" target="#b26">Ding et al. (2017b)</ref> reported a much lower score in cross-domain performance compared to in-domain. Most noticeably, the performance of models varies significantly between domains. Evidence comes from the SemEval 2014 dataset, where performance in the restau- <ref type="table" target="#tab_0">Table 13</ref> Performance of DNN models in aspect category detection tasks from <ref type="bibr" target="#b11">SemEval -2016</ref> rant domain is reported to be higher than in the laptop domain, for all studies and tasks ( <ref type="bibr" target="#b57">Li et al., 2018;</ref>.</p><p>One possible explanation is that the prevalence of aspect phrases is higher within the laptop domain (i.e. 36.55% versus 24.56%), making it more difficult to predict than in the case of single-word aspects . Furthermore, consumer reviews in general are highly product-oriented, which means that most of the aspects or opinions are expressed with nouns or noun phrases, while in reality, aspects and opinion can be represented in different formats, and the co-existence of opinionated texts and non-opinionated texts is frequent <ref type="bibr" target="#b24">( De Clercq, 2016 )</ref>. In unsupervised machine learning approaches, the issue can be resolved by incorporating a domain-specific lexicon <ref type="bibr" target="#b59">( Liu, 2015 )</ref>, which could also be considered for DNN models. <ref type="bibr">Ruder et al. (2017)</ref> have attempted to overcome the issue through domain adaptation, simplified by domain similarity metrics to guide the selection of appropriate training data. Another way to overcome the domain adaption is to pre-train the word embeddings in a large similar corpora <ref type="bibr" target="#b87">( Ruder et al., 2016 )</ref>, which has shown promise as discussed in session 3.1.1. Similarly, <ref type="bibr" target="#b29">Dragoni et al. (2017)</ref> suggested that the domain adaptation will be more effective if the word embeddings is created from an opinion-based corpus rather than a general purpose one (such as Wikipedia). They proposed NeuroSent, a tool for calculating the linguistic overlaps between different domains for conjecturing sentiment polarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Multilingual application</head><p>Lo, <ref type="bibr" target="#b61">Cambria, Chiong, &amp; Cornforth (2017)</ref> argued that multilingual sentiment analysis has certain challenges, including word sense ambiguity, language-specific structure and translation errors. <ref type="bibr" target="#b75">Peng et al. (2018)</ref> have illustrated this in the case of Chinese, where each sub-word may encode semantics. Thus, the verb 'shine' contains 'sun' and 'moon' as sub-elements. This is radically different from English where only character N-grams (i.e. "pre", "sub") contain semantics. Therefore, it requires a higher effort to encode and decode the former type of language.</p><p>Despite the fact that DNN models require less language-specific features ( <ref type="bibr" target="#b98">Tamchyna et al., 2016)</ref> , this review has highlighted that ABSA has not yet achieved its potential in a multilingual environment. The first issue stems from the fact that there are insufficient resources for many languages to construct NLP models. This is particularly the case for low-resource languages, which lack of large monolingual or parallel corpora such as Hindi or Tegulu. Is observed that word vectors in those type of languages obtain lower quality than others ( <ref type="bibr" target="#b40">Grave et al., 2018 )</ref>. It is also clear that there are yet no benchmark ABSA datasets on different languages. Apart from SemEval 2016, there is merely a small number of product review datasets in Chinese ( <ref type="bibr" target="#b34">Feng et al., 2018;</ref><ref type="bibr" target="#b42">Gu et al., 2017</ref><ref type="bibr">), in Hindi ( Akhtar, Kumar, et al., 2016</ref>, and in Vietnamese <ref type="bibr" target="#b66">( Mai &amp; Le, 2018 )</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 14</head><p>Performance in opinion target extraction with SemEval 2014 dataset (restaurant and laptop domains). Cells in shading indicate the best model in SemEval competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 15</head><p>Performance on Twitter dataset by <ref type="bibr" target="#b28">Dong et al. (2014)</ref> . It is, therefore, hardly surprising that there are few successful attempts at using DL methods on ABSA with different languages, with the exception of <ref type="bibr" target="#b87">Ruder et al. (2016)</ref> and  for French, Spanish, Russian, Dutch and Turkish. From SemEval 2016, it is evident that the performance of models varies between languages, with higher scores recorded in English and Chinese but lower ones in French, Spanish, Dutch, and Russian ( <ref type="bibr" target="#b79">Pontiki et al., 2016)</ref> . <ref type="bibr" target="#b87">Ruder et al. (2016)</ref> have suggested incorporation of different embeddings trained on a range of corpora in different languages. <ref type="bibr" target="#b75">Peng et al. (2018)</ref> successfully incorporated radical, character and word embeddings into Chinese to overcome the issue of multiword aspect representation. Therefore, it is expected that training the models to associate with different surface forms could help to reduce the performance differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Technical requirements</head><p>Best performing ABSA systems generally use manually labelled data and language specific resources for training on a particular domain and language <ref type="bibr" target="#b81">( Pontiki et al., 2014</ref><ref type="bibr" target="#b80">( Pontiki et al., , 2015</ref><ref type="bibr" target="#b79">( Pontiki et al., , 2016</ref> . Particularly the DL based systems require a significant amount of labelled data for training <ref type="bibr" target="#b8">( Araque et al., 2017 ;</ref><ref type="bibr" target="#b17">T. Chen et al., 2017 )</ref>. For example, one major issue in Tang, <ref type="bibr" target="#b99">Qin, Feng, et al. (2015)</ref> is the failure to produce consistent results, possibly due to a small training corpus ( .</p><p>Another issue is related to computational resources and time. Despite the improvement in technology that reduces computational time in training for DNN models, reported by <ref type="bibr" target="#b42">Gu et al. (2017)</ref> and <ref type="bibr" target="#b4">Al-Smadi et al. (2017)</ref> , when compared to conventional machine learning, time is still a current issue for DNN models. For example, <ref type="bibr" target="#b16">Chen et al. (2016)</ref> reported an acceptable time span of four hours to process over 30 0 0 sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Linguistic complications</head><p>As discussed in <ref type="bibr" target="#b91">Schouten et al. (2016)</ref> , there are still challenges in terms of language that have not yet been satisfactorily addressed in current studies. They include the issue of comparative sentences, where it is difficult to detect whether any aspect is preferred. Similar problems exist with conditional sentences (irrealis phenomenon), where it is difficult to extract sentiment from an unknown/unreal situation. Also highly complex is analysis from sentences that contain negation and valence-shifting, where the polarity can be flipped, and sentiment value can be decreased or increased.</p><p>It is also challenging to extract implicit aspects, which can only be read between the lines ( <ref type="bibr" target="#b85">Rana et al., 2016)</ref> . The same text may be read differently in a different situation/ A classic example comes from <ref type="bibr" target="#b72">Pang &amp; Lee (2008)</ref> "go read the book" expresses positive sentiments in the case of a book review, but implies a negative sentiment as a film review. One consideration is to undertake "co-reference", reflecting aspects with pronouns or synonymous phrases; however, not much research exists as yet <ref type="bibr" target="#b24">( De Clercq, 2016 )</ref>.</p><p>NLP methods also need to catch up with the evolution of usergenerated content, which is quite different from standard text. It is characterized by its "noisiness" from highly expressive tokens such as emoticons, flooding (repetition of some characters such as "loooool") as well as misspellings, grammatical errors, abbreviations and more use of sarcasm, irony, humour and metaphor, particularly for twitters <ref type="bibr" target="#b24">( De Clercq, 2016;</ref><ref type="bibr" target="#b36">Giachanou &amp; Crestani, 2016 )</ref>. This makes it more difficult to train with tools that were originally trained from a standard text <ref type="bibr" target="#b24">( De Clercq, 2016 )</ref>.</p><p>The difficulties increase with regard to other languages, with Chinese with words that are ambiguous in terms of semantics and syntax <ref type="bibr" target="#b74">( Peng, Cambria, &amp; Hussain, 2017 )</ref>, in Hindi and Arabic through the issue of multi-dialects and lately also for Arabizi -Arabic words with Latin characters <ref type="bibr" target="#b31">( El-Masri, Altrabsheh, &amp; Mansour, 2017 )</ref>. A promising approach suggested by <ref type="bibr" target="#b91">Schouten et al. (2016)</ref> is to evolve to the more concept-centric approach of the knowledge base. Recent works in SenticNet and SSWE suggest that the incorporation of the knowledge base and recent language evolution is promising.</p><p>the relationship between aspect and opinion, improved performance can be obtained by joint extraction and classification of aspect, category and sentiment. However, many robust studies opt to perform only aspect extraction or categorization, and those who jointly perform aspect detection and sentiment analysis, have not yet achieved optimal performance. Therefore, there is the need for a combined approach that can undertake both tasks and create more pervasive sentiment analysis at aspect level. Research would further benefit from a more concept-centric approach to connect knowledge bases with deep learning methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: List of Abbreviations</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Three tasks of ABSA in a sample sentence from SemEval ABSA dataset 2016. The sentence has two opinion targets: sushi &amp; service. The category of "sushi" is "Food", with the attribute being "Quality" and polarity "Positive". The category is "Service", with an attribute of "General" and polarity of "Positive".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The opinion target is not explicitly stated, but the category and polarity of the sentence can still be inferred. Sentence from SemEval data 2016.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison of classic machine learning and deep learning processes for NLP. Deep learning architecture is characterized by dense embeddings and hidden layersadapted from Thanaki (2017) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. CBOW and skip-gram model. Figure from Mikolov et al. (2013) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>.</head><label></label><figDesc>Others, such as Poria, Cambria, et al. (2016) and Wang et al. (2016) created their own embeddings by applying word2vec to a selected corpora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>observed average gains of 5% in evalua- tion criteria thanks to fine-tuning word embeddings according to dependency-based word vectors and specialized features in their RNN models. Furthermore, pre-training word embeddings in large corpora with similar domain have shown important to the successful im- plementation of DNN models ( Ruder, Ghaffari, &amp; Breslin, 2016 ). For Liu et al. (2015), Poria, Cambria, et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>in Poria, Cambria, et al. (2016) ; 4 tags according to Stanford Tagger in Liu et al. (2015), Ye, Yan, Luo, and Chao (2017) ; or even 45 tag based on Penn Tagger ( Jebbara &amp; Cimiano, 2017; Zainuddin, Selamat, &amp; Ibrahim, 2018 ). In Liu et al. (2015) and Ye et al. (2017) , the four POS types are noun, adjective , verb and adverb , and the five classes of chunks are: NP (noun phrase), VP (verb phrase), PP (prepositional phrase), ADJP (adjective phrase) and ADVP (adverb phrase) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Conditional random fields with other models. Taken from ( Sutton, 2012 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Basic CNN model with 4 layers. Adapted from Kim (2014) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Max-pooling layer: this layer selects the maximum valu¨º c = max { c} as the feature corresponding to one particular filter Softmax layer : The final feature vector with m filters w is ob- tained as z = [ ? c 1 , ? c 2 , . . . ? c m ] , the final output softmax layer is ob- tained using softmax function:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. An example of CNN architecture for aspect category and sentiment polarity. Adapted from Gu, Gu, &amp; Wu (2017) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Other papers utilise multiple or multi-tasking CNNs, showing that the CNN can provide other advantages. Examples of multi- ple CNN can be found in Xu et al. (2017) who incorporated CNN with non-linear CRF to extract the aspect term, then concate- nated aspect vector with word embeddings and used another CNN model to determine the sentiment. They achieved a competitive performance in Yelp datasets. Meanwhile, Gu et al. (2017) pro- posed a cascaded model with two levels of CNNs -CNN aspect mappers and a CNN sentiment classifier. Aspect-mapping CNN and sentiment-classification CNN are organized in a cascaded way. Each mapper determines whether the input sentence belongs to its cor-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Basic RNN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Bidirectional RNN. Taken from ( Fan et al., 2014 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Attention mechanism in a bidirectional RNN. Taken from ( L. Zhang et al., 2018 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Further works by P. Chen et al. (2017) and Tay, Tuan, et al. (2017) also focused on attention mechanisms for the LSTM to in- corporate aspect information into the model. While P. Chen et al. (2017) adopted a multiple-attention mechanism, Tay, Tuan, et al. (2017) introduced a novel association layer with holographic re- duced representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. A model with three computation layers (hops) model that encodes both context and aspect words, and output of lower hop is recorded as input for higher hop. Taken from Tang, Qin, et al. (2016) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Bidirectional LSTM-CRF in opinion target extraction. Taken from T. Chen et al. (2017) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Focus of attention on different aspects. Taken from Y. Wang et al. (2016) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Target-dependent LSTM &amp; target-connection LSTM. Source: Tang, Qin, Feng, et al. (2015) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Constituency tree and dependency tree. Adapted from Nguyen &amp; Shirai (2015) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>Study Task: Sentiment polarity Features Performance 1 B. Wang et al. (2017) TDParse + (dependency parser and three sentiment lexica) F1: 69.8 (3-way)Acc: 72.1 (3-way) 2 M. Zhang et al. (2016) Bidirectional GRNN &amp; 3-way gate Acc: 71.96 (3-way) 3 Vo &amp; Zhang (2015) Dependency parser &amp; sentiment lexicon F1: 69.9 (3-way)Acc: 71.1 (3-way) 4 Tang, Qin, Feng, et al. (2015) Target Dependent LSTM Acc: 70.8 (3-way) Target Connection LSTM Acc: 71.5 (3-way) 5 P. Chen et al. (2017) Recurrent Attention on Memory (RAM) + attention layers Acc: 69.36 (3-way) 6 Dong et al. (2014) Adaptive Recursive Neural Network + uni/bigram features + SVM classifier Acc: 66.3 (3 way)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 Publicly available datasets for ABSA.</head><label>1</label><figDesc></figDesc><table>No Dataset and Author Domain &amp; Language &amp; Size 
Format 
Example 
URL 

1 
Customer review 
data ( Hu et al., 
2004 ) 

Digital products (EN): 3945 
sentences 

Text format with tags of 
aspect terms and polarities 
(-3, -2, -1, 1, 2, 3) 

speaker phone[ + 2], radio[ + 2], infrared[ + 2] ##my 
favourite features, although there are many, are the 
speaker phone, the radio and the infrared . 

https://www.cs.uic. 
edu/ ?liub/FBS/ 
sentiment-analysis. 
html 
2 
SemEval 2014 
( Pontiki et al., 
2014 ) 

Restaurants (EN): 3841 
sentences 
Laptops (EN): 3845 sentences 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>mentionClass &gt; indicate the aspect term)</head><label></label><figDesc></figDesc><table>http://alt.qcri.org/ 
semeval2016/task5/ 

Restaurant (DU): 400 reviews 
(2286 sentences) 
Mobile phone (DU): 270 
reviews (1697 sentences) 
Restaurant (FR): 455 reviews 
(2429 sentences) 
Restaurant (RU): 405 reviews 
(4699 sentences) 
Restaurant (ES): 913 reviews 
(2951 sentences) 
Restaurant (TU): 339 reviews 
(1248 sentences) 
Hotel (AR): 2291 reviews (3309 
sentences) 

XML tag of {E#A, OTE, 
polarity} 

5 
ICWSM 2010 JDPA 
Sentiment Corpus 
for the Automotive 
Domain 
( Kessler, Eckert, 
Clark, &amp; Nicolov, 
2010 ) 

Automotive &amp; digital devices: 
515 documents (19,322 
sentences) 

XML tags ( &lt; </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 1 ( continued )</head><label>1</label><figDesc></figDesc><table>No Dataset and Author Domain &amp; Language &amp; Size 
Format 
Example 
URL 

7 
FiQA ABSA ( Maia 
et al., 2018 ) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>= i w i x i + b. Thus, the output o is calculated with the activation function:</head><label></label><figDesc></figDesc><table>onion 
rings 
are 
great 
! 

Labels: 
O 
B 
I 
O 
O 
O 

ron is written as a </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="true"><head>Table 3 Pre-trained word embeddings datasets.</head><label>3</label><figDesc></figDesc><table>Pre-trained Word Embeddings 
Authors 
Dimension 
Vocabulary size 

SENNA/Wikipedia 
Collobert et al. (2011) 
50d 
130K 
word2vec CBOW/Google News 
Mikolov et al. (2013) 
300d 
3M 
word2vec CBOW/Amazon 
Liu et al. (2015) 
50d, 300d 
1M 
SSWE h , SSWE r , SSWE u 
Tang et al. (2014) 
50d 
137K 
GloVe/ Wikipedia 2014 + Gigaword 5 
Pennington, Socher, &amp; Manning (2014) 
50d, 100d, 200d, 300d 
400K 
GloVe Twitter 
Pennington et al. (2014) 
25d, 50d, 100d, 200d 
1.2M 
Learning word vectors for 157 languages/fastText 
Grave, Bojanowski, Gupta, Joulin and Mikolov (2018) 
Varied between languages 
Varied between languages 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="true"><head>Table 4 Application of the CNN model in the consumer review domain.</head><label>4</label><figDesc></figDesc><table>No 
Study 
Domain 
Dataset &amp; Language 
Model 
Performance 

Opinion target extraction 
1 
Poria, Cambria, 
et al. (2016) 

12 electronic 
products 

Hu and Liu (2004) 
English 
Deep CNN + Amazon WE + POS + LP Precision: 82 0.65 -92.75% 
Recall: 85.02 -88.32% 
F1: 84.87 -90.44% 
Laptop 
SemEval '14 
English 
Precision: 86.72% 
Recall: 78.35% 
F1: 82.32% 
Restaurant 
SemEval '14 
English 
Precision: 88.27% 
Recall: 86.10% 
F1: 87.17% 
2 
Feng et al. (2018) 
Mobile phone 
PM from Amazon, 
Jingdong, and Lynx 

Chinese 
Deep CNN + WE + POS + dependent 
syntactic-(explicit aspects) 

Precision: 77.75% 
Recall: 72.61% 
F1: 75.09% 
Aspect category extraction 
3 
Toh &amp; Su (2016) 
Restaurant 
SemEval '16 
English 
CNN + WE + head word + name 
list + word cluster 

F1: 75.10% 

Laptop 
SemEval '16 
English 
F1: 59.83% 
4 
Ruder et al. (2016) 
Mobile phone 
SemEval '16 
Dutch 
CNN + concatenated vectors 
F1: 45.55% 
Hotel 
SemEval '16 
Arabic 
F1: 52.11% 
5 
Gu et al. (2017) 
Smartphone 
PM from Amazon 
English 
Multiple CNNs for each aspect 
F1: 72.67 -83.74% 
Shirt 
PM from Taobao 
Chinese 
F1: 92.26 -97.34% 
6 
Wu et al. (2016) 
Smartphone 
PM from Amazon 
English 
Multi-task 
CNN + word2vec/Wikipedia 

F1:71.6-81.2% 

Sentiment polarity 
7 
Gu et al. (2017) 
Smartphone 
PM from Amazon 
English 
Single CNN 
Acc: 84.87% (binary) 
Shirt 
PM from Taobao 
Chinese 
Acc: 98.26% (binary) 
8 
Ruder et al. (2016) 
Hotel 
SemEval '16 
Arabic 
CNN + aspect tokens 
Acc: 82.72% 
Mobile phone 
SemEval '16 
Dutch 
CNN + aspect tokens 
Acc: 83.33% 
9 
Du et al. (2016) 
Electronics 
PM from Amazon 
English 
Aspect specific sentiment 
WE + CNN 

Acc: 92.08% (binary) 

Movies and TV 
English 
Acc: 92.05% (binary) 
CDs and vinyl 
English 
Acc: 94.38% (binary) 
Clothing, shoes and 
jewellery 

English 
Acc: 93.22% (binary) 

10 
Wu et al. (2016) 
Smartphone 
PM from Amazon 
English 
Multi-task 
CNN + word2vec/Wikipedia 

Acc: 84.1% (binary) 

11 
Xu et al. (2017) 
Laptop 
PM from Yelp 
English 
CNN + CRF 
Acc: 70.90% (binary, lower than 
SVM model) 
Restaurant 
PM from Yelp 
English 
CNN + CRF 
Acc: 68.34% (binary, lower than 
SVM model) 
12 
Akhtar, Kumar, 
et al. (2016) 

12 personal 
electronic products 

PM ( Akhtar, Ekbal, 
&amp; Bhattacharyya, 
2016 ) 

Hindi 
CNN + SVM 
Acc: 65.96% (3-way) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>, at each time step t , a hidden forward layer h ¡û is computed based on the previous hidden state h t?1 and the cur- rent input x t . Similarly, a hidden backward layer ¡û ? h is computed ¡û h t = s ( W hh h t?1 + W xh x t?1 )</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="true"><head>Table 6 Application of RNN model in the consumer review domain.</head><label>6</label><figDesc></figDesc><table>No 
Study 
Domain 
Dataset &amp; Language 
Model 
Performance 

Opinion target extraction 
1 
Toh &amp; Su (2016) 
Restaurant 
SemEval '16 
English 
RNN + WE + Name List + DP Name 
List + Word Cluster 

Precision: 75.49% 
Recall: 69.44% 
F1: 72.34% 
2 
T. Chen et al. (2017) Restaurant 
SemEval '16 
English 
BiLSTM + Google WE + CRF 
F1: 72.44% 
Restaurant 
SemEval '16 
Spanish 
F1: 71.70% 
Restaurant 
SemEval '16 
French 
F1: 73.50% 
Restaurant 
SemEval '16 
Russian 
F1: 67.08% 
Restaurant 
SemEval '16 
Dutch 
F1: 64.29 % 
Restaurant 
SemEval '16 
Turkish 
F1: 63.76% 
3 
Liu et al. (2015) 
Laptop 
SemEval '14 
English 
LSTM-RNN + POS + chunk + Amazon 
WE 

F1: 75.00% 

Restaurant 
SemEval '14 
English 
Bi-Elman-
RNN + POS + chunk + Amazon 
WE 

F1: 82.06% 

4 
Jebbara &amp; 
Cimiano (2016) 

Restaurant 
Laptop 
Hotel 

ESWC Challenge 
2016 

English 
BiGRU + Amazon WE + POS 
Precision: 65.9% 
Recall: 71.0% 
F1: 68.4% 
5 
Tay, Tuan and 
Hui (2017) 

Restaurant 
SemEval '14 
English 
Holo DyMemNN 
Precision: 81.87% 
Recall: 79.73% 
F1: 79.73% 
Laptop 
SemEval '14 
English 
Holo DyMemNN 
Precision: 75.16% 
Recall: 73.19% 
F1: 74.03% 
6 
Al-
Smadi, Qawasmeh, 
Al-Ayyoub, 
Jararweh, and 
Gupta (2017) 

Hotel 
SemEval '16 
Arabic 
RNN 
F1: 48% 

SVM + morphological, N-grams, 
syntactic, and semantic 

F1: 89.8% 

7 
Yuan, Zhao, Qin, 
and Liu (2017) 

Restaurant 
SemEval '14 
English 
LSTM + Local Context + Senna WE 
F1: 80.62% (lower than CRF models) 

Laptop 
SemEval '14 
English 
BiLSTM + Local Context + Senna WE F1: 74.78% (lower than CRF models) 
8 
X. Wang et al. 
(2016) 

Restaurant 
SemEval '14 
English 
Uni-directional Elman RNN 
F1: 82.12% (lower than the CRF model) 

Laptop 
SemEval '14 
English 
F1: 75.45% 
9 
Ding, Yu, and 
Jiang (2017b) 

Restaurant 
SemEval '14 + '15 
English 
Hierarchical LSTM 
F1: 77.9% 

Laptop 
SemEval '15 
English 
F1: 76.6% 
Digital device 
( Kessler et al., 
2010 ) 

English 
F1: 45.1% 

Web service 
( Toprak et al., 2010 ) English 
F1: 43.8% 
10 
W. Wang, Pan, &amp; 
Dahlmeier (2017) 

Restaurant 
SemEval '14 
English 
Coupled Multi-layer Attentions 
(CMLA) based on GRU 

F1: 85.29% 

Laptop 
SemEval '14 
English 
F1: 77.80% (lower than RNCRF) 
Restaurant 
SemEval '15 
English 
F1: 70.73% 
11 
Li &amp; Lam (2017) 
Laptop 
SemEval '14 
English 
Memory Interaction Network (MIN) 
based on LSTM with extended 
memory 

F1: 77.58% 

Restaurant 
SemEval '16 
English 
F1: 73.44% 
12 
Li, Bing, Li, Lam, &amp; 
Yang (2018) 

Laptop 
SemEval '14 
English 
Truncated History-Attention (THA) 
and Selective Transformation 
Network (STN) built on two LSTMs 

F1: 79.52% 

Restaurant 
SemEval '14 
English 
F1: 85.61% 
Restaurant 
SemEval '15 
English 
F1: 71.46% 
Restaurant 
SemEval '16 
English 
F1: 73.61% 
13 
Mai &amp; Le (2018) 
Mobile phone 
review 

PM from Youtube 
Vietnamese 
Bidirectional RNN + CRF 
Precision: 68.12% 
Recall: 75.87% 
F1: 71.79% 
Aspect category extraction 
14 
Tamchyna &amp; 
Veselovsk¨¢ (2016) 

Restaurant 
SemEval '16 
Turkish 
Binary classifier (deep LSTM) for 
each category 

F1: 61.03% 
Restaurant 
SemEval '16 
Russian 
F1: 64.83% 
15 
Tay et al. (2017) 
Restaurant 
SemEval '14 
English 
Tensor DyMemNN 
F1 (binary): 81.68% 
Reviews 
Merge dataset of 
SemEval '14-15 

English 
Tensor DyMemNN 
F1 (binary): 81.66% 

16 
Ding, Yu, &amp; 
Jiang (2017a) 

Restaurant 
Yelp SG dataset 
English 
RNN + finetune WE 
F1: 72.42% 
Laptop 
Amazon Product 
Reviews ( H. 
Wang, Lu, &amp; Zhai, 
2011 ) 

English 
F1: 66.17% 

17 
Chaudhuri &amp; 
Ghosh (2016) 

Hotel 
DBS Text Mining 
Challenge 2015 
data 

English 
Weighted Hierachi Bidirectional 
RNN (mini-batches) 

65% (10 aspects); 55% (20 aspects) 

( continued on next page ) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="true"><head>Table 6 ( continued )</head><label>6</label><figDesc></figDesc><table>No 
Study 
Domain 
Dataset &amp; Language 
Model 
Performance 

18 
Y. Ma et al. (2018) 
Reviews 
SemEval'15 
modified (no 
explicit aspects) 

English 
LSTM + Target 
attention + Sentence-level 
attention + SenticNet 

Macro F1: 76.44% 
Micro F1: 73.82% 

19 
Ding et al. (2017b) 
Restaurant 
SemEval '14 + '15 
English 
Hierarchical LSTM 
F1: 77.9% 
Laptop 
SemEval '15 
English 
F1: 76.6% 
Digital device 
JPDA Corpus 
( Kessler et al., 
2010 ) 

English 
F1: 45.1% 

Web service 
Darmstadt Corpus 
( Toprak et al., 2010 ) 

English 
F1: 43.8% 

20 
W. Wang et al. 
(2017) 

Restaurant 
SemEval '14 
English 
Coupled Multi-layer Attentions 
(CMLA) based on GRU 

F1: 85.29% 
Laptop 
SemEval '14 
English 
F1: 77.80% (lower than RNCRF) 
Restaurant 
SemEval '15 
English 
F1: 70.73% 
21 
Li &amp; Lam (2017) 
Laptop 
SemEval '14 
English 
Memory Interaction Network (MIN) 
based on LSTM with extended 
memory 

F1: 77.58% 
Restaurant 
SemEval '16 
English 
F1: 73.44% 
Sentiment polarity 
22 
Tay, Luu, &amp; 
Hui (2017) 

Restaurant 
SemEval '14 
English 
Aspect Fusion LSTM 
Acc: 75.44 (3 way 
 *  ); 87.78 (binary) 
Laptop 
SemEval '14 
English 
Acc: 68.81 (3 way); 83.58 (binary) 
23 
Cheng et al. (2017) Restaurant 
SemEval '14 
English 
BiGRU + aspect 
attention + sentiment attention 

Acc: 91.3 (binary); 85.1 (3-way) 
Restaurant 
SemEval '15 
English 
Acc: 83.4 (binary); 80.5 (3-way) 
Restaurant 
SemEval '16 
English 
Acc: 91.1 (binary); 87.5 (3-way) 
Laptop 
SemEval '15 
English 
Acc: 88.0 (binary); 85.1 (3-way) 
24 
Y. Wang et al. 
(2016) 

Restaurant 
SemEval '16 
English 
LSTM + aspect attention + aspect 
embeddings 

Acc: 77.2 (3 way); 90.9 (binary) 
Laptop 
SemEval '15 
English 
Acc: 68.9 (3 way); 87.6 (binary) 
25 
Y. Ma et al. (2018) 
Restaurant 
SemEval'15 ABSA 
modified (no 
implicit target) 

English 
Sentic LSTM + Target 
attention + Sentence-level attention 

Acc: 76.47% (binary) 

26 
P. Chen et al. (2017) Restaurant 
SemEval '14 
English 
Recurrent Attention on Memory 
(RAM) + attention layers 

Acc: 80.59% (3-way) 
Laptop 
SemEval '14 
English 
Acc: 74.65% (3-way) 
27 
R. Ma et al. (2017) 
Restaurant 
SemEval '14 
English 
Feature-based Compositing 
Memory Networks 

Acc: 82.03% (3-way) 
Laptop 
SemEval '14 
English 
Acc: 73.94% (3-way) 
28 
Tang, Qin, et al. 
(2016) 

Restaurant 
SemEval '14 
English 
Deep memory network 
Acc: 80.95% (3-way) 
Laptop 
SemEval '15 
English 
Acc: 72.37% (3-way) 
29 
Peng et al. (2018) 
Notebook 
Chinese aspect 
dataset 

Chinese 
Aspect target sequence model 
(ATSM), working on word level, 
character and radical level. 

Acc: 75.59% (binary); F1: 60.09% 
Car 
Acc: 82.94% (binary); F1: 64.18% 
Camera 
Acc: 84.86% (binary); F1: 75.35% 
Phone 
Acc: 85.95% (binary); F1: 80.13% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" validated="true"><head>Table 7 Application of the RNN model in targeted sentiment analysis.</head><label>7</label><figDesc></figDesc><table>No 
Study 
Domain 
Datasets 
Language 
Model 
Performance 

Opinion category detection 
1 
Tay, Tuan, et al. (2017) 
Tweets 
SemEval 2016 
Tweet task 

English 
Tensor DyMemNN 
F1: 72.42% Precision: 72.11% 
Recall: 72.79% 
Debates 
Internet 
Argument 
Corpus v2 

English 
Tensor DyMemNN 
F1: 66.17% Precision: 66.53% 
Recall: 66.07% 

2 
Y. Ma et al. (2018) 
London 
locations 

SentiHood 
English 
LSTM + Target 
attention + Sentence-level 
attention + SenticNet 

Acc: 67.43% Macro F1: 78.18% 
Micro F1: 77.66% 

Sentiment polarity 
3 
Tang, Qin, Feng, &amp; 
Liu (2015) 

Twitter data 
Dong et al. 
(2014) 

English 
Target Connection LSTM 
Acc: 71.5% Macro F1: 69.5% 

4 
M. Zhang, Zhang, &amp; 
Vo (2016) 

Twitter data 
Dong et al. 
(2014) 

English 
Bidirectional GRNN &amp; 3-way gate 
Acc: 71.96 (3-way) 

5 
Y. Ma et al. (2018) 
London 
locations 

SentiHood 
English 
Sentic LSTM + Target 
attention + Sentence-level attention 

Acc: 89.32% (binary) 

6 
P. Chen et al. (2017) 
Twitter data 
Dong et al. 
(2014) 

English 
Recurrent Attention on Memory 
(RAM) + attention layers 

Acc: 69.36 (3-way) 

Chinese news 
comments 

Primarily 
collected data 

Chinese 
Acc: 73.89 (3-way) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" validated="true"><head>Table 8 Application of RecNN in opinion target extraction in customer reviews and targeted Twitter sentiment analysis.</head><label>8</label><figDesc></figDesc><table>No 
Study 
Domain 
Dataset &amp; Language 
Model 
Performance 

Opinion target extraction 
1 
Nguyen &amp; Shirai (2015) Restaurant 
SemEval '14 
English 
PhaseRecNN + CRF + multiple global 
functions 

Precision: 62.40% 
Recall: 63.94% 
F1: 62.21% 
2 
W. Wang, Pan, 
Dahlmeier, &amp; 
Xiao (2016) 

Laptop 
SemEval '14 
English 
RecNN + CRF + POS + Name 
list + Sentiment Lexicon 

F1: 78.42% 

Restaurant 
SemEval '14 
English 
RecNN + CRF + POS + Name 
list + Sentiment Lexicon 

F1: 84.93% (lower than CRF 
models) 
3 
Lakkaraju, Socher, &amp; 
Manning (2014) 

Beer 
Stanford Beer 
Advocate Dataset 

English 
Joint Multi-Aspect Sentiment 
Model + Recursive Neural Tensor 
Network 

Accuracy for aspect terms: 77.04% 

Camera 
Amazon 
English 
Joint Multi-Aspect Sentiment 
Model + Recursive Neural Tensor 
Network 

Accuracy for aspect terms: 81.02% 

Sentiment polarity 
4 
Dong et al. (2014) 
Twitter data 
PM 
English 
Adaptive Recursive Neural 
Network + uni/bigram features + SVM 
classifier 

Acc: 66.3 (3 way) 

5 
Vo &amp; Zhang (2015) 
Twitter data 
Dong et al. (2014) 
English 
Dependency parser &amp; sentiment lexicon F1: 69.9% (3-way) 
Acc: 71.1% (3-way) 
6 
Zainuddin et al. (2018) 
Twitter data 
Hate Crime Twitter 
Sentiment (HCTS) 

English 
Association rule mining (ARM) 
+ POS + Stanford Dependency Parser 
(SDP) + Sentiwordnet + PCA + SVM 

Acc: 71.62% 

Stanford Twitter 
Sentiment (STS) 
dataset 

English 
Acc: 76.55% 

Sanders Twitter 
Corpus (STC) 

English 
Acc: 74.24% 

7 
B. Wang, Liakata, 
Zubiaga and 
Procter (2017) 

Twitter data 
Dong et al. (2014) 
English 
TDParse + (dependency parser and the 
three sentiment lexica) 

F1: 69.8% (3-way) 
Acc: 72.1% (3-way) 

Tweets from 
2015 UK 
general election 
campaign 

PM 
English 
TDParse (dependency parser) 
Acc: 56.45% 
F1: 46.09% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" validated="true"><head>Table 9 Application of hybrid models.</head><label>9</label><figDesc></figDesc><table>No 
Study 
Domain 
Dataset &amp; Language 
Model 
Performance 

Opinion target extraction 
1 
Ye et al. (2017) 
Restaurant 
SemEval '14 
English 
Dependency-Tree Based 
CNN + POS + chunk 

F1: 83.97% (lower than the CRF 
winning model) 
Laptop 
SemEval '14 
English 
F1: 75.66% 
2 
Xue et al. (2017) 
Restaurant 
SemEval '14 
English 
Multi-task learning neural 
network combines BiLSTM and 
CNN layers 

F1: 83.65% (lower than the top 
models) 

SemEval '15 
English 
F1: 67.73% 
SemEval '16 
English 
F1: 72.95% 
Aspect category detection 
3 
Xue et al. (2017) 
Restaurant 
SemEval '14 
English 
Multi-task learning neural 
network combines BiLSTM 
and CNN layers 

F1: 88.91% 

SemEval '15 
English 
F1: 65.97% 
SemEval '16 
English 
F1: 76.42% 
Sentiment polarity 
4 
P. Chen et al. (2016) 
Phone 
COAE2014 task5 
Chinese 
LSTM + CNN 
Acc: 90.91% (3-way polarity) 

Car 
COAE2012 task1 
and the autohome 
website 

Chinese 
Acc: 78.62% (3-way polarity) 

tics to be analysed. Yin et al. (2017) evaluated sentiment classi-
fication, question answering and POS tagging and concluded that 
model choice depended on the global semantics of a classification 
task and that, therefore, the focus should be on hyper-parameters 
such as layer size. 
This observation is validated by a comparison of models. It is 
clear that CNN models can be highly effective </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" validated="false"><head>Table 10 Summary of model comparison. CNN RNN RecNN Advantages ? Ability to extract meaningful local patterns (n-grams) ? Distributed hidden state that can store past computations ? Simpler architecture ? Non-linear dynamics ? Ability to produce a fixed size vector that takes into account the weighted combination of all words and summarizes the sequence ? Ability to learn tree-like structures ? Fast computation ? Do not require large dataset ? Ability to</head><label>10</label><figDesc></figDesc><table>construct representations for any 
new word 
</table></figure>

			<note place="foot" n="1"> In this paper, &quot;opinion target&quot; and &quot;aspect term&quot; are used interchangeably.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">This analysis has presented a comprehensive overview of major deep learning approaches and provided a precise comparison of these approaches for sentiment analysis at aspect level. For this analysis, more than 40 approaches were summarised and categorised according to their main architecture and classification tasks. Common approaches include standard and variants of Convolutional Neural Networks (CNN), Long-Short Term Memory (LSTM) and Gated Recurrent Unit (GRU)</title>
	</analytic>
	<monogr>
		<title level="m">the advent of user-generated content as a rich source of subjective information, there have been vigorous attempts to analyze, classify</title>
		<imprint/>
	</monogr>
	<note>To boost the performance of models, studies have included pre-trained and fine-tuned word embeddings, and incorporating linguistic factors in the form of part-of-speech and grammatical rules as well as exploring concept-based knowledge. However, from the review of the state-of-the-art in aspectlevel sentiment analysis and. presented in this paper, it is clear that ABSA and deep learning are still in the early stages. Given</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Aspect based sentiment analysis in hindi: Resource creation and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th edition of the Language Resources and Evaluation Conference (LREC)</title>
		<meeting>the 10th edition of the Language Resources and Evaluation Conference (LREC)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Feature selection and ensemble construction: A two-step method for aspect based sentiment analysis. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<idno type="doi">10.1016/j.knosys.2017.03.020</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2017.03.020" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="116" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A hybrid deep learning architecture for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C16-1047" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="482" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep recurrent neural network vs. support vector machine for aspect-based sentiment analysis of Arabic hotels&apos; reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Smadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Qawasmeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jararweh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="doi">10.1016/j.jocs.2017.11.006</idno>
		<ptr target="https://doi.org/10.1016/j.jocs.2017.11.006" />
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Science</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint multi-grain topic sentiment: Modeling semantic aspects for online reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<idno type="doi">10.1016/j.ins.2016.01.013</idno>
		<ptr target="https://doi.org/10.1016/j.ins.2016.01.013" />
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">339</biblScope>
			<biblScope unit="page" from="206" to="223" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>?lvarez-L¨®pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Juncal-Mart¨ªnez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fern¨¢ndez-Gavilanes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Costamontenegro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gonz¨¢lez-Casta?o</surname></persName>
		</author>
		<idno>GTI at SemEval-2016</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SVM and CRF for aspect detection and unsupervised aspectbased sentiment analysis</title>
		<ptr target="http://aclweb.org/anthology/S16-1049" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)</meeting>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="306" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enhancing deep learning sentiment analysis with ensemble techniques in social applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Araque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Corcuera-Platas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>S¨¢nchez-Rada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Iglesias</surname></persName>
		</author>
		<idno type="doi">10.1016/j.eswa.2017.02.002</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2017.02.002" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="236" to="246" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>ArXiv: 1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Sen¨¦cal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Gauvain</surname></persName>
		</author>
		<idno type="doi">10.1007/10985687_6</idno>
		<ptr target="https://doi.org/10.1007/10985687_6" />
	</analytic>
	<monogr>
		<title level="j">Studies in Fuzziness and Soft Computing</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="137" to="186" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">XRCE at SemEval-2016 Task 5: Feedbacked ensemble modelling on syntactico-semantic knowledge for aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
		<meeting>the 10th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="282" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sentiment analysis is a big suitcase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thelwall</surname></persName>
		</author>
		<idno type="doi">10.1109/MIS.2017.4531228</idno>
		<ptr target="https://doi.org/10.1109/MIS.2017.4531228" />
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="74" to="80" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">TGB at SemEval-2016 Task 5 : Multi-lingual constraint system for as-pect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>?etin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Y?ld?r?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>?zbey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Eryi ? Git</surname></persName>
		</author>
		<idno type="doi">10.1007/978-3-642-19460-3</idno>
		<ptr target="http://link.springer.com/10.1007/978-3-642-19460-3" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval 2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sentiment analysis of customer reviews using robust hierarchical bidirectional recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="doi">10.1007/978-3-319-33625-1</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-33625-1" />
	</analytic>
	<monogr>
		<title level="m">Advances in Intelligent Systems and Computing : 464</title>
		<editor>R. Silhavy, R. Senkerik, Z. K. Oplatkova, P. Silhavy, &amp; Z. Prokopova</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D17-1047" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clause sentiment identification based on convolutional neural network with context embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="doi">10.1109/FSKD.2016.7603403</idno>
		<ptr target="https://doi.org/10.1109/FSKD.2016.7603403" />
	</analytic>
	<monogr>
		<title level="m">12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery, ICNC-FSKD 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1532" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving sentiment analysis via sentence type classification using BiLSTM-CRF and CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="doi">10.1016/j.eswa.2016.10.065</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2016.10.065" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="221" to="230" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Aspect-level Sentiment Classification with HEAT (HiErarchical ATtention) Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="doi">10.1145/3132847.3133037</idno>
		<ptr target="https://doi.org/10.1145/3132847.3133037" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management -CIKM &apos;17</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management -CIKM &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chernyshevich</surname></persName>
		</author>
		<title level="m">IHS R &amp; D Belarus : Cross-domain extraction of product features using conditional random fields. Semeval , (SemEval)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="309" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nichols</surname></persName>
		</author>
		<idno>ArXiv: 1511.08308</idno>
		<ptr target="http://arxiv.org/abs/1511.08308" />
		<title level="m">Named entity recognition with bidirectional LSTMCNNs</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/D14-1179</idno>
		<idno>ArXiv: 1406.1078</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1179" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>ArXiv: 1412.3555</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Natural language processing (Almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
		<ptr target="https://doi.org/10.1.1.231.4614" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The many aspects of fine-grained sentiment analysis: An overview of the task and its main challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>De Clercq</surname></persName>
		</author>
		<ptr target="https://biblio.ugent.be/publication/8500953/file/8500962" />
	</analytic>
	<monogr>
		<title level="m">HUSO 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="23" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A neural network model for semi-supervised review aspect identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="doi">10.1007/978-3-319-57529-2_52</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-57529-2_52" />
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="10235" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Recurrent neural networks with auxiliary labels for cross-domain opinion target extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Aaai</publisher>
			<biblScope unit="page" from="3436" to="3442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improving implicit semantic role labeling by predicting semantic frame arguments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">N T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Moens</surname></persName>
		</author>
		<idno>ArXiv: 1704.02709</idno>
		<ptr target="http://arxiv.org/abs/1704.02709" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A neural word embeddings approach for multidomain sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dragoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Petrucci</surname></persName>
		</author>
		<idno type="doi">10.1109/TAFFC.2017.2717879</idno>
		<ptr target="https://doi.org/10.1109/TAFFC.2017.2717879" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="457" to="470" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Aspect-specific sentimental word embedding for sentiment analysis of online reviews. International Conference Companion on World Wide Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<idno type="doi">10.1145/2872518.2889403</idno>
		<ptr target="https://doi.org/10.1145/2872518.2889403" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="29" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Successes and challenges of Arabic sentiment analysis research: a literature review. Social Network Analysis and Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Masri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Altrabsheh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mansour</surname></persName>
		</author>
		<idno type="doi">10.1007/s13278-017-0474-x</idno>
		<ptr target="https://doi.org/10.1007/s13278-017-0474-x" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distributed representations, simple recurrent networks, and grammatical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
		<idno type="doi">10.1023/A:1022699029236</idno>
		<ptr target="https://doi.org/10.1023/A:1022699029236" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="195" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">TTS synthesis with bidirectional LSTM based recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</title>
		<meeting>the Annual Conference of the International Speech Communication Association, INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1964" to="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Enhanced sentiment labeling and implicit aspect identification by integration of deep convolution neural network and sequential algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fern¨¢ndez-Gavilanes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>?lvarez-L¨®pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Juncal-Mart¨ªnez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Costamontenegro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Gonz¨¢lez-Casta?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
		<idno type="doi">10.1007/s10586-017-1626-5</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2016.03.031" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="57" to="75" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Unsupervised method for sentiment analysis in online texts</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">W2VLDA: Almost unsupervised system for Aspect Based Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garc¨ªa-Pablos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuadros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigau</surname></persName>
		</author>
		<idno type="doi">10.1016/j.eswa.2017.08.049</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2017.08.049" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="127" to="137" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Like it or not: A survey of Twitter sentiment analysis methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<idno type="doi">10.1145/2938640</idno>
		<ptr target="https://doi.org/10.1145/2938640" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput Surv</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Article</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A primer on neural network models for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="345" to="420" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural network methods for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="doi">10.2200/S00762ED1V01Y201703HLT037</idno>
		<ptr target="https://doi.org/10.2200/S00762ED1V01Y201703HLT037" />
	</analytic>
	<monogr>
		<title level="m">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="doi">10.1038/nmeth.3707</idno>
		<ptr target="https://doi.org/10.1038/nmeth.3707" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">800</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning word vectors for 157 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1802.06893" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the International Conference on Language Resources and Evaluation (LREC 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Supervised sequence labelling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="doi">10.1145/2911451.2926729</idno>
		<idno>ArXiv: 1308.0850</idno>
		<ptr target="https://doi.org/10.1145/2911451.2926729" />
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="126" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cascaded Convolutional neural networks for aspectbased opinion summary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<idno type="doi">10.1007/s11063-017-9605-7</idno>
		<ptr target="https://doi.org/10.1007/s11063-017-9605-7" />
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="581" to="594" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning approach for sentiment analysis of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<idno type="doi">10.1109/ICCAR.2017.7942788</idno>
		<ptr target="https://doi.org/10.1109/ICCAR.2017.7942788" />
	</analytic>
	<monogr>
		<title level="m">2017 3rd International Conference on Control, Automation and Robotics (ICCAR)</title>
		<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="705" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="doi">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="doi">10.1145/1014052.1014073</idno>
		<ptr target="https://doi.org/10.1145/1014052.1014073" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining -KDD &apos;04</title>
		<meeting>the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining -KDD &apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">168</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Opinion mining with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/D14-1080</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1080" />
	</analytic>
	<monogr>
		<title level="m">Emnlp-2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="720" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Aspect-based sentiment analysis using a two-step neural network architecture. Semantic Web Evaluation Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jebbara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
		<idno type="doi">10.3233/978-1-61499-672-9-1123</idno>
		<ptr target="https://doi.org/10.3233/978-1-61499-672-9-1123" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="153" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Aspect-based relational sentiment analysis using a stacked neural network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jebbara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
		<idno type="doi">10.3233/978-1-61499-672-9-1123</idno>
		<idno>ArXiv: 1709.06309</idno>
		<ptr target="https://doi.org/10.3233/978-1-61499-672-9-1123" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Combining resources to improve unsupervised sentiment analysis at aspect-level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Jim¨¦nez-Zafra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Mart¨ªn-Valdivia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mart¨ªnez-C¨¢mara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Ure?a-L¨®pez</surname></persName>
		</author>
		<idno type="doi">10.1177/0165551515593686</idno>
		<ptr target="https://doi.org/10.1177/0165551515593686" />
	</analytic>
	<monogr>
		<title level="j">Journal of Information Science</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="229" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nicolov</surname></persName>
		</author>
		<ptr target="http://www.cs.indiana.edu/?jaskessl/icwsm10.pdf" />
		<title level="m">The ICWSM 2010 JDPA sentiment corpus for the automotive domain. 4th Int&apos;l AAAI Conference on Weblogs and Social Media Data Workshop Challenge (ICWSM-DWC 2010)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="doi">10.1145/1599272.1599278</idno>
		<ptr target="https://doi.org/10.1145/1599272.1599278" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="23" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">NRC-Canada-2014: Detecting Aspects and Sentiment in Customer Reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohammad</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/S14-2076</idno>
		<ptr target="https://doi.org/10.3115/v1/S14-2076" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">IIT-TUDA at SemEval-2016 Task 5: Beyond Sentiment Lexicon: combining domain dependency and distributional semantics features for aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kohail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
		<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>SemEval</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1129" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Aspect specific sentiment analysis using hierarchical deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS WS on Deep Neural Networks and Representation Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/N16-1030</idno>
		<idno>ArXiv: 1603.01360</idno>
		<ptr target="https://doi.org/10.18653/v1/N16-1030" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="doi">10.1038/nature14539</idno>
		<ptr target="https://doi.org/10.1038/nature14539" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="440" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>4 4.</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Aspect term extraction with history attention and selective transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<idno>ArXiv: 1805.00760</idno>
		<ptr target="http://arxiv.org/abs/1805.00760" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D17/D17-1310.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Sentiment analysis: Mining opinions, sentiments, and emotions. Sentiment Analysis: Mining Opinions. Sentiments, and Emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="doi">10.1017/CBO9781139084789</idno>
		<ptr target="https://doi.org/10.1017/CBO9781139084789" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Fine-grained opinion mining with recurrent neural networks and word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1168" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1433" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multilingual sentiment analysis: from formal to informal and scarce resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cornforth</surname></persName>
		</author>
		<idno type="doi">10.1007/s10462-016-9508-4</idno>
		<ptr target="https://doi.org/10.1007/s10462-016-9508-4" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="499" to="527" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Interactive attention networks for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4068" to="4074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Featurebased compositing memory networks for aspect-based sentiment classification in social internet of things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Sangaiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liaqat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin</surname></persName>
		</author>
		<idno type="doi">10.1016/j.future.2017.11.036</idno>
		<ptr target="https://doi.org/10.1016/j.future.2017.11.036" />
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">End-to-end sequence labeling via Bi-directional LSTMCNNs-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-1101</idno>
		<idno>ArXiv: 1603.01354</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1101" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<ptr target="http://sentic.net/sentic-lstm.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Aspect-based sentiment analysis of vietnamese texts with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Le</surname></persName>
		</author>
		<idno type="doi">10.1007/978-3-319-75417-8_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-75417-8_14" />
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10751</biblScope>
			<biblScope unit="page" from="149" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">WWW&apos;18 Open Challenge: Financial opinion mining and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Handschuh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zarrouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balahur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;18 Companion: The 2018 Web Conference Companion</title>
		<meeting><address><addrLine>New York, NY, USA; Lyon, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-04-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Aspect term extraction for sentiment analysis in large movie reviews using Gini Index feature selection method and SVM classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Manek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Venugopal</surname></persName>
		</author>
		<idno type="doi">10.1007/s11280-015-0381-x</idno>
		<ptr target="https://doi.org/10.1007/s11280-015-0381-x" />
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="154" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<idno>ArXiv: 1701.02593</idno>
		<ptr target="http://arxiv.org/abs/1701.02593" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="doi">10.1162/153244303322533223</idno>
		<ptr target="https://doi.org/10.1162/153244303322533223" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR 2013)</title>
		<meeting>the International Conference on Learning Representations (ICLR 2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">PhraseRNN: Phrase recursive neural network for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shirai</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1298" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="2509" to="2514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis. Foundations and Trends? in InformatioPang</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">; B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<idno type="doi">1561/1500000001.doi:10.1561/1500000001n</idno>
		<ptr target="https://doi.org/10" />
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends? in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="231" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Retrieval</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Sentiment analysis of movie reviews: finding most important movie aspects using driving factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Parkhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biswas</surname></persName>
		</author>
		<idno>10 07/s0 050 0-015-1779-1</idno>
		<ptr target="https://doi.org/10" />
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3373" to="3379" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A review of sentiment analysis research in Chinese Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<idno type="doi">10.1007/s12559-017-9470-8</idno>
		<ptr target="https://doi.org/10.1007/s12559-017-9470-8" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Learning multi-grained aspect target sequence for Chinese sentiment analysis. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<idno type="doi">10.1016/j.knosys.2018.02.034</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2018.02.034" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="55" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/D14-1162</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Movie Prism: A novel system for aspect level sentiment profiling of movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piryani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<idno type="doi">10.3233/JIFS-169272</idno>
		<ptr target="https://doi.org/10.3233/JIFS-169272" />
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent and Fuzzy Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>S?gaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-2067</idno>
		<idno>ArXiv: 1604.05529</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-2067" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">SemEval-2016 Task 5: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alsmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Eryi \ Ugit</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/S16-1055</idno>
		<ptr target="https://doi.org/10.18653/v1/S16-1055" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
		<meeting>the 10th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">SemEval-2015 Task 12: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<idno type="doi">10.1007/978-3-642-40837-3</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-40837-3" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">SemEval-2014 Task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<idno type="doi">10.15520/ajcsit.v4i8.9</idno>
		<ptr target="https://doi.org/10.15520/ajcsit.v4i8.9" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Aspect extraction for opinion miningwith a deep convolutional neural network. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<idno type="doi">10.1016/j.knosys.2016.06.009</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2016.06.009" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Sentiment data flow analysis by means of dynamic linguistic patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bisio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<idno type="doi">10.1109/MCI.2015.2471215</idno>
		<ptr target="https://doi.org/10.1109/MCI.2015.2471215" />
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="26" to="36" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Sentic LDA: Improving on LDA with semantic similarity for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bisio</surname></persName>
		</author>
		<idno type="doi">10.1109/IJCNN.2016.7727784</idno>
		<ptr target="https://doi.org/10.1109/IJCNN.2016.7727784" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks (pp. 4 465-4 473). 2016-Octob</title>
		<meeting>the International Joint Conference on Neural Networks (pp. 4 465-4 473). 2016-Octob</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Aspect extraction in sentiment analysis : comparative analysis and survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-N</forename><surname>Cheah</surname></persName>
		</author>
		<idno type="doi">10.1007/s10462-016-9472-z</idno>
		<ptr target="https://doi.org/10.1007/s10462-016-9472-z" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="459" to="483" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Deep learning for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<idno type="doi">10.1111/lnc3.12228</idno>
		<ptr target="https://doi.org/10.1111/lnc3.12228" />
	</analytic>
	<monogr>
		<title level="j">Language and Linguistics Compass</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="701" to="719" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">INSIGHT-1 at SemEval-2016 Task 5: Deep learning for multilingual aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghaffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Breslin</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1609.02748" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016) Retrieved from</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016) Retrieved from</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">SentiHood: Targeted aspect based sentiment analysis dataset for urban neighbourhoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saeidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno>ArXiv: 1610.03771</idno>
		<ptr target="http://arxiv.org/abs/1610.03771" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Sentiue: target and aspect based sentiment analysis in SemEval-2015 Task 12</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation , (SemEval)</title>
		<meeting>the 9th International Workshop on Semantic Evaluation , (SemEval)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="767" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="doi">10.1016/j.neunet.2014.09.003</idno>
		<ptr target="https://doi.org/10.1016/j.neunet.2014.09.003" />
		<title level="m">Deep learning in neural networks: An overview. Neural Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Survey on aspect-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schouten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frasincar</surname></persName>
		</author>
		<idno type="doi">10.1109/TKDE.2015.2485209</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2015.2485209" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="813" to="830" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Deep active learning for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kronrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="doi">10.18653/v3</idno>
		<idno>1-15. ArXiv: 1707.05928</idno>
		<ptr target="https://doi.org/10.18653/v3" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="doi">10.1371/journal.pone.0073791</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0073791" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ¡­</title>
		<meeting>the ¡­</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Fast and accurate sequence labeling with iterated dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1702.02098" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<ptr target="https://doi.org/v5" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">An introduction to conditional random fields. Foundations and Trends? in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<idno>1561/ 220 0 0 0 0 013</idno>
		<ptr target="https://doi.org/10" />
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="267" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="doi">10.1515/popets-2015-0023</idno>
		<ptr target="https://doi.org/10.1515/popets-2015-0023" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">UFAL at SemEval-2016 Task 5: Recurrent neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamchyna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Veselovsk¨¢</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/S/S16/S16-1059.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="367" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Effective LSTMs for target-dependent sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1512.01100" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv: 1605.08900</idno>
		<ptr target="http://arxiv.org/abs/1605.08900" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Sentiment embeddings with applications to sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="doi">10.1109/TKDE.2015.2489653</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2015.2489653" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="496" to="509" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Learning sentiment-specific word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<idno type="doi">10.3115/1220575.1220648</idno>
		<ptr target="https://doi.org/10.3115/1220575.1220648" />
		<imprint>
			<date type="published" when="2014" />
			<publisher>Acl</publisher>
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui</surname></persName>
		</author>
		<idno>ArXiv: 1712.05403</idno>
		<ptr target="http://arxiv.org/abs/1712.05403" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Dyadic memory networks for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui</surname></persName>
		</author>
		<idno type="doi">10.1145/3132847.3132936</idno>
		<ptr target="https://doi.org/10.1145/3132847.3132936" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management -CIKM &apos;17</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management -CIKM &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">NLANGP: supervised machine learning system for aspect category classification and opinion target extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">; Z</forename><surname>Thanaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="496" to="501" />
		</imprint>
	</monogr>
	<note>Python Natural Language Processing Packt Publishing Ltd . Toh,</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">NLANGP at SemEval-2016 Task 5: Improving aspect based sentiment analysis using neural network features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S16-1045" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2016</title>
		<meeting>SemEval-2016</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="282" to="288" />
		</imprint>
	</monogr>
	<note>Subtask 1)</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">DLIREC: Aspect term extraction and term polarity classification system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="235" to="240" />
		</imprint>
	</monogr>
	<note>SemEval</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Sentence and expression level annotation of opinions in user-generated discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toprak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P10-1059" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Coverage-based neural machine translation. Arxiv , 1-19</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="doi">10.1145/2856767.2856776</idno>
		<ptr target="https://doi.org/10.1145/2856767.2856776" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Combination of deep recurrent neural networks and conditional random fields for extracting adverse drug reactions from user reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tutubalina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nikolenko</surname></persName>
		</author>
		<idno type="doi">10.1155/2017/9451342</idno>
		<ptr target="https://doi.org/10.1155/2017/9451342" />
	</analytic>
	<monogr>
		<title level="j">Journal of Healthcare Engineering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">EliXa: A modular and flexible ABSA platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Saralegi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agerri</surname></persName>
		</author>
		<idno>ArXiv: 1702.01944</idno>
		<ptr target="http://arxiv.org/abs/1702.01944" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Target-dependent twitter sentiment classification with rich automatic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">DCU: Aspect-based polarity classification for SemEval Task 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bogdanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tounsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="223" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">TDParse : Multi-target-specific sentiment recognition on Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Procter</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/E/E17/E17-1046.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4" to="83" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Latent aspect rating analysis without aspect keyword supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<idno type="doi">10.1145/2020408.2020505</idno>
		<ptr target="https://doi.org/10.1145/2020408.2020505" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining -KDD &apos;11</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining -KDD &apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">618</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Coupled multi-layer attentions for coextraction of aspect and opinion terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1702.01776" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>Aaai</publisher>
			<biblScope unit="page" from="3316" to="3322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Recursive neural conditional random fields for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Extended dependencybased word embeddings for aspect extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="doi">10.1007/978-3-319-46681-1_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46681-1_13" />
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9950</biblScope>
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Aspectbased extraction and analysis of affective knowledge from social media streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weichselbraun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gindl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vakulenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Scharl</surname></persName>
		</author>
		<idno type="doi">10.1109/MIS.2017.57</idno>
		<ptr target="https://doi.org/10.1109/MIS.2017.57" />
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="80" to="88" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Aspect-based opinion summarization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<idno type="doi">10.1109/IJCNN.2016.7727602</idno>
		<ptr target="https://doi.org/10.1109/IJCNN.2016.7727602" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="3157" to="3163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network based approach for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="doi">10.14257/astl.2017.143.41</idno>
		<ptr target="https://doi.org/10.14257/astl.2017.143.41" />
	</analytic>
	<monogr>
		<title level="j">Advanced Science and Technology Letters</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="199" to="204" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Ast)</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Aspect based sentiment analysis for online reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computer Science and Ubiquitous Computing</title>
		<editor>J. Park, V. Loia, G. Yi, &amp; Y. Sung</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">474</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">MTNA: A neural multi-task model for aspect category classification and aspect term extraction on restaurant reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="151" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Multi-task cross-lingual sequence tagging from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno>ArXiv: 1603.06270</idno>
		<ptr target="http://arxiv.org/abs/1603.06270" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Dependency-tree based convolutional neural networks for aspect term extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chao</surname></persName>
		</author>
		<idno type="doi">10.1007/978-3-319-57529-2_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-57529-2_28" />
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="350" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Comparative Study of CNN and RNN for Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sch¨¹tze</surname></persName>
		</author>
		<idno>ArXiv: 1708.02709</idno>
		<ptr target="http://arxiv.org/abs/1702.01923" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1708.02709" />
		<title level="m">Recent Trends in Deep Learning Based Natural Language Processing. Arxiv</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Local contexts are effective for neural aspect extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<idno type="doi">10.1007/978-981-10-6805-8_20</idno>
		<ptr target="https://doi.org/10.1007/978-981-10-6805-8_20" />
	</analytic>
	<monogr>
		<title level="m">Communications in Computer and Information Science</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">774</biblScope>
			<biblScope unit="page" from="244" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Hybrid sentiment classification on twitter aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zainuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selamat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ibrahim</surname></persName>
		</author>
		<idno type="doi">10.1007/s10489-017-1098-6</idno>
		<ptr target="https://doi.org/10.1007/s10489-017-1098-6" />
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1218" to="1232" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Learning to Execute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="doi">10.1016/S0893-6080(96</idno>
		<idno>ArXiv: 1410.4615</idno>
		<ptr target="https://doi.org/10.1016/S0893-6080(96" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="73" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Deep learning for sentiment analysis: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="doi">10.1002/widm.1253</idno>
		<ptr target="https://doi.org/10.1002/widm.1253" />
	</analytic>
	<monogr>
		<title level="m">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Gated Neural Networks for Targeted Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vo</surname></persName>
		</author>
		<ptr target="http://zhangmeishan.github.io/targeted-sentiment.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Conference on Artificial Intelligence (AAAI 2016)</title>
		<meeting>the 30th Conference on Artificial Intelligence (AAAI 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3087" to="3093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Weaklysupervised Deep Embedding for Product Review Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<idno type="doi">10.1109/TKDE.2017.2756658</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2017.2756658" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
