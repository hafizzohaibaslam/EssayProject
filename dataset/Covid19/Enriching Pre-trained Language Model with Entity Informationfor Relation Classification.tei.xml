<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2020-05-07T04:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enriching Pre-trained Language Model with Entity Information for Relation Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date type="published" when="2019-03-03">2019-03-03</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanchan</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>He</surname></persName>
						</author>
						<title level="a" type="main">Enriching Pre-trained Language Model with Entity Information for Relation Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
						<meeting>the 28th ACM International Conference on Information and Knowledge Management						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<date type="published" when="2019-03-03" />
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3357384.3358119</idno>
					<note>Session: Short -Classification CIKM &apos;19, November 3-7, 2019, Beijing, China ACM Reference Format: Shanchan Wu and Yifan He. 2019. Enriching Pre-trained Language Model with Entity Information for Relation Classification . In The 28th ACM In-ternational Conference on Information and Knowledge Management (CIKM &apos;19), November 3-7, 2019, Beijing, China. ACM, New York, NY, USA, 4 pages.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Information systems ¡ú Language models</term>
					<term>? Computing methodologies ¡ú Information extraction</term>
					<term>Ar- tificial intelligence KEYWORDS relation classification</term>
					<term>relation extraction</term>
					<term>Language Model</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Relation classification is an important NLP task to extract relations between entities. The state-of-the-art methods for relation classification are primarily based on Convolutional or Recurrent Neural Networks. Recently, the pre-trained BERT model achieves very successful results in many NLP classification / sequence labeling tasks. Relation classification differs from those tasks in that it relies on information of both the sentence and the two target entities. In this paper, we propose a model that both leverages the pre-trained BERT language model and incorporates information from the target entities to tackle the relation classification task. We locate the target entities and transfer the information through the pre-trained architecture and incorporate the corresponding encoding of the two entities. We achieve significant improvement over the state-of-the-art method on the SemEval-2010 task 8 relational dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The task of relation classification is to predict semantic relations between pairs of nominals. Given a sequence of text (usually a sentence) s and a pair of nominals e 1 and e 2 , the objective is to identify the relation between e 1 and e 2 <ref type="bibr" target="#b3">[4]</ref>. It is an important NLP task which is normally used as an intermediate step in variety of NLP applications. The following example shows the ComponentWhole relation between the nominals "kitchen" and "house": "The <ref type="bibr">[kitchen]</ref> e1 is the last renovated part of the <ref type="bibr">[house]</ref> e1 . "</p><p>Recently, deep neural networks have applied to relation classification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22]</ref>. These methods usually use some features derived from lexical resources such as Word-Net or NLP tools such as dependency parsers and named entity recognizers (NER).</p><p>Language model pre-training has been shown to be effective for improving many natural language processing tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>. The pretrained model BERT proposed by Devlin et al. has especially significant impact. It has been applied to multiple NLP tasks and obtains new state-of-the-art results on eleven tasks. The tasks that BERT has been applied to are typically modeled as classification problems and sequence labeling problems. It has also been applied to the SQuAD question answering <ref type="bibr" target="#b11">[12]</ref> problem, in which the objective is to find the starting point and ending point of an answer span.</p><p>As far as we know, the pretrained BERT model <ref type="bibr" target="#b1">[2]</ref> has not been applied to relation classification, which relies not only on the information of the whole sentence but also on the information of the specific target entities. In this paper, we apply the pretrained BERT model for relation classification. We insert special tokens before and after the target entities before feeding the text to BERT for fine-tuning, in order to identify the locations of the two target entities and transfer the information into the BERT model. We then locate the positions of the two target entities in the output embedding from BERT model. We use their embeddings as well as the sentence encoding (embedding of the special first token in the setting of BERT) as the input to a multi-layer neural network for classification. By this way, it captures both the semantics of the sentence and the two target entities to better fit the relation classification task.</p><p>Our contributions are as follows: (1) We put forward an innovative approach to incorporate entity-level information into the pretrained language model for relation classification. (2) We achieve the new state-of-the-art for the relation classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. There has been some work with deep learning methods for relation classification, such as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22]</ref> MVRNN model <ref type="bibr" target="#b16">[17]</ref> applies a recursive neural network (RNN) to relation classification. They assign a matrix-vector representation to every node in a parse tree and compute the representation for the complete sentence from bottom up according to the syntactic structure of the parse tree. <ref type="bibr" target="#b22">[22]</ref> propose a CNN model by incorporating both word embeddings and position features as input. Then they concatenate lexical features and the output from CNN into a single vector and feed them into a softmax layer for prediction. <ref type="bibr" target="#b21">[21]</ref> propose a Factor-based Compositional Embedding Model (FCM) by constructing sentence-level and substructure embeddings from word embeddings, through dependency trees and named entities.</p><p>[15] tackle the relation classification task by ranking with a convolutional neural network named CR-CNN. Their loss function is based on pairwise ranking. In our work, we take advantage of a pre-trained language model for the relation classification task, without relying on CNN or RNN architecutures. <ref type="bibr" target="#b15">[16]</ref> utilize a CNN encoder in conjunction with a sentence representation that weights the words by attention between the target entities and the words in the sentence to perform relation classification. <ref type="bibr" target="#b19">[19]</ref> propose a convolutional neural network architecture with two levels of attention in order to catch the patterns in heterogeneous contexts to classify relations. <ref type="bibr" target="#b6">[7]</ref> develop an end-to-end recurrent neural model which incorporates an entity-aware attention mechanism with a latent entity typing for relation classification.</p><p>There are some related work on the relation extraction based on distant supervision, for example, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">20]</ref>. The difference between relation classification on regular data and on distantly supervised data is that the latter may contain a large number of noisy labels. In this paper, we focus on the regular relation classification problem, without noisy labels.</p><p>For example, after insertion of the special separate tokens, for a sentence with target entities "kitchen" and "house" will become to:</p><p>"[CLS] The $ kitchen $ is the last renovated part of the # house # . " Given a sentence s with entity e 1 and e 2 , suppose its final hidden state output from BERT module is H . Suppose vectors H i to H j are the final hidden state vectors from BERT for entity e 1 , and H k to H m are the final hidden state vectors from BERT for entity e 2 . We apply the average operation to get a vector representation for each of the two target entities. Then after an activation operation (i.e. tanh), we add a fully connected layer to each of the two vectors, and the output for e 1 and e 2 are H ¡ä 1 and H ¡ä 2 respectively. This process can be mathematically formalized as Equation <ref type="formula" target="#formula_0">(1)</ref>.</p><formula xml:id="formula_0">j H ¡ä 1 = W 1 tanh 1 j ? i + 1 H t + b 1 t =i m<label>(1)</label></formula><formula xml:id="formula_1">H ¡ä 2 = W 2 tanh 1 m ? k + 1 H t + b 2 t =k</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY 3.1 Pre-trained Model BERT</head><p>We make W 1 and W 2 , b 1 and b 2 share the same parameters. In other words, we set W 1 = W 2 , b 1 = b 2 . For the final hidden state vector of the first token (i.e. ' <ref type="bibr">[CLS]</ref>'), we also add an activation operation and a fully connected layer, which is formally expressed as:</p><formula xml:id="formula_2">H ¡ä 0 = W 0 (tanh(H 0 )) + b 0<label>(2)</label></formula><p>The pre-trained BERT model <ref type="bibr" target="#b1">[2]</ref> is a multi-layer bidirectional Transformer encoder <ref type="bibr" target="#b18">[18]</ref>. The design of input representation of BERT is to be able to represent both a single text sentence and a pair of text sentences in one token sequence. The input representation of each token is constructed by the summation of the corresponding token, segment and position embeddings.</p><p>' <ref type="bibr">[CLS]</ref>' is appended to the beginning of each sequence as the first token of the sequence. The final hidden state from the Transformer output corresponding to the first token is used as the sentence representation for classification tasks. In case there are two sentences in a task, ' <ref type="bibr">[SEP]</ref>' is used to separate the two sentences.</p><p>BERT pre-trains the model parameters by using a pre-training objective: the "masked language model" (MLM), which randomly masks some of the tokens from the input, and set the optimization objective to predict the original vocabulary id of the masked word according to its context. Unlike left-to-right language model pretraining, the MLM objective can help a state output to utilize both the left and the right context, which allows a pre-training system to apply a deep bidirectional Transformer. Besides the masked language model, BERT also trains a "next sentence prediction" task that jointly pre-trains text-pair representations.</p><formula xml:id="formula_3">Matrices W 0 , W 1 , W 2 have the same dimensions, i.e. W 0 ¡Ê R d¡Ád , W 1 ¡Ê R d¡Ád , W 2 ¡Ê R d¡Ád ,</formula><p>where d is the hidden state size from BERT.</p><p>We concatenate H ¡ä 0 , H ¡ä 1 , H ¡ä 2 and then add a fully connected layer and a softmax layer, which can be expressed as following:</p><formula xml:id="formula_4">h ¡ä¡ä = W 3 concat H ¡ä ¡ä 0 , H 1 , H ¡ä 2 + b 3 (3) p = so f tmax(h ¡ä¡ä )</formula><p>where W 3 ¡Ê R L¡Á3d (L is the number of relation types), and p is the probability output. In Equations (1),(2),(3), b 0 , b 1 , b 2 , b 3 are bias vectors. We use cross entropy as the loss function. We apply dropout before each fully connected layer during training. We call our approach as R-BERT. <ref type="figure">Figure 1</ref> shows the architecture of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Dataset and Evaluation Metric</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Architecture</head><p>For a sentence s with two target entities e 1 and e 2 , to make the BERT module capture the location information of the two entities, at both the beginning and end of the first entity, we insert a special token '$', and at both the beginning and end of the second entity, we insert a special token '#'. We also add ' <ref type="bibr">[CLS]</ref>' to the beginning of each sentence.</p><p>We use the SemEval-2010 Task 8 dataset in our experiments. The dataset contains nine semantic relation types and one artificial relation type Other, which means that the relation does not belong to any of the nine relation types. The nine relation types are CauseEffect, Component-Whole, Content-Container, Entity-Destination, Entity-Origin, Instrument-Agency, Member-Collection, Message-Topic and Product-Producer. The dataset contains 10,717 sentences, with each containing two nominals e1 and e2, and the corresponding relation type in the sentence. The relation is directional, which means that Component-Whole(e1, e2) is different from ComponentWhole(e2, e1). The dataset has already been partitioned into 8,000 training instances and 2,717 test instances. We evaluate our solution by using the <ref type="bibr">SemEval-2010</ref>  Figure 1: The model architecture.</p><p>the macro-averaged F1-scores for the nine actual relations (excluding Other) and considers directionality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parameter Settings</head><p>Method F1 We add dropout before each add-on layer. For the pre-trained BERT model, we use the uncased basic model. For the parameters of the pre-trained BERT model, please refer to <ref type="bibr" target="#b1">[2]</ref> for details.</p><p>Att-Pooling-CNN <ref type="bibr" target="#b19">[19]</ref> 88.0</p><p>Entity Attention Bi-LSTM <ref type="bibr" target="#b6">[7]</ref> 85.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with other Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R-BERT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>89.25</head><p>We compare our method, R-BERT, against results by multiple methods recently published for the SemEval-2010 Task 8 dataset, including SVM, RNN, MVRNN, CNN+Softmax, FCM, CR-CNN, Attention-CNN, Entity Attention Bi-LSTM. The SVM method by <ref type="bibr" target="#b12">[13]</ref> uses a rich feature set in a traditional way, which was the best result during the SemEval-2010 task 8 competition. Details of all other methods are briefly reviewed in Section 2. <ref type="table" target="#tab_2">Table 2</ref> reports the results. We can see that R-BERT significantly beats all the baseline methods. The MACRO F1 value of R-BERT is 89.25, which is much better than the previous best solution on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>4.4.1 Effect of Model Components. We have demonstrated the strong empirical results based on the proposed approach. We further want to understand the specific contributions by the components besides the pre-trained BERT component. For this purpose, we create three more configurations.</p><p>The first configuration is to discard the special separate tokens (i.e. '$' and '#') around the two entities in the sentence and discard the hidden vector output of the two entities from concatenating with the hidden vector output of the sentence. In other words, we add ' <ref type="bibr">[CLS]</ref>' at the beginning of the sentence and feed the sentence with the two entities into the BERT module, and use the first output vector for classification. We label this method as BERT-NO-SEP-NO-ENT.</p><p>The second configuration is to discard the special separate tokens (i.e. '$' and '#') around the two entities in the sentence, but keep the hidden vector output of the two entities in concatenation for classification. We label this method as BERT-NO-SEP.</p><p>The third configuration is to discard the hidden vector output of the two entities from concatenation for classification, but keep the special separate tokens. We label this method as BERT-NO-ENT. <ref type="table" target="#tab_4">Table 3</ref> reports the results of the ablation study with the above three configurations. We observe that the three methods all perform worse than R-BERT. Of the methods, BERT-NO-SEP-NO-ENT performs worst, with its F1 8.16 absolute points worse than R-BERT. This ablation study demonstrates that both the special separate tokens and the hidden entity vectors make important contributions to our approach.</p><p>In relation classification, the relation label is dependent on both the semantics of the sentence and the two target entities. BERT without special separate tokens cannot locate the target entities and lose this key information. The reason why the special separate tokens help to improve the accuracy is that they identify the locations of the two target entities and transfer the information into the BERT model, which make the BERT output contain the location information of the two entities. On the other hand, incorporating the output of the target entity vectors further enriches the information and helps to make more accurate prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method F1</head><p>R-BERT-NO-SEP-NO-ENT 81.09 R-BERT-NO-SEP 87.98 R-BERT-NO-ENT 87.99 R-BERT 89. <ref type="bibr">25</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we develop an approach for relation classification by enriching the pre-trained BERT model with entity information. We add special separate tokens to each target entity pair and utilize the sentence vector as well as target entity representations for classification. We conduct experiments on the SemEval-2010 benchmark dataset and our results significantly outperform the state-of-the-art methods. One possible future work is to extend the model to apply to distant supervision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Task 8 official scorer script. It computes</head><label>8</label><figDesc></figDesc><table>Fully-connected 
+ activation 
Fully-connected 
+ activation 

H 0 
H i 
H j 
H k 
H m 
Average 
Average 
Fully-connected 
+ activation 

Softmax 

BERT 

Fully-connected 

[CLS] T 1 

$ 
T i 
T j 
T j+2 
$ 
# 
T k 
T m 
T m+2 
# 
T n 

Entity 1 
Entity 2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Comparison with results in the literature.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table shows the major parameters used in our experiments.</head><label>shows</label><figDesc></figDesc><table>SVM [13] 
82.2 

RNN [17] 
77.6 

Table 1: Parameter settings. 

MVRNN [17] 
82.4 

Batch size 
16 
Max sentence length 128 
Adam learning rate 2e-5 
Number of epochs 
5 
Dropout rate 
0.1 

CNN+Softmax [22] 
82.7 

FCM [21] 
83.0 

CR-CNN [15] 
84.1 

Attention CNN [16] 
85.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison of the BERT based methods with dif-
ferent components. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-supervised Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classifying Relations by Ranking with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C¨ªcero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SemEval-2010 Task 8: Multi-way Classification of Semantic Relations Between Pairs of Nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid</forename><forename type="middle">?</forename><surname>S¨¦aghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Pad¨®</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation (SemEval &apos;10)</title>
		<meeting>the 5th International Workshop on Semantic Evaluation (SemEval &apos;10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 49th Annual Meeting of the Association for Computational Linguistics, ACL 2011</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017-06" />
			<biblScope unit="page" from="3060" to="3066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joohong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Suk</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural Relation Extraction with Selective Attention over Instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distant Supervision for Relation Extraction Without Labeled Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL &apos;09</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL &apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00108</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improving language understanding with unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Utd: Classifying semantic relations by combining lexical and semantic resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="256" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Universal Language Model Finetuning for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Classifying Relations by Ranking with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero Nogueira Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention-based Convolutional Neural Network for Semantic Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2526" to="2536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic Compositionality Through Recursive Matrix-vector Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Stroudsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Usa</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation Classification via Multi-Level Attention CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving Distantly Supervised Relation Extraction with Neural Noise Converter and Conditional Optimal Selector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanchan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Third AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Factor-based compositional embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Learning Semantics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relation Classification via Convolutional Deep Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
