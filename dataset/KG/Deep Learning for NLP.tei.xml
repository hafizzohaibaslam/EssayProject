<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-10-04T09:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning for NLP (without Magic) Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Manning</surname></persName>
						</author>
						<title level="a" type="main">Deep Learning for NLP (without Magic) Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>ACL 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#4 Using a deep architecture</head><p>? Deep architectures learn good intermediate representa:ons that can be shared across tasks task 1 output y1 task 3 output y3 task 2 output y2</p><p>Task A Task B Task C</p><p>? Insufficient depth of representa:on can be exponen:ally inefficient  1+ e</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?¦Ë?x</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>From Maxent Classifiers to Neural Networks</head><p>? Output of one class:</p><formula xml:id="formula_0">P(c 1 | x, ¦Ë) = 1 1+ e ?¦Ë?x</formula><p>? We o\en have an "always on" feature for a class, which gives a class prior. We can separate it out as a bias term:</p><formula xml:id="formula_1">P(c 1 | x, ¦Ë) = 1 1+ e ?[¦Ë?x+b]</formula><p>? Or we can have x 1 be an always-?-on input ? We separate things into the vector dot product and applying a non-?-linearity. Let f(z) = 1/(1 + exp(-?-z)), the logis:c func:on.</p><p>? Then:</p><formula xml:id="formula_2">P(c 1 | x, ¦Ë) = f (¦Ë ? x + b)</formula><p>This is exactly what a neuron computes </p><formula xml:id="formula_3">h w,b (x) = f (w ? x + b) f (z) = 1 1+ e ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matrix notation for a layer</head><p>We have</p><formula xml:id="formula_4">a 1 = f (W 11 x 1 + W 12 x 2 + W 13 x 3 + b 1 ) a 1 a 2 = f (W 21 x 1 + W 22 x 2 + W 23 x 3 + b 2 )</formula><p>In matrix nota:on etc.</p><formula xml:id="formula_5">a 2 a 3 z = Wx + b a = f (z)</formula><p>where f is applied element-?-wise:</p><formula xml:id="formula_6">f ([z 1 , z 2 , z 3 ]) = [ f (z 1 ), f (z 2 ), f (z 3 )] 36</formula><p>How do we train the weights w?</p><p>? For a single layer neural net, we can train the model just like a logis:c regression model</p><p>? We can do stochas:c gradient descent ? We can use fancier methods, as we commonly do for maxent models like conjugate gradient or L-?-BFGS</p><p>? For a mul:layer net it is poten:ally more complex because the internal ("hidden") logis:c units make the func:on non-?-convex ¡­ just as for hidden CRFs <ref type="bibr">[QuaBoni et al. 04, Gunawardana et al. 05]</ref> ? But we use the same ideas ? This leads into "backpropaga:on", which we cover later <ref type="bibr">37</ref> Non-linearities: Why they're needed</p><p>? For logis:c regression, they're mo:vated by mapping to probabili:es: [0,1]</p><p>? Here, they're mo:vated by being able to do func:on approxima:on</p><p>? Without non-?-lineari:es, neural networks can't do anything more than a linear transform: extra layers could just be compiled down into a single linear transform</p><p>? The probabilis:c interpreta:on for hidden units is usually unnecessary except in the Boltzmann machine models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>38</head><p>Non-linearities: What's used logis:c ("sigmoid") tanh tanh is just a rescaled and shi\ed sigmoid (2 ¡Á as steep,</p><formula xml:id="formula_7">[?1,1]):</formula><p>tanh is what is most used and o\en performs best for deep nets <ref type="bibr">[Glorot and Bengio AISTATS 2010]</ref> tanh(z) = 2logistic(2z) ?1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-linearities: There are various other choices</head><p>hard tanh so\ sign rec:fier</p><formula xml:id="formula_8">rect(z) = max(z, 0) softsign(z) = a 1+ a</formula><p>? hard tanh is a mathema:cally awkward but computa:onally cheap tanh ? <ref type="bibr">[Glorot and Bengio AISTATS 2010]</ref> discuss uses of so\sign and rec:fier</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary: Knowing the meaning of words!</head><p>You should now understand the basics and how they relate to other models you use</p><p>? Neuron = logis:c regression or similar func:on ? Input layer = input training/test vector ? Bias unit = intercept term ? Ac:va:on func:on is a sigmoid (or similar nonlinearity)</p><p>? Ac:va:on = response ? Backpropaga:on = running stochas:c gradient descent across a mul:layer network</p><p>? Weight decay = regulariza:on / Bayesian prior Effective deep learning became possible through unsupervised pre-training <ref type="bibr">[Erhan et al., JMLR 2010]</ref> How does conventional IR try to solve this problem?</p><p>Query expansion (synonyms)</p><p>EXPAND <ref type="formula">(</ref>  In probabilis:c approaches, all the numbers are non-?-nega:ve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>53</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distributed Representations</head><p>These are distributed representaHons: each word has a weigh:ng in each dimension or cluster</p><p>In contrast to the the "atomic" or "localist" representa:ons employed in most of NLP, a distributed representa:on is one in which "each en:ty is represented by a paBern of ac:vity distributed over many compu:ng elements, and each compu:ng element is involved in represen:ng many different en: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advantages of the neural word embedding approach</head><p>Compared to a method like LSA:</p><p>? A neural embedding can learn higher-?-level features/abstrac:ons (beyond the word)</p><p>? Learning such an embedding forces a representa:on on the words themselves that is beBer and more meaningful</p><p>? Because everything is trained together ? By adding supervision from either one task or mul:ple tasks simultaneously, we can improve the representa:on of the words for handling language analysis tasks ? Idea: A word and its context is a posi:ve training sample, a random word in that same context is a nega:ve training sample.</p><p>? score(cat chills on a mat) &gt; score(cat chills Jeju a mat)</p><p>? How to compute the score?</p><p>? With a neural network ? Each word is associated with an n-?-dimensional vector</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>61</head><p>Word embedding matrix </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word vectors as input to a neural network</head><p>? score(cat chills on a mat)</p><p>? To describe a phrase, retrieve (via index) the corresponding vectors from L cat chills on a mat</p><p>? Then concatenate them to (5n) vector:</p><formula xml:id="formula_9">? x =[ ]</formula><p>? How do we then compute score(x)? <ref type="bibr">63</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Single Layer of a Neural Network</head><p>? A single layer is a combina:on of a linear layer and a nonlinearity:</p><p>? The neural ac:va:ons can then be used to compute some func:on.</p><p>? For instance, the score we care about: ? s = score(cat chills on a mat)</p><p>? s c = score(cat chills Jeju a mat)</p><p>? Idea for training objec:ve: make score of true window larger and corrupt window's score lower (un:l they're good enough): minimize</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>66</head><p>Training with Backpropagation Training with Backpropagation</p><p>? We just almost learned the backpropaga:on rule by carefully taking deriva:ves and using the chain rule!</p><p>? The only remaining trick is to re-?-use deriva:ves that we computed once, for lower layers.</p><p>? Let's apply that idea for the last deriva:ves of this model, the word vectors in x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>73</head><p>Training with Backpropagation</p><p>? Take deriva:ve of score with respect to single word vector (for simplicity a 1d vector, but same if it was longer)</p><p>? Now, we cannot just take into considera:on one ai because each xj is connected to all the neurons above and hence xj influences the overall score through all of these, hence:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>74</head><p>Training with Backpropagation</p><p>? So in the last line, we re-?-used parts of the deriva:ve of a computa:on from a layer above.</p><p>? In the next sec:on, we will see how -with more deeper layerseven more of the computa:on of higher layers can be re-?-used in lower layers</p><p>Single scalar output ¡­ ¡­ ? E.g. Max over configura:ons or sum weighted by posterior ? The loss to be op:mized depends on these choices ? The inference opera:ons are flow graph nodes ? If con:nuous, can perform stochas:c gradient descent</p><p>? Max(a,b) is con:nuous.</p><p>? Collobert &amp; Weston 2008: max-?-pooling layer <ref type="bibr">85</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic Differentiation</head><p>? The gradient computa:on can be automa:cally inferred from the symbolic expression of the fprop.</p><p>? Makes it easier to quickly and safely try new models.</p><p>? Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output.</p><p>? Theano Library (python) does it symbolically. Other neural network packages (Torch, Lush) do it numerically (at run-?-:me).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>86</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part 1.6: The Basics</head><p>Learning word-level classifiers: POS and NER</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>87</head><p>The Model <ref type="bibr">? Collobert &amp; Weston (2008)</ref> ? Similar to the word vector learning but replaces the single scalar score with a</p><p>? so@max (maxent) classifier Auto-Encoders</p><p>? MLP whose target output = input ? Reconstruc:on=decoder(encoder(input)), e.g.</p><p>? Probable inputs have small reconstruc:on error </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auto-Encoder Variants</head><p>? Discrete inputs: cross-?-entropy or log-?-likelihood reconstruc:on criterion (similar to used for discrete targets for MLPs)</p><p>? Preven:ng them to learn the iden:ty everywhere:</p><p>? Undercomplete (eg PCA): boBleneck code smaller than input ? Sparsity: penalize hidden unit ac:va:ons so at or near 0 [Goodfellow et al 2009]</p><p>? Denoising: predict true input from corrupted input <ref type="bibr">[Vincent et al 2008]</ref> ? Contrac:ve: force encoder to have small deriva:ves [ <ref type="bibr">Rifai et al 2011]</ref> Manifold Learning</p><p>? Addi:onal hypothesis: examples concentrate near a lower dimensional "manifold" (region of high density with only few opera:ons allowed which allow small changes while staying on the manifold) Auto-Encoders Learn Salient Variations, like a non-linear PCA</p><p>? Minimizing reconstruc:on error forces to keep varia:ons along manifold.</p><p>? Regularizer wants to throw away all varia:ons.</p><p>? Both: keep ONLY sensi:vity to varia:ons ON the manifold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>110</head><p>Stacking Auto-Encoders</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>111</head><p>Why is Unsupervised Pre-Training Working So Well?</p><p>? Regulariza:on hypothesis:</p><p>? Unsupervised component forces model close to P(x)</p><p>? Representa:ons good for P(x) are good for P(y|x)</p><p>? Op:miza:on hypothesis:</p><p>? Unsupervised ini:aliza:on near beBer local minimum of supervised training error ? We don't want invariant representa:ons because it is not clear to what aspects, but disentangling factors would help a lot</p><p>? Sparse/saturated units seem to help ? Why?</p><p>? How to train more towards that objecHve?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>113</head><p>Invariance and Disentangling</p><p>? Invariant features</p><p>? Which invariances?</p><p>? Alterna:ve: learning to disentangle factors</p><p>? Good disentangling ¨¤? avoid the curse of dimensionality <ref type="bibr">114</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advantages of Sparse Representations</head><p>? Just add a penalty on learned representa:on</p><p>? Informa:on disentangling (compare to dense compression)</p><p>? More likely to be linearly separable (high-?-dimensional space)</p><p>? Locally low-?-dimensional representa:on = local chart ? Hi-?-dim. sparse = efficient variable size representa:on = data structure Few bits of informa:on Many bits of informa:on <ref type="bibr">115</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task Learning</head><p>? Generalizing beBer to new tasks is crucial to approach AI task 1 output y1 task 3 output y3 task 2 output y2</p><p>?  <ref type="formula">0 1 2 3 4 5 6 7 8 9 10</ref> the country of my birth the place where I was born But how can we represent the meaning of longer phrases?</p><p>By mapping them into the same vector space!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How should we map phrases into a vector space?</head><p>Use principle of composi:onality</p><formula xml:id="formula_10">x 2</formula><p>The meaning (vector) of a sentence is determined by (1) the meanings of its words and (2) the rules that combine them.  Neural " Network" on the mat. Related Work to Socher et al. 2011a</p><p>? Pollack (1990): Recursive auto-?-associa:ve memories</p><p>? Previous Recursive Neural Networks work by <ref type="bibr">[Goller &amp; K¨¹chler 1996</ref><ref type="bibr">, Costa et al. 2003</ref>] assumed fixed tree structure and used one hot vectors.</p><p>? Hinton (1990) and BoBou <ref type="formula">(2011)</ref> Neural " Network"</p><p>Neural " Network"</p><p>Neural " Network"</p><p>Neural " Network"</p><p>Neural " Network" The cat sat on the mat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>128</head><p>Parsing a sentence The cat sat on the mat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>129</head><p>Parsing a sentence </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recursive Autoencoders</head><p>? Similar to Recursive Neural Net but instead of a supervised score we compute a reconstruc:on error at each node. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-supervised Recursive Autoencoder</head><p>? In order for representa:ons to capture sen:ment, we add a so\max classifier</p><p>? Error is a weighted combina:on of reconstruc:on error and cross-?-entropy (distribu:on likelihood)</p><p>? Socher et al. 2011b</p><p>Reconstruction error Cross-?-entropy error</p><formula xml:id="formula_11">W (2) W (label) W (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentiment Detection and Bag-of-Words Models</head><p>? Sen:ment detec:on is crucial to business intelligence, stock trading, ¡­</p><p>? Most methods start with a bag of words + linguis:c features/processing/lexica</p><p>? But such methods (including ¡­-?-idf) can't dis:nguish:</p><p>+ white blood cells destroying an infec:on -?-an infec:on destroying white blood cells <ref type="bibr">145</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Scale Experiments: Movies</head><p>Stealing Harvard doesn't care about cleverness, wit or any other kind of intelligent humor.</p><p>a film of ideas and wry comic mayhem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy of Positive/Negative Sentiment Classification</head><p>? Results on movie reviews (MR) and opinions (MPQA).</p><p>? All other methods use hand-?-designed polarity shi\ing rules or sen:ment lexica.</p><p>? RAE: no hand-?-designed features, learns vector representa:ons for n-?-grams ¨¤? Use SENNA (Collobert 2010) = embedding-?-based NLP tagger for Seman:c Role Labeling, breaks sentence into (subject part, verb part, object part)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MR MPQA</head><p>¨¤? Use max-?-pooling to aggregate embeddings of words inside each part <ref type="bibr">183</ref> Open-Text Semantic Parsing</p><p>? 3 steps:</p><p>? last formula defines the Meaning Representa:on (MR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>184</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Criterion</head><p>? Intui:on: if an en:ty of a triplet was missing, we would like our model to predict it correctly i.e. to give it the lowest energy. For example, this would allow us to answer ques:ons like "what is part of a car?"</p><p>? Hence, for any training triplet x i = (lhs i , rel i , rhs i ) we would like:</p><p>(1) E(lhs i , rel i , rhs i ) &lt; E(lhs j , rel i , rhs i ), <ref type="bibr">(2)</ref> E(lhs i , rel i , rhs i ) &lt; E(lhs i , rel j , rhs i ), (3) E(lhs i , rel i , rhs i ) &lt; E(lhs i , rel i , rhs j ), That is, the energy func:on E is trained to rank training samples below all other triplets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>From</head><label></label><figDesc>Maxent Classifiers to Neural Networks ¦Ë? f (c,d ) ? Vector form: P(c | d, ¦Ë) = e e ¦Ë? f ( " c ,d ) ¡Æ " c ? Make two class: ¦Ë? f (c 1 ,d ) ¦Ë? f (c 1 ,d ) ?¦Ë? f (c 1 ,d ) P(c 1 | d, ¦Ë) = e = e ? e e ¦Ë? f (c 1 ,d ) + e ¦Ë? f (c 2 ,d ) e ¦Ë? f (c 1 ,d ) + e ¦Ë? f (c 2 ,d ) e ?¦Ë? f (c 1 ,d ) = 1 1+ e ¦Ë?[ f (c 2 ,d )? f (c 1 ,d )] = for x = f (c 1 , d) ? f (c 2 , d) 1 1+ e ?¦Ë?x ¦Ë? f (c 2 ,d ) ¦Ë? f (c 2 ,d ) ?¦Ë? f (c 1 ,d ) P(c 2 | d, ¦Ë) = e = e ? e e ¦Ë? f (c 1 ,d ) + e ¦Ë? f (c 2 ,d ) e ¦Ë? f (c 1 ,d ) + e ¦Ë? f (c 2 ,d ) e ?¦Ë? f (c 1 ,d ) ¦Ë?[ f (c 2 ,d )? f (c 1 ,d ) ?¦Ë?x = e = e 1+ e ¦Ë?[ f (c 2 ,d )? f (c 1 ,d )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>64 Summary:</head><label>64</label><figDesc>Feed-forward Computation ? Compu:ng a window's score with a 3-?-layer Neural Net: s = score(cat chills on a mat) cat chills on a mat 65 Summary: Feed-forward Computation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>?</head><label></label><figDesc>Can be stacked successfully (Bengio et al NIPS'2006) to form highly non-?-linear representa:ons input reconstruc:on</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>PCA = Linear Manifold = Linear Auto-Encoder Linear manifold input x, 0-?-mean features=code=h(x)=W x reconstruc:on(x)=W T h(x) = W T W x W = principal eigen-?-basis of Cov(X) reconstruc:on(x) reconstruc:on error vector x LSA example: x = (normalized) distribu:on of co-?-occurrence frequencies</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Recursive Neural Networks for Structure Prediction Inputs: two candidate children's representa:ons Outputs: 1. The seman:c representa:on if the two nodes are merged. 2. Score of how plausible the new node would be.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>by L: x = Lo</head><label></label><figDesc></figDesc><table>? Ini:alize all word vectors randomly to form a word embedding 
matrix 
|V| 

L = 

¡­ 
n 


the cat mat ¡­ 

[ ] 

? These are the word features we want to learn 
? Also called a look-?-up table 
? Conceptually you get a word's vector by le\ mul:plying a 
one-?-hot vector o </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>and s c wrt all the involved variables: W score , W, b, x 67</head><label></label><figDesc></figDesc><table>? Assuming cost is &gt; 0, we compute the 
deriva:ves of s Training with Backpropagation 

? Let's consider the deriva:ve of a single weight Wij 

? This only appears inside ai 
? For example: W23 is only 
used to compute a2 

s 

a1 a2 

W23 

x1 x2 x3 +1 

68 

Training with Backpropagation 

? Deriva:ve of weight Wij: 

s 

Wscore,2 

a1 a2 

W23 

x1 x2 x3 +1 

69 

Training with Backpropagation 

? Deriva:ve of single weight Wij: 

s 

Wscore,2 

a1 a2 

W23 

x1 x2 x3 +1 

Local error 
signal 
Local input 
signal 

70 

Training with Backpropagation 

? From single weight Wij to full W: 

? We want all combina:ons of 
i=1,2 and j=1,2,3 

s 

Wscore,2 

? Solu:on: Outer product: 
? where is the 
"responsibility" coming from each ac:va:on a 

a1 a2 

W23 

x1 x2 x3 +1 

71 

Training with Backpropagation 

? For biases b, we get: 

s 

Wscore,2 

a1 a2 

W23 

x1 x2 x3 +1 

72 

</table></figure>

			<note place="foot">? Before we know it, we have a mul:layer neural network¡­.</note>

			<note place="foot">p = sigmoid(W + b) c 1 c 2 ? But what if words act mostly as an operator, e.g. &quot;very&quot; in  very good</note>

			<note place="foot">p = sigmoid(W + b) c 1 c 2 p = sigmoid(W + b) C 2 c 1 C 1 c 2</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>131</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max-Margin Framework -Details</head><p>? The score of a tree is computed by the sum of the parsing decision scores at each node.</p><p>8 3</p><p>1.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN"</head><p>8 5 3 3</p><p>? Similar to max-?-margin parsing ( <ref type="bibr">Taskar et al. 2004</ref>), we can formulate a supervised max-?-margin objec:ve</p><p>? The loss penalizes all incorrect decisions <ref type="bibr">132</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backpropagation Through Structure (BTS)</head><p>? Introduced by <ref type="bibr">[Goller et al. 1996]</ref> ? Principally the same as general backpropaga:on (efficient matrix deriva:ve)</p><p>? Two differences resul:ng from the tree structure:</p><p>? Split deriva:ves at each node</p><p>? Sum deriva:ves of W from all nodes <ref type="bibr">133</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BTS: Split derivatives at each node</head><p>? During forward prop, the parent is computed using 2 children ? Hence, the errors need to be computed wrt each of them: where each child's error is n-?-dimensional ? If take separate deriva:ves of each occurrence, we get same:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>135</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BTS: Optimization</head><p>? As before, we can plug the gradients into a standard off-?-the-?-shelf L-?-BFGS op:mizer</p><p>? For non-?-con:nuous objec:ve use subgradient method <ref type="bibr">[Ratliff et al. 2007]</ref> 136</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details of Recursive Neural Networks</head><p>? Structure search was maximally greedy Paraphrase detection task</p><p>? Goal is to say which of candidate phrases are a good paraphrase of a given phrase  <ref type="formula">(4)</ref> united <ref type="formula">(1)</ref> the <ref type="formula">(1)</ref> of the united states (3) america (5) na:ons <ref type="formula">(2)</ref> we <ref type="formula">(3)</ref> around the world around the globe(5) throughout the world(5) across the world(5) over the world(2) in the world(5) of the budget(2) of the world <ref type="formula">(5)</ref> it would be it would represent (5) there will be (2) that would be (3) it would be ideal (2) it would be appropriate (2) it is (3) it would <ref type="formula">(2)</ref> of capital punishment of the death penalty (5) to death (2) the death penalty <ref type="formula">(2)</ref> of <ref type="formula">(1)</ref> in the long run in the long term (5) in the short term (2) for the longer term (5) in the future (5) in the end (3) in the long-?-term (5) in :me (5) of the <ref type="formula">(1)</ref> Paraphrase Detection  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Criticisms</head><p>? You're learning everything. BeBer to encode prior knowledge about structure of language.</p><p>? Wasn't there a similar machine learning vs. linguists debate in NLP ~20 years ago¡­.</p><p>? Research goal: Just like we now use off-?-the-?-shelf SVM packages, we can try to build off-?-the-?-shelf NLP classifica:on packages that are given as input only raw text and a label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>211</head><p>Problems with model interpretability</p><p>? Ways to interpret the output of models/ weights on features ? More difficult compared to systems with small sets of hand-?-designed features, but possible through low-?-dimensional projec:ons of word vectors</p><p>? Learning about language through doing nlp ? Can be done by careful visualiza:on, e.g. "flawed" in sen:ment analysis</p><p>? No discrete categories or words, everything is a con:nuous vector. We'd like have symbolic features like NP, VP, etc. and see why their combina:on makes sense.</p><p>? True, but most of language is fuzzy and many words have so\ rela:onships to each other. Also, many NLP features are already not human-?-understandable (e.g, concatena:ons/combina:ons of different features).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>212</head><p>Inference Challenges</p><p>? Many latent variables involved in understanding language (sense ambiguity, parsing, seman:c role)</p><p>? Almost any inference mechanism can be combined with deep learning ? Complex inference can be hard (exponen:ally) and needs to be approximate ¨¤? learn to perform inference <ref type="bibr">213</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Multiple Levels of Abstraction</head><p>? The big payoff of deep learning is to allow learning higher levels of abstrac:on</p><p>? Going from symbols to embeddings already helps, and recent years have shown that phrase and sentence representa:ons work too, but the space of possibili:es is much wider there</p><p>? Higher-?-level abstrac:ons disentangle the factors of varia:on, which allows much easier generaliza:on and transfer <ref type="bibr">214</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advantages</head><p>? Despite a small community in the intersec:on of deep learning and NLP, already many state of the art results on a variety of language tasks</p><p>? O\en very simple matrix deriva:ves (backprop) for training and matrix mul:plica:ons for tes:ng</p><p>? Fast inference and well suited for mul:-?-core CPUs/GPUs <ref type="bibr">215</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transfer Learning</head><p>? Applica:on of deep learning could be in areas where there are not enough labeled data but a transfer is possible</p><p>? Domain adapta:on already showed that effect, thanks to unsupervised feature learning</p><p>? Transfer to resource-?-poor languages would be a great applica:on [Gouws, <ref type="bibr">PhD proposal 2012]</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<idno>Mo:va:on</idno>
	</analytic>
	<monogr>
		<title level="j">Recursive Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Recursive Neural Networks for Parsing 3. Theory: Backpropaga:on Through Structure</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">Recursive Autoencoders 5. Applica:on to Sen:ment Analysis and Paraphrase Detec:on 6. Composi:onality Through Recursive Matrix-?-Vector Spaces 7. Rela:on classifica:on 1. Mo:va:on</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">Recursive Neural Networks for Parsing 3. Theory: Backpropaga:on Through Structure</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<title level="m">Recursive Autoencoders 5. Applica:on to Sen:ment Analysis and Paraphrase Detec:on 6. Composi:onality Through Recursive Matrix-?-Vector Spaces 7. Rela:on classifica</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Applica:ons 1. Neural language models 2. Structured embedding of knowledge bases 3. Assorted other speech and NLP applica:ons</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Resources (readings, code, ¡­)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<title level="m">Tricks of the trade 4. Discussion: Limita:ons, advantages, future direc:ons</title>
		<imprint>
			<biblScope unit="page">168</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<title level="m">Existing NLP Applications ? Language Modeling ? Speech Recogni:on ? Machine Transla:on ? Part-?-Of-?-Speech Tagging ? Chunking ? Named En:ty Recogni:on ? Seman:c Role Labeling ? Sen:ment Analysis ? Paraphrasing ? Ques:on-?-Answering ? Word-?-Sense Disambigua:on 169 Part 3.1: Applica:ons</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<title level="m">? Standard approach: output = P</title>
		<imprint/>
	</monogr>
	<note>next word | previous word</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Applica</surname></persName>
		</author>
		<title level="m">ons to Speech, Transla:on and Compression ? Computa:onal boBleneck: large vocabulary V means that compu:ng the output costs #hidden units x |V|</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Language Modeling Output Bottleneck</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">only predict most frequent words (short list) and use n-?-gram for the others</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Morin &amp; Bengio</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Blitzer et al</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih &amp;amp; Hinton</surname></persName>
		</author>
		<title level="m">hierarchical representa:ons, mul:ple output groups</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Mikolov et al 2011. condi:onally computed, predict ? P(word category | context</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
		</author>
		<imprint/>
	</monogr>
<note type="report_type">sub-?-category | context</note>
	<note>category</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bigger is beBer¡­ experiments on Broadcast News NIST-?-RT04 perplexity goes from 140 to 102 Paper shows how to train a recurrent neural net with a single core in a few days</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>P(word | Context, Sub-?-Category</surname></persName>
		</author>
		<ptr target="http://www.fit.vutbr.cz/~imikolov/rnnlm/!" />
	</analytic>
	<monogr>
		<title level="m">category) Recurrent Neural Net Language Modeling for ASR ?</title>
		<imprint/>
	</monogr>
	<note>with &gt; 1% absolute improvement in WER Code</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Acoustic Models ?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phoneme-Level</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Mohamed et al, 2011, IEEE Tr.ASLP</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<title level="m">? Unsupervised pre-?-training as Deep Belief Nets (a stack of RBMs), supervised fine-?-tuning to predict phonemes ? Phoneme classifica:on on TIMIT: ? CD-?-HMM: 27.3% error ? CRFs: 26.6%</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? Triphone</forename><surname>Hmms</surname></persName>
		</author>
		<idno>w. BMMI: 22.7% ? Unsupervised DBNs: 24.5% ? Fine-?-tuned DBNs: 20.7%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<title level="m">? Improved version by Dong Yu is RELEASED IN MICROSOFT&apos;S ASR system for Audio Video Indexing Service Sentiment Analysis on Bag-of-word representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? [</forename><surname>Glorot</surname></persName>
		</author>
		<title level="m">ICML 2011] beats SOTA on Amazon benchmark ? Bag-?-of-?-words input ? Embeddings pre-?-trained in denoising auto-?-encoder with rec:fier ac:va:on func:ons and sampled reconstruc:on ? Disentangling effect</title>
		<imprint/>
	</monogr>
	<note>features specialize to domain or sen:ment</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<title level="m">Sentiment Analysis: Transfer Learning ? 25 Amazon</title>
		<imprint/>
	</monogr>
	<note>com domains: toys, so\ware, video, books, music, beauty</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<title level="m">? Unsupervised pre-?-training of input space on all domains ? Supervised SVM on 1 domain, generalize out-?-of-?-domain ? Baseline: bag-?-of-?-words + SVM 196 Part 3.2: Resources Resources: Code and References</title>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Related</forename><surname>Papers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tutorials</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>See</surname></persName>
		</author>
		<ptr target="//deeplearning.net/tutorials/" />
		<title level="m">Neural Net Language Models&quot; Scholarpedia entry ? Deep Learning tutorials ? hBp</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep learning tutorials with simple programming assignments and reading list ? hBp://deeplearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Stanford</surname></persName>
		</author>
		<ptr target="stanford.edu/wiki/?RecursiveAutoencoderclassproject:hBp://cseweb.ucsd.edu/~elkan/250B/learningmeaning.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">State-?-of-?-the-?-art performance on many tasks ? hBp://ronan.collobert.com/senna/ ? 3500 lines of C, extremely fast and using very liBle memory ? Recurrent Neural Network Language Model ? hBp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cpu</forename><surname>Software ? Theano ; Python</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gpu Mathema ; ? Pos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ner</forename><surname>Chunking</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srl ?</surname></persName>
		</author>
		<ptr target="//www.fit.vutbr.cz/~imikolov/rnnlm/" />
		<imprint/>
	</monogr>
	<note>Collobert et al 2011</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<title level="m">? Recursive Neural Net and RAE models for paraphrase detec:on, sen:ment analysis, rela:on classifica:on ? www.socher.org</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Prac:cal Recommenda:ons for Gradient-?-Based Training of Deep Architectures&quot; ? Unsupervised pre-?-training ? Stochas:c gradient descent and se ?ng learning rates ? Main hyper-?-parameters ?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>hyper-?-parameter configura:ons 201</idno>
	</analytic>
	<monogr>
		<title level="m">Learning rate schedule ? Early stopping ? Minibatches ? Parameter ini:aliza:on ? Number of hidden units ? L1 and L2 weight decay ? Sparsity regulariza:on ? Debugging ? How to efficiently search for</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Stochastic Gradient Descent (SGD)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">? Gradient descent uses total gradient over all examples per update</title>
	</analytic>
	<monogr>
		<title level="m">SGD updates a\er only 1 or few examples</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? L = Loss</forename><surname>Func</surname></persName>
		</author>
		<title level="m">on, z t = current example, ¦È = parameter vector, and ¦Å t = learning rate</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">On large datasets, SGD usually wins over all batch methods. On smaller datasets LBFGS or Conjugate Gradients win. Large-?-batch LBFGS extends the reach of LBFGS</title>
		<imprint/>
	</monogr>
	<note>? Ordinary gradient descent is a batch method, very slow, should never be used. Le et al ICML&apos;2011</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning Rates ? Simplest recipe: keep it fixed and use the same for all parameters</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Collobert scales them by the inverse of square root of the fan-?-in of each neuron</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<title level="m">? BeBer results can generally be obtained by allowing learning rates to decrease, typically in O(1/t) because of theore:cal convergence guarantees</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">the gradient is a product of Jacobian matrices, each associated with a step in the forward computa:on. This can become very small or very large quickly</title>
	</analytic>
	<monogr>
		<title level="m">Long-Term Dependencies and Clipping Trick ? In very deep networks such as recurrent networks (or possibly recursive ones)</title>
		<imprint/>
	</monogr>
	<note>Bengio et al 1994], and the locality assump:on of gradient descent breaks down</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">? The solu:on first introduced by Mikolov is to clip gradients to a maximum value. Makes a big difference in RNNs</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">ful FREE LUNCH (no need to launch many different training runs for each value of hyper-?-parameter for #itera:ons)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Beau</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Monitor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valida</forename></persName>
		</author>
		<title level="m">on error during training (a\er visi:ng # examples a mul:ple of valida:on set size</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Keep track of parameters with best valida:on error and report them at the end</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">? If error does not improve enough (with some pa:ence)</title>
		<idno>stop. 205</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Parameter Initialization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Ini</surname></persName>
		</author>
		<title level="m">alize hidden layer biases to 0 and output (or reconstruc:on) biases to op:mal value if weights were 0 (e.g. mean target or inverse sigmoid of mean target)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">r inversely propor:onal to fan-?-in (previous layer size) and fan-?-out (next layer size): for tanh units (and 4x bigger for sigmoid units)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Ini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">alize weights ~ Uniform(-?-r,r)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Note: for embedding weights, fan-?-in=1 and we don&apos;t care about fan-?-out</title>
		<imprint/>
		<respStmt>
			<orgName>Collobert uses Uniform</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Sampled Reconstruction Trick</title>
		<imprint/>
	</monogr>
	<note>Dauphin et al, ICML 2011</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">RBMs reconstruct the input, which is sparse and high-?-dimensional to bag-?-of-?-words input for sen:ment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Auto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-?-Encoders</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>with denoising auto-?-encoders</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Criticisms ? Many algorithms and variants (burgeoning field)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">?-core machines, clusters and random sampling for cross-?-valida:on ? Not always obvious how to combine with exis:ng NLP ¨¤? learn more abstract features, separate parsing and seman:c analysis, your research here ? Slower to train train linear models ¨¤? only by a small constant factor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Many Hyper-?-Parameters</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and much more compact that non-?-parametric (e.g. n-?-gram models</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
