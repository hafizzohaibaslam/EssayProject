<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-10-04T10:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Graph Embedding: A Survey of Approaches and Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
				<availability status="unknown"><p>Copyright Institute of Electrical and Electronics Engineers (IEEE)</p>
				</availability>
				<date type="published" when="2017">DECEMBER 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
						</author>
						<title level="a" type="main">Knowledge Graph Embedding: A Survey of Approaches and Applications</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE Transactions on Knowledge and Data Engineering</title>
						<title level="j" type="abbrev">IEEE Trans. Knowl. Data Eng.</title>
						<idno type="ISSN">1041-4347</idno>
						<imprint>
							<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
							<biblScope unit="volume">29</biblScope>
							<biblScope unit="issue">12</biblScope>
							<biblScope unit="page" from="2724" to="2743"/>
							<date type="published" when="2017">DECEMBER 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/tkde.2017.2754499</idno>
					<note>2724</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Statistical relational learning</term>
					<term>knowledge graph embedding</term>
					<term>latent factor models</term>
					<term>tensor/matrix factorization models</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Knowledge graph (KG) embedding is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. It can benefit a variety of downstream tasks such as KG completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-the-arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the KG are first introduced. We describe the overall framework, specific model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus specifically on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we briefly introduce how KG embedding can be applied to and benefit a wide variety of downstream tasks such as KG completion, relation extraction, question answering, and so forth.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Graph Embedding: A Survey of Approaches and Applications</head><p>Quan Wang , Zhendong Mao , Bin Wang, and Li Guo</p><p>Abstract-Knowledge graph (KG) embedding is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. It can benefit a variety of downstream tasks such as KG completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-the-arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the KG are first introduced. We describe the overall framework, specific model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus specifically on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we briefly introduce how KG embedding can be applied to and benefit a wide variety of downstream tasks such as KG completion, relation extraction, question answering, and so forth.</p><p>Index Terms-Statistical relational learning, knowledge graph embedding, latent factor models, tensor/matrix factorization models ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R</head><p>ECENT years have witnessed rapid growth in knowledge graph (KG) construction and application. A large number of KGs, such as Freebase <ref type="bibr" target="#b1">[1]</ref>, DBpedia <ref type="bibr" target="#b2">[2]</ref>, YAGO <ref type="bibr" target="#b3">[3]</ref>, and NELL <ref type="bibr" target="#b4">[4]</ref>, have been created and successfully applied to many real-world applications, from semantic parsing <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref> and named entity disambiguation <ref type="bibr" target="#b8">[7]</ref>, <ref type="bibr" target="#b9">[8]</ref>, to information extraction <ref type="bibr" target="#b10">[9]</ref>, <ref type="bibr" target="#b11">[10]</ref> and question answering <ref type="bibr" target="#b12">[11]</ref>, <ref type="bibr" target="#b13">[12]</ref>. A KG is a multi-relational graph composed of entities (nodes) and relations (different types of edges). Each edge is represented as a triple of the form (head entity, relation, tail entity), also called a fact, indicating that two entities are connected by a specific relation, e.g., <ref type="bibr">(AlfredHitchcock, DirectorOf, Psycho)</ref>. Although effective in representing structured data, the underlying symbolic nature of such triples usually makes KGs hard to manipulate.</p><p>To tackle this issue, a new research direction known as knowledge graph embedding has been proposed and quickly gained massive attention <ref type="bibr" target="#b14">[13]</ref>, <ref type="bibr" target="#b15">[14]</ref>, <ref type="bibr" target="#b16">[15]</ref>, <ref type="bibr" target="#b17">[16]</ref>, <ref type="bibr" target="#b18">[17]</ref>, <ref type="bibr" target="#b19">[18]</ref>, <ref type="bibr" target="#b20">[19]</ref>.</p><p>The key idea is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. Those entity and relation embeddings can further be used to benefit all kinds of tasks, such as KG completion <ref type="bibr" target="#b15">[14]</ref>, <ref type="bibr" target="#b16">[15]</ref>, relation extraction <ref type="bibr" target="#b21">[20]</ref>, <ref type="bibr" target="#b22">[21]</ref>, entity classification <ref type="bibr" target="#b14">[13]</ref>, <ref type="bibr" target="#b23">[22]</ref>, and entity resolution <ref type="bibr" target="#b14">[13]</ref>, <ref type="bibr" target="#b19">[18]</ref>.</p><p>Most of the currently available techniques perform the embedding task solely on the basis of observed facts. Given a KG, such a technique first represents entities and relations in a continuous vector space, and defines a scoring function on each fact to measure its plausibility. Entity and relation embeddings can then be obtained by maximizing the total plausibility of observed facts. During this whole procedure, the learned embeddings are only required to be compatible within each individual fact, and hence might not be predictive enough for downstream tasks <ref type="bibr" target="#b24">[23]</ref>, <ref type="bibr" target="#b25">[24]</ref>. As a result, more and more researchers have started to further leverage other types of information, e.g., entity types <ref type="bibr" target="#b26">[25]</ref>, <ref type="bibr" target="#b27">[26]</ref>, relation paths <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b29">[28]</ref>, <ref type="bibr" target="#b30">[29]</ref>, textual descriptions <ref type="bibr" target="#b31">[30]</ref>, <ref type="bibr" target="#b32">[31]</ref>, <ref type="bibr" target="#b34">[32]</ref>, <ref type="bibr" target="#b35">[33]</ref>, and even logical rules <ref type="bibr" target="#b24">[23]</ref>, <ref type="bibr" target="#b36">[34]</ref>, <ref type="bibr" target="#b37">[35]</ref>, to learn more predictive embeddings.</p><p>In this article, we provide a thorough review of currently available KG embedding techniques, including those that use facts alone, as well as those that further leverage additional information. We further introduce how the learned embeddings can be applied to and benefit a wide variety of entity-oriented tasks. Nickel et al. <ref type="bibr" target="#b38">[36]</ref> have made a survey of statistical relational learning methods on KGs, including embedding techniques, path ranking algorithms <ref type="bibr" target="#b39">[37]</ref>, <ref type="bibr" target="#b40">[38]</ref>, <ref type="bibr" target="#b41">[39]</ref>, and Markov logic networks <ref type="bibr" target="#b42">[40]</ref>, <ref type="bibr" target="#b43">[41]</ref>, <ref type="bibr" target="#b44">[42]</ref>. In contrast to their work, we focus specifically on KG embedding, and make a systematic review of existing techniques, including not only the state-of-the-arts but also those with latest trends. Particularly, we make the review based on the type of information used in these techniques.</p><p>The rest of this article is organized as follows. Section 2 briefly introduces basic notations. Section 3 reviews techniques that conduct embedding using only facts observed in KGs. We describe the overall framework, specific model design, typical training procedures, as well as pros and cons of such techniques. Section 4 discusses embedding techniques that further incorporate other information besides facts. We focus on the use of entity types, relation paths, textual descriptions, and logical rules. Section 5 further explores the application of KG embedding in downstream tasks like KG completion, relation extraction and question answering. Finally, we present our concluding remarks in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NOTATIONS</head><p>Throughout this article, we use a boldface lower-case letter x to represent a vector, with its ith entry denoted as ?x? i . The ' p norm of a vector for p ! 1 is denoted as kxk p , and kxk 1=2 means either the ' 1 norm or the ' 2 norm. Let diag?x? be a diagonal matrix, the ith diagonal entry of which is ?x? i . Let jxj denote the absolute value function, tanh?x? the hyperbolic tangent function, and ReLU?x? the rectified linear unit. All of them are entry-wise operations. A matrix is represented by a boldface upper-case letter X, with its ijth entry denoted as ?X? ij . kXk F is the Frobenius norm of a matrix, tr?X? and det?X? the trace and determinant of a square matrix, respectively. We use an underlined boldface capital letter X to represent a three-mode tensor. The ijkth entry of a tensor is denoted as ?X? ijk . We further use X ?i;:;:? , X ?:;j;:? , and X ?:;:;k? to denote the ith, jth, and kth slice along the first, second, and third mode, respectively.</p><p>Let : R n ? R n ! R n denote the Hadamard product between two vectors, i.e., deterministic points in the vector space <ref type="bibr" target="#b14">[13]</ref>, <ref type="bibr" target="#b15">[14]</ref>, <ref type="bibr" target="#b16">[15]</ref>, <ref type="bibr" target="#b17">[16]</ref>, <ref type="bibr" target="#b20">[19]</ref>. Recent work in <ref type="bibr" target="#b47">[45]</ref> further takes into account uncertainties of entities, and models them through multivariate Gaussian distributions. Relations are typically taken as operations in the vector space, which can be represented as vectors <ref type="bibr" target="#b15">[14]</ref>, <ref type="bibr" target="#b16">[15]</ref>, matrices <ref type="bibr" target="#b17">[16]</ref>, <ref type="bibr" target="#b19">[18]</ref>, tensors <ref type="bibr" target="#b20">[19]</ref>, multivariate Gaussian distributions <ref type="bibr" target="#b47">[45]</ref>, or even mixtures of Gaussians <ref type="bibr" target="#b48">[46]</ref>. Then, in the second step, a scoring function f r ?h; t? is defined on each fact ?h; r; t? to measure its plausibility. Facts observed in the KG tend to have higher scores than those that have not been observed. Finally, to learn those entity and relation representations (i.e., embeddings), the third step solves an optimization problem that maximizes the total plausibility of observed facts (i.e., facts contained in ID ? ). We roughly categorize such embedding techniques into two groups: translational distance models and semantic matching models. The former use distance-based scoring functions, and the latter similarity-based ones. In this section, we first introduce these two groups of embedding techniques, and then discuss the training process for them. After that, we compare these embedding techniques in terms of efficiency and effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Translational Distance Models</head><p>Translational distance models exploit distance-based scoring functions. They measure the plausibility of a fact as the distance between the two entities, usually after a translation carried out by the relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">TransE and Its Extensions</head><formula xml:id="formula_0">?a b? i ? ?a? i ? ?b? i ;</formula><p>and ? : R n ? R n ! R n the circular correlation, 1 i.e.,</p><formula xml:id="formula_1">?a ? b? i ? X n?1 ?a? k ? ?b? ?k?i? mod n : k?0</formula><p>For details about these operations, refer to <ref type="bibr" target="#b45">[43]</ref>, <ref type="bibr" target="#b46">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">KG EMBEDDING WITH FACTS ALONE</head><p>Suppose we are given a KG consisting of n entities and m relations. Facts observed in the KG are stored as a collection of triples ID ? ? f?h; r; t?g. Each triple is composed of a head entity h 2 E, a tail entity t 2 E, and a relation r 2 IR between them, e.g., <ref type="bibr">(AlfredHitchcock, DirectorOf, Psycho)</ref>. Here, E denotes the set of entities, and IR the set of relations. KG embedding aims to embed entities and relations into a low-dimensional continuous vector space, so as to simplify computations on the KG. Most of the currently available techniques use facts stored in the KG to perform the embedding task, enforcing embedding to be compatible with the facts.</p><p>A typical KG embedding technique generally consists of three steps: (i) representing entities and relations, (ii) defining a scoring function, and (iii) learning entity and relation representations. The first step specifies the form in which entities and relations are represented in a continuous vector space. Entities are usually represented as vectors, i.e., <ref type="bibr">TransE. TransE [14]</ref> is the most representative translational distance model. It represents both entities and relations as vectors in the same space, say R d . Given a fact ?h; r; t?, the relation is interpreted as a translation vector r so that the embedded entities h and t can be connected by r with low error, i.e., h ? r % t when ?h; r; t? holds. The intuition here originates from <ref type="bibr" target="#b49">[47]</ref>, which learns distributed word representations to capture linguistic regularities such as Psycho ? AlfredHitchcock % Avatar ? JamesCameron. In multi-relational data, such an analogy holds because of the certain relation of DirectorOf, and through this relation we can get AlfredHitchcock ? DirectorOf % Psycho and JamesCameron ? DirectorOf % Avatar. <ref type="figure" target="#fig_0">Fig. 1a</ref> gives a simple illustration of this idea. The scoring function is then defined as the (negative) distance between h ? r and t, i.e.,</p><formula xml:id="formula_2">f r ?h; t? ? ?kh ? r ? tk 1=2 :</formula><p>The score is expected to be large if ?h; r; t? holds.</p><p>Despite its simplicity and efficiency, TransE has flaws in dealing with 1-to-N, N-to-1, and N-to-N relations <ref type="bibr" target="#b16">[15]</ref>, <ref type="bibr" target="#b17">[16]</ref>. Take 1-to-N relations for example. Given such a relation r, i.e., 9i ? 1; . . . ; p such that ?h; r; t i ? 2 ID ? , TransE enforces h ? r % t i for all i ? 1; . . . ; p, and then t 1 % ? ? ? % t p . That means, given a 1-to-N relation, e.g., DirectorOf, TransE might learn very similar vector representations for Psycho, Rebecca, and RearWindow which are all films directed by AlfredHitchcock, even though they are totally different entities. Similar disadvantages exist for N-to-1 and N-to-N relations.  <ref type="bibr" target="#b16">[15]</ref>, <ref type="bibr" target="#b17">[16]</ref>.</p><p>Introducing Relation-Specific Entity Embeddings. To overcome the disadvantages of TransE in dealing with 1-to-N, N-to-1, and N-to-N relations, an effective strategy is to allow an entity to have distinct representations when involved in different relations. In this way, even if the embeddings of Psycho, Rebecca, and RearWindow might be very similar given the relation DirectorOf, they could still be far away from each other given other relations.</p><p>TransH <ref type="bibr" target="#b16">[15]</ref> follows this general idea, by introducing relation-specific hyperplanes. As shown in <ref type="figure" target="#fig_0">Fig. 1b</ref>, TransH models entities again as vectors, but each relation r as a vector r on a hyperplane with w r as the normal vector. Given a fact ?h; r; t?, the entity representations h and t are first projected onto the hyperplane, resulting in associated with two matrices, one to project head entities and the other tail entities.</p><p>TransD <ref type="bibr" target="#b52">[50]</ref> simplifies TransR by further decomposing the projection matrix into a product of two vectors. Specifically, for each fact ?h; r; t?, TransD introduces additional mapping vectors w h ; w t 2 R d and w r 2 R k , along with the entity/relation representations h; t 2 R d and r 2 R k . Two projection matrices M 1 2 r and M r are accordingly defined as</p><formula xml:id="formula_3">M 1 &gt; 2 &gt; r ? w r w h ? I; M r ? w r w t ? I:</formula><p>These two projection matrices are then applied on the head entity h and the tail entity t respectively to get their projections, i.e.,</p><formula xml:id="formula_4">1 2 h ? ? h ? w &gt; &gt; h ? ? M r h; t ? ? M r t: r hw r ; t ? ? t ? w r tw r :</formula><p>The projections are then assumed to be connected by r on the hyperplane with low error if ?h; r; t? holds, i.e., h ? ? r % t ? . The scoring function is accordingly defined as f r ?h; t? ? ?kh ? ? r ? t ? k 2 2 ; similar to the one used in TransE. By introducing the mechanism of projecting to relation-specific hyperplanes, TransH enables different roles of an entity in different relations.</p><p>TransR <ref type="bibr" target="#b17">[16]</ref> shares a very similar idea with TransH. But it introduces relation-specific spaces, rather than hyperplanes. In TransR, entities are represented as vectors in an entity space R d , and each relation is associated with a specific space R k and modeled as a translation vector in that space. Given a fact ?h; r; t?, TransR first projects the entity representations h and t into the space specific to relation r, i.e., With the projected entities, the scoring function is defined in the same way as in TransR. TransD requires O?nd ? mk? parameters and is more efficient than TransR (which requires O?nd ? mdk? parameters).</p><p>TranSparse <ref type="bibr" target="#b53">[51]</ref> is another work that simplifies TransR by enforcing sparseness on the projection matrix. It has two versions: TranSparse (share) and TranSparse (separate). The former uses the same sparse projection matrix M r ?u r ? for each relation r, i.e.,</p><formula xml:id="formula_5">h ? ? M r ?u r ?h; t ? ? M r ?u r ?t:</formula><p>The latter introduces two separate sparse projection matrices M r ? for that relation, one to project head entities, and the other tail entities, i.e.,</p><formula xml:id="formula_6">h ? ? M 1 1 2 2 r ?u r ?h; t ? ? M r ?u r ?t: h ? ? M r h; t ? ? M r t:</formula><p>Here, u r , u 1 2 r , and u Here M r 2 R k?d is a projection matrix from the entity space to the relation space of r. Then, the scoring function is again defined as f r ?h; t? ? ?kh ? ? r ? t ? k 2 2 : <ref type="figure" target="#fig_0">Fig. 1c</ref> gives a simple illustration of TransR. Although powerful in modeling complex relations, TransR introduces a projection matrix for each relation, which requires O?dk? parameters per relation. So it loses the simplicity and efficiency of TransE/TransH (which model relations as vectors and require only O?d? parameters per relation). An even more complicated version of the same approach was later proposed in <ref type="bibr" target="#b50">[48]</ref>, <ref type="bibr" target="#b51">[49]</ref>. In this version, each relation is r denote sparseness degrees of these projection matrices. The scoring function is again the same with that used in TransR. By introducing sparse projection matrices, TranSparse reduces the number of parameters to O?nd ? ?1 ? u?mdk?, where u is the average sparseness degree of projection matrices.</p><p>Relaxing Translational Requirement h ? r % t. Besides allowing entities to have distinct embeddings when involved in different relations, another line of research improves TransE by relaxing the overstrict requirement of h ? r % t. TransM <ref type="bibr" target="#b54">[52]</ref> associates each fact ?h; r; t? with a weight u r specific to the relation, and defines the scoring function as f r ?h; t? ? ?u r kh ? r ? tk 1=2 :</p><p>By assigning lower weights to 1-to-N, N-to-1, and N-to-N relations, TransM allows t to lie farther away from h ? r in those relations. ManifoldE <ref type="bibr" target="#b55">[53]</ref> relaxes h ? r % t to kh ? r ? tk 2 2 2 % u r for each ?h; r; t? 2 ID ? . As such, t can lie approximately on a manifold, i.e., a hyper-sphere centered at h ? r with a radius of u r , rather than close to the exact point of h ? r. The scoring function is hence defined as Here m m ? m m h ? m m r ? m m t and S ? S h ? S r ? S t . With the help of Gaussian embeddings, KG2E can effectively model uncertainties of entities and relations in KGs.</p><p>TransG <ref type="bibr" target="#b48">[46]</ref> also models entities with Gaussian distributions, i.e.,</p><p>h $ N ?m m h ; s  which is a mixture of translational distances introduced by different semantics of the relation. These semantic components can be learned automatically from the data using the Chinese restaurant process <ref type="bibr" target="#b63">[60]</ref>, <ref type="bibr" target="#b64">[61]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Other Distance Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Gaussian Embeddings</head><p>Methods introduced so far model entities as well as relations as deterministic points in vector spaces. Some recent works take into account their uncertainties, and model them as random variables <ref type="bibr" target="#b47">[45]</ref>, <ref type="bibr" target="#b48">[46]</ref> h $ N ?m m h ; S h ?; t $ N ?m m t ; S t ?; r to project head and tail entities for each relation r, and the score is where m m h ; m m t ; m m r 2 R d are mean vectors, and S h ; S t ; S r 2 R d?d covariance matrices. Then, inspired by the translational assumption, KG2E scores a fact by measuring the distance between the two random vectors of t ? h and r, i.e., the two distributions of N ?m m t ? m m h ; S t ? S h ? and N ?m m r ; S r ?. Two types of distance measures are used. One is the Kullback-Leibler divergence <ref type="bibr" target="#b61">[58]</ref> which defines <ref type="table">Table 1</ref> summarizes entity/relation representations and scoring functions used in these translational distance models. For all the models, there are constraints imposed on them, e.g., enforcing vector embeddings to have, at most, a unit ' 2 norm. Some of the constraints are converted into regularization terms during optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic Matching Models</head><formula xml:id="formula_7">f r ?h; t? ? ? Z N x ?m m t ? m m h ;S t ? S h ?ln N x ?m m t ? m m h ; S t ? S h ? N x ?m m r ; S r ? dx / ?tr?S ?1 ?1 r ?S h ? S t ?? ? m m &gt; S r m m ? ln det?S r ? det?S h ? S t ? ;</formula><p>Semantic matching models exploit similarity-based scoring functions. They measure plausibility of facts by matching latent semantics of entities and relations embodied in their vector space representations. and the other is the probability inner product <ref type="bibr" target="#b62">[59]</ref> which introduces</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">RESCAL and Its Extensions</head><formula xml:id="formula_8">f r ?h; t? ? Z N x ?m m t ? m m h ; S t ? S h ? ? N x ?m m r ; S r ?dx /?m m &gt; S ?1 m m ? ln?det?S??:</formula><p>RESCAL. RESCAL <ref type="bibr" target="#b14">[13]</ref> (a.k.a. the bilinear model <ref type="bibr" target="#b18">[17]</ref>) associates each entity with a vector to capture its latent semantics. Each relation is represented as a matrix which models pairwise interactions between latent factors. The score of a fact ?h; r; t? is defined by a bilinear function</p><formula xml:id="formula_9">X d?1 f r ?h; t? ? h &gt; M r t ? X d?1 ?M r ? ij ? ?h? i ? ?t? j ; i?0 j?0</formula><p>2. To make f r ?h; t? a Mahalanobis distance, M r ought to be positive semidefinite. But <ref type="bibr" target="#b58">[55]</ref> did not impose this constraint, and only required M r to be symmetric and non-negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 1 Summary of Translational Distance Models (See Section 3.1 for Details)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Ent. embedding Rel. embedding Scoring function f r ?h; t? </p><formula xml:id="formula_10">Constraints/Regularization TransE [14] h; t 2 R d r 2 R d ?kh ? r ? tk 1=2 khk 2 ? 1; ktk 2 ? 1 TransH [15] h; t 2 R d r; w r 2 R d ?k?h ? w &gt; r hw r ? ? r ? ?t ? w &gt; r tw r ?k 2 2 khk 2 1; ktk 2 1 jw &gt; r rj=krk 2 ; kw r k 2 ? 1 TransR [16] h; t 2 R d r 2 R k ; M r 2 R k?d ?kM r h ? r ? M</formula><formula xml:id="formula_11">h; t 2 R d r 2 R d ?u r kh ? r ? tk 1=2 khk 2 ? 1; ktk 2 ? 1 ManifoldE [53] h; t 2 R d r 2 R d ??kh ? r ? tk 2 2 2 2 ? u r ? khk 2 1; ktk 2 1; krk 2 1 TransF [54] h; t 2 R d r 2 R d ?h ? r? &gt; t ? ?t ? r? &gt; h khk 2 1; ktk 2 1; krk 2 1 TransA [55] h; t 2 R d r 2 R d ; M r 2 R d?d ??jh ? r ? tj? &gt; M r ?jh ? r ? tj? khk 2 1; ktk 2 1; krk 2 1 kM r k F 1; ?M r ? ij ? ?M r ? ji ! 0 KG2E [45] h $ N ?m m h ; S h ? ? tr?S ?1 ?1 r ?S h ? S t ?? ? m m &gt; S r m m ? ln det?Sr? det?S h ?S t ? km m h k 2 1; km m t k 2 1; km m r k 2 1 t $ N ?m m t ; S t ? r $ N ?m m r ; S r ? ? m m &gt; S ?1 m m ? ln?det?S?? c min I S h c max I m m h ; m m t 2 R d m m r 2 R d ; S r 2 R d?d m m ? m m h ? m m r ? m m t c min I S t c max I S h ; S t 2 R d?d S ? S h ? S r ? S t c min I S r c max I TransG [46] h $ N ?m m h ; s 2 i 2 2 i km m h ?m m i r ?m m t k 2 i h I? m m r $ N m m t ? m m h ; ?s h ? s t ?I ? ? P i p r exp ? 2 s 2 h ?s 2 t km m h k 2 1; km m t k 2 1; km m r k 2 1 t $ N ?m m t ; s 2 i i d t I? r ? P i p r m m r 2 R m m h ; m m t 2 R d UM [56] h; t 2 R d - ?kh ? tk 2 2 khk 2 ? 1; ktk 2 ? 1 SE [57] h; t 2 R d M 1 2 d?d 1 2 r ; M r 2 R ?kM r h ? M r tk 1 khk 2 ? 1; ktk 2 ? 1</formula><p>where h; t 2 R d are vector representations of the entities, and M r 2 R d?d is a matrix associated with the relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>This score captures pairwise interactions between all the components of h and t (see also <ref type="figure" target="#fig_3">Fig. 2a</ref>), which requires O?d 2 ? parameters per relation. <ref type="bibr" target="#b18">[17]</ref> further assumes that all M r decompose over a common set of rank-1 matrices, i.e., <ref type="bibr" target="#b67">[64]</ref> models not only the threeway interaction h &gt; M r t but also two-way interactions, e.g., those between an entity and a relation. The scoring function is f r ?h; The compositional vector is then matched with the relation representation to score that fact, i.e.,</p><formula xml:id="formula_12">M r ? P i p i r u i v &gt; i . TATEC</formula><formula xml:id="formula_13">t? ? h &gt; M r t ? h &gt; r ? t &gt; r ? h &gt; Dt,</formula><formula xml:id="formula_14">?r? i X d?1 f r ?h; t? ? r &gt; ?h ? t? ? X d?1 f r ?h; t? ? h &gt; diag?r?t ? X d?1 ?r? i ? ?h? i ? ?t? i : ?h? k ? ?t? ?k?i? mod d : i?0 k?0 i?0</formula><p>This score captures pairwise interactions between only the components of h and t along the same dimension (see also <ref type="figure" target="#fig_3">Fig. 2b</ref>), and reduces the number of parameters to O?d? per relation. However, since h &gt; diag?r?t ? t &gt; diag?r?h for any h and t, this over-simplified model can only deal with Circular correlation makes a compression of pairwise interactions (see also <ref type="figure" target="#fig_3">Fig. 2c</ref>). So HolE requires only O?d? parameters per relation, which is more efficient than RESCAL. Meanwhile, since circular correlation is not commutative, i.e., h ? t 6 ? t ? h, HolE is able to model asymmetric relations as RESCAL does.</p><p>Complex Embeddings <ref type="bibr">(ComplEx)</ref>. <ref type="bibr">ComplEx [66]</ref> extends DistMult by introducing complex-valued embeddings so as to better model asymmetric relations. In ComplEx, entity and relation embeddings h; r; t no longer lie in a real space <ref type="bibr" target="#b3">3</ref>. Note that we use zero-indexed vectors and matrices for notation brevity in this section.</p><p>4. TATEC can actually use different entity representations in the 2-way and 3-way terms. Here, we omit this for notation brevity.  <ref type="bibr" target="#b19">[18]</ref>, <ref type="bibr" target="#b20">[19]</ref>, <ref type="bibr" target="#b66">[63]</ref>.</p><p>but a complex space, say C d . The score of a fact ?h; r; t? is defined as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Matching with Neural Networks</head><formula xml:id="formula_15">! f r ?h; t? ? Re ? h &gt; diag?r? t ? ? Re X d?1 ?r? i ? ?h? i ? ? t? i ; i?0</formula><p>where t is the conjugate of t and Re??? means taking the real part of a complex value. This scoring function is not symmetric any more, and facts from asymmetric relations can receive different scores depending on the order of entities involved. <ref type="bibr" target="#b70">[67]</ref> recently showed that every ComplEx has an equivalent HolE, and conversely, HolE is subsumed by ComplEx as a special case in which the conjugate symmetry is imposed on embeddings. ANALOGY. ANALOGY <ref type="bibr" target="#b72">[68]</ref> extends RESCAL so as to further model the analogical properties of entities and relations, e.g., "AlfredHitchcock is to Psycho as JamesCameron is to Avatar". It follows RESCAL and employs a bilinear scoring function Semantic Matching Energy (SME). SME <ref type="bibr" target="#b19">[18]</ref> conducts semantic matching using neural network architectures. Given a fact ?h; r; t?, it first projects entities and relations to their vector embeddings in the input layer. The relation r is then combined with the head entity h to get g u h; r ? ?, and with the tail entity t to get g v t; r ? ? in the hidden layer. The score of a fact is finally defined as matching g u and g v by their dot product, i.e., f r ?h; t? ? g u ?h; r? &gt; g v ?t; r?:</p><p>There are two versions of SME: a linear version as well as a bilinear version. SME (linear) defines</p><formula xml:id="formula_16">g u ?h; r? ? M 1 2 u h ? M u r ? b u ; g v ?t; r? ? M 1 2 v t ? M v r ? b v ;</formula><p>while SME (bilinear) defines</p><formula xml:id="formula_17">f r ?h; t? ? h &gt; M r t;</formula><p>g u ?h; r? ? ?M </p><formula xml:id="formula_18">Here, M 1 2 1 2 u ; M u ; M v ; M v 2 R d?d are weight matrices and b u ; b v 2 R d bias vectors shared across different relations. 5 normality : M r M &gt; &gt; r ? M r M r ; 8r 2 IR; commutativity : M r M r 0 ? M r 0 M r ; 8r; r 0 2 IR:</formula><p>Although ANALOGY represents relations as matrices, these matrices can be simultaneously block-diagonalized into a set of sparse almost-diagonal matrices, each of which consists of only O?d? free parameters. It has been shown that the previously introduced methods of DistMult, HolE, and ComplEx can all be subsumed by ANALOGY as special cases in a principled manner.  <ref type="bibr" target="#b20">[19]</ref> is another neural network architecture. Given a fact, it first projects entities to their vector embeddings in the input layer. Then, the two entities h; t 2 R d are combined by a relation-specific tensor M r 2 R d?d?k (along with other parameters) and mapped to a non-linear hidden layer. Finally, a relation-specific linear output layer gives the score 5. In <ref type="bibr" target="#b19">[18]</ref>, SME (bilinear) is defined with three-mode tensors instead of matrices. The two forms are equivalent <ref type="bibr" target="#b74">[70]</ref>. Note also that M</p><formula xml:id="formula_19">1 2 u , M u , M 1 2 v , M v are not necessarily square matrices.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 2 Summary of Semantic Matching Models (See Section 3.2 for Details)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Ent. embedding Rel. embedding Scoring function f r ?h; t?</p><formula xml:id="formula_20">Constraints/Regularization RESCAL [13] h; t 2 R d M r 2 R d?d h &gt; M r t khk 2 1; ktk 2 1; kM r k F 1 M r ? P i p i r u i v &gt; i (required in [17]) TATEC [64] h; t 2 R d r 2 R d ; M r 2 R d?d h &gt; M r t ? h &gt; r ? t &gt; r ? h &gt; Dt khk 2 1; ktk 2 1; krk 2 1 kM r k F 1 DistMult [65] h; t 2 R d r 2 R d h &gt; diag?r?t khk 2 ? 1; ktk 2 ? 1; krk 2 1 HolE [62] h; t 2 R d r 2 R d r &gt; ?h ? t? k hk 2 1; ktk 2 1; krk 2 1 ComplEx [66] h; t 2 C d r 2 C d Re ? h &gt; diag?r? t ? khk 2 1; ktk 2 1; krk 2 1 khk 2 1; ktk 2 1; kM r k F 1 ANALOGY [68] h; t 2 R d M r 2 R d?d h &gt; M r t M r M &gt; &gt; r ? M r M r M r M r 0 ? M r 0 M r SME [18] h; t 2 R d r 2 R d ?M 1 2 1 2 u h ? M u r ? b u ? &gt; ?M v t ? M v r ? b v ? ?M ? ? khk 2 ? 1; ktk 2 ? 1 1 2 1 2 u h??M u r? ? b u ? ? &gt; ?M v t??M v r? ? b v k d?d?k NTN [19] h; t 2 R d r; b r 2 R ; M r 2 R M 1 2 k?d 1 2 khk 2 1; ktk 2 1; krk 2 1 kb r k 2 1; kM ?:;:;i? r k F 1 r ; M r 2 R r &gt; tanh?h &gt; M r t ? M r h ? M r t ? b r ? kM 1 2 r k F 1; kM r k F 1 SLM [19] h; t 2 R d r 2 R k ; M 1 2 k?d 1 2 khk 2 1; ktk 2 1; krk 2 1 r ; M r 2 R r &gt; tanh?M r h ? M r t? kM 1 2 r k F 1; kM r k F 1 MLP [69] h; t 2 R d r 2 R d w &gt; tanh?M 1 h ? M 2 r ? M 3 t? k hk 2 1; ktk 2 1; krk 2 1 ?L? NAM [63] h; t 2 R d r 2 R d f r ?h; t? ? t &gt; z - z ?'? ? ReLU?a ?'? ?; a ?'? ? M ?'? z ?'?1? ? b ?'? z ?0? ? ?h; r? f r ?h; t? ? r &gt; tanh?h &gt; M r t ? M 1 2 r h ? M r t ? b r ?;</formula><p>where M 1 2 r ; M r 2 R k?d and b r 2 R k are relation-specific weight matrices and bias vectors, respectively. The bilinear tensor product h &gt; M r t results in a vector, with the ith entry head entity and the relation in the input layer, which gives z ?0? ? ?h; r? 2 R 2d . The input z ?0? is then fed into a deep neural network consisting of L rectified linear hidden layers such that</p><formula xml:id="formula_21">a ?'? ? M ?'? z ?'?1? ? b ?'? ; ' ? 1; . . . ; L;</formula><p>computed as h &gt; M ?;;;;i? r t. <ref type="figure">Fig. 3b</ref> gives a simple illustration of NTN. By setting all M r ? 0 and b r ? 0, NTN degenerates to the single layer model (SLM) <ref type="bibr" target="#b20">[19]</ref>. NTN might be the most expressive model to date. But it requires O?d 2 k? parameters per relation, and is not sufficiently simple and efficient to handle large-scale KGs.</p><p>Multi-Layer Perceptron (MLP). MLP <ref type="bibr" target="#b73">[69]</ref> is a simpler approach where each relation (as well as entity) is associated with a single vector. As illustrated in <ref type="figure">Fig. 3c</ref>, given a fact ?h; r; t?, the vector embeddings h, r, and t are concatenated in the input layer, and mapped to a non-linear hidden layer. The score is then generated by a linear output layer, i.e.,</p><formula xml:id="formula_22">z ?'? ? ReLU?a ?'? ?; ' ? 1; . . . ; L;</formula><p>where M ?'? and b ?'? represent the weight matrix and bias for the 'th layer respectively. After the feedforward process, the score is given by matching the output of the last hidden layer and the embedding of the tail entity, i.e.,  </p><formula xml:id="formula_23">f r ?h; t? ? t &gt; z ?L? : f r ?h; t? ? w &gt; tanh?M 1 h ? M 2 r ? M 3 t?:</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Training under Open World Assumption</head><p>The open world assumption (OWA) states that KGs contain only true facts and non-observed facts can be either false or just missing <ref type="bibr" target="#b76">[72]</ref>. In this case, ID ? stores only positive examples. Negative examples can be generated by heuristics such as the local closed world assumption <ref type="bibr" target="#b73">[69]</ref> (detailed later in this section). Given the positive set ID ? and a negative set ID ? constructed accordingly, we can learn entity and relation representations Q by minimizing, for instance, the logistic loss, i.e., and initialize word vectors with those pre-trained on a text corpus.</p><p>Generating Negative Training Examples. Given a positive fact t ? ? ?h; r; t? 2 ID ? , negative facts can be generated by replacing either the head h or the tail t with a random entity sampled uniformly from E <ref type="bibr" target="#b15">[14]</ref>, i.e.,</p><formula xml:id="formula_24">ID ? ? f?h 0 ; r; t?jh 0 2 E ^ h 0 6 ? h ^ ?h; r; t? 2 ID ? g [ f?h; r; t 0 ?jt 0 2 E ^ t 0 6 ? t ^ ?h; r; t? 2 ID ? g: X min Q log 1 ? exp??y hrt ? f r ?h; t?? ? ? ;<label>(1)</label></formula><p>Sometimes, negative facts can also be generated by randomly corrupting the relation <ref type="bibr" target="#b19">[18]</ref>, <ref type="bibr" target="#b47">[45]</ref>, i.e.,</p><formula xml:id="formula_25">t2ID ? [ID ?</formula><p>where t ? ?h; r; t? is a training example in ID ? [ ID ? , and y hrt ? AE1 the label (positive or negative) of that example. It has been shown that minimizing the logistic loss can help to find compact representations for some complex relational patterns such as transitive relations <ref type="bibr" target="#b77">[73]</ref>. Besides the logistic loss, we can also use a pairwise ranking loss such as</p><formula xml:id="formula_26">ID ? ? f?h 0 ; r; t?jh 0 2 E ^ h 0 6 ? h ^ ?h; r; t? 2 ID ? g [ f?h; r; t 0 ?jt 0 2 E ^ t 0 6 ? t ^ ?h; r; t? 2 ID ? g [ f?h; r 0 ; t?jr 0 2 ID ^ r 0 6 ? r ^ ?h; r; t? 2 ID ? g: X X min Q max 0; g ? f r ?h; t? ? f r 0 ?h 0 ; t 0 ? ? ? (2) t ? 2ID ? t ? 2ID ?</formula><p>However, this way of uniformly sampling might introduce false-negative training examples, e.g., (AlfredHitchcock, Gender, Male) might be a false-negative example generated for (JamesCameron, Gender, Male) by replacing the head.</p><p>to make the scores of positive facts higher than those of negative ones. 8 Here, t ? ? ?h; r; t? is a positive example, t ? ? ?h 0 ; r 0 ; t 0 ? a negative one, and g a margin separating them. Minimizing the pairwise ranking loss has an additional advantage: it does not assume that negative examples are necessarily false, just that they are more invalid than those positive ones <ref type="bibr" target="#b38">[36]</ref>. Note that in both types of optimization, there are constraints and/or regularization terms specified by different embedding models (see <ref type="table" target="#tab_3">Tables 1 and 2</ref> for details). Trouillon et al. <ref type="bibr" target="#b69">[66]</ref> have shown that the logistic loss generally yields better results for semantic matching models such as DistMult and ComplEx, while the pairwise ranking loss may be more suitable for translational distance models like TransE.</p><p>The optimization Eqs. <ref type="formula" target="#formula_24">(1)</ref> and <ref type="formula">(2)</ref> can be easily carried out by stochastic gradient descent (SGD) <ref type="bibr" target="#b78">[74]</ref> in minibatch mode. After initializing entity and relation embeddings, at each iteration, a small set of positive facts is sampled from ID ? , and for each positive fact, one or more negative facts are generated accordingly. These positive and negative facts then serve as training examples in a minibatch. After the minibatch, embeddings are updated by a gradient step with constant or adaptive learning rates. AdaGrad <ref type="bibr" target="#b79">[75]</ref> is usually used to tune the learning rate. Algorithm 1 summarizes this training procedure, where a single negative is generated for each positive fact. Next, we discuss the initialization step (line 1) and negative example generation step (line 6).</p><p>Initializing Entity and Relation Embeddings. Embeddings for entities and relations are usually initialized randomly from uniform distributions <ref type="bibr" target="#b15">[14]</ref> or Gaussian distributions <ref type="bibr" target="#b69">[66]</ref>. Some complicated models also initialize their embeddings with results of simple models such as TransE <ref type="bibr" target="#b17">[16]</ref>. Another solution is to represent an entity as the average word vector of its name <ref type="bibr" target="#b20">[19]</ref>  To reduce such false-negative examples, <ref type="bibr" target="#b16">[15]</ref> proposed to set different probabilities for replacing the head and the tail, i.e., to give more chance to replacing the head if the relation is 1-to-N and the tail if the relation is N-to-1. Consider, for example, the relation Gender. For triples from this relation, replacing the tail is obviously more likely to generate truenegative examples. Specifically, given a relation and all its positive facts, <ref type="bibr" target="#b16">[15]</ref> first calculates (i) the average number of tail entities per head (denoted as tph), and (ii) the average number of head entities per tail (denoted as hpt). Then, for any positive fact from that relation, <ref type="bibr" target="#b16">[15]</ref> corrupts the fact by replacing the head with probability tph=?tph ? hpt?, and the tail with probability hpt=?tph ? hpt?.</p><p>[77] adopted a different strategy to generate negative facts. Given a positive fact, it corrupts a position (i.e., head or tail) using only entities that have appeared in that position with the same relation. That means, given (JamesCameron, Gender, Male), it could generate a negative fact (JamesCameron, Gender, Female) but never (JamesCameron, Gender, Avatar), as Avatar never appears as a tail entity of the relation.</p><p>8. Sometimes a sigmoid function is imposed on f r ?h; t? to make the score a probability within the range of ?0; 1?.</p><note type="other">Trouillon et al. [66] further investigated the influence of the number of negative facts generated for each positive one.</note><p>Their study showed that generating more negatives usually leads to better results, and 50 negatives per positive example is a good trade-off between accuracy and training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 3 Comparison of Models in Space and Time Complexity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Space complexity Time complexity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Training under Closed World Assumption</head><p>The closed world assumption (CWA) assumes that all facts that are not contained in ID ? are false. In this case, we can learn entity and relation representations Q by minimizing, for instance, the squared loss, i.e.,</p><formula xml:id="formula_27">X min Q y hrt ? f r ?h; t? ? ? 2 ;<label>(3)</label></formula><p>h;t2IE;r2IR</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TransE [14]</head><p>O?nd ? md? O ? d? TransH <ref type="bibr" target="#b16">[15]</ref> O?nd ? md? O ? d? TransR <ref type="bibr" target="#b17">[16]</ref> O?nd</p><note type="other">? mdk? O ? dk? TransD [50] O?nd ? mk? O ? max?d; k?? TranSparse [51] O?nd ? ?1 ? u?mdk? O ? dk? TransM [52] O?nd ? md? O ? d? ManifoldE [53] O?nd ? md? O ? d? TransF [54] O?nd ? md? O ? d? TransA [55] O?nd ? md 2 ? O ? d 2 ? KG2E [45] O?nd ? md? O ? d? TransG [46] O?nd ? mdc? O ? dc? UM [56] O?nd? O ? d? SE [57] O?nd ? md 2 ? O ? d 2 ?</note><p>where y hrt ? 1 if ?h; r; t? 2 ID ? , and y hrt ? 0 otherwise. The squared loss sums over all h; t 2 IE and r 2 IR, enforcing observed facts to have scores close to 1, while non-observed facts scores close to 0. Other possible loss functions include the logistic loss <ref type="bibr" target="#b82">[78]</ref> and absolute loss <ref type="bibr" target="#b83">[79]</ref>, <ref type="bibr" target="#b84">[80]</ref>.</p><p>Minimizing the squared loss amounts to factorization of a three-mode tensor represented by the KG. For example, using the squared loss, the optimization for RESCAL becomes a tensor factorization (or collective matrix factorization) problem</p><note type="other">RESCAL [13] O?nd ? md 2 ? O ? d 2 ? TATEC [64] O?nd ? md 2 ? O ? d 2 ? DistMult [65] O?nd ? md? O ? d? HolE [62] O?nd ? md? O ? d log d? ComplEx [66] O?nd ? md? O ? d? ANALOGY [68]</note><p>O?nd ? md? O ? d? SME (linear) <ref type="bibr" target="#b19">[18]</ref> O?nd ? md? O ? d 2 ? SME (bilinear) <ref type="bibr" target="#b19">[18]</ref> O?nd ? md?</p><formula xml:id="formula_28">O ? d 3 ? NTN [19] O?nd ? md 2 k? O ? d 2 k? SLM [19] O?nd ? mdk? O ? dk? MLP [69] O?nd ? md? O ? d 2 ? NAM [63] O?nd ? md? O ? Ld 2 ? X X min 2 2 2 E;fM r g kY ?:;:;r? ? EM r E &gt; k F ? 1 kEk F ? 2 kM r k F : r2IR r2IR</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Comparison</head><p>Here, Y 2 f0; 1g n?n?m is a three-mode tensor encoding the KG whose entries are set such that ?Y? hrt ? 1 if and only if ?h; r; t? 2 ID ? ; Y ?:;:;r? 2 f0; 1g n?n is the slice holding all facts from relation r; E 2 R n?d stores entity embeddings where each row is a vector representation of an entity; M r 2 R d?d holds the embedding for relation r; and 1 ; 2 ! 0 are regularization coefficients. This optimization problem can be solved efficiently via an alternating least squares (ALS) algorithm, which alternately fixes fM r g to update E and then fixes E to update fM r g. Besides RESCAL, other tensor factorization models such as the CANDECOMP/PAR-AFAC decomposition <ref type="bibr" target="#b85">[81]</ref> and Tucker decomposition <ref type="bibr" target="#b86">[82]</ref> have also been applied on KGs to model multi-relational data <ref type="bibr" target="#b87">[83]</ref>, <ref type="bibr" target="#b88">[84]</ref>, <ref type="bibr" target="#b89">[85]</ref>. For more details about tensor representation of KGs and tensor factorization models, refer to <ref type="bibr" target="#b14">[13]</ref>, <ref type="bibr" target="#b38">[36]</ref> and references therein.</p><p>The closed world assumption has several disadvantages. First, it will not hold for incomplete KGs where a lot of true facts are missing from observed data. However, despite their seemingly huge size, most KGs (e.g., Freebase, DBpedia, and YAGO) are highly incomplete <ref type="bibr" target="#b90">[86]</ref>, making the open world assumption a better choice in this case. In fact, it has been shown that CWA-based models generally perform worse than OWA-based ones in downstream tasks <ref type="bibr" target="#b91">[87]</ref>. <ref type="bibr">And [62]</ref> has also demonstrated that the CWA-based RESCAL model can perform substantially better if trained under the open world assumption, i.e., minimizing the pairwise ranking loss defined in Eq. (2) rather than the squared loss defined in <ref type="figure">Eq. (3)</ref>. Second, the closed world assumption will introduce a huge number of negative examples, which might lead to scalability issues in model training. <ref type="table">Table 3</ref> compares space and time complexity of the different models we have discussed. Here, n and m are the number of entities and relations respectively; d and k the dimensionality of entity and relation embedding space respectively (we usually have d ? k); u in TranSparse the average sparseness degree of projection matrices; c in TransG the average number of semantic components per relation; and L in NAM the total number of hidden layers in the network. See Sections 3.1 and 3.2 for details. During the analysis we assume that d; k ( n and all the models are trained under the open world assumption. We can draw the following conclusions. First, models which represent entities and relations as vectors (e.g., TransE, TransH, DistMult, and ComplEx) are more efficient. They usually have space and time complexity that scales linearly with d. HolE is an exception, as it computes circular correlation via the discrete Fourier transform whose time complexity is O?dlog d?. Second, models which represent relations as matrices (e.g., TransR, SE, and RESCAL) or tensors (e.g., NTN) usually have higher complexity in both space and time, scaling quadratically or cubically with the dimensionality of embedding space. Here ANALOGY is an exception, since the relational matrices can be block-diagonalized into a set of sparse almost-diagonal matrices, each of which consists of only O?d? free parameters. Finally, models based on neural network architectures (e.g., SME, NTN, MLP, and NAM) generally have higher complexity in time, if not in space, since matrix or even tensor computations are often required in these models.</p><p>After comparison in complexity, we discuss performance of these models in downstream tasks. Clearly which model is best depends on the task, and also the data. We limit our discussion to the link prediction task (see Section 5.1.1 for details) and WordNet and Freebase data, which is standard practice in previous studies. An interesting observation is that those seemingly more expressive models do not necessarily have better performance. For instance, it was demonstrated that the NTN model performed slightly worse than the simpler MLP model <ref type="bibr" target="#b73">[69]</ref>, and it even performed worse than TransE and DistMult <ref type="bibr" target="#b68">[65]</ref>, which are almost the simplest KG embedding models. The reason could be that expressive models often require a large number of parameters and tend to overfit on small-and medium-sized datasets. Nickel et al. <ref type="bibr" target="#b65">[62]</ref> </p><note type="other">recently reimplemented some of the models and compared them in the identical setting, i.e., using the ranking loss of Eq. (2) solved by SGD with AdaGrad. They found that HolE performed substantially better than TransE, TransR, RESCAL, and MLP. Trouillon et al. [66] later showed that ComplEx which defines complex-valued embeddings performed even better than HolE.</note><p>ANALOGY, which subsumes DistMult, HolE, and ComplEx as special cases, gave the best link prediction results to date reported on WordNet and Freebase data.</p><p>triples, e.g., <ref type="bibr">(Psycho, IsA, CreativeWork)</ref>. <ref type="bibr" target="#b10">9</ref> A straightforward method to model such information, as investigated in <ref type="bibr" target="#b23">[22]</ref>, is to take IsA as an ordinary relation and the corresponding triples as ordinary training examples.</p><p>Guo et al. <ref type="bibr" target="#b26">[25]</ref> proposed semantically smooth embedding (SSE), which requires entities of the same type to stay close to each other in the embedding space, e.g., Psycho is supposed to stay closer to Avatar than to JamesCameron. SSE employs two manifold learning algorithms, i.e., Laplacian eigenmaps <ref type="bibr" target="#b95">[91]</ref> and locally linear embedding <ref type="bibr" target="#b96">[92]</ref> to model this smoothness assumption. The former requires an entity to lie close to every other entity in the same category, giving a smoothness measure of</p><formula xml:id="formula_29">R 1 ? 1 2 X n X n ke i ? e j k 2 1 2 w ij ; i?1 j?1</formula><p>where e i and e j are the vector embeddings of entities e i and e j respectively; w 1 ij ? 1 if the two entities belong to the same category and w 1 ij ? 0 otherwise. By minimizing R 1 , we expect a small distance between e i and e j whenever w 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Other Approaches</head><p>Besides the aforementioned models, there are other works which learn representations for head-tail entity pairs rather than individual entities <ref type="bibr" target="#b22">[21]</ref>. Specifically, given a triple ?h; r; t?, the relation r is represented as a vector r 2 R d , and the entity pair ?h; t? another vector p 2 R d . The plausibility of the fact can be measured by the inner product of r and p. These vector representations are then learned by minimizing a pairwise ranking loss similar to the one defined in Eq. (2). Such entity pair representation is particularly prevalent in relation extraction, which aims to identify possible relations holding between a pair of entities <ref type="bibr" target="#b92">[88]</ref>. Similarly, one can also model the head entity h as a vector h 2 R d and the relation-tail entity pair ?r; t? as another vector p 2 R d <ref type="bibr" target="#b93">[89]</ref>, <ref type="bibr" target="#b94">[90]</ref>. Nevertheless, such paired formulations have their disadvantages. For instance, if the head-tail entity pairs ?h 1 ; t? and ?h 2 ; t? are modeled via different vector representations, the information that they share the same tail entity t will be lost. And also, relations between unpaired entities, e.g., h 3 and t, cannot be effectively discovered. It also leads to an increased space complexity, since a vector representation is computed for each entity pair which requires O?n 2 d ? md? parameters in total.</p><p>ij ? 1. The latter represents an entity as a linear combination of its nearest neighbors, i.e., entities within the same category. The smoothness measure is defined as</p><formula xml:id="formula_30">R 2 ? X n ke i ? X w 2 2 ij e j k 2 ; i?1 e j 2IN e i</formula><p>where IN e i is the set containing nearest neighbors of entity e i ; w </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">INCORPORATING ADDITIONAL INFORMATION</head><p>ij ? 0 otherwise. 10 By minimizing R 2 , we expect each entity to be linearly reconstructed from its nearest neighbors with low error. R 1 and R 2 are then incorporated as regularization terms to constrain the embedding task. SSE was proved to perform better than the straightforward method in both KG embedding and downstream tasks. A major limitation of SSE is that it assumes entities' semantic categories are non-hierarchical and each entity belongs to exactly one category. This is obviously not the case in typical real-world KGs.</p><p>Xie et al. <ref type="bibr" target="#b27">[26]</ref> devised type-embodied knowledge representation learning (TKRL), which can handle hierarchical entity categories and multiple category labels. TKRL is a translational distance model with type-specific entity projections. Given a fact ?h; r; t?, it first projects h and t with type-specific projection matrices, and then models r as a translation between the two projected entities. The scoring function is accordingly defined as</p><p>The methods introduced so far conduct the embedding task using only facts observed in the KG. In fact, there is a wide variety of additional information that can be incorporated to further improve the task, e.g., entity types, relation paths, textual descriptions, as well as logical rules. In this section, we discuss how such information can be integrated.</p><formula xml:id="formula_31">f r ?h; t? ? ?kM rh h ? r ? M rt tk 1 ;</formula><p>where M rh and M rt are projection matrices for h and t. To handle multiple category labels, M rh is represented as a weighted sum of all possible type matrices, i.e.,  where n h ! 1 is the number of categories to which h belongs; c i the ith category among them; M c i the projection matrix of c i ; a i the corresponding weight; and I C r h the set of types that a head entity can have in relation r. To further handle hierarchical categories, M c i is represented as a composition of the projection matrices associated with all subcategories of c i . Two types of composition operations are used, i.e., Although TKRL achieves good performance in downstream tasks such as link prediction and triple classification, it has a relatively high space complexity since it associates each category with a specific projection matrix.</p><p>Entity types can also be used as constraints of head and tail positions for different relations, e.g., head entities of relation DirectorOf should be those with the type of Person, and tail entities those with the type of CreativeWork. <ref type="bibr" target="#b81">[77]</ref> and <ref type="bibr" target="#b27">[26]</ref> tried to impose such constraints in the training process, particularly during the generation of negative training examples. Negative examples that violate entity type constraints are excluded from training <ref type="bibr" target="#b81">[77]</ref>, or generated with substantially low probabilities <ref type="bibr" target="#b27">[26]</ref>. <ref type="bibr" target="#b97">[93]</ref> imposed similar constraints on RESCAL, a tensor factorization model. The idea is to discard invalid facts with wrong entity types, and factorize only a sub-tensor composed of remaining facts. See <ref type="bibr" target="#b24">[23]</ref>, <ref type="bibr" target="#b98">[94]</ref> for other approaches in which entity type information is also incorporated. strategies. Typical composition operations include addition, multiplication, and recurrent neural network (RNN) <ref type="bibr" target="#b99">[95]</ref>.</p><p>Lin et al. <ref type="bibr" target="#b28">[27]</ref> proposed an extension of TransE to model relation paths, referred to as path-based TransE (PTransE). Given a path p ? r 1 ! ? ? ? ! r ' linking two entities h and t, as well as the vector representations r 1 ; . . . ; r ' of the constituent relations, PTransE considers all the three types of composition operations, i.e.,</p><formula xml:id="formula_32">addition : p ? r 1 ? ? ? ? ? r ' ; multiplication : p ? r 1 ? ? ? r ' ; RNN : c i ? f W?c i?1 ; r i ? ? ? :</formula><p>Here, c i indicates the accumulated path vector at the ith relation; W is a composition matrix shared by all relations; ?c i?1 ; r i ? denotes the concatenation of c i?1 and r i ; and f is a non-linearity function. By setting c 1 ? r 1 and recursively traversing from left to right, one can finally get p ? c ' . The path p is then required to be consistent with a direct relation r between the two entities, i.e., kp ? rk 1 tends to be small if ?h; r; t? holds. For each fact ?h; r; t? 2 ID ? , PTransE defines a loss w.r.t. the paths, i.e., The second kind of additional information we consider is relation paths, i.e., multi-hop relationships between entities. 11 A relation path is typically defined as a sequence of relations r 1 ! ? ? ? ! r ' through which two entities can be connected on the graph. For example, BornIn ! LocatedIn is a path linking AlfredHitchcock to England, via an intermediate node Leytonstone. Relation paths contain rich semantic cues and are extremely useful for KG completion, e.g., BornIn ! LocatedIn is indicative of the relation Nationality between AlfredHitchcock and England.</p><p>Relation paths have long been studied in multi-relational data. For instance, the path ranking algorithms directly use paths connecting two entities as features to predict potential relations between them <ref type="bibr" target="#b39">[37]</ref>, <ref type="bibr" target="#b40">[38]</ref>, <ref type="bibr" target="#b41">[39]</ref>. They have, very recently, been integrated into KG embedding. A key challenge then is how to represent such paths in the same vector space along with entities and relations. A straightforward solution is to represent a path as a composition of the representations of its constituent relations, since the semantic meaning of the path depends on all these relations. <ref type="figure" target="#fig_11">Fig. 4</ref> provides a simple illustration of this idea. <ref type="bibr" target="#b13">12</ref> Actually, almost all the existing approaches handle relation paths using composition where IP?h; t? is the set of all paths connecting h and t; R?pjh; t? indicates the reliability of a path p given the two entities; Z ? P p2IP?h;t? R?pjh; t? is a normalization factor; and '?p; r? is a loss specified on the path-relation pair ?p; r?. The path reliability R?pjh; t? can be calculated by a networkbased resource-allocation mechanism <ref type="bibr" target="#b100">[96]</ref>, and the loss '?p; r? is defined as</p><formula xml:id="formula_33">X '?p; r? ? max?0; g ? kp ? rk 1 ? kp ? r 0 k 1 ?; r 0</formula><p>which prefers a lower value of kp ? rk 1 than of any kp ? r 0 k 1 . Finally, to learn entity and relation representations, L path is aggregated over all the facts in ID ? and then combined with the original TransE loss. Experimental results showed that by further incorporating relation paths, PTransE can perform substantially better than TransE in KG completion and relation extraction.</p><p>Guu et al. <ref type="bibr" target="#b29">[28]</ref> proposed a similar framework, the idea of which is to build triples using entity pairs connected not only with relations but also with relation paths. For example, given a pair of entities ?h; t? and a path p ? r 1 ! ? ? ? ! r ' between them, a new triple ?h; p; t? can be constructed. To model such path-connected triples, Guu et al. devised extensions of both the TransE model and the RESCAL <ref type="bibr" target="#b12">11</ref>. Facts themselves are regarded as one-hop relationships. 12. For simplicity, relations are represented as vectors in this figure. But, they can also be represented in other forms, e.g., matrices <ref type="bibr" target="#b29">[28]</ref>. model. The former uses the addition composition, and defines the score of ?h; p; t? as</p><formula xml:id="formula_34">f p ?h; t? ? ?kh ? ?r 1 ? ? ? ? ? r ' ? ? tk 1 ;</formula><p>while the latter chooses the multiplication composition and defines the score as product (similarity) between them are hence meaningful. The joint model has three components: knowledge model, text model, and alignment model. The knowledge model is to embed entities and relations in the KG. It is a variant of TransE, with a loss L K to measure the fitness to KG facts. The text model is to embed words in the text corpus. It is a variant of Skip-gram <ref type="bibr" target="#b106">[101]</ref>, with a loss L T to measure the fitness to co-occurring word pairs. Finally, the alignment model guarantees the embeddings of entities/relations and words lie in the same space. Different mechanisms of alignment are introduced, e.g., alignment by entity names <ref type="bibr" target="#b31">[30]</ref>, by Wikipedia anchors <ref type="bibr" target="#b31">[30]</ref>, and by entity descriptions <ref type="bibr" target="#b32">[31]</ref>. A loss L A is defined to measure the quality of alignment. The joint model then amounts to minimizing a loss aggregated from the three components, i.e.,</p><formula xml:id="formula_35">f p ?h; t? ? h &gt; ?M 1 ? ? ? M ' ?t: L ? L K ? L T ? L A :</formula><p>Path-connected triples are then treated the same as those relation-connected ones during training. This approach was shown to perform well in answering path queries on KGs.</p><p>A more limited version of the same approach was simultaneously introduced in <ref type="bibr" target="#b101">[97]</ref>. While incorporating relation paths improves model performance, the huge number of paths poses a critical complexity challenge. Both <ref type="bibr" target="#b28">[27]</ref> and <ref type="bibr" target="#b29">[28]</ref> had to make approximations by sampling or pruning. To enable efficient path modeling, <ref type="bibr" target="#b30">[29]</ref> proposed a dynamic programming algorithm which can incorporate all relation paths of bounded length. Moreover, in this work, not only relations but also intermediate nodes (i.e., entities) are modeled in compositional path representations. For other related work, please refer to <ref type="bibr" target="#b102">[98]</ref>, <ref type="bibr" target="#b104">[99]</ref>, <ref type="bibr" target="#b105">[100]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Textual Descriptions</head><p>This section discusses the incorporation of textual descriptions for entities. Actually, in most KGs there are concise descriptions for entities which contain rich semantic information about them. <ref type="figure" target="#fig_13">Fig. 5</ref> shows the descriptions for AlfredHitchcock and Psycho in Freebase. Besides entity descriptions stored in KGs, it can be extended to incorporate more general textual information such as news releases <ref type="bibr" target="#b20">[19]</ref> and Wikipedia articles <ref type="bibr" target="#b31">[30]</ref>, <ref type="bibr" target="#b32">[31]</ref>, <ref type="bibr" target="#b35">[33]</ref>.</p><p>Embedding KGs with textual information dates back to the NTN model <ref type="bibr" target="#b20">[19]</ref>, where textual information is simply used to initialize entity representations. Specifically, NTN first learns word vectors from an auxiliary news corpus, and then initializes the representation of each entity by averaging the vectors of words contained in its name. For example, the embedding of AlfredHitchcock is initialized by the average word vectors of "alfred" and "hitchcock". A similar method was later proposed in <ref type="bibr" target="#b80">[76]</ref>, which represents entities as average word vectors of their descriptions rather than just their names. This kind of methods model textual information separately from KG facts, and hence fail to leverage interactions between them.</p><p>Wang et al. <ref type="bibr" target="#b31">[30]</ref> proposed the first joint model which can make better use of textual information during embedding.</p><p>The key idea is to align the given KG with an auxiliary text corpus, and then jointly conduct KG embedding and word embedding. As such, entities/relations and words are represented in the same vector space and operations like inner Jointly embedding utilizes information from both structured KGs and unstructured text. KG embedding and word embedding can thus be enhanced by each other. Moreover, by aligning these two types of information, jointly embedding enables the prediction of out-of-KG entities, i.e., phrases appearing in web text but not included in the KG yet.</p><p>Xie et al. <ref type="bibr" target="#b34">[32]</ref> proposed description-embodied knowledge representation learning (DKRL), the aim of which is to extend TransE so as to further handle entity descriptions. DKRL associates each entity e with two vector representations, i.e., a structure-based e s and a description-based e d . The former captures structural information conveyed in KG facts, while the latter captures textual information expressed in the entity description. The description-based representation is constructed by the constituent word embeddings, via either a continuous bag-of-words encoder or a convolutional neural network encoder. Given a fact ?h; r; t?, DKRL defines the scoring function as</p><formula xml:id="formula_36">f r ?h; t? ? ? kh s ? r ? t s k 1 ? kh d ? r ? t d k 1 ? kh s ? r ? t d k 1 ? kh d ? r ? t s k 1 ;</formula><p>where r is the vector representation of the relation, shared by both structure-based h s /t s and description-based h d /t d . Entity, relation, and word embeddings can then be learned simultaneously by minimizing the ranking loss defined in Eq. <ref type="formula">(2)</ref>. Experimental results demonstrated the superiority of DKRL over TransE, particularly in the zero-shot scenario with out-of-KG entities. <ref type="bibr">Wang et al. [33]</ref> recently proposed a text-enhanced KG embedding model, referred to as TEKE. Given a KG and a text corpus, TEKE first annotates entities in the corpus and constructs a co-occurrence network composed of entities and words. Then, for each entity e, TEKE defines its textual context n?e? as its neighbors in the co-occurrence network, i.e., words co-occurring frequently with the entity in the text corpus. A textual context embedding n?e? is further introduced for that entity, defined as the weighted average of the word vectors in n?e?. <ref type="bibr" target="#b14">13</ref> For each relation r in a fact <ref type="bibr" target="#b14">13</ref>. Word vectors are pre-trained using the word2vec tool <ref type="bibr" target="#b106">[101]</ref>. They are not learned jointly with entity and relation embeddings.</p><note type="other">?h; r; t?, TEKE defines its textual context as the common neighbors of h and t, i.e., n?h; t? ? n?h? \ n?t?. A textual context embedding</note><p>n?h; t? is similarly defined for that relation. Textual context embeddings are then incorporated into traditional methods, e.g., TransE, to learn more expressive entity and relation representations such as</p><formula xml:id="formula_37">^ h ? An?h? ? h; ^ t ? An?t? ? t; ^ r ? Bn?h; t? ? r:</formula><p>Here, A, B are weight matrices, and h, t, r bias vectors. This extension also applies for TransH and TransR. By incorporating textual context embeddings, TEKE was proved to outperform the original models of <ref type="bibr">TransE, TransH, and TransR. See [102]</ref>, <ref type="bibr" target="#b108">[103]</ref>, <ref type="bibr" target="#b109">[104]</ref>, <ref type="bibr" target="#b110">[105]</ref> for other approaches where textual information is also considered. the constituent ground atoms, via specific t-norm based logical connectives, e.g.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Logical Rules</head><formula xml:id="formula_38">I?f 1 ) f 2 ? ? I?f 1 ? ? I?f 2 ? ? I?f 1 ? ? 1; I?f 1 ^ f 2 ) f 3 ? ? I?f 1 ? ? I?f 2 ? ? I?f 3 ? ? I?f 1 ? ? I?f 2 ? ? 1:</formula><p>Finally we consider the incorporation of logical rules, particularly those represented in terms of first-order Horn clauses, e.g., 8x; y : HasWife?x; y? ) HasSpouse?x; y? stating that any two entities linked by the relation HasWife should also be linked by the relation HasSpouse. Such logical rules contain rich background information and have been widely studied in knowledge acquisition and inference, usually on the basis of Markov logic networks <ref type="bibr" target="#b42">[40]</ref>, <ref type="bibr" target="#b43">[41]</ref>, <ref type="bibr" target="#b44">[42]</ref>. There are also systems such as WARMR <ref type="bibr" target="#b111">[106]</ref>, ALEPH <ref type="bibr" target="#b112">[107]</ref>, and AMIE <ref type="bibr" target="#b113">[108]</ref>, <ref type="bibr" target="#b114">[109]</ref> which can extract logical rules automatically from KGs. Recently, there has been growing interest in combining logical rules with KG embedding models. <ref type="bibr">Wang et al. [23]</ref> tried to utilize rules to refine embedding models during KG completion. In their work, KG completion is formulated as an integer linear programming problem, with the objective function generated from embedding models and the constraints from rules. Facts inferred in this way will be the most preferred by the embedding models and comply with all the rules. A similar work that combines rules and embedding models via Markov logic networks was later introduced in <ref type="bibr" target="#b25">[24]</ref>. However, in <ref type="bibr" target="#b24">[23]</ref> and <ref type="bibr" target="#b25">[24]</ref>, rules are modeled separately from embedding models, serving as post-processing steps, and thus will not help to obtain better embeddings.</p><p>Guo et al. <ref type="bibr" target="#b36">[34]</ref> proposed a joint model which embeds KG facts and logical rules simultaneously. A key ingredient of this approach, called KALE, is to represent and model facts and rules in a unified framework. Specifically, a fact ?h; r; t? is taken as a ground atom, with its truth value defined as I?h; r; t?</p><formula xml:id="formula_39">? 1 ? 1 3 ffiffi ffi d p kh ? r ? tk 1 ;</formula><p>where d is the dimension of the embeddings, and I?h; r; t? 2 ?0; 1? a linearly transformed version of the TransE score indicating how likely the fact holds. Logical rules are first instantiated into ground rules, e.g., the universally quantified rule 8x; y : HasWife?x; y? ) HasSpouse?x; y? can be grounded into HasWife?AlfredHitchcock; AlmaReville? ) HasSpouse?AlfredHitchcock; AlmaReville?. Ground rules are then interpreted as complex formulae constructed by combining ground atoms with logical connectives (e.g., ^ and )), and modeled by t-norm fuzzy logics <ref type="bibr" target="#b115">[110]</ref>. The truth value of a ground rule is a composition of the truth values of This value also lies within the range of ?0; 1?, indicating to what degree the ground rule is satisfied. In this way, KALE represents facts and rules in a unified framework, as atomic and complex formulae respectively. <ref type="figure" target="#fig_14">Fig. 6</ref> provides a simple illustration of this framework. After unifying facts and rules, KALE minimizes a global loss involving both of them to learn entity and relation embeddings. The learned embeddings are hence compatible not only with facts but also with rules, which can be more effective for knowledge acquisition and inference. Rockt aschel et al.</p><p>[35] devised a model similar to KALE. In their work, however, vector embeddings are introduced for entity pairs rather than individual entities, making it particularly useful for relation extraction. Since entities do not have their own embeddings, relations between unpaired entities cannot be effectively discovered. Both <ref type="bibr" target="#b36">[34]</ref> and <ref type="bibr" target="#b37">[35]</ref> share a common drawback: they have to instantiate universally quantified rules into ground rules before learning their models. This grounding procedure could be extremely time and space inefficient especially when there are a huge number of entities in the KG and/ or the rules are complicated themselves. To address this drawback, Demeester et al. <ref type="bibr" target="#b116">[111]</ref> recently proposed an extension of <ref type="bibr" target="#b37">[35]</ref>, the key idea of which is to impose logical implications by regularizing relation embeddings so as to avoid grounding. For example, given a universally quantified rule 8x; y : HasWife?x; y? ) HasSpouse?x; y?, <ref type="bibr" target="#b116">[111]</ref> tried to model it by using only the embeddings of the two relations HasWife and HasSpouse, without instantiating x and y with concrete entities in the KG. Nevertheless, this strategy works only for rules in the simplest form of 8x; y : r i ?x; y? ) r j ?x; y?, and cannot be generalized to more complicated rules. For other related work, please refer to <ref type="bibr" target="#b117">[112]</ref>, <ref type="bibr" target="#b118">[113]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Other Information</head><p>Besides the four types of additional information mentioned above, there are also a few studies which tried to incorporate other information into KG embedding.</p><p>Entity Attributes. Nickel et al. <ref type="bibr" target="#b23">[22]</ref> argued that relations in KGs can indicate both relationships between entities (e.g., (AlfredHitchcock, HasWife, AlmaReville)) and attributes of entities (e.g., <ref type="bibr">(AlfredHitchcock, Gender, Male)</ref>), but most KG embedding techniques do not explicitly distinguish between relations and attributes. Take the tensor factorization model RESCAL as an example. In this model, each KG relation is encoded as a slice of the tensor, no matter it indicates a true relation or just an attribute. This naive handling of attributes will dramatically increase the dimensionality of the tensor, while a huge amount of entries in this tensor, however, will be wasted. <ref type="bibr" target="#b15">14</ref> To address this problem, <ref type="bibr" target="#b23">[22]</ref> proposed to explicitly distinguish attributes from relations. Relations are still encoded in a tensor, while attributes in a separate entity-attribute matrix. This matrix is then factorized jointly with the tensor to learn representations simultaneously for entities, relations, and attributes. A similar idea was later studied in a translational distance model <ref type="bibr" target="#b119">[114]</ref>.</p><p>Temporal Information. Jiang et al. <ref type="bibr" target="#b120">[115]</ref> observed that KG facts are usually time-sensitive, e.g., (AlfredHitchcock, BornIn, Leytonstone) happened in the year of 1899, while (AlfredHitchcock, DiedIn, BelAir) took place in the year of 1980. Based on this observation they proposed a time-aware embedding model. The idea of this model is to impose temporal order constraints on time-sensitive relation pairs, e.g., BornIn and DiedIn. Given such a pair ?r i ; r j ?, the prior relation is supposed to lie close to the subsequent relation after a temporal transition, i.e., Mr i % r j , where M is a transition matrix capturing the temporal order information between relations. After imposing such temporal order constraints, <ref type="bibr" target="#b120">[115]</ref> is able to learn temporally consistent relation embeddings. <ref type="bibr">Esteban et al. [116]</ref> tried to model the temporal evolution of KGs. In their model, changes in a KG always arrive as events, represented by labeled quadruples such as ?h; r; t; s; True? or ?h; r; t; s; False?, indicating that the fact ?h; r; t? appears or vanishes at time s, respectively. Each quadruple is then modeled as a four-way interaction among h, r, t, and s, where s is the vector representation of the time stamp. This model was shown to perform well in dynamic domains, e.g., medical and sensor data. <ref type="bibr">Trivedi et al. [117]</ref> recently proposed to learn non-linearly evolving entity representations over time so as to perform temporal reasoning over dynamic KGs. Each fact in a dynamic KG is represented as a quadruple ?h; r; t; s?, indicating the creation of relation r between head entity h and tail entity t at time s. A temporal point process <ref type="bibr" target="#b123">[118]</ref> is then employed to model occurrence of facts, with a bilinear scoring function to capture multi-relational interactions between entities, and a deep recurrent neural network to learn non-linearly evolving entity representations. This approach performed quite well in link prediction, in particular, time prediction.</p><p>Graph Structures. Feng et al. <ref type="bibr" target="#b124">[119]</ref> proposed a graphaware embedding model which learns entity and relation representations by leveraging three types of graph structures. The first is neighbor context, which is actually equivalent to triples observed in a KG. The second is path context, very similar to relation paths discussed in Section 4.2. <ref type="bibr" target="#b16">15</ref> The last is edge context, which has not been considered in previously introduced methods. Given a specific entity, its edge context is defined as all kinds of relations linking to and from that entity, based simply on the intuition that all these relations are representative of that entity. For example, the edge context of AlfredHitchcock might include relations such as BornIn, DiedIn, HasWife, and DirectorOf, all indicating AlfredHitchcock to be a Person or more specifically, a Director. Experimental results further demonstrated the effectiveness of modeling these graph structures. <ref type="bibr">Jiang et al. [121]</ref> suggested to estimate the plausibility of a fact from its immediate context. The immediate context of a fact ?h; r; t? is defined as: (i) triples where h is the head, (ii) triples where h is the tail, (iii) triples where t is the head, (iv) triples where t is the tail, and (v) triples with arbitrary relations but where the two entities are h and t. This work was shown effective in predicting links on multi-relational data.</p><p>Evidence from Other Relational Learning Methods. There is another line of research which combines KG embedding with other relational learning methods, e.g., the path ranking algorithm (PRA), to take the strengths of different types of methods. <ref type="bibr">Dong et al. [69]</ref> proposed to combine MLP with PRA via a fusion system. Specifically, after fitting the two models separately, they used the outputs of MLP and PRA as scalar features, and learned a final fusion layer by training a binary classifier. They found that fusing these two models improves performance: the fused system obtained a result of 0.911 for the area under the ROC curve, as compared to 0.882 for MLP and 0.884 for PRA on their specific dataset. Nickel et al. <ref type="bibr" target="#b127">[122]</ref> designed a generic framework to combine latent and observable variable models. In particular, if it combines RESCAL with PRA, the scoring function becomes f r ?h; t? ? h &gt; M r t ? w &gt; r f f ht . The first term is the RESCAL score and the second term the PRA score, in which f f ht is a feature vector composed of path features and w r holds the weights of these features. This is a joint model which can be trained by alternately optimizing the RESCAL parameters with the PRA parameters. After the combination, RESCAL only needs to model the "residual errors" that cannot be modeled by PRA, which requires lower latent dimensionality and speeds up training. For more details about these combined models, refer to <ref type="bibr" target="#b38">[36]</ref> and references therein.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">APPLICATIONS IN DOWNSTREAM TASKS</head><p>After a systematic review of currently available KG embedding techniques, this section explores how the learned entity and relation embeddings can be applied to and benefit a wide variety of downstream tasks. We categorize such tasks into (i) in-KG applications and (ii) out-of-KG applications, discussed as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">In-KG Applications</head><p>In-KG applications are those conducted within the scope of the KG where entity and relation embeddings are learned. We introduce four such applications, i.e., link prediction, triple classification, entity classification, and entity resolution, which have been extensively studied in the literature. All these applications are sorts of refinement (e.g., completion or de-duplication) of the input KG <ref type="bibr" target="#b128">[123]</ref>, from different viewpoints and application context. 14. Note that as opposed to true entities (e.g., AlfredHitchcock and AlmaReville), attribute values (e.g., Male) might never occur as head entities in a relation or even as tail entities in other relations except for the specific attribute.</p><p>15. The difference lies in that Section 4.2 considers only relations in a path, while path context further takes into account intermediate nodes in that path, as practiced in <ref type="bibr" target="#b30">[29]</ref>, <ref type="bibr" target="#b105">[100]</ref>, <ref type="bibr" target="#b125">[120]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Link Prediction</head><p>Link prediction is typically referred to as the task of predicting an entity that has a specific relation with another given entity, i.e., predicting h given ?r; t? or t given ?h; r?, with the former denoted as ??; r; t? and the latter as ?h; r; ??. For example, (?, DirectorOf, Psycho) is to predict the director of the film, while <ref type="bibr">(AlfredHitchcock, DirectorOf, ?)</ref> amounts to predicting films directed by that specific person. This is essentially a KG completion task, i.e., adding missing knowledge to the graph, and has been tested extensively in previous literature <ref type="bibr" target="#b15">[14]</ref>, <ref type="bibr" target="#b16">[15]</ref>, <ref type="bibr" target="#b17">[16]</ref>, <ref type="bibr" target="#b65">[62]</ref>. This link prediction task is also sometimes called entity prediction <ref type="bibr" target="#b28">[27]</ref> or entity ranking <ref type="bibr" target="#b19">[18]</ref>. A similar idea can also be used to predict relations between two given entities, i.e., ?h; ?; t?, which is usually referred to as relation prediction <ref type="bibr" target="#b27">[26]</ref>, <ref type="bibr" target="#b28">[27]</ref>.</p><p>With entity and relation representations learned beforehand, link prediction can be carried out simply by a ranking procedure. Take the prediction task ??; r; t? as an example. To predict the head, one can take every entity h 0 in the KG as a candidate answer and calculate a score f r ?h 0 ; t? for each ?h 0 ; r; t?. This can easily be achieved by using the learned embeddings and scoring function once an embedding model has been trained on the KG, e.g., f r ?h 0 ; t? ? ?kh 0 ? r ? tk 1=2 if TransE has been employed for KG embedding. Ranking these scores in descending order will result in a ranked list of candidate answers. For instance, given the prediction task (?, DirectorOf, Psycho), one may generate an ordered list {JamesCameron, AlfredHitchcock, GeorgeLucas, QuentinTarantino} by using this ranking procedure. The prediction task ?h; r; ?? or ?h; ?; t? can be carried out in a similar manner.</p><p>For evaluation, a common practice is to record ranks of correct answers in such ordered lists, so as to see whether correct answers can be ranked before incorrect ones. In the forementioned example of <ref type="bibr">(?, DirectorOf, Psycho)</ref>, the correct answer AlfredHitchcock gets a rank of 2. Lower ranks indicate better performance. Various evaluation metrics have been designed based on such ranks, e.g., mean rank (the average of predicted ranks), mean reciprocal rank (the average of reciprocal ranks), Hits@n (the proportion of ranks no larger than n), and AUC-PR (the area under the precision-recall curve).</p><p>as true if its score f r ?h; t? is higher than d r , and as false otherwise. <ref type="bibr" target="#b17">16</ref> In this way, we obtain a triple classifier for each relation. Traditional metrics for classification can be used to evaluate this task, e.g., micro-and macro-averaged accuracy <ref type="bibr" target="#b26">[25]</ref>. Since for each triple a real valued score will be output along with the binary label, ranking metrics can also be used here, e.g., mean average precision <ref type="bibr" target="#b36">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Entity Classification</head><p>Entity classification aims to categorize entities into different semantic categories, e.g., AlfredHitchcock is a Person, and Psycho a CreativeWork. Given that in most cases the relation encoding entity types (denoted as IsA) is contained in the KG and has already been included into the embedding process, entity classification can be treated as a specific link prediction task, i.e., ?x; IsA; ??. Similar prediction and evaluation procedures can be applied here (see Section 5.1.1 for details). Entity classification is obviously a KG completion problem, and has been studied in <ref type="bibr" target="#b14">[13]</ref> and <ref type="bibr" target="#b23">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Entity Resolution</head><p>Entity resolution consists in verifying whether two entities refer to the same object. In some KGs many nodes actually refer to identical objects, e.g., in the Cora dataset <ref type="bibr" target="#b129">[124]</ref> which contains citations with the fields of author, title, and venue, the name of an author or a venue can be written in different ways. Entity resolution is the task that de-duplicates such nodes <ref type="bibr" target="#b14">[13]</ref>, <ref type="bibr" target="#b19">[18]</ref>.</p><p>Bordes et al. <ref type="bibr" target="#b19">[18]</ref> considered a scenario where the KG already contains a relation stating whether two entities are equivalent (denoted as EqualTo) and an embedding has been learned for that relation. In this case, entity resolution degenerates to a triple classification problem, i.e., to judge whether the triple ?x; EqualTo; y? holds or how likely this triple holds. Triple scores output by an embedding model can be directly used for such prediction (see Section 5.1.2 for details). This intuitive strategy, however, does not always work since not all KGs encode the EqualTo relation. <ref type="bibr">Nickel et al. [13]</ref> proposed to perform entity resolution solely on the basis of entity representations. More specifically, given two entities x, y and their vector representations x, y, the similarity between x and y is computed as k?x; y? ? e ?kx?yk 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Triple Classification</head><p>Triple classification consists in verifying whether an unseen triple fact ?h; r; t? is true or not, e.g., <ref type="bibr">(AlfredHitchcock, DirectorOf, Psycho)</ref> should be classified as a true fact while (JamesCameron, DirectorOf, Psycho) a false one. This task, again, can be seen as some sort of completion of the input KG, which has also been studied extensively in previous works <ref type="bibr" target="#b16">[15]</ref>, <ref type="bibr" target="#b17">[16]</ref>, <ref type="bibr" target="#b20">[19]</ref>.</p><p>Recall that once an embedding model has been learned on the KG, we can calculate a score for any triple ?h; r; t? as long as h; t 2 IE and r 2 IR, e.g., f r ?h; t? ? ?kh ? r ? tk 1=2 if the TransE model has been learned. Triple classification can then be carried out simply on the basis of such triple scores. Triples with higher scores tend to be true facts. Specifically, we introduce for each relation r a threshold d r . Then any unseen fact from that relation, say ?h; r; t?, will be predicted 2 =s , and this similarity score is used to measure the likelihood that x and y refer to the same entity. The new strategy works even if the EqualTo relation is not encoded in the input KG. AUC-PR is the most widely adopted evaluation metric for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Out-of-KG Applications</head><p>Out-of-KG applications are those which break through the boundary of the input KG and scale to broader domains. We introduce three such applications as examples, including relation extraction, question answering, and recommender systems. We do not seek to provide systematic reviews of these tasks or introduce the state-of-the-arts. But instead we focus particularly on showing how KG embedding can be <ref type="bibr" target="#b17">16</ref>. The relation-specific threshold d r can be determined by using a small set of facts observed for that relation, either a subset of ID ? or a separate development set. applied to these domains. And we hope they can provide new insights into future application of KG embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Relation Extraction</head><p>Relation extraction aims to extract relational facts from plain text where entities have already been detected. For example, given a sentence "Alfred Hitchcock directed Psycho" with the entities h ? AlfredHitchcock and t ? Psycho detected, a relation extractor should predict the relation DirectorOf between these two entities. Relation extraction has long been a crucial task in natural language processing, and provides an effective means to enrich KGs. Much research has tried to leverage KGs for this task, but usually as distant supervision to automatically generate labeled data <ref type="bibr" target="#b10">[9]</ref>, <ref type="bibr" target="#b130">[125]</ref>, <ref type="bibr" target="#b131">[126]</ref>, <ref type="bibr" target="#b132">[127]</ref>. Such approaches are still text-based extractors, ignoring the capability of a KG itself to reason new facts.</p><p>Recently, Weston et al. <ref type="bibr" target="#b21">[20]</ref> proposed to combine TransE with a text-based extractor so as to better perform relation extraction. Specifically, in the training phase, they learned a text-based extractor from a text corpus and a TransE model from a KG aligned to that corpus. The text-based extractor scores the similarity between each relation r and its textual mention m, i.e., S text ?m; r?. These scores can then be used to predict relations from their textual mentions, i.e., evidence from the text corpus. Meanwhile, the TransE model scores the plausibility of each missing fact ?h; r; t? in the KG, i.e., S KG ?h; r; t?. 17 These scores can be used to predict relations from their interactions with entities in the KG, i.e., evidence from the KG. In the test phase, given two entities h, t and all relation mentions M h;t associated with them, a prediction ^ r is first made with the text-based extractor, and a composite score is then introduced for the candidate fact, <ref type="bibr" target="#b19">18</ref> i.e., [93] later devised a tensor-based variant, which encodes plain text and KGs in a three-mode tensor and then factorizes the tensor by using the RESCAL model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Question Answering</head><p>This article considers a specific question answering task, i.e., question answering over KGs. Given a question expressed in natural language, the task is to retrieve the correct answer supported by a triple or set of triples from a KG <ref type="bibr" target="#b12">[11]</ref>, <ref type="bibr" target="#b13">[12]</ref>.</p><p>Here we show some examples of questions, answers, and supporting triple(s):</p><p>X S text?KG ?h; ^ r; t? ? S text ?m; ^ r? ? S KG ?h; ^ r; t?: m2M h;t</p><p>This composite model favors predictions that agree with not only the textual mentions but also the KG. Experimental results further showed that incorporating the TransE model can successfully improve over traditional text-based extractors. Similar improvements have also been observed after incorporating TransH <ref type="bibr" target="#b16">[15]</ref> and TransR <ref type="bibr" target="#b17">[16]</ref>. <ref type="bibr">Riedel et al. [21]</ref> devised a different framework which performs relation extraction by jointly embedding plain text and KGs. In their work, text and KGs are represented in the same matrix. Each row of the matrix stands for a pair of entities, and each column a textual mention or a KG relation. If two entities co-occur with a mention in plain text or with a relation in KGs, the corresponding entry is set to one, and otherwise to zero. For training instances (entity pairs), one can observe both textual mentions and KG relations, with the latter as distant supervision. But for test instances, only textual mentions are available. Relation extraction then is to predict missing KG relations for test instances. <ref type="figure" target="#fig_15">Fig. 7</ref> gives a simple illustration of this scenario. Collaborative filtering techniques are further employed for this task, which factorize the input matrix to learn vector embeddings for entity pairs, textual mentions, Who directed Psycho? -AlfredHitchcock (AlfredHitchcock, DirectorOf, Psycho) Where was A. Hitchcock born? -Leytonstone (AlfredHitchcock, BornIn, Leytonstone) What was the nationality of A. Hitchcock? -England (AlfredHitchcock, BornIn, Leytonstone) (Leytonstone, LocatedIn, England)</p><p>The use of KGs simplifies question answering by organizing a great variety of answers in a structured format. However, it remains a challenging task because of the great variability of natural language and of the large scale of KGs.</p><p>Bordes et al. <ref type="bibr" target="#b12">[11]</ref>, <ref type="bibr" target="#b13">[12]</ref> introduced an embedding-based framework for this task. The key idea of their approach is to learn low-dimensional vector embeddings of words and of KG constituents, so that representations of questions and of their corresponding answers are close to each other in the embedding space. Specifically, let q denote a question and a a candidate answer. A function S?q; a?, based on vector embeddings, is designed to score the similarity between the question and the answer, i.e.,</p><formula xml:id="formula_40">S?q; a? ? ? Wf f?q? ? &gt; ? Wc c?a? ? :</formula><p>Here W is a matrix containing embeddings of words, entities, and relations; f f?q? and c c?a? are two sparse vectors, the former indicating occurrences of words in the question, and the latter occurrences of entities and relations in the answer. <ref type="bibr" target="#b20">19</ref> 17. S KG ?h; r; t? is a variant of the TransE score f r ?h; t?.</p><p>18. If the prediction ^ r is NA, i.e., a marker indicating that there is no relation between h and t, S KG ?h; ^ r; t? will not be introduced.</p><p>19. The answer can be a single entity, a triple, a relation path, or even a subgraph <ref type="bibr" target="#b13">[12]</ref>. If, for example, it is a single entity, c c?a? is a onehot vector with 1 corresponding to the entity, and 0 elsewhere.</p><p>Wf f?q? and Wc c?a? are vector representations of the question and answer respectively in the embedding space. Both are combinations of embeddings of their constituents, i.e., words, entities, and relations. S??; ?? generates a high score if a is the correct answer to the question q, and a low score otherwise. Given a training set consisting of questions paired with their correct answers, the embeddings W can be learned by using typical pairwise ranking optimization, which enforces the score of a correct pair to be higher than that of any incorrect one. The training set can be created by crowdsourcing <ref type="bibr" target="#b134">[129]</ref> or by automatically generalizing seed patterns over KGs <ref type="bibr" target="#b135">[130]</ref>. Once W is trained, at test time, for a given question q the answer is predicted as</p><formula xml:id="formula_41">i : j 1 1 j 2 1 ? ? ? 1 j n , u &gt; &gt; &gt; i e j 1 &gt; u i e j 2 &gt; ? ? ? &gt; u i e jn ;</formula><p>where i : j s 1 j t means that user i prefers item j s over j t . Experimental results demonstrated the effectiveness of the three types of item representations learned from the KG in recommender systems. where A?q? is the candidate answer set. Bordes et al. empirically demonstrated that this intuitive approach achieves promising results without using any lexicon, rules or additional steps for part-of-speech tagging, syntactic or dependency parsing during training as most traditional question answering systems do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUDING REMARKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Recommender Systems</head><p>Recommender systems provide advice to users about items they may wish to purchase or examine. Among different recommendation strategies, collaborative filtering techniques which model the interaction between a user and an item as a product of their latent representations, have achieved significant success <ref type="bibr" target="#b136">[131]</ref>. Such techniques, however, do not always work well, since user-item interactions can be very sparse. In this case, hybrid recommender systems, which combine user-item interactions and auxiliary information of users or items, can usually achieve better performance <ref type="bibr" target="#b137">[132]</ref>.</p><p>Zhang et al. <ref type="bibr" target="#b138">[133]</ref> recently proposed a hybrid recommendation framework which leverages heterogeneous information in a KG to improve the quality of collaborative filtering. Specifically, they used three types of information stored in the KG, including structural knowledge (triple facts), textual knowledge (e.g., the textual summary of a book or a movie), and visual knowledge (e.g., a book's front cover or a movie's poster image), to derive semantic representations for items. To model the structural knowledge, a typical KG embedding technique, i.e., TransR, is applied which learns a structural representation for each item. For the other two types of information, stacked de-noising auto-encoders and stacked convolutional auto-encoders are employed to extract items' textual representations and visual representations, respectively. Then, to conduct collaborative filtering, each user i is represented as a latent vector u i , and each item j a latent vector KG embedding, which aims to embed entities and relations into continuous vector spaces, has found important applications in various entity-oriented tasks and quickly gained massive attention. This article provided a systematic review of currently available techniques, particularly based on the type of information used in KG embedding. State-of-the-art techniques which conduct embedding using only facts observed in a given KG were first introduced. We described the overall framework, specific model design, typical training procedures, as well as pros and cons of such techniques. After that, some more advanced techniques which perform KG embedding with other information besides facts were later discussed. We focused specifically on the incorporation of four types of additional information, i.e., entity types, relation paths, textual descriptions, and logical rules. The investigation on incorporating additional information has just started, and might receive increasing attention in the near future. Finally, this article explored the application of KG embedding. Two types of applications were introduced, i.e., in-KG applications conducted within the scope of the input KG and out-of-KG applications that scale to broader domains. We hope this brief exploration can provide new insights into future application of KG embedding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Simple illustrations of TransE, TransH, and TransR. The figures are adapted from [15], [16].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>hr</head><label></label><figDesc>I?; t $ N ?m m t ; s t I?: f r ?h; t? ? ??kh ? r ? tk 2 2 2 ? u r ? 2 : TransF [54] uses a similar idea. Instead of enforcing the strict translation h ? r % t, TransF only requires t to lie in the same direction with h ? r, and meanwhile h in the same direction with t ? r. The scoring function then is to match t with h ? r, and also h with t ? r, i.e., But it believes that a relation can have multiple semantics, and hence should be represented as a mixture of Gaussian distributions, i.e.,is the embedding for the ith semantic, and p f r ?h; t? ? ?h ? r? &gt; t ? ?t ? r? &gt; h: r the weight of that semantic. The scoring function is accordingly defined as ! TransA [55] introduces for each relation r a symmetric non- negative matrix M r , and defines the scoring function using an adaptive Mahalanobis distance, i.e., X i f r ?h; t? ? p i ?km m h ? m m r ? m m t k 2 2 r exp ; i s 2 h ? s 2 t f r ?h; t? ? ??jh ? r ? tj? &gt; M r ?jh ? r ? tj?: By learning the distance metric M r , TransA is more flexible in dealing with complex relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>r</head><label></label><figDesc>$ N ?m m r ; S r ?; f r ?h; t? ? ?kM r h ? M r tk 1 :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Simple illustrations of RESCAL, DistMulti, and HolE. The figures are adapted from [62].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>are vector embeddings for the entities, and M r 2 R d?d is a linear map associated with the relation. To model the analogical structures, it further requires relations' linear maps to be normal and mutually commutative, i.e., u r? ? b u ; g v ?t; r? ? ?M v t? ?M v r? ? b v :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig</head><label></label><figDesc>Fig. 3a provides a simple illustration of SME. Neural Tensor Network (NTN). NTN [19] is another neural network architecture. Given a fact, it first projects entities to their vector embeddings in the input layer. Then, the two entities h; t 2 R d are combined by a relation-specific tensor M r 2 R d?d?k (along with other parameters) and mapped to a non-linear hidden layer. Finally, a relation-specific linear output layer gives the score</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig.</head><label></label><figDesc>Fig. 3d provides a simple illustration of NAM. It has a more complicated version which connects the relation embedding r to all hidden layers in the network. Table 2 summarizes entity/relation representations and scoring functions used in these semantic matching models. Constraints imposed on these models are also presented.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>ij</head><label></label><figDesc>? 1 if e j 2 IN e i and w</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>&amp;</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Path representations are semantic compositions of their relation embeddings. The figure is adapted from [27].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Examples of entity descriptions. The figure is adapted from [32].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Simple illustration of KALE. The figure is adapted from [34].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. A matrix encoding text and KGs. The figure is adapted from [21]. and KG relations. This framework also improves over traditional text-based extractors. Fan et al. [88] proposed a similar approach to relation extraction. But in their work, the first group of columns in the matrix correspond to text features rather than textual mentions, and matrix completion techniques [128] are employed instead of matrix factorization ones. Chang et al. [93] later devised a tensor-based variant, which encodes plain text and KGs in a three-mode tensor and then factorizes the tensor by using the RESCAL model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2</head><label>2</label><figDesc></figDesc><table>summarizes 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>4.1 Entity Types M rh ? The first kind of additional information we consider is entity types, i.e., semantic categories to which entities belong. For example, AlfredHitchcock has the type of Person, and Psycho the type of CreativeWork. This kind of information is available in most KGs, usually encoded by a specific relation and stored also in the form of</figDesc><table>P n h 
i?1 a i M c i 
P n h 
i?1 a i 

; a i ? 
1; c i 2 I 
C r h ; 
0; c i 6 2 I 
C r h ; 

9. Here, we use "IsA" to denote the relation that links entities to 
their semantic categories. Different KGs actually name this relation in 
their own way, e.g., Freebase adopts /type/object/type and NELL 
uses Generalization. 
10. w 

2 

2 

ij are further normalized so that 

P n 
j?1 w 

ij ? 1 for each i. </table></figure>

			<note place="foot">&quot; For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank all the reviewers for their insightful and valuable suggestions, which significantly improve the quality of this survey. </p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">visual representations associated to that item respectively, and h h j is an offset vector. The preference of user i for item j is then modeled as a product of the two latent vectors, i.e., u &gt; i e j . Ranking optimization over pair-wise preference is used to learn these latent vectors. Finally</title>
		<imprint/>
	</monogr>
	<note>at test time, given a target user i, item recommendation can be made according to the following ranking criterion</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD Int. Conf. Manage. Data</title>
		<meeting>ACM SIGMOD Int. Conf. Manage. Data</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DBpedia: A large-scale, multilingual knowledge base extracted from Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web J</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="195" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">YAGO: A core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Int. Conf. World Wide Web</title>
		<meeting>16th Int. Conf. World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th AAAI Conf</title>
		<meeting>24th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1306" to="1313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process</title>
		<meeting>Conf. Empirical Methods Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Leveraging knowledge graphs for web-scale unsupervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-T Ur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>T</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Annu. Conf. Int. Speech Commun. Assoc</title>
		<imprint>
			<biblScope unit="page" from="1594" to="1598" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Named entity disambiguation using linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damljanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bontcheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Extended Semantic Web Conf</title>
		<meeting>9th Extended Semantic Web Conf</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Entity disambiguation with Freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/WIC/ACM Int. Joint Conf. Web Intell</title>
		<meeting>IEEE/WIC/ACM Int. Joint Conf. Web Intell</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="82" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 49th Annu. Meeting Assoc. Comput. Linguistics</title>
		<meeting>49th Annu. Meeting Assoc. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving efficiency and accuracy in multilingual entity extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int. Conf. Semantic Syst</title>
		<meeting>9th Int. Conf. Semantic Syst</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="121" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Open question answering with weakly supervised embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint Eur. Conf. Mach. Learn. Knowl. Discovery Databases</title>
		<meeting>Joint Eur. Conf. Mach. Learn. Knowl. Discovery Databases</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="165" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process</title>
		<meeting>Conf. Empirical Methods Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="615" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Int. Conf. Mach. Learn</title>
		<meeting>28th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garc Ia-Dur An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th AAAI Conf</title>
		<meeting>28th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th AAAI Conf</title>
		<meeting>29th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A latent factor model for highly multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf</title>
		<meeting>Adv. Neural Inf</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3167" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="259" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process</title>
		<meeting>Conf. Empirical Methods Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1366" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics: Human Language Technol</title>
		<meeting>Conf. North Amer. Chapter Assoc. Comput. Linguistics: Human Language Technol</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Factorizing YAGO: Scalable machine learning for linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Conf. World Wide Web</title>
		<meeting>21st Int. Conf. World Wide Web</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Knowledge base completion using embeddings and rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Int. Joint Conf</title>
		<meeting>24th Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1859" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale knowledge base completion: Inferring via grounding network sampling over selected instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th ACM Int. Conf. Inf. Knowl. Manage</title>
		<meeting>24th ACM Int. Conf. Inf. Knowl. Manage</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1331" to="1340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantically smooth knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 53rd Annu. Meeting Assoc. Comput. Linguistics 7th Int. Joint Conf. Natural Language Process</title>
		<meeting>53rd Annu. Meeting Assoc. Comput. Linguistics 7th Int. Joint Conf. Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="84" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with hierarchical types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Int. Joint Conf</title>
		<meeting>25th Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2965" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling relation paths for representation learning of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process</title>
		<meeting>Conf. Empirical Methods Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Traversing knowledge graphs in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process</title>
		<meeting>Conf. Empirical Methods Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="318" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Compositional learning of embeddings for relation paths in knowledge base and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 54th Annu. Meeting Assoc. Comput. Linguistics</title>
		<meeting>54th Annu. Meeting Assoc. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1434" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge graph and text jointly embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process</title>
		<meeting>Conf. Empirical Methods Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1591" to="1601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aligning knowledge and text embeddings by entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<title level="m">Conf. Empirical Methods Natural Language Process</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="267" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th AAAI Conf</title>
		<meeting>30th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2659" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Text-enhanced representation learning for knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Int. Joint Conf</title>
		<meeting>25th Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1293" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Jointly embedding knowledge graphs and logical rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process</title>
		<meeting>Conf. Empirical Methods Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="192" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Injecting logical background knowledge into embeddings for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics: Human Language Technol</title>
		<meeting>Conf. North Amer. Chapter Assoc. Comput. Linguistics: Human Language Technol</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2016-01" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="11" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Relational retrieval using a combination of path-constrained random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="67" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient and expressive knowledge base completion using subgraph feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process</title>
		<meeting>Conf. Empirical Methods Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1488" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Knowledge base completion via coupled path ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 54th Annu. Meeting Assoc. Comput. Linguistics</title>
		<meeting>54th Annu. Meeting Assoc. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1308" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="107" to="136" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A short introduction to probabilistic soft logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kimmig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Broecheler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS Workshop Probab</title>
		<meeting>NIPS Workshop Probab</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Using semantics and statistics to turn data into knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pujara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Mag</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="74" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The Hadamard product</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Symp. Appl. Math</title>
		<imprint>
			<biblScope unit="page" from="87" to="169" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Holographic reduced representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Plate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="623" to="641" />
			<date type="published" when="1995-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to represent knowledge graphs with Gaussian embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th ACM Int. Conf. Inf. Knowl. Manage</title>
		<meeting>24th ACM Int. Conf. Inf. Knowl. Manage</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">TransG: A generative model for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 54th Annu. Meeting Assoc. Comput. Linguistics</title>
		<meeting>54th Annu. Meeting Assoc. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2316" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics: Human Language Technol</title>
		<meeting>Conf. North Amer. Chapter Assoc. Comput. Linguistics: Human Language Technol</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A translationbased knowledge graph embedding preserving logical property of relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-G</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics: Human Language Technol</title>
		<meeting>Conf. North Amer. Chapter Assoc. Comput. Linguistics: Human Language Technol</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="907" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">STransE: A novel embedding model of entities and relationships in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics: Human Language Technol</title>
		<meeting>Conf. North Amer. Chapter Assoc. Comput. Linguistics: Human Language Technol</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="460" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 53rd Annu. Meeting Assoc. Comput. Linguistics 7th Int. Joint Conf. Natural Language Process</title>
		<meeting>53rd Annu. Meeting Assoc. Comput. Linguistics 7th Int. Joint Conf. Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Knowledge graph completion with adaptive sparse transfer matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th AAAI Conf</title>
		<meeting>30th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="985" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Transition-based knowledge graph embedding with relational mapping properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Pacific Asia Conf. Language Inf</title>
		<meeting>28th Pacific Asia Conf. Language Inf</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="328" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">From one point to a manifold: Knowledge graph embedding for precise link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Int. Joint Conf</title>
		<meeting>25th Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1315" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by flexible translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th</title>
		<meeting>15th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Int</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Principles Knowl. Represent. Reasoning</title>
		<imprint>
			<biblScope unit="page" from="557" to="560" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">TransA: An adaptive approach for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.05490</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Joint learning of words and meaning representations for open-text semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Int. Conf</title>
		<meeting>15th Int. Conf</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th AAAI Conf</title>
		<meeting>25th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Information Theory and Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Courier Corporation</publisher>
			<pubPlace>North Chelmsford, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Probability product kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="819" to="844" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Exchangeability and related topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Aldous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ecole d&apos; Et e de Probabilit es de Saint-Flour, XIII-1983</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1985" />
			<biblScope unit="page" from="1" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The nested Chinese restaurant process and Bayesian nonparametric inference of topic hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th AAAI Conf</title>
		<meeting>30th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Evdokimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07704</idno>
		<title level="m">Probabilistic reasoning via deep learning: Neural association models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Effective blending of two and three-way interactions for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garc Ia-Dur An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint Eur. Conf. Mach. Learn. Knowl. Discovery Databases</title>
		<meeting>Joint Eur. Conf. Mach. Learn. Knowl. Discovery Databases</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="434" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 33rd Int. Conf. Mach. Learn</title>
		<meeting>33rd Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">On the equivalence of holographic and complex embeddings for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 55th</title>
		<meeting>55th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Annu. Meeting Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="page" from="554" to="559" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Analogical inference for multi-relational embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th Int. Conf. Mach. Learn</title>
		<meeting>34th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2168" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Knowledge vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</title>
		<meeting>20th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Predicting RDF triples in incomplete knowledge bases with tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Drumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th Annu</title>
		<meeting>27th Annu</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="326" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">On approximate reasoning capabilities of low-rank vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Spring Symp. Knowl. Representation Reasoning</title>
		<meeting>AAAI Spring Symp. Knowl. Representation Reasoning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="400" to="407" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Leveraging lexical resources for learning entity embeddings in multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C K</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 54th Annu. Meeting Assoc. Comput. Linguistics</title>
		<meeting>54th Annu. Meeting Assoc. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="112" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Type-constrained representation learning in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krompa?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Int. Semantic Web Conf</title>
		<meeting>14th Int. Semantic Web Conf</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="640" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Logistic tensor factorization for multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML Workshop Structured Learn.: Inferring Graphs Structured Unstructured Inputs</title>
		<meeting>ICML Workshop Structured Learn.: Inferring Graphs Structured Unstructured Inputs</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Boolean tensor factorizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Miettinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th IEEE Int. Conf. Data Mining</title>
		<meeting>11th IEEE Int. Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Discovering facts with Boolean tensor Tucker decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erdos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Miettinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM Int. Conf. Inf. Knowl. Manage</title>
		<meeting>22nd ACM Int. Conf. Inf. Knowl. Manage</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1569" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Analysis of individual differences in multidimensional scaling via an N-way generalization of &quot;Eckart-Young&quot; decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="283" to="319" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Some mathematical notes on three-mode factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="311" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">TripleRank: Ranking semantic web data by tensor decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sizov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Staab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int. Semantic Web Conf</title>
		<meeting>8th Int. Semantic Web Conf</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A tensor-based factorization model of semantic compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van De Cruys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poibeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics: Human Language Technol</title>
		<meeting>Conf. North Amer. Chapter Assoc. Comput. Linguistics: Human Language Technol</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1142" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Multi-relational latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process</title>
		<meeting>Conf. Empirical Methods Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1602" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Knowledge base completion via search-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd Int. Conf. World Wide Web</title>
		<meeting>23rd Int. Conf. World Wide Web</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="515" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">SSE: Semantically smooth embedding for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="884" to="897" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 52nd Annu. Meeting Assoc. Comput. Linguistics</title>
		<meeting>52nd Annu. Meeting Assoc. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Materializing and querying learned knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bundschus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rettinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st ESWC Workshop Inductive Reasoning Mach. Learn. Semantic Web</title>
		<meeting>1st ESWC Workshop Inductive Reasoning Mach. Learn. Semantic Web</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A scalable approach for statistical learning in semantic graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="22" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Typed tensor decomposition of knowledge bases for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process</title>
		<meeting>Conf. Empirical Methods Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1568" to="1579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Entity hierarchy embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 53rd Annu. Meeting Assoc. Comput. Linguistics 7th Int. Joint Conf. Natural Language Process</title>
		<meeting>53rd Annu. Meeting Assoc. Comput. Linguistics 7th Int. Joint Conf. Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1292" to="1300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Backpropagation through time: What it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1990-10" />
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Bipartite network projection and personal recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Medo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Composing relationships with translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garc Ia-Dur An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process</title>
		<meeting>Conf. Empirical Methods Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="286" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Compositional vector space models for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 53rd</title>
		<meeting>53rd</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Annu</surname></persName>
		</author>
		<title level="m">Meeting Assoc. Comput. Linguistics 7th Int. Joint Conf. Natural Language Process</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="156" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Chains of reasoning over entities, relations, and text using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Conf</title>
		<meeting>15th Conf</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Context-dependent knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process</title>
		<meeting>Conf. Empirical Methods Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1656" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Joint semantic relevance learning with text data and graph knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Workshop Continuous Vector Space Models Compositionality</title>
		<meeting>3rd Workshop Continuous Vector Space Models Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="32" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">SSP: Semantic space projection for knowledge graph embedding with text descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st AAAI Conf. Artif. Intell</title>
		<meeting>31st AAAI Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3104" to="3110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Distributed representation learning for knowledge graphs with entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="31" to="37" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Knowledge graph representation with jointly structural and textual encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08661</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Discovery of frequent DATALOG patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dehaspe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining Knowl. Discovery</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="36" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Inverse entailment and Progol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muggleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Generation Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="286" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">AMIE: Association rule mining under incomplete evidence in ontological knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teflioudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Int. Conf. World Wide Web</title>
		<meeting>22nd Int. Conf. World Wide Web</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Fast rule mining in ontological knowledge bases with AMIE+</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teflioudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="707" to="730" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">The Metamathematics of Fuzzy Logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Kluwer</publisher>
			<pubPlace>Norwell, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Lifted rule injection for relation embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process</title>
		<meeting>Conf. Empirical Methods Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1389" to="1399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Low-dimensional embeddings of logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bo Snjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL Workshop Semantic Parsing</title>
		<meeting>ACL Workshop Semantic Parsing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="45" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Learning first-order logic embeddings via matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Int. Joint Conf</title>
		<meeting>25th Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2132" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Knowledge representation learning with entities, attributes and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Int. Joint Conf</title>
		<meeting>25th Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2866" to="2872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Encoding temporal information for time-aware link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process</title>
		<meeting>Conf. Empirical Methods Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2350" to="2354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Predicting the co-evolution of event and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Esteban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krompa?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th Int. Conf. Inf. Fusion</title>
		<meeting>19th Int. Conf. Inf. Fusion</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="98" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Know-evolve: Deep temporal reasoning for dynamic knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th Int. Conf. Mach. Learn</title>
		<meeting>34th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3462" to="3471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Multivariate point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A W</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Berkeley Symp</title>
		<meeting>6th Berkeley Symp</meeting>
		<imprint>
			<date type="published" when="1972" />
			<biblScope unit="page" from="401" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">GAKE: Graph aware knowledge embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Int. Conf. Comput. Linguistics</title>
		<meeting>26th Int. Conf. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="641" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">DeepWalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</title>
		<meeting>20th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Link prediction in multi-relational graphs using additive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Semantic Technol. Meet Recommender Syst. Big Data</title>
		<meeting>Int. Conf. Semantic Technol. Meet Recommender Syst. Big Data</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Reducing the rank in relational factorization models by including observable patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1179" to="1187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Knowledge graph refinement: A survey of approaches and evaluation methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="489" to="508" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Entity resolution with Markov logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Conf. Data Mining</title>
		<meeting>6th Int. Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="572" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint Conf. 47th Annu. Meeting ACL 4th Int. Joint Conf. Natural Language Process</title>
		<meeting>Joint Conf. 47th Annu. Meeting ACL 4th Int. Joint Conf. Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Mach. Learn. Knowl. Discovery Databases</title>
		<meeting>Eur. Conf. Mach. Learn. Knowl. Discovery Databases</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Relation extraction with multi-instance multi-label convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Int. Conf. Comput. Linguistics</title>
		<meeting>26th Int. Conf. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1471" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Cand Es</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Comput. Math</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="717" to="772" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process</title>
		<meeting>Conf. Empirical Methods Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Paraphrase-driven learning for open question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 51st Annu. Meeting Assoc</title>
		<meeting>51st Annu. Meeting Assoc</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1608" to="1618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Personalized entity recommendation: A heterogeneous information network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th ACM Int. Conf. Web Search Data Mining</title>
		<meeting>7th ACM Int. Conf. Web Search Data Mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="283" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</title>
		<meeting>22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
