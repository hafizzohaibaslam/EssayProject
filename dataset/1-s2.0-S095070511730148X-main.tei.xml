<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2021-05-15T08:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature selection and ensemble construction: A two-step method for aspect based sentiment analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Elsevier BV</publisher>
				<availability status="unknown"><p>Copyright Elsevier BV</p>
				</availability>
				<date type="published" when="2017">2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Md</roleName><forename type="first">Md</forename><forename type="middle">Shad</forename><surname>Akhtar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asif</forename><surname>Ekbal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
						</author>
						<title level="a" type="main">Feature selection and ensemble construction: A two-step method for aspect based sentiment analysis</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Knowledge-Based Systems</title>
						<title level="j" type="abbrev">Knowledge-Based Systems</title>
						<idno type="ISSN">0950-7051</idno>
						<imprint>
							<publisher>Elsevier BV</publisher>
							<biblScope unit="volume">125</biblScope>
							<biblScope unit="page" from="116" to="135"/>
							<date type="published" when="2017">2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.knosys.2017.03.020</idno>
					<note type="submission">Article history: Received 21 September 2016 Revised 15 February 2017 Accepted 25 March 2017</note>
					<note>Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/knosys a r t i c l e i n f o a b s t r a c t</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>Sentiment analysis Aspect term extraction Feature selection Ensemble Conditional random field Support vector machine Maximum entropy Particle swarm optimization</keywords>
			</textClass>
			<abstract>
				<p>In this paper we present a cascaded framework of feature selection and classifier ensemble using particle swarm optimization (PSO) for aspect based sentiment analysis. Aspect based sentiment analysis is performed in two steps, viz. aspect term extraction and sentiment classification. The pruned, compact set of features performs better compared to the baseline model that makes use of the complete set of features for aspect term extraction and sentiment classification. We further construct an ensemble based on PSO, and put it in cascade after the feature selection module. We use the features that are identified based on the properties of different classifiers and domains. As base learning algorithms we use three classifiers, namely Maximum Entropy (ME), Conditional Random Field (CRF) and Support Vector Machine (SVM). Experiments for aspect term extraction and sentiment analysis on two different kinds of domains show the effectiveness of our proposed approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent past has witnessed a phenomenal growth of internet users globally, and the third world countries like India, China etc. are not the exceptions. Use of social media and messaging applications grew 203% year-on-year in 2013, with overall application users rising 115% over the same period, as reported by Statista, citing data from Flurry Analytics. This growth means that 1.61 billion people are now active in social media around the world and this is expected to advance to 2 billion users in 2016, led by India. The research shows that consumers are now spending daily approximately 8 h on digital media including social media and mobile internet usages. This has completely changed the lifestyles of people. Before buying any product or acquiring any service, customers or users nowadays depend heavily on the web-based information available through several shopping portals, online sites, blogs, tweets etc. They want to make sure that the products that they buy or the services that they acquire are of high quality. Be it buying a product from an e-commerce website or going for a dinner/lunch to a restaurant or planning for watching a movie at the theater, they always seek other users' opinions about the product and/or services before experiencing themselves. The unprece- * Corresponding author.</p><p>E-mail addresses: asif.ekbal@gmail.com , asif@iitp.ac.in (A. Ekbal). dented volume and variety of user-generated contents make it difficult to go through all the information manually. But, in order to get an unbiased opinion of a product or service one has to extract and read all the reviews. Unfortunately this is not an easy task to perform considering the huge amount of time and effort s involved. Therefore, there is a need to develop tools and techniques that will aid users in mining the desired information from the collection of reviews.</p><p>Sentiment Analysis <ref type="bibr" target="#b0">[1]</ref> is a well-established task that targets to determine the polarity of opinion expressed in a given user's review. In general, polarity can either be positive, negative or neutral depending on the sentiment expressed in a review <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> . These information not only help an individual seeking for information, but also facilitate the process of refining various business decisions to improve the quality of products or services. There have been quite a significant number of existing methods for sentiment analysis in various domains <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref> . It is to be noted that most of these existing works focus on discovering sentiments at the document <ref type="bibr" target="#b3">[4]</ref> or sentence <ref type="bibr" target="#b4">[5]</ref> level. However, the techniques focused on such a coarselevel analysis (i.e. document level or sentence level ) do not always satisfy the users' needs as per their expectations. They often require more finer information from a review in terms of specific aspects (or, features or attributes ) of a product or service. For example, an user may be interested in only specific aspects such as the 'design', 'battery life' or 'display screen' of a laptop. Different opinions may have been expressed by different users on each of these http://dx.doi.org/10.1016/j.knosys.2017.03.020 0950-7051/? 2017 Elsevier B.V. All rights reserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Example of user's review, aspect terms and their sentiment polarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># Reviews</head><p>Aspect term Polarity 1. The fried rice is amazing here. fried rice positive 2. But the staff was so horrible to us. staff negative 3. The menu is limited but almost all of the dishes are excellent.</p><p>menu, dishes negative, positive 4. The food was delicious but do not come here on a empty stomach.</p><p>food conflict 5. I grew up eating Dosa and have yet to find a place in NY to satisfy my taste buds.</p><p>dosa neutral aspects. Therefore, it is important to detect the sentiments with respect to different aspects, and this process is known as 'Aspect based Sentiment Analysis'. The term "aspect" refers to an attribute or a component of the product/service that has been commented on in a review. In aspect level sentiment analysis, we are interested in determining the polarity orientation towards each aspect term, which appears in a review sentence. Polarity values of the aspect terms in a sentence could be same or different. Aspect term extraction is a sub-problem of the aspect level sentiment analysis. It only concerns with detecting the aspect terms present in a sentence. If the aspect term is known beforehand then there is no need to perform aspect term extraction. Once the aspect terms are known/identified, the second sub-problem (i.e. sentiment classification) deals with assigning a polarity value (i.e. positive, negative, neutral and conflict) to each aspect term. Aspect term extraction and opinion target extraction are the related terms. The term "Aspect term extraction" was introduced in SemEval 2014 shared task <ref type="bibr" target="#b8">[9]</ref> and the term "opinion term extraction" was coined a year later in SemEval 2015 shared task <ref type="bibr" target="#b9">[10]</ref> . Since we are using SemEval 2014 datasets for experiments we use the term "aspect term extraction" in the paper. In <ref type="table">Table 1</ref> we present some examples that describe the aspects and their corresponding polarities. The first review contains only one aspect term i.e. fried rice and the sentiment expressed towards it is positive . Similarly, review 2 expresses negative sentiment for the aspect term staff. Third review contains two aspect terms, namely menu and dishes which carry opposite sentiments, i.e. negative for menu and positive for dishes . The reviewer has made both positive and negative comments on an aspect term food in the fourth review, therefore, its polarity is marked as conflict . In the last review, aspect term is dosa and the sentiment is neither positive nor negative , and hence it is neutral .</p><p>The primary focus of aspect based sentiment analysis can be thought of as the processes of extracting aspects and then determining the sentiments that are expressed in the review <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> . Aspect term extraction can be modeled as a sequence labeling problem since it depends heavily on the structural and contextual information. For example, in the first review sentence of <ref type="table">Table 1</ref> , token "fried " can not be termed as part of an aspect term if the context "rice " is not provided. However, in the presence of contextual information the multi-word token "fried rice " together forms an aspect term. The sentiment can be classified either at the coarsegrained level denoting such positive and negative or at the more fine-grained level that corresponds to positive, negative, neutral or conflict <ref type="bibr" target="#b8">[9]</ref> . Also, aspect terms can influence sentiment polarity within a single domain. As an example, for the restaurant domain, cheap is usually positive with respect to food , but it denotes a negative polarity when discussing decor or ambiance <ref type="bibr" target="#b10">[11]</ref> . In contrast, sentiment analysis at aspect level can guide users to gain more insights on the sentiments of various aspects of the target entity. Hence, the decision taken thereafter becomes more informative and practical.</p><p>According to <ref type="bibr" target="#b11">[12]</ref> , opinions are personal interpretation of information whereas sentiment refers to an expression constrained on social expectation. Therefore, in light of above definitions the term "opinion mining" would be the more suitable in the work. However, to make it consistent with the SemEval 2014 shared task on aspect based sentiment analysis we choose to use the term "sentiment analysis" throughout the paper. Also, literature shows the evidence that "sentiment analysis" and "opinion mining" are often used inter-changeably to refer the study of polarity orientation in a user written text.</p><p>Literature survey shows that the concept of aspect based sentiment analysis has recently drawn the attention of researchers worldwide. Earlier approaches to aspect extraction are based on the frequently used nouns and noun phrases <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> . Such approaches work well when many aspects are strongly associated with certain categories of words (such as nouns), but often fail when many low frequency terms are used as the aspects. Nowadays, with the emergence of few labeled datasets, supervised learning approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> are predominantly being used. Some other approaches include the techniques, such as those that define aspect terms using a manually specified subset of Wikipedia category <ref type="bibr" target="#b14">[15]</ref> hierarchy, unsupervised clustering <ref type="bibr" target="#b12">[13]</ref> and semantically motivated technique <ref type="bibr" target="#b3">[4]</ref> . In 2014 a shared task, SemEval-2014 <ref type="bibr" target="#b8">[9]</ref> was organized for addressing the challenges of aspect based sentiment analysis and to provide a common benchmark setup. Participation to this particular task were quite overwhelming. The best performing model as reported in <ref type="bibr" target="#b15">[16]</ref> was based on CRF that uses lexical and syntactic features. The performance of machine learner was further boosted with a set of hand-crafted heuristics. For sentiment classification the best system was reported by Wagner et al. <ref type="bibr" target="#b16">[17]</ref> . A two-step method was proposed for the task. First, a rule-based method using sentiment lexicons (e.g. BingLiu, SentiWordNet, MPQA etc.) was applied to find the polarity of an aspect term. Subsequently, output of the rule-based system is combined with bag-of-n -gram features to train SVM classifier.</p><p>In <ref type="bibr" target="#b17">[18]</ref> , an application of recurrent neural network (RNN) has been discussed for the aspect term extraction task. Further, it was shown that LSTM-RNN based system with the assistance of extra set of features performs better than a feature-rich system based on CRF. In <ref type="bibr" target="#b18">[19]</ref> , a deep convolutional neural network (CNN) based architecture has been proposed for aspect term extraction task. The authors employed two different word embeddings (pretrained embedding trained on Google News corpus and Amazon word embedding trained on 4.7-billion-word corpus from Amazon) along with Part-of-Speech (PoS) tag information and few linguistic patterns for training a CNN. Recently, tree-kernel based approach <ref type="bibr" target="#b19">[20]</ref> has been proposed for capturing the lexical and syntactic information for identifying the polarity orientation towards the aspect terms.</p><p>Existing literature on sentiment analysis does not show much effort s f or systematic feature selection. Most of the existing systems rely on heuristic based methods for selecting the most relevant set of features or classifier candidates for constructing an ensemble. The process is time-consuming because different combinations of features/classifiers should be exhaustively tried to finally fix a model. Another crucial issue is domain adaptation where the system, developed targeting a specific domain, often fails to perform reasonably when the domain is altered. The set of features or classifiers which show acceptable performance for a domain may not be equally effective for the other. Thus, careful feature selection plays an important role. An effective usage of Z-score and Information gain for feature selection in sentiment analysis has been reported in <ref type="bibr" target="#b20">[21]</ref> . A computationally efficient feature selection technique is proposed in <ref type="bibr" target="#b21">[22]</ref> that is based on document frequency for sentiment analysis. However, this is shown to be weaker than the information gain based feature selection in terms of reported accuracy. In <ref type="bibr" target="#b22">[23]</ref> , a PSO based method has been proposed for effective selection of optimal parameter values for SVM. Three different techniques of ensemble (fixed rules, weighted com-bination and meta-classifiers) were used in <ref type="bibr" target="#b23">[24]</ref> for sentiment classification. They conclude that weighted combination based ensemble method performs better over the other two. In <ref type="bibr" target="#b24">[25]</ref> , it has been shown that the problem of sentiment classification can be overcome by classifier ensemble. A bootstrap ensemble framework for twitter sentiment classification is proposed in <ref type="bibr" target="#b25">[26]</ref> . An ensemble based approach for Chinese sentiment analysis by incorporating English sentiment resource was proposed in <ref type="bibr" target="#b26">[27]</ref> . <ref type="bibr">Wang et al. [28]</ref> conducted their experiments on ten publicly available datasets to prove the effectiveness of ensemble learning in sentiment analysis. However, it is to be noted that none of these existing works focused on determining sentiment at such a fine-grained level.</p><p>In the first part of the paper we propose a technique for feature selection that automatically determines the most relevant set of features for aspect term extraction and sentiment classification. Our algorithm is based on Particle Swarm Optimization (PSO) <ref type="bibr" target="#b28">[29]</ref> . Feature selection, also known as variable subset selection or dimensionality reduction, is a technique that selects the most relevant features for the target problem. By removing the most irrelevant and redundant features from the data, feature selection aids to improve the performance of a classifier. We use Conditional Random Field (CRF) <ref type="bibr" target="#b29">[30]</ref> , Support Vector Machine (SVM) <ref type="bibr" target="#b30">[31]</ref> and Maximum Entropy (ME) model <ref type="bibr" target="#b31">[32]</ref> as the learning algorithms. We implement a diverse set of features for each of the tasks, i.e. aspect term extraction and sentiment classification. One appealing characteristics of these features being the fact that we limited ourselves in not using much domain-dependent information for the spirit of their easy adaptability to new domains and applications. Most of the features used for aspect term extraction or sentiment classification exploit lexical, syntactic or semantic level features as discussed in Section 3 . Initially, we perform a series of experiments with the various combination of features. The best configuration, thus obtained, is used as the baseline model on which PSO based feature selection technique is applied. Each classifier, when subjected to PSO based feature selection, yields a set of solutions. Each solution represents a particular feature combination. The classifiers are then trained, tested and evaluated on these feature combinations. Next we select the most promising models (based on F-measure or accuracy) for each of the classifiers. In order to further improve the performance, in the second part of our algorithm we propose a PSO based ensemble learning method. The most promising models are selected and combined using a majority or weighted voting.</p><p>Experiments on the benchmark setups of <ref type="bibr">SemEval-2014</ref> show that our proposed techniques achieve state-of-the-art performance for aspect term extraction and sentiment classification. Evaluation shows that systematic method of feature selection can produce improved performance, even with a much reduced set of features. In our earlier attempt we have proposed a feature selection technique for aspect based sentiment analysis in <ref type="bibr" target="#b32">[33]</ref> . The present research differs from our previous works with respect to the following points: current work is more extensive as we substantially extend our algorithmic view by developing more models based on the classifiers, which are heterogeneous in nature; developed feature selection technique for three different classifiers, namely CRF, SVM and ME; developed a PSO based ensemble selection method for combining the most promising models in order to further improve the performance. The contributions of the present work can be summarized as follows: (i). use of a very diverse and rich feature set for aspect term extraction and sentiment classification; (ii). efficient feature selection technique based on PSO that shows superior performance with a compact and pruned feature set for aspect term extraction and sentiment classification both; (iii). efficient ensemble construction techniques based on PSO for aspect term extraction and sentiment classification; (iv). proposal of a generalized technique that attains state-of-the-art performance for aspect based sentiment analysis in two different domains, viz. laptop and restaurant reviews.</p><p>The rest of the paper is structured as follows. A brief introduction to PSO is presented in Section 2 . Features used for aspect term extraction and sentiment classification are presented in Section 3 . In Section 4 , we present our proposed method for feature selection and ensemble construction. Experimental results with detailed analysis are presented in Section 5 . Finally, we conclude in Section 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Brief introduction to particle swarm optimization</head><p>Particle Swarm Optimization (PSO) <ref type="bibr" target="#b28">[29]</ref> is a population based stochastic optimization method which is founded on the behavior of bird flocking. PSO starts with a set of random solutions and searches for the global optima by updating the generations. In PSO, the potential solutions of the given problem are called as particles and denoted as ? ¡ú</p><formula xml:id="formula_0">X (k ) = (x (k, 1) , x (k, 2) , .........., x (k,n ) ) in an n- dimensional search space. Each co-ordinate x ( k, d</formula><p>) of these particles can change with some rate, known as the velocity</p><formula xml:id="formula_1">v ( k, d ) d = 1,2,.</formula><p>..,n. Every particle keeps a record of the best position that it has ever visited. Such a record is called the particle's previous best position and denoted by ? ¡ú B (k ) . The global best position attained by any particle so far is also recorded and stored in a particle denoted by ? ¡ú G . An iteration comprises evaluation of each particle, then stochastic adjustment of v <ref type="bibr">( k, d )</ref> </p><formula xml:id="formula_2">in the direction of particle ? ¡ú X (k ) 's previ-</formula><p>ous best position and the previous best position of any particle in the neighborhood <ref type="bibr" target="#b28">[29]</ref> . Entire process of PSO is governed by three operations, namely evaluate, compare and imitate . The evaluation phase measures how well each particle, i.e. the candidate solution solves the problem at hand. The comparison process attempts to identify the best particle by comparing different solutions. The imitation process produces new particles based on some of the best particles found so far. These three processes are repeated until a given stopping criterion is met. The objective is to find the particle that best solves the target problem. Velocity and neighborhood are the two important concepts in PSO. There are other popular approaches like the well-known genetic algorithm (GA) <ref type="bibr" target="#b33">[34]</ref> and simulated annealing (SA) <ref type="bibr" target="#b34">[35]</ref> . In order to solve the global optimization problems these techniques are widely used to find the good set of solutions in the search space. While SA is a probabilistic meta-heuristic approach, GA relies on the concept of survival of the fittest. Unlike GA, PSO does not retain only the good solutions. It allows the particles to move in the search space on the basis of the number of cases, and it generates good set of possible solutions without eliminating any weak particle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Feature set</head><p>In this section we describe the features that we use for aspect term extraction and sentiment classification. Most of the lexical and syntactic features that we use are domain-independent and generic in nature, and therefore may be used for the applications of similar nature. We restrict ourselves in not using much domaindependent external resources. Few of the features e.g. word cluster, WordNet, NER, head word, dependency relation, semantic orientation, lexicons feature etc. separately and/or collectively have been proved to be efficient for many different NLP problems. So, we have rigorously studied our datasets and defined a common set of features for both the restaurant and laptop domains. Some of the features such as Word cluster, Semantic orientation, BingLiu, BingLiu Direct etc. have been re-implemented in a better and representative way to exploit generalization across multiple domains. The feature such as prefix and suffix of fixed length character sequences have not been used for aspect term extraction as such.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Features for aspect term extraction</head><p>We use the following features for aspect term extraction from the reviews of both the domains, viz . restaurant and laptop. </p><formula xml:id="formula_3">w i ?2 w i ?1 w i w i +1 w i +2</formula><p>But the sta f f was so hor r ible to us.</p><p>3. Part-of-Speech (PoS) tag: PoS information of the token provides useful evidence to identify aspect term. The potential aspect candidates correspond mostly to noun, adjective, verb or adverb. Here, we use PoS tags of the current and the surrounding tokens as feature. 4. Head word: We observe that significant percentage of constituent words that belong to the noun phrases have the chances of being an aspect term. We identify and implement a binary feature that fires if the current token is a head word of the noun phrase. A '0' value is assigned for the words that do not belong to a noun phrase. For example, in the following review spicy tuna roll and asian salad are noun phrases, whereas roll and salad denote the two head words, respectively.</p><p>8. Stop word: In general, stop words cannot be the aspect terms (e.g., the, is, at etc.). A feature is defined that takes the value equal to 1 or 0 depending upon whether it is a stop word or not. 9. Word length: Length of a token may be effective in identifying the aspect terms. We observe that aspect terms are generally longer in length. We define a binary-valued feature that is set to high if the length of the candidate token exceeds a predetermined threshold. In our case we assume the token to be an aspect term if its length is more than 5 characters. 10. Prefix and suffix: Prefix and suffix of fixed-length character sequences are stripped from each token and used as the features of classifier. Here, we use prefixes and suffixes of length up to three characters of the current word as features. 11. Frequent aspect term: We extract and complie a list of frequent aspect terms from the training dataset. A binary-valued feature is then defined that fires if and only if the current token appears in this list. Here, the threshold is set to 5. 12. Dependency relation: Grammatical relationship among the words in a sentence can be represented by the dependency relations. We define two different dependency relation features in our work. One denotes the relation in case the current token is the governor (i.e head of the relation), while the other represents the relation if it is the dependent . For the first feature we look for the dependency relations of types: 'amod'(adjectival modifier), 'nsubj'(nominal subject) and 'dep'(dependent). The second feature corresponds to the relation types: 'nsubj'(nominal subject), 'dobj'(direct object) and 'dep'(dependent). Let us consider the following example review.</p><p>? Review: Best spicy tuna roll , great asian salad .</p><p>? HeadWord: 0 0 0 1 0 0 0 1 0 5. Head word PoS: PoS of the head word is used as a feature of the model. 6. Chunk information: Many aspect terms are multiword in nature. For example, "battery life", "spicy tuna rolls" etc. We use chunk information that provides useful evidence to identify the boundaries of aspect terms. An example is shown in the following:</p><p>Review It contains an aspect term staff. The token staff is dependent on horrible via relation 'nsubj'. No other above mentioned relations (neither governor nor dependent) are present for the token staff. Therefore, only the feature that corresponds to dependent 'nsub' will fire. Stanford dependency parser 1 is used to extract the dependency relations from a sentence. These features are defined in line with <ref type="bibr" target="#b35">[36]</ref> . 13. WordNet: In WordNet <ref type="bibr" target="#b36">[37]</ref> , different words that are semantically similar (or synonymous to each other) are categorized into synsets. Synset information as a feature enables the model to group tokens with identical senses. For example, the tokens lunch and dinner are related as the homonyms of meal in the WordNet hierarchy. This feature is particularly very crucial in identifying an unseen aspect term whose synonyms are present in the training set. We consider only the noun synsets. We define this feature following the one as mentioned in <ref type="bibr" target="#b35">[36]</ref> . 14. Named entity information: As per definition, only attribute of a product can be tagged as aspect term and not the product itself. Therefore, a named entity (NE) is not treated as an aspect term. Also, some tokens (which are normally a part of an aspect term) can not be considered as an instance of aspect term if they belong to any NE. For example in <ref type="table">Table 2</ref> , a token 'sushi' is present in both review sentences. It is an aspect term in the first case ( an attribute of a restaurant ). However, in second sentence it is not treated as an aspect term because it belongs to a NE 'Go Sushi' ( a restaurant name ). We, therefore, define and use a binary-valued feature, which fires when a token is part of a NE.</p><p>7. Lemma: We use lemmas of the words as features. For example, the words like serve, serves, served and serving in restaurant domain can be identified as different inflectional forms of the word serve . <ref type="table">Table 2</ref> Named entity information and aspect term relation.</p><p>Review: Certainly not the best sushi in New York. NE:</p><formula xml:id="formula_4">0 0 0 0 0 0 0 0 0 Aspect terms: O O O O B-ASP O O O O</formula><p>Review: I trust the people at Go Sushi , it never disappoints. NE:</p><formula xml:id="formula_5">0 0 0 0 0 1 1 0 0 0 0 0 Aspect term: O O O O O O O O O O O O</formula><p>15. Character n-grams: Character n -gram is a contiguous sequence of n characters extracted from a given word by striping off few characters from the beginning and/or end positions. We extract character n -grams by varying n in the range of 1-5 and use these as features of the classifier. 16. Aspect term list: For each domain (i.e. restaurant &amp; laptop) we compile two different lists of aspect terms from the respective training set.</p><p>(a) The first list contains the aspect terms that appear more than a predefined threshold count (here, f 1 ) in the training set. (b) The second list is created in order to handle the multiword aspect terms. At first we compile single-word aspect terms whose counts are above a predefined threshold f 2 in the training data. Then, a probability p is computed in line with <ref type="bibr" target="#b35">[36]</ref> for each word in the collection.</p><p>The list is then compiled by selecting only those aspect terms whose corresponding probabilities are above a cercompute SO scores on the training sets of the respective domains, and used them as features in our work. 19. Orthographic features: We define two features based on the constructions of words. These check whether the token starts with a capitalized letter or starts with a digit. We observe that many aspect terms are capitalized and contains numeric symbols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sentiment classification</head><p>User's opinion expressed in a review are classified into the following semantic classes i.e. positive, negative, neutral and conflict . For sentiment classification we directly adapt few features used for aspect term extraction (e.g., local context, sentiment orientation score etc.). Along with these we define and implement some other problem specific features, as listed below, for the task at hand. tain threshold (say, ¦È ).</p><p>Two binary-valued features are defined that take the value of 0 and 1 depending upon whether the current word appears in the compiled lists or not. As a result of cross validation we set 18. Semantic orientation (SO) score: For each word, sentiment orientation (SO) score <ref type="bibr" target="#b38">[39]</ref> is computed that measures how much it is associated with the positive or negative sentiments. Point-wise Mutual Information (PMI), a measure of association of token t with respect to positive or negative review, is used to determine the sentiment score as follow:</p><formula xml:id="formula_6">f 1 , f 2 &amp; ¦È as</formula><formula xml:id="formula_7">SO (t) = P MI(t, posRe v ) ? P MI(t, negRe v ) and P MI(t, negRe v ) = log f req (t, negRe v ) * N f req (t) * M</formula><p>where freq ( t, negRev ) is the frequency of word t in negative review, freq ( t ) is frequency of t in the corpus, M is the number of tokens in negative review and N is the number of tokens in the corpus. Similarly, PMI ( t, posRev ) is the PMI scores with respect to the positive review. A positive SO score implies that the token is more inclined to the positive than negative reviews. We For each token we define the polarity score to be 1, ?1, 0 for a positive, negative and neutral token. respectively. For the token that does not appear in this lexicon a score of 2 is assigned. An integer-valued feature is then defined that computes the sum of polarity scores of all the terms that appear in the context window of size five. (b) Bing Liu lexicon : Bing Liu lexicon <ref type="bibr" target="#b40">[41]</ref> is a list of positive and negative sentiment words. This lexicon contains approximately 6,800 words. Similar to the process used in MPQA lexicon we assign the following scores: 1 for a positive token, ?1 for negative token and 2 for the token that does not appear in the lexicon. Based on this configuration, we define the following two features:</p><p>(1) Bing: Polarity score of all the tokens that fall into the context window of a target aspect term are summed up and used as a feature. Size of the context window is set to five. (2) Bing Direct: In this feature we compute the sum of the polarity scores of only those words which have a direct dependency relation with the target aspect term. (c) SentiWordNet lexicon : This is one of the most popularly used lexicons for sentiment analysis. SentiWordNet 4 <ref type="bibr" target="#b41">[42]</ref> lexicon is based on WordNet that assigns sentiment score of positivity and negativity to each synset. The sentiment scores of all the words that appear in the surrounding context of previous five and next five words of the target aspect term are retrieved and summed up. The value obtained as a result of this is used as a feature. 3. Domain-specific words : All the above lexicons are generic in nature and do not cover many domain specific words that express specific sentiments. Some of these examples are 'mouth watering', 'yummy' and 'over cooked' for the restaurant domain.</p><p>We manually compile a lexicon of such words from the web <ref type="bibr" target="#b4">5</ref> and from the general intuition. Following the same scoring method (1: positive, ?1: negative and 2: words that don't appear), we compute sum of all the scores for the words that appear in the context of size 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed method</head><p>In this section we describe our proposed method of PSO based feature selection and ensemble construction. At first we determine the best fitting feature sets for aspect term extraction and sentiment classification. As base classifiers we use three classification models, namely CRF, SVM and ME. The proposed method of ensemble selection automatically determines the best set of classifiers, that when combined together using a PSO based ensemble, improves the classification performance most. The entire process can be outlined as a sequence of three steps as follows:</p><p>1. Identification and Implementation of a diverse and rich feature sets for aspect term extraction and sentiment classification; 2. PSO based feature selection that yields a set of solutions for each classifier; and 3. Ensemble construction using a PSO to combine the outputs of classifiers. These steps are generic in nature and can be adapted to any other application domain. In our current paper we evaluate our proposed techniques for two different problems, namely aspect term extraction and sentiment classification. For each of these two problems we use the reviews from two domains, namely restaurant and laptop. The schematic diagram of the proposed method is depicted in <ref type="figure" target="#fig_1">Fig. 1</ref> . The features that we use for our tasks cover lexical, syntactic as well as semantic level information. The PSO based feature selection is single objective optimization (SOO) in nature, where we optimize only one function. We choose the most relevant features in such a way, that when the classifiers are trained with these particular combinations, maximize the objective functions. For aspect term extraction we optimize F-measure, whereas for sentiment classification we optimize the classification accuracy. Output of this process is a set of vectors, each of which corresponds to a particular feature combination. We choose a set of promising models based on their effectiveness (i.e. based on Fmeasure, recall and precision, or accuracy values). We select the best models in two different ways: 3 N effective classifiers ( N each from a particular classification technique) are selected based on Fmeasure values; and then 3 N models are selected in such a way that half of these are promising with respect to recall and half are with respect to precision. Our proposed method of ensemble construction operates in two steps: first step of which selects the most eligible candidate models (out of N as described above) for combination; and in the second step these are combined using majority or weighted voting approach. The best ensemble is obtained by optimizing F-measure or accuracy depending upon whether the problem deals with aspect term extraction or sentiment classification. Entire scheme is represented in the form of an algorithm, as mentioned below. In subsequent sections we present more detailed discussions on these steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1</head><p>Classi f ied <ref type="bibr">Test</ref>  is governed by the corresponding element v <ref type="bibr">( i, j )</ref> in V ( i ). Each element v <ref type="bibr">( i, d )</ref> in V ( i ) is updated following the process as mentioned below <ref type="bibr" target="#b42">[43]</ref> :</p><formula xml:id="formula_8">v (i, j) = w * v (i, j) + ¦Ì 1 (b i, j ? x (i, j) ) + ¦Ì 2 (g j ? x (i, j) )</formula><p>where w (0 &lt; w &lt; 1), ¦Ì 1 and ¦Ì 2 are known as inertia weight, cognitive and social scaling parameters, respectively. The ? ¡ú G , respectively. The concept of inertia weight was not there in the original version of PSO <ref type="bibr" target="#b43">[44]</ref> . It was first introduced by <ref type="bibr">Shi et al.</ref> [45] to better control the exploration and exploitation of the particles.</p><formula xml:id="formula_9">b ( i, j ) , x ( i, j ) , g ( j ) denote the j th components of ? ¡ú B (i ) , ? ¡ú X (i ) and</formula><p>ticles can be formulated as:</p><formula xml:id="formula_10">? ¡ú X (i ) = (x (i, 1) , x (i, 2) , ..., x (i,n ) )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Generating new particles:</head><p>For each particle ? ¡ú X (i ) and each bit position</p><formula xml:id="formula_11">x ( i, j ) in ? ¡ú X (i ) can</formula><p>where</p><formula xml:id="formula_12">x ( i, j ) ¡Ê {0, 1}, i = 1 , 2 , .</formula><p>.., N , N is the number of particles. The length of each particle ( n ) is equal to the number of features present. Each bit position of a particle corresponds to a feature. The value of this bit denotes whether the respective feature participates in classifier's training or not. A feature f j is used for classifier's training if and only if its corresponding bit position j contains a value of '1', otherwise, it is not considered for training. <ref type="figure" target="#fig_3">Fig. 2</ref> represents an example of particle representation for N = 4 and n = 10 . In Particle 1 , only five features i.e. f 3 , f 4 , f 6 , f 8 and f 9 , have the bit values 1 and hence only these participate in classifier's training.</p><p>Similarly Particle 2 encodes a feature combination where f 1 , f 3 , f 4 , f 8 and f 10 are considered for classifiers' training. At the beginning, a fixed number of N particles are initialized and encoded in the swarm. Encoding process is described as follows:</p><p>be either 0 or 1 based on the following criteria:</p><formula xml:id="formula_13">x (i, j) = 1 if r &gt; ¦Î (v (i, j) ) 0 otherwise</formula><p>where 0 ¡Ü r ¡Ü 1 is a uniform random number and is updated based on the fitness function (or, objective function). In our case the fitness function is the F-measure value of the classifier trained using the feature combination as represented in the particle ? ¡ú</p><formula xml:id="formula_14">¦Î v ( i, j ) = 1 1 + exp ? ¡ú v ( i,d ) 4.2. PSO based ensemble construction x (i, j) = 1 , if ¦Ì ¡Ý 0 . 5 0 , Otherwise</formula><p>It is a fact that a single learning algorithm is not always sufficient to produce the acceptable performance whenever the application domain is altered. If a learner L shows good accuracy for any domain m , it is not guaranteed that it will show similar performance for any other domain n as well. In classifier ensemble, several classifiers are combined together to produce the final output. For many applications the approaches were proved to be useful. The feature selection step described earlier yields a set of solutions for each of the learning algorithms, i.e. MEMM, CRF and SVM. Each solution represents a particular feature combination, which is used to construct a classification model. Our proposed PSO based ensemble selection method finds a good subset of models, and combine them together using majority and weighed voting techniques. The basic steps that we describe earlier in Section 4.1 also apply here. The notable difference is in the step of problem encoding, where each particle represents a set of individual classifiers in this case. As an example, for a given set of classifiers, </p><formula xml:id="formula_15">C = (c 1 , c 2 , c 3 , c 4 ,<label>c</label></formula><formula xml:id="formula_16">X ( 1 ) = ( 1 , 0 , 1 , 0 , 1 )</formula><p>date the best position by setting it to the current position (representing the best position, the particle has seen so far). Otherwise,</p><formula xml:id="formula_17">¡ú X ( 2 ) = ( 1 , 0 , 1 , 1 , 0 ) ¡ú it remains unaltered. This means that if f ( ? ¡ú P (i )) &gt; f ( ? ¡ú B (i )) , up- X ( 3 ) = ( 1 , 1 , 1 , 0 , 0 )</formula><p>date the value of ? ¡ú P (i ) . Here, the fitness value is computed to be equal to the F-measure value of the classifier. For the global best position, i.e. for ? ¡ú G we also follow the same process for updating. Initially, the global best position is set to empty. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Updating the velocities:</head><p>Every particle ? ¡ú X (i ) in the swarm has an associated velocity vector</p><formula xml:id="formula_18">V (i ) = (v (i, 1) , v (i, 2) , ..., v (i,n ) ) . Rate of change of x ( i, j ) in ? ¡ú</formula><p>Here for the first particle, three classifiers, namely c 1 , c 3 and c 5 are chosen as the candidates for constructing the ensemble as only these positions have the values of 1.</p><p>Once the eligible classifiers are identified, these are combined using either majority or weighted voting. In majority voting we analyse the outputs of several classifiers, and finally assign the class label that has the maximum occurrences. Thus, we always choose the particular class for which majority of the classifiers agree. In case of weighted voting, rather than uniform weights we assign different weights depending upon the strengths or weaknesses of the classifiers. For an instance, we add the weights of those classifiers that predict the same class. Finally, we assign the Classifier ensemble using majority and weighted voting. class that has the highest weighted vote. The weight is the Fmeasure or accuracy of the classifier. It is to be noted that Fmeasure or accuracy corresponds to the fitness value of the classifier (feature selection) or ensemble.</p><formula xml:id="formula_19">X (i )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier ( C i ) &amp; Weight ( f i ) Ensemble</head><formula xml:id="formula_20">C 1 C 2 C 3 C 4 C 5 Majority Weighted f 1 = 0 . 60 f 2 = 0 . 60 f 3 = 0 . 90 f 4 = 0 . 92 f 5 = 0 . 61 max (( 5 i =1 ,x f i , 5 i =1 ,</formula><p>In <ref type="table" target="#tab_4">Table 3</ref> , we provide an example for two kinds of voting schemes. Suppose, for a two-class ('x' and 'y') problem we have the five classifiers denoted by C i , i = 1... </p><formula xml:id="formula_21">f 1 + f 2 + f 5 = 1 . 81 ).</formula><p>ity or weighted voting techniques. Rest of the processes are similar to the PSO based feature set optimization. An example of PSO based classifier ensemble for sentiment classification is depicted in <ref type="table">Table 5</ref> . At iteration t , five particles are initialized and combined using the selected candidates (corresponding to the bit positions having values of 1). Output of the combined model is then evaluated and assigned a fitness value d i to each particle p i . As similar to the feature selection technique, PSO finds out a set of near-(optimal) solutions at the end of each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Datasets, experiments and analysis</head><p>In this section we first provide a brief description of the datasets that we use for our experiments, and then report the experimental results along with necessary analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Discussion on optimization using PSO:</head><p>After discussing various aspects of PSO, we try to realize the processes that it follows to optimize the feature set and the candidates for classifier ensemble.</p><p>Feature Selection: Every token in the training or test dataset is represented by a vector of length n , which is equal to the length of a particle in PSO, i.e. each bit position of a particle corresponds to exactly one feature. Once a particle is initialized, as defined in Section 4.1.1 , it represents a subset of features whose corresponding bit positions are '1'. We consider this feature subset as an input for classifier's training and its evaluation. Hence, each particle has an associated fitness value, say d i t for i th particle, at iteration t . In the next iteration t + 1 , a new set of particles is generated and evaluated. At the end of each iteration t , PSO selects a particle p (a.k.a global solution) that has the highest fitness value seen up to the t th iterations. Feature set represented by particle p is the optimized feature set up to the t th iterations. We continue this process up to the maximum number of iterations, and on termination PSO yields a set of near-(optimal) solutions. <ref type="table">Table 4</ref> shows one such example. It depicts 5 particles, showing the encoding of two iterations ( t &amp; t + 1 ) along with their corresponding fitness values. Particle P 1 at iteration t selects word, next, stop &amp; PoS as elements of its feature set. We only use these 4 features for training the models. Evaluation of the model yields the fitness value, i.e. d 1 = 69 . 78% . Similarly, P 2 at iteration t makes use of prev, next &amp; PoS as its feature set. Fittest particle and optimized feature set at the end of each iteration, i.e. t : P 4 and t + 1 : P 2 , are listed at the bottom of <ref type="table">Table 4</ref> .</p><p>Classifier Ensemble: For classifier ensemble, length of a particle in the swarm is set equal to the number of classifiers that participate in the ensemble process. In contrast to feature selection, each bit position of the particle encodes a classifier. Particles are initialized following the same technique as discussed in Section 4.1.1 . Presence of bit '1' in the particle selects the corresponding classifier's predicted output for ensemble process. Subsequently, outputs of the selected classifiers are combined and evaluated using majorWe use the benchmark datasets of SemEval-2014 shared task for our experiments. The dataset consists of user generated reviews from two domains, namely restaurant and laptop. The restaurant dataset comprises of 3,044 user reviews in the training set while gold test set contains 800 review instances. There are 3,045 &amp; 800 user reviews present in the training and gold test datasets, respectively, for the laptop domain. Number of aspect terms present in the two training sets counts to 3,699 and 2,358, respectively. Brief statistics of the datasets are shown in <ref type="table" target="#tab_8">Table 6</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Preprocessing:</head><p>At first the datasets are pre-processed to remove the XML tags, and extract the relevant bits of information. For this Stanford CoreNLP 6 suite is used to tokenize the reviews and extract various basic information such as lemma, Part-of-Speech (PoS) and named entity (NE) information. The aspect term can also be a multi-word token. In order to properly denote the boundaries of aspect term we use a IOB encoding scheme, where B, I and O denote the beginning, internal and outside tokens of aspect term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experimental results</head><p>As mentioned earlier, we use three robust classifiers, viz. CRF, SVM &amp; ME, as our base learners. For implementation of these algorithms we use CRF++, 7 Yamcha 8 and Stanford ME classifier, 9 respectively. Evaluation of all the systems are performed following the SemEval-2014 evaluation script. Since context information provides crucial information in aspect based sentiment analysis, we define four baseline systems, i.e. Baseline 1 , Baseline 2 , Baseline 3 and <ref type="table">Table 4</ref> Representative feature pruning scenario; features whose values are '0' are pruned; fitness values are hypothetical.  Results of the baseline models for each domain are reported in <ref type="table">Table 7</ref> .</p><p>Parameter settings of PSO: Different parameters of PSO guide its behavior and efficacy in optimizing the problem at hand. It has been shown in <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref> that we can achieve satisfactory performance with PSO if we tune its parameters properly. Several ways for assigning the parameters have been discussed in <ref type="bibr" target="#b47">[48]</ref> . We perform various experiments in order to choose the best fitting parameters for our problem. As a result, we found four different parameter settings that were (near)-optimal for the different tasks at hand. In order to maintain the uniformity we make use of all the four settings for the separate runs of PSO. <ref type="table" target="#tab_10">Table 8</ref> shows our choice of parameters for the ex- <ref type="table">Table 5</ref> Representative classifier pruning scenario for the sentiment classification task; Classifier whose values are '0' are pruned; Fitness values are hypothetical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aspect terms</head><p>Classifier's Output Ensemble Output Fitness Value  For each pair of instance and output label, feature vectors are generated. The classifiers are trained with these vectors, and PSO based feature selection technique is employed to find the most relevant set of features. In order to optimize the feature selection model, part of the training set was used as a development set in the work. Classifiers were trained on 90% of the training set while the development set, which constitutes of 10% of training set, was used for evaluating the performance of the trained models. Based on different parameter combinations, we execute PSO for four different runs as mentioned above. For a particular domain-classifier combination, we combine all these runs. Thereafter we select the top-most N models based on the F-measure measure (for aspect term extraction) or accuracy (for sentiment classification) value. Here we set the value of N as 20. It is also to be noted that, for aspect term extraction we select the models that show either good recall or precision values. This is done in order to choose the diverse classifiers (i.e. complimentary in nature), so that when they are combined, produces higher performance. The underlying assumption, being that the classifiers which perform good with respect to recall may not (in most cases) have the similar characteristics to those, performing good for precision. We assign these extracted models an unique name X Y i where Here we show the results of only the top 5 systems, which were selected based on F-measure or accuracy value. Results of these models are reported in <ref type="table" target="#tab_11">Table 9</ref> . Comparisons between the results of baselines and the models selected through PSO based feature selection (c.f. <ref type="table" target="#tab_11">Table 7 and Table 9 )</ref> show that we can achieve better performance if we are able to find out the most appropriate set of features. For aspect term extraction in the restaurant domain, best model of ME, M f 1 , obtains a F-measure of 72.86%. This is higher compared to all the other baseline models. For CRF and SVM we obtain the F-measures of 83.11% ( C f 1 ) and 81.76% ( S f 1 ), respectively, which are above all the baseline models. For sentiment classification we obtain the accuracies of 74.95%, 78.65% and 77.24% for ME, <ref type="table">Table 7</ref> Results of baseline systems. Here, f n is the total number of features for respective domain/classifiers.</p><formula xml:id="formula_22">C 1 C 2 C 2 C 4 C 5 (</formula><formula xml:id="formula_23">i = 1 ...N, Y ¡Ê { p(recision ) , r(ecal l ) , f (?measure ) } and X ¡Ê { M ( EM ), C ( RF ), S ( VM )}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>Restaurant  synsets, dependency relation, named entity etc., have greater impact than the others. In <ref type="table" target="#tab_4">Table 13</ref> , we present the optimized feature sets for sentiment classification, which clearly suggests that the use of sentiment lexicons is the primary cause in achieving good accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Results of classifier ensemble</head><p>CRF and SVM, respectively, for the restaurant domain. In case of laptop domain the system yields the F-measure values of 59.39%, 72.75% and 72.78% for aspect term extraction. The system for sentiment classification on this dataset yields the accuracies of 66.81%, 72.17% and 66.97%, for ME, CRF and SVM, respectively. In <ref type="table" target="#tab_11">Table 9</ref> , we also exhibit the number of features f n that participates in classifier's training. It shows how we can achieve better performance even with the use of a much reduced sets of features. As an instance, the PSO based feature selection model only makes use of 35 ( C f 1 ) and 27 ( S f 1 ) features for the training of CRF while the domains are restaurant and laptop, respectively. This proves that, indeed, feature selection helps to obtain better performance with a less complex model that utilizes less number of features. We show the experimental results in <ref type="table" target="#tab_12">Table 10</ref> , where 10 best models are selected based on the good recall and precision values (5 each).</p><p>In <ref type="table" target="#tab_14">Tables 11 and 12</ref> we show the optimized feature sets as determined by PSO for aspect term extraction for the restaurant and laptop domains, respectively. It shows that some of the features, e.g. context information, PoS tag, chunk, word cluster, WordNet Models extracted in the previous subsection are chosen as the potential candidates for constructing classifier ensemble. In order to find the best candidates for ensemble construction we employ the PSO based method that we described in the earlier section. Similar to feature selection approach we keep the same set of parameter combinations. The best set of classifiers obtained are combined using both majority voting and weighted voting techniques. Evaluation results are reported in <ref type="table" target="#tab_18">Table 14</ref> that shows the effectiveness of the weighted voting technique ( En Weighted ) over majority voting ( En Majority ) for all the cases. The proposed ensemble achieves the F-measure scores of 84.52% and 74.93% for aspect term extraction for restaurant and laptop domains, respectively. For sentiment classification we obtain the accuracies of 80.07% and 75.22% for restaurant and laptop domains, respectively. It is clearly evident that ensemble has been effective with considerable performance improvement (approximately 2% increase on an average) as a result of PSO based classifier selection process. In <ref type="table">Table 15</ref> we present the optimal subsets of classifier candidates which are used in final ensemble construction. Results show that ME based classifiers have the least contributions in comparison to CRF or SVM for all the  Result of top 5 models obtained through PSO based feature selection (w.r.t precision and recall). problems. Contribution of SVM and CRF based models are comparable to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifiers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparisons with the existing systems</head><p>Our proposed system performs convincingly better with respect to the baseline models that we defined. In order to further establish the effectiveness of our proposed approach we provide comparisons with some other state-of-the-art models as well. Comparisons with the existing systems are reported in <ref type="table" target="#tab_8">Table 16</ref> . In SemEval-2014 shared task, the best performing systems for aspect term extraction exhibit the F-measure values of 84.01% for the restaurant domain <ref type="bibr" target="#b48">[49]</ref> and 74.55% for the laptop domain <ref type="bibr" target="#b15">[16]</ref> . Both of these systems were developed based on CRF. This is lower in comparison to what we obtain,i.e. 84.52% and 74.91% for the restaurant and laptop domains, respectively. It is to be noted that the CRF based model developed in <ref type="bibr" target="#b48">[49]</ref> makes use of additional external resources and large amount of unlabeled data to generate word clusters, which were used as the features. The CRF based system of <ref type="bibr" target="#b15">[16]</ref> incorporates more extensive additional resources and a rule-based sentiment analysis module to improve the performance. The most distinctive characteristics of our present work compared to these two is that we don't make use of any heavy domain-specific resources and/ or tools. However, due to the use of systematic approaches for feature selection and classifier ensemble, we obtain better accuracies with much reduced sets of features. In comparison to the system of Liu et al. <ref type="bibr" target="#b17">[18]</ref> for aspect term extraction, our proposed model achieves more than 2% increase in F-measure value for the restaurant domain. For laptop domain, <ref type="bibr" target="#b17">[18]</ref> reported 75.00% F-measure value using Long-ShortTerm-Memory (LSTM) along with an extra set of features. In sentiment classification our system for aspect term extraction obtains Fmeasure values of 80.07% and 74.46% for the restaurant and laptop domains, respectively. For sentiment classification, the best system <ref type="bibr" target="#b16">[17]</ref> of the shared task reports the accuracies of 80.95% and 70.49% for these two domains. However, this is to be noted that the method proposed in <ref type="bibr" target="#b16">[17]</ref> was based on SVM that made use of some extra features extracted from the bag-of-word concept, rule-based system, and combined lexicons (BingLiu, SentiWordNet, MPQA). Like aspect term extraction task we also achieve better performance with less number of features. Our system also performs convincingly better as compared to <ref type="bibr">Kaljahi et al. [20]</ref> who uses tree-kernels based technique for the sentiment classifications.</p><p>The current work is an extension of our earlier work reported in <ref type="bibr" target="#b32">[33]</ref> , where a feature selection approach is developed for CRF classifier. The system proposed in the current work clearly performs better than our previous systems proposed in <ref type="bibr" target="#b32">[33]</ref> . The reasons behind this better performance are due to better re-implementation of some of the previous features, implementation of some new features, use of three different classification techniques and the PSO based classifier ensemble technique. We also present detailed experiments, thorough analysis of the results and error analysis. It should be noted that our system uses considerably less number of features and external resources as compared to the existing systems. Hence, the complexity of our proposed model is lower compared to the others.</p><p>We also perform Analysis of variance (ANOVA) <ref type="bibr" target="#b49">[50]</ref> to measure the statistical significance of the results obtained. For this the algorithm was executed 10 times. It was observed that differences (proposed model vs. existing state-of-the-art systems) in mean Fmeasure are statistically significant as p value is less than 0.05 in each case.</p><p>If P = # particles, I = # iterations, F v = # 1's in a particle on avg and ¦Î = model training time , then the time complexity of the proposed algorithm, i.e. PSO based feature selection and      eigen values and eigen vectors of the covariance matrix. These vectors provide information about the patterns in the data. Eigen vector corresponds to the highest eigen value contains the most important pattern, where as vector corresponds to next highest eigen value contains relatively lesser information than the first but more than others. Thus, by keeping only top k vectors, we can ignore/remove n ? k redundant features from the datasets. We then train, test and evaluate a model using the reduced subset of feature sets. We perform the following steps for PCA based dimensionality reduction:</p><formula xml:id="formula_24">M f 1 ?2..+3 ¡Ì ¡Ì ¡Ì - ¡Ì - - ¡Ì - ¡Ì ¡Ì ¡Ì 1,3,4 ¡Ì ¡Ì ¡Ì - M f 2 ?2..+2 ¡Ì ¡Ì - - - - - - - - ¡Ì ¡Ì 2,3,4,5 ¡Ì - ¡Ì - M f 3 ?1..+2 ¡Ì - - - - - ¡Ì ¡Ì - - ¡Ì - 3,4,5 ¡Ì - - - M f 4 ?3..+2 ¡Ì - ¡Ì - ¡Ì - - ¡Ì - ¡Ì ¡Ì ¡Ì 1,3,4,5 ¡Ì ¡Ì - - M f 5 ?3..+2 ¡Ì ¡Ì ¡Ì - - - ¡Ì ¡Ì ¡Ì - - ¡Ì 1,2,5 - ¡Ì - - C f 1 ?2..0 - ¡Ì - - ¡Ì - - ¡Ì - ¡Ì ¡Ì ¡Ì 2,5 ¡Ì - ¡Ì - C f 2 ?2..+3 - ¡Ì - - - - ¡Ì ¡Ì ¡Ì ¡Ì ¡Ì ¡Ì 2 ¡Ì - ¡Ì - C f 3 ?2..+3 ¡Ì - ¡Ì - ¡Ì - - - - - ¡Ì - 3 , 5 ¡Ì - - - C f 4 ?2..0 ¡Ì ¡Ì ¡Ì - - - - ¡Ì - - ¡Ì - 3,4,5 ¡Ì - ¡Ì - C f 5 ?2..+2 ¡Ì - - - - - - ¡Ì - ¡Ì ¡Ì - 3 ¡Ì - ¡Ì - S f 1 ?1..+1 - ¡Ì - - - ¡Ì - - - ¡Ì ¡Ì ¡Ì 1,5 ¡Ì ¡Ì - - S f 2 ?1..+2 ¡Ì - ¡Ì - ¡Ì - ¡Ì ¡Ì ¡Ì ¡Ì - ¡Ì 1 ¡Ì ¡Ì - - S f 3 ?1..+2 ¡Ì ¡Ì ¡Ì - ¡Ì - ¡Ì ¡Ì ¡Ì - ¡Ì ¡Ì 1,2,4 ¡Ì - ¡Ì - S f 4 ?1..+2 - ¡Ì ¡Ì - ¡Ì ¡Ì ¡Ì - ¡Ì ¡Ì - ¡Ì ¡Ì ¡Ì - - - S f 5 ?1..+1 ¡Ì ¡Ì - - - ¡Ì - - ¡Ì ¡Ì ¡Ì ¡Ì 1,3,4 ¡Ì ¡Ì ¡Ì - M p 1 ?2..+3 ¡Ì ¡Ì ¡Ì - ¡Ì - - ¡Ì - ¡Ì ¡Ì ¡Ì 1,3,4 ¡Ì ¡Ì ¡Ì - M p 2 ?3..+2 ¡Ì ¡Ì ¡Ì - - - ¡Ì ¡Ì ¡Ì - - ¡Ì 1,2,5 - ¡Ì - - M p 3 ?3..+2 ¡Ì ¡Ì ¡Ì - ¡Ì - ¡Ì ¡Ì ¡Ì ¡Ì - - 2,3,5 ¡Ì ¡Ì ¡Ì - M p 4 ?3..+2 ¡Ì - ¡Ì - ¡Ì - - ¡Ì - ¡Ì ¡Ì ¡Ì 1,3,4,5 ¡Ì ¡Ì - - M p 5 ?2..+3 ¡Ì ¡Ì - - - - - ¡Ì - ¡Ì ¡Ì ¡Ì 1 ¡Ì - ¡Ì - M r 1 ?1..+2 ¡Ì - - - - - ¡Ì ¡Ì - - ¡Ì - 3,4,5 ¡Ì - - - M r 2 ?2..+2 ¡Ì ¡Ì - - - - - - - - ¡Ì ¡Ì 2,3,4,5 ¡Ì - ¡Ì - M r 3 ?1..+2 ¡Ì ¡Ì ¡Ì - ¡Ì - - ¡Ì - ¡Ì ¡Ì - 1 , 5 ¡Ì - ¡Ì - M r 4 ?3..+2 ¡Ì - ¡Ì - ¡Ì - - ¡Ì - ¡Ì ¡Ì ¡Ì 1,3,4,5 ¡Ì ¡Ì - - M r 5 ?3..+2 ¡Ì ¡Ì - - - - ¡Ì ¡Ì - ¡Ì ¡Ì ¡Ì 2,5 ¡Ì - ¡Ì - C p 1 0 ¡Ì ¡Ì - - ¡Ì - ¡Ì ¡Ì ¡Ì ¡Ì ¡Ì ¡Ì 1,2 ¡Ì ¡Ì - - C p 2 ?2..+3 ¡Ì - ¡Ì - ¡Ì - - - - - ¡Ì - 3 , 5 ¡Ì - - - C p 3 ?2..+3 - ¡Ì - - - - ¡Ì ¡Ì ¡Ì ¡Ì ¡Ì ¡Ì 2 ¡Ì - ¡Ì - C p 4 ?3..+3 ¡Ì ¡Ì ¡Ì - ¡Ì - ¡Ì ¡Ì ¡Ì ¡Ì ¡Ì ¡Ì 2,3,4 ¡Ì ¡Ì - - C p 5 0.. + 1 ¡Ì - - - - - ¡Ì ¡Ì ¡Ì - ¡Ì - 1 , 4 ¡Ì - ¡Ì - C r 1 ?2..0 - ¡Ì - - ¡Ì - - ¡Ì - ¡Ì ¡Ì ¡Ì 2,5 ¡Ì - ¡Ì - C r 2 ?2..0 ¡Ì ¡Ì ¡Ì - - - - ¡Ì - - ¡Ì - 3,4,5 ¡Ì - ¡Ì - C r 3 ?3..+3 ¡Ì ¡Ì ¡Ì - ¡Ì - ¡Ì - - ¡Ì ¡Ì ¡Ì 4,5 ¡Ì ¡Ì - - C r 4 ?2..+3 - ¡Ì - - - - ¡Ì ¡Ì ¡Ì ¡Ì ¡Ì ¡Ì 2 ¡Ì - ¡Ì - C r 5 ?2..+3 ¡Ì - ¡Ì - ¡Ì - - - - - ¡Ì - 3 , 5 ¡Ì - - - S p 1 ?1..+1 ¡Ì ¡Ì - - ¡Ì ¡Ì - - ¡Ì ¡Ì - ¡Ì 2 ¡Ì ¡Ì ¡Ì - S p 2 ?1..+1 ¡Ì ¡Ì - - ¡Ì ¡Ì - - ¡Ì ¡Ì - ¡Ì 2,3 ¡Ì ¡Ì ¡Ì - S p 3 ?2..+1 ¡Ì ¡Ì - - ¡Ì ¡Ì - - ¡Ì ¡Ì - - 3 , 5 ¡Ì ¡Ì ¡Ì - S p 4 ?2..+2 ¡Ì ¡Ì - - ¡Ì - - ¡Ì ¡Ì ¡Ì - ¡Ì 3,4 ¡Ì ¡Ì ¡Ì - S p 5 ?2..+1 ¡Ì - ¡Ì - - ¡Ì - - ¡Ì - ¡Ì - 1,2,3,5 ¡Ì ¡Ì ¡Ì - S r 1 ?1..+1 - ¡Ì - - - ¡Ì - - - ¡Ì ¡Ì ¡Ì 1,5 ¡Ì ¡Ì - - S r 2 ?1..+1 ¡Ì ¡Ì - - ¡Ì ¡Ì ¡Ì - ¡Ì ¡Ì ¡Ì ¡Ì 1,3 ¡Ì ¡Ì ¡Ì - S r 3 ?1..+1 ¡Ì ¡Ì - - - ¡Ì - - ¡Ì ¡Ì ¡Ì ¡Ì 1,3,4 ¡Ì ¡Ì ¡Ì - S r 4 ?1..+2 ¡Ì - ¡Ì - ¡Ì - ¡Ì ¡Ì ¡Ì ¡Ì - ¡Ì 1 ¡Ì ¡Ì - - S r 5 ?1..+2 ¡Ì - - - ¡Ì ¡Ì - - ¡Ì ¡Ì ¡Ì ¡Ì 2,5 ¡Ì ¡Ì - -</formula><formula xml:id="formula_25">M f 1 ?2..+3 ¡Ì ¡Ì - - - - ¡Ì ¡Ì - ¡Ì ¡Ì - 2 , 4 ¡Ì - ¡Ì - M f 2 0.. + 1 ¡Ì - - - ¡Ì - - - - - ¡Ì - 1,4,5 ¡Ì - ¡Ì - M f 3 ?1..+1 ¡Ì ¡Ì - - - - ¡Ì ¡Ì - - - - 2,3,4 ¡Ì - - - M f 4 ?1..+1 ¡Ì - - ¡Ì ¡Ì - ¡Ì ¡Ì - ¡Ì - - 4 , 5 ¡Ì - ¡Ì - M f 5 ?1..+1 ¡Ì - ¡Ì - - - - - - ¡Ì - - 3 , 4 ¡Ì - ¡Ì - C f 1 ?2..0 ¡Ì - ¡Ì ¡Ì ¡Ì - - ¡Ì ¡Ì - ¡Ì ¡Ì 2,3,5 ¡Ì ¡Ì ¡Ì d C f 2 ?1..+1 ¡Ì - ¡Ì ¡Ì ¡Ì - ¡Ì ¡Ì ¡Ì ¡Ì ¡Ì ¡Ì 1,4 ¡Ì - - c C f 3 ?1..+1 ¡Ì ¡Ì ¡Ì - - - - - - ¡Ì - ¡Ì 1,3,5 ¡Ì ¡Ì - ¡Ì M.S</formula><formula xml:id="formula_26">C f 4 ?2..+2 ¡Ì - ¡Ì ¡Ì - - ¡Ì ¡Ì - - ¡Ì ¡Ì 2,3 ¡Ì ¡Ì ¡Ì ¡Ì C f 5 0.. + 2 ¡Ì ¡Ì ¡Ì ¡Ì ¡Ì - ¡Ì ¡Ì ¡Ì - ¡Ì - 4 , 5 ¡Ì ¡Ì ¡Ì d S f 1 ?1..+1 ¡Ì ¡Ì ¡Ì - ¡Ì ¡Ì ¡Ì - - - ¡Ì - 4 ¡Ì - - c S f 2 ?2..+1 ¡Ì ¡Ì - ¡Ì ¡Ì - - - - - ¡Ì ¡Ì 1,4,5 ¡Ì - - d S f 3 ?2..+1 ¡Ì - - - - - ¡Ì ¡Ì - - ¡Ì - 4 , 5 ¡Ì - - c S f 4 ?1..+1 - - ¡Ì - ¡Ì - - ¡Ì ¡Ì - ¡Ì - 2,3,4 ¡Ì ¡Ì - - S f 5 ?1..+2 ¡Ì ¡Ì - - ¡Ì ¡Ì - ¡Ì ¡Ì - ¡Ì - - ¡Ì - - d M p 1 ?2..+3 ¡Ì ¡Ì - - - - ¡Ì ¡Ì - ¡Ì ¡Ì - 2 , 4 ¡Ì - ¡Ì - M p 2 ?1..+1 ¡Ì ¡Ì - - - - ¡Ì ¡Ì - - - - 2,3,4 ¡Ì - - - M p 3 ?3..+2 ¡Ì ¡Ì - - - - ¡Ì ¡Ì - ¡Ì - - 1 , 3 , 5 ¡Ì - ¡Ì - M p 4 ?3..+2 ¡Ì ¡Ì - ¡Ì - - - - - ¡Ì ¡Ì - 1 ¡Ì - ¡Ì ¡Ì M p 5 ?3..+2 ¡Ì ¡Ì - - - - - - - ¡Ì ¡Ì - 1,2,4 ¡Ì - ¡Ì - M r 1 ?3..+3 ¡Ì ¡Ì ¡Ì - - - ¡Ì - - ¡Ì ¡Ì - 2 , 3 ¡Ì - ¡Ì - M r 2 ?2..+2 ¡Ì ¡Ì - - - - - - - - ¡Ì ¡Ì 2,3,4,5 ¡Ì - ¡Ì - M r 3 ?2..+3 ¡Ì ¡Ì ¡Ì - ¡Ì - - ¡Ì - ¡Ì ¡Ì ¡Ì 1,3,4 ¡Ì ¡Ì ¡Ì - M r 4 ?3..+2 ¡Ì - ¡Ì - ¡Ì - - ¡Ì - ¡Ì ¡Ì ¡Ì 1,3,4,5 ¡Ì ¡Ì - - M r 5 ?3..+2 ¡Ì ¡Ì ¡Ì - - - ¡Ì ¡Ì ¡Ì - - ¡Ì 1,2,5 - ¡Ì - - C p 1 0.. + 2 ¡Ì ¡Ì ¡Ì ¡Ì ¡Ì - ¡Ì ¡Ì ¡Ì - ¡Ì - 4 , 5 ¡Ì ¡Ì ¡Ì d C p 2 ?1..+1 ¡Ì - ¡Ì ¡Ì ¡Ì - ¡Ì ¡Ì ¡Ì ¡Ì ¡Ì ¡Ì 1,4 ¡Ì - - c C p 3 ?3..+3 - ¡Ì ¡Ì ¡Ì ¡Ì - - - ¡Ì ¡Ì - - 5 ¡Ì ¡Ì - - C p 4 ?2..+2 ¡Ì - ¡Ì ¡Ì - - ¡Ì ¡Ì - - ¡Ì ¡Ì 2,3 ¡Ì ¡Ì ¡Ì ¡Ì C p 5 ?2..0 ¡Ì - - ¡Ì - - - - ¡Ì ¡Ì ¡Ì ¡Ì 1,3 ¡Ì ¡Ì - - C r 1 ?1..+2 ¡Ì ¡Ì ¡Ì - ¡Ì - ¡Ì ¡Ì ¡Ì - - ¡Ì 3,4 ¡Ì ¡Ì ¡Ì - C r 2 ?1..+1 ¡Ì ¡Ì ¡Ì - - - - - - ¡Ì - ¡Ì 1,3,5 ¡Ì ¡Ì - ¡Ì C r 3 0.. + 2 ¡Ì ¡Ì ¡Ì ¡Ì ¡Ì - ¡Ì ¡Ì ¡Ì - ¡Ì - 4 , 5 ¡Ì ¡Ì ¡Ì d C r 4 ?2..+2 ¡Ì - ¡Ì ¡Ì - - ¡Ì ¡Ì - - ¡Ì ¡Ì 2,3 ¡Ì ¡Ì ¡Ì ¡Ì C r 5 0 ¡Ì ¡Ì - - ¡Ì - - - ¡Ì ¡Ì ¡Ì ¡Ì 1,3 ¡Ì - - - S p 1 ?2..+1 ¡Ì - - - - - ¡Ì ¡Ì - - ¡Ì - 4 , 5 ¡Ì - - c S p 2 ?2..+1 - ¡Ì - - - - - ¡Ì - - ¡Ì - 1,3,4 ¡Ì - ¡Ì ¡Ì S p 3 ?2..+2 ¡Ì - - - - - - - - ¡Ì - ¡Ì 1 ¡Ì - ¡Ì ¡Ì S p 4 ?2..+1 ¡Ì - - - ¡Ì - ¡Ì - - ¡Ì ¡Ì ¡Ì 2,3,4 ¡Ì - - d S p 5 ?2..+1 ¡Ì - - ¡Ì ¡Ì ¡Ì - - - - ¡Ì ¡Ì ¡Ì ¡Ì - - ¡Ì S r 1 ?1..+1 ¡Ì ¡Ì ¡Ì - ¡Ì ¡Ì ¡Ì - - - ¡Ì - 4 ¡Ì - - c S r 2 ?2..+1 ¡Ì - - ¡Ì ¡Ì ¡Ì - ¡Ì - ¡Ì - ¡Ì 1,3,4 ¡Ì - - ¡Ì S r 3 ?2..+1 ¡Ì ¡Ì - ¡Ì ¡Ì - - - - - ¡Ì ¡Ì 1,4,5 ¡Ì - - d S r 4 ?1..+1 - - ¡Ì - ¡Ì - - ¡Ì ¡Ì - ¡Ì - 2,3,4 ¡Ì ¡Ì - - S r 5 ?1..+1 ¡Ì ¡Ì - ¡Ì - - - ¡Ì - ¡Ì ¡Ì ¡Ì 1,2,3,4 ¡Ì - ¡Ì d M.S</formula><formula xml:id="formula_27">M f 1 ?2..+3 ?1..+2 - - - ¡Ì ¡Ì ?2..+3 - - - ¡Ì ¡Ì - M f 2 ?2..+1 0.. + 2 - ¡Ì - ¡Ì ¡Ì 0.. + 3 - - - ¡Ì - - M f 3 ?2..+1 ?2..+1 - - - - ¡Ì 0.. + 3 0.. + 1 - - ¡Ì ¡Ì - M f 4 ?2..+2 ?4..+1 - ¡Ì - ¡Ì ¡Ì ?4..+3 - - ¡Ì - ¡Ì - M f 5 ?2..+5 - ¡Ì - ¡Ì - - ?2..+1 0.. + 2 - - ¡Ì ¡Ì - C f 1 ?1..+1 0.. + 2 ¡Ì - ¡Ì - - 0 ?2..+1 - ¡Ì - ¡Ì - C f 2 ?1..+1 ?3..+2 ¡Ì ¡Ì ¡Ì ¡Ì - 0 ?2..0 - ¡Ì - - - C f 3 ?4...0 0.. + 2 - - ¡Ì ¡Ì ¡Ì 0 ?1..+1 - ¡Ì - ¡Ì - C f 4 0.. + 1 ?4..0 ¡Ì - ¡Ì ¡Ì - ?1..0 ?2..+2 - ¡Ì ¡Ì ¡Ì - C f 5 ?2..0 0.. + 2 ¡Ì - ¡Ì ¡Ì - 0 ?1..+1 - ¡Ì ¡Ì ¡Ì - S f 1 0.. + 3 ?1..0 - ¡Ì ¡Ì - - -1 . . + 1 ?1..+2 ¡Ì ¡Ì ¡Ì ¡Ì - S f 2 0.. + 1 0.. + 3 - ¡Ì ¡Ì - - ?1..+1 - - ¡Ì ¡Ì ¡Ì - S f 3 0.. + 2 - - ¡Ì ¡Ì - - 0 . . + 2 ?1..0 ¡Ì ¡Ì ¡Ì - - S f 4 0 ?4..0 ¡Ì ¡Ì ¡Ì - - -1 . . + 2 - ¡Ì ¡Ì ¡Ì - - S f 5 0.. + 2 - - ¡Ì ¡Ì - - ?1..+1 ?1..0 ¡Ì ¡Ì ¡Ì - -</formula><p>classifier ensemble, would be O ((</p><formula xml:id="formula_28">P * I * F v * ¦Î ) + (P * I * F v )) ¡Ö O (P * I * F v * (¦Î + 1)) .</formula><p>The key characteristics of the current work are as follows: (i). proposal of a two-step, first step of which performs feature selection and the second step performs ensemble learning for aspect based sentiment analysis; (ii). use of extensive feature sets for aspect term extraction and sentiment classification; (iii). proposal of an aspect term extractor and sentiment analyzer that yield stateof-the-art performance on benchmark datasets; (iv). finding that small set of relevant features can actually improve the classifier's performance; and (v). determining proper subsets of classifiers further improves the performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Comparison among feature selection techniques: PSO, PCA and I nformation gain</head><p>As a comparison to PSO based feature selection, we exploits PCA and Information gain for reducing the dimension of feature space for our problems. Principal Component Analysis (PCA) <ref type="bibr" target="#b50">[51]</ref> is a useful statistical technique to compress the data by reducing the number of dimensions. It finds the patterns in data of high dimension and transforms it into lower dimension by leaving out redundant information. PCA starts its processing by calculating the Information gain corresponds to the gain obtained due to the reduction in entropy when the data is distributed among the different classes with respect to a particular feature. We calculate information gain <ref type="bibr" target="#b9">10</ref> for each feature and sort them in ascending order. Top few features are then selected and used for the training of classifier. We report the result of PCA and Information gain based feature space reduction techniques in <ref type="table">Table 17</ref> along with the result of PSO. It also reports the value of f n / k (i.e. number of features) that were required for the respective models. Results show that PSO based methods achieve better result as compared to both PCA and Information gain for all the cases. Also, it should be noted that PSO based model requires relatively less number of features than the other two techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 17</head><p>Comparison between PSO, PCA and Information gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifiers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Restaurant Laptop</head><p>Aspect term Sentiment Aspect term Sentiment  </p><formula xml:id="formula_29">P R F f n / k Acc f n / k P R F f n / k Acc f n / k ME</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Comparison among ensemble techniques: B agging, B oosting, S tacking and V oting</head><p>We also perform experiments with some of the well-known ensemble techniques such as bagging <ref type="bibr" target="#b51">[52]</ref> , AdaBoost <ref type="bibr" target="#b52">[53]</ref> , stacking <ref type="bibr" target="#b53">[54]</ref> and voting <ref type="bibr" target="#b54">[55]</ref> . We choose sequential minimal optimization (SMO) algorithm for support vector machine as a base classifier for bagging and AdaBoost. For stacking and voting we use logistics regression, SMO and naive Bayes as our base classifiers. In addition, we opt for SMO as our meta classifier in stacking and majority class technique for voting. For implementation of these algorithms we used WEKA 3.6 <ref type="bibr" target="#b10">11</ref> .</p><p>We observe that, in all cases, our proposed approach yields better results compared to these existing methods. Results are reported in <ref type="table" target="#tab_10">Table 18</ref> . Performance improvements were also shown to be statistically significant. system's performance. In sentiment classification task, majority of the 'positive' instances were correctly predicted for both the domains. However, the system misclassifies most of the time for the 'neutral' instances. For the restaurant domain it misclassifies 116 instances, while only 80 instances are classified correctly. Similarly in laptop domain, proposed system performs slightly better than the restaurant domain for the 'neutral' class with 100 correct classifications and 69 misclassifications. For 'conflict' class our system fails to correctly predict all the instances. This could be because the proposed model was trained with a very fewer number of instances of 'conflict' class. We hope the system could do better with a greater number of training instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.">Error analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.2.">Qualitative analysis</head><p>We analyze the outputs of our proposed system and found that it mainly lags behind in the following scenarios:</p><p>Aspect Term Extraction:</p><p>In this section we present an analysis of the errors encountered by our proposed system. We present both quantitative as well as qualitative analysis with respect to the across-domain and acrossclassifier phenomenon of feature selection.</p><p>? System fails in correctly identifying an aspect term which include braces. For example, our system predicted installation disk as an aspect term but fails to tag the braces (including inner text i.e. 'DVD') as 'I-ASP'. This results in incorrect identification of the term in question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.1.">Quantitative analysis</head><p>Error analysis is performed in terms of confusion matrix as shown in <ref type="figure" target="#fig_11">Fig. 3</ref> . For aspect term extraction in restaurant domain, quite a good number of 'B-ASP' (Beginning of an aspect term) and 'I-ASP' (Intermediate tokens of an aspect term) are wrongly predicted as 'O' (others). A total of 347 instances were misclassified for these two classes. Our system also faces the same problem for the laptop domain as there are 400 misclassified instances for 'B-ASP' and 'I-ASP' classes. Possible reason behind this anomaly could be the presence of relatively large number of 'O' (other than aspect term) tokens in the training set. Also, few instances of 'I-ASP' and 'O' were misclassified as 'B-ASP' which further hampers the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Review:</head><p>No</p><formula xml:id="formula_30">installation disk ( DVD ) is included . Actual: O B-ASP I-ASP I-ASP I-ASP I-ASP O O O Predicted: O B-ASP I-ASP O O O O O O</formula><p>? Aspect terms which are made up of words and digits are shown to have been misclassified. For example, aspect terms like ' Windows 7 ' and ' i5 ' are not predicted correctly by our proposed system. In the first case our system predicts only ' Windows ' as aspect leaving ' 7 ' alone. However, in the second case it does not predict it at all.</p><p>? We found few cases where the last word of any sentence, even being a part of an aspect term, is misclassified. This may be attributed to the fact that due to the lack of right contextual information, the system has not been able to identify these properly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Review:</head><p>Apple is unmatched in ... and customer service . Actual:</p><formula xml:id="formula_31">O O O O ... O B-ASP I-ASP O Predicted: O O O O ... O B-ASP O O Sentiment Classification:</formula><p>? Negative words close to the aspect term bias the sentiment towards it. For the example given below, installation disk (DVD) was an aspect term and the actual polarity towards it was neutral , but the presence of No changes the polarity of installation disk (DVD) to 'negative'. Few of the features that we have implemented in our proposed model have a higher degree of relevance against others, irrespective of the classification algorithms we used. For example, in aspect term extraction task 'context information', 'word cluster', 'PoS tag' and 'WordNet' are the most prominent features in most of the models (for at least 11 out of 15 best performing models). Here, we closely analyze these features and present the possible explanations behind their selection by majority of the models. Sometimes the combination of features vary because of the random behavior of PSO. In PSO, each bit position of the feature vector randomly oscillates between 0 and 1 depending upon the random number generated.</p><p>Since the task of aspect term extraction can be seen as the sequence labeling problem, and therefore word and its context information are unarguably required, thus justifying its selection in the model construction. Word cluster feature tries to group different words that are semantically similar in nature. Therefore, it helps in grouping the tokens that have similar characteristics, for example, all the nouns and noun groups have the tendency of being in the same cluster(s). We induce 21 and 20 clusters for the restaurant and laptop domains, respectively. We observe that all the aspect terms in the test dataset lie in the same training clusters for each domain.</p><p>Part of Speech (PoS) tag feature is selected in most of the cases. Aspect terms generally refer to the entities that belong to the noun and noun groups. The use of PoS tag as a feature encourages the model to better capture the properties of aspect terms. There are approximately 84% and 82% noun aspect terms present in the training and gold dataset for restaurant domain, respectively. Similarly in laptop domain, training and gold datasets contain approximately 80% and 74% aspect terms that denote the noun PoS tag. Therefore, it is evident that PoS information of a token does play an important role in correctly identifying the aspect terms. Majority of the aspect terms belong to the noun PoS categories. For the restaurant and laptop domains, these are approximately 89% and 82%. Ou of these 81% and 72% have been correctly identified. The WordNet related feature has been very useful for handling the unseen aspect terms. There are 353 (31%) and 273 (41.74%) instances of unseen aspect terms in the restaurant and laptop domain, respectively. For example, an unseen aspect term 'hotdogs' is correctly predicted by our model whose synset element 'food' was present in the training dataset.</p><p>In sentiment classification task, three lexicons features i.e. Bing, Bing Direct and SentiwordNet along with the word and context features are chosen most of the time as candidates for training and testing in both the domains. However, Bing and Bing direct features have higher impact in laptop domain as compared to the other lexicon features. This could be because Bing Liu opinion mining lexicon <ref type="bibr" target="#b40">[41]</ref> itself was created from the electronics corpus. Now, we analyze the results of feature selection across the various classifiers. Closer observations to the various experimental setups suggest that selection of features heavily depends on the classification algorithm and the PSO. Due to the nature of randomness of PSO, the feature vectors do change over the iterations, and this has significant effect on the decision behind a features' inclusion or exclusion from the final feature set. Feature vectors change depending upon the velocity, which is mainly controlled by three important parameters of PSO, viz. inertia weight, social scaling and cognitive parameters. Hence the performance could vary from one iteration to the other iteration. Again due to randomness of the search process, selection of features could vary even within the different models, built using the same classification algorithm. Our experiments show that among the top-performing models there are features which are not always selected in all the models of CRF, SVM or ME.</p><p>We also try to analyze the classifier's behavior by adding or deleting a feature to the optimized feature set in model training. We choose 'chunk' feature for this analysis. We add 'chunk' feature to the optimized feature sets of the top models (i.e. M f 1 , C f 1 and S f 1 ) if it was not selected by PSO.</p><p>Restaurant domain: From <ref type="table" target="#tab_14">Table 11</ref> it can be observed that 'chunk' feature was selected for the ME model, M f 1 , but not for the models C f 1 and S f 1 . So, we drop 'chunk' feature from the first model and add it to the last two models. The model based on ME (i.e. M f 1 ) reports an F-measure of 72.64% (without chunk) as compared to 72.86% (with chunk). Similarly, C f 1 and S f 1 with chunk information yield 82.77% and 81.38% F-measure, respectively as compared to 83.11% and 81.76% (i.e. without chunk). Hence the adding (deleting) any feature to (or from) the feature set actually hurts the systems' performance. This entails that our feature set as determined by PSO is optimized in nature. It is also be noted that although this feature is not selected in the best models of SVM or CRF, this is included in the other top models. The reason behind this is the random behavior of PSO. As discussed earlier, chunk information is useful mainly for detecting the multiword aspect terms. However, the restaurant dataset contains relatively lesser percentage (24%) of multiword aspect terms, which could be a possible justification for the absence of chunk information from the optimized feature sets of few models.</p><p>Laptop domain: Similarly, we perform the above analysis for the laptop domain. Chunk information was absent from M f 1 but present in both C f 1 and S f 1 models. This suggest that 'chunk' feature has a relatively higher degree of relevance in laptop dataset which has approximately 45% multiwords aspect terms. The other cause is again the random behavior of PSO.</p><p>Closer analysis to the various experimental setups suggests that selection of features heavily depends on two important aspects, viz. classification algorithm used and the PSO. Due to randomness of PSO, particles do change over the iterations, and this has significant effect on the decision behind a feature's/classifier's inclusion or exclusion into/from the final optimized set. Any change in the particle depends upon the velocity, which is mainly controlled by three important parameters of PSO, viz. inertia weight, social scaling and cognitive parameters. Hence, the performance could vary from one iteration to the other. We found that PSO indeed produces the optimized feature sets, and any perturbation to these led to lower performance. Our experiments show that the use of ensemble techniques in our proposed method did improve the result by a good margin, which is in line with <ref type="bibr" target="#b27">[28]</ref> who proved the effectiveness of ensemble learning is sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper we have presented an efficient method for feature selection and ensemble learning for aspect based sentiment analysis. The algorithm is based on single objective PSO. As base learning algorithms we use CRF, SVM and ME. In the first step we determine the best feature sets for aspect term extraction and sentiment classification. This yields a set of solutions, each of which represents a particular feature combination. Based on certain criteria we choose the most promising solutions from the final population of PSO. The models developed with these feature combinations are combined together using a PSO based ensemble technique. The ensemble learner finds out the most eligible models, that when combined together, maximizes some classification quality measures like F-measure (for aspect term extraction) or accuracy (for sentiment classification). As the base learning algorithms we use three classifiers, namely ME, CRF and SVM. We have identified and implemented various lexical, syntactic or semantic level features for solving the problems. Experiments on the benchmark datasets of SemEval-2014 show our proposed techniques attain state-of-the-art performance for both aspect term extraction and sentiment classification. We compare the performance with the best performing systems that were developed using the same setups, several baseline models and the existing systems. In all the settings our proposed methods showed the effectiveness with reasonable performance increments. The key contributions of the current work can be summarized as below: (i). proposal of a two-step process for feature selection and ensemble learning using PSO; (ii). developing a PSO based feature selection and ensemble learning technique for the application like sentiment analysis; (iii). building domain-independent models for aspect based sentiment analysis that achieves state-of-the-art performance; (iv). finding how efficiently we can improve the classifiers' performance if it is trained with the most relevant set of features (particularly for sentiment analysis).</p><p>The current work focuses on single objective optimization technique, where we deal with only one objective function at a time. In future we would like to explore how multiobjective optimization that deals with the optimization of more than one objective function be effective for solving the problems. Future work also includes the studies of how the proposed system works for sentiment analysis in other domains and languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Every particle ? ¡ú X (k ) is associ- ated with a velocity vector. The velocity vector is updated at every generation, and used to generate a new particle ? ¡ú X (k ) . The neigh- borhood defines how other particles in the swarm, such as ? ¡ú B (k ) and ? ¡ú G , interact with ? ¡ú X (k ) to modify its respective velocity vector and position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 .</head><label>1</label><figDesc>Aspect term and its context : Surface form of an aspect term along with its lower case form are used as features. As sen- timent of a review heavily depends on the context where an aspect term appears, we use five tokens to the left and right as the local context information (i.e. a context word window of size 11 including the current one). 2. Sentiment lexicon : Sentiment lexicons are the important and useful sources for analyzing the opinions. Based on the follow- ing lexicons we extract few features: (a) MPQA lexicon : MPQA subjectivity lexicon [40] catego- rizes word sentiment into positive, negative and neu- tral classes. It contains 8,0 0 0 words along with its corre- sponding sentiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Proposed methodology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Particle encoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>A</head><label></label><figDesc>uniform random number ¦Ì on the interval (0, 1) is generated for each bit position x ( i, j ) of ? ¡ú X (i ) and if ¦Ì is greater than a threshold of 0.5 we choose the bit value as '1', otherwise it is initialized with '0'. 4.1.2. Updating the global and best position value: At the very beginning, previous best position of the particle ? ¡ú X (i ) , represented by ? ¡ú B (i ) , is set to null. As the initial particle is generated, we set the value of ? ¡ú B (i ) to the position vector of the particle ? ¡ú X (i ) . At each iteration the best position vector ? ¡ú B (i )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>5 ) and N = 3, the swarm can be represented as follows: X (i ) . If the current position vector ? ¡ú P (i ) is better than its ¡ú best position vector ? ¡ú B (i ) with respect to the fitness value, we up-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>The update only happens after all the values of ? ¡ú B (i ) are determined. The value of ? ¡ú G is set to the fittest ? ¡ú B (i ) found so far. It is updated only when the fittest solution represented by f ( ? ¡ú B (i )) in the swarm is supe- rior to f ( ? ¡ú G ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>5, along with their respec- tive weights f i . Predicted class labels of each classifier are shown in the respective columns. For the first instance, predicted class labels are 'x', 'x', 'y', 'y' &amp; 'x' respectively. Three classifiers i.e. C 1 , C 2 &amp; C 5 have predicted 'x' whereas two classifiers C 3 &amp; C 4 have predicted 'y' as the class labels. Since the count of class 'x' is higher than the count of 'y', majority voting technique picks 'x' as the final class label. However, in case of weighted voting technique, class label 'y' is chosen because weighted sum of 'y' class ( f 3 + f 4 = 1 . 82 ) is greater than the weighted sum of class 'x' (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Features</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>For example, C f 1 and C f 2 represent two CRF based models with highest F-measure values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>1 .</head><label>1</label><figDesc>Convert training and test datasets into numerical form. 2. Find covariance matrix of datasets. 3. Calculate eigen values and Eileen vector of the covariance ma- trix. 4. Sort eigen vectors w.r.t non-increasing eigen values. 5. Keep the top k vectors. 6. Train, test and evaluate the reduced datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Confusion matrix for different problems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Review: No installation disk (DVD) is included. Aspect Term: installation disk (DVD) Actual sentiment: neutral Predicted sentiment: neg- ative ? Our system fails to correctly classify sentiment in smaller sen- tences. An example is given below: Review: The sangria 's -watered down. Aspect Term: sangria Actual sentiment: negative Predicted sentiment: positive</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>( was ) contains four tokens, i.e. w i ?2 ( the ), w i ?1 ( staff), w i +1 ( so ) and w i +2 ( horrible ) as its contexts.</head><label></label><figDesc></figDesc><table>1. Words: Surface forms and their converted lower-cased versions 
are used as two separate features. 
2. Local context information: Local contextual information plays 
an important role to properly identify aspect terms. For context 
information, for each token a window is defined that consti-
tutes the preceding and following few tokens. Here, we use the 
context of size 5, i.e. 2 words to the left and 2 words to the 
right. For the example shown below, local context for the token 
w i </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>= PSO _ Asent (F eature T rain , F eature De v , F eature Test , c 1 , c 2 , c 3 , N) Input: F eature T rain , F eature De v , F eature Test -Feature files, c 1 , c 2 , c 3 -Three classifiers corresponds to CRF, SVM &amp; ME, N -Top N models. Output: Classi f ied Test -Final classified output. 1: For each classifiers c i do 2: Models c i ¡û PSO (F eature T rain , F eature De v , c i ) 3: T op c i ¡û GetTopModels (Models c i , N) 4: end for 5: Cand id ates Classi f ier ¡û FeatureVector (T op c 1 , T op c 2 , T op c 3 ). 6: BestCand id ates M ¡û PSO (Cand id ates Classi f ier , Ma j orit yV ot ing) 7: BestCand id ates W ¡û PSO (Cand id ates Classi f ier , W eighted V oting) 8: EnsembleMod el M ¡û BuildEnsembleModel (BestCand id ates M ) 9: EnsembleMod el W ¡û BuildEnsembleModel (BestCand id ates W ) 10: M odel Ensemble ¡û BetterPerformance (EnsembleM odel M , EnsembleModel W ) 11: Classi f ied Test ¡û PredictClass (Model</head><label></label><figDesc></figDesc><table>Ensemble , F eature Test ) 

4.1. Feature selection using PSO 

We develop our feature selection technique using a binary ver-
sion of PSO. Basic steps of the proposed approach are described as 
below: 

4.1.1. Particle encoding and initialization: 
Potential solutions 
? ¡ú 

X (i ) , particles in PSO, are initialized with a 

fixed-length binary valued-string of 0's &amp; 1's. Mathematically, par-</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Baseline 2 takes all the features in context window of w i +3 i ?3 for training. Baseline 3 is based on PSO optimization. It is similar to Baseline 1 and Baseline 2 except context information was obtained from PSO. The baseline system, Baseline SemEval was defined by the organizers of the SemEval 2014 task. Below, we describe the base- line systems: 4. Baseline SemEval : This fourth baseline is derived from2 : This is similar to Baseline 1 except the context window w i +3 i ?3</head><label></label><figDesc></figDesc><table>Fitness Value 

Word 
Prev 
Next 
WordLen 
Stop 
POS 
d i 

Sample feature file 
The 
NULL 
food 
0 
1 
DT 
-
food 
The 
was 
0 
0 
NN 
was 
food 
good 
0 
1 
VBD 
good 
was 
but 
0 
0 
JJ 
but 
good 
service 
0 
1 
CC 
service 
but 
was 
1 
0 
NN 
was 
service 
poor 
0 
1 
VBD 
poor 
was 
. 
0 
0 
JJ 
. 
poor 
NULL 
0 
0 
. 
Particle Swarm Optimization (PSO) based feature selection 

Particle P i 
Iteration t 
P 1 
1 
0 
1 
0 
1 
1 
69.78 
P 2 
0 
1 
1 
0 
0 
1 
70.68 
P 3 
1 
0 
0 
1 
1 
0 
70.23 
P 4 
1 
0 
1 
0 
0 
1 
71.24 
P 5 
1 
1 
0 
1 
0 
0 
67.94 
Iteration t+1 
P 1 
1 
1 
0 
0 
0 
1 
69.54 
P 2 
1 
0 
1 
0 
1 
1 
72.87 
P 3 
1 
1 
0 
1 
0 
0 
71.43 
P 4 
0 
1 
1 
0 
1 
0 
67.98 
P 5 
1 
0 
1 
1 
1 
1 
68.76 
Fittest particle and optimized feature set at Iteration t 

P 4 
1 
0 
1 
0 
0 
1 
71.24 

Features for P 4 
The 
food 
DT 
71.24 
food 
was 
NN 
was 
good 
VBD 
good 
but 
JJ 
but 
service 
CC 
service 
was 
NN 
was 
poor 
VBD 
poor 
. 
JJ 
. 
NULL 
. 
Fittest particle and optimized feature set at Iteration t+1 

P 2 
1 
0 
1 
0 
1 
1 
72.87 

Feature for P 2 
The 
food 
1 
DT 
72.87 
food 
was 
0 
NN 
was 
good 
1 
VBD 
good 
but 
0 
JJ 
but 
service 
1 
CC 
service 
was 
0 
NN 
was 
poor 
1 
VBD 
poor 
. 
0 
JJ 
. 
NULL 
0 
. 

Baseline SemEval for comparative analysis. In Baseline 1 , we fix the 
context window as w 

i +2 
i ?2 

(i.e. previous two, current and next two 
instances) and select all the features in that window for training. 
Similarly, SemEval-
2014 shared task for both aspect term extraction and sentiment 
classification. These are defined as below: 
a. A list containing all the aspect terms of the training set is 
generated. A sequence of tokens is tagged as aspect term in a 
test sentence, if it appears in the list. 
b. For each aspect term of the test set, all the training sentences 
containing this aspect term are retrieved. The final sentiment 
class is then determined based on the most frequent class. 

1. Baseline 1 : This baseline system is developed by training with 
all the features mentioned in Section 3 within the local context 
window of w 

i +2 
i ?2 

: all features of previous two ( ?2, ?1), current 
(0) and next two (+1, +2) instances. 
2. Baseline is considered i.e. all features of previous three ( ?3, ?2, 
?1), current (0) and next three (+1, +2, +3) instances. 
3. Baseline 3 : This model is dependent on the best model obtained 
after we execute PSO. Context information, obtained through 
PSO based feature selection, is considered along with all the 
other features. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 Statistics of datasets.1 , P SO Run 2 , P SO Run 3 and P SO Run 4 .</head><label>6</label><figDesc></figDesc><table>Domain 
Dataset 
#Reviews 
#Aspect 
Sentiment Class 

#Pos 
#Neg 
#Neu 
#Conf 

Restaurant 
training 
3044 
3699 
2164 
805 
633 
91 
test 
800 
1134 
728 
196 
196 
14 
Laptop 
training 
3045 
2358 
987 
866 
460 
45 
test 
800 
654 
341 
128 
169 
16 

periment. We denote these runs as: P SO Run For each run we fix the number of particles and iterations 
as 50 and 100, respectively. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="true"><head>Table 8 Various parameter settings of PSO.</head><label>8</label><figDesc></figDesc><table>Run 
# Particle 
# Iteration 
Inertia weight( w ) 

¦Ì 1 
¦Ì 2 

PSO Run 1 
50 
100 
0.3593 
?0.7238 
2.0289 
PSO Run 2 
0.7298 
1.49618 
1.49618 
PSO Run 3 
?0.3699 
?0.1207 
3.3657 
PSO Run 4 
?0.4349 
?0.6504 
2.2073 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="true"><head>Table 9 Results of top 5 models obtained through PSO based feature selection (w.r.t F-measure).</head><label>9</label><figDesc></figDesc><table>Classifiers 
Models 
Restaurant 
Laptop 

Aspect term 
Sentiment 
Aspect term 
Sentiment 

P 
R 
F 
f n 
Acc 
f n 
P 
R 
F 
f n 
Acc 
f n 

ME 
M f 1 
71.44 
74.33 
72.86 
38 
74.95 
20 
61.43 
57.49 
59.39 
41 
66.81 
13 
M f 2 
70.78 
74.77 
72.72 
41 
74.69 
20 
60.25 
57.95 
59.08 
39 
66.20 
12 
M f 3 
70.24 
74.95 
72.52 
31 
74.60 
17 
61.38 
56.88 
59.04 
46 
66.05 
14 
M f 4 
70.53 
74.51 
72.47 
41 
74.51 
18 
60.48 
57.33 
58.86 
48 
65.90 
16 
M f 5 
71.17 
73.80 
72.46 
37 
74.42 
23 
60.61 
57.18 
58.85 
46 
65.74 
13 
CRF 
C f 1 
85.39 
80.95 
83.11 
35 
78.65 
16 
83.39 
64.52 
72.75 
44 
72.17 
11 
C f 2 
85.63 
80.42 
82.94 
39 
78.48 
18 
83.83 
64.22 
72.72 
42 
72.01 
11 
C f 3 
85.76 
80.46 
82.91 
31 
78.39 
20 
83.23 
64.52 
72.69 
40 
71.71 
10 
C f 4 
85.03 
82.68 
82.80 
32 
78.21 
23 
83.49 
64.22 
72.60 
44 
71.55 
17 
C f 5 
84.68 
80.95 
82.77 
34 
78.13 
16 
83.20 
64.37 
72.58 
46 
71.40 
13 
SVM 
S f 1 
83.53 
80.07 
81.76 
29 
77.24 
11 
81.99 
65.44 
72.78 
27 
66.97 
11 
S f 2 
83.54 
79.71 
81.58 
30 
76.89 
11 
82.17 
64.83 
72.47 
32 
66.81 
13 
S f 3 
83.41 
79.80 
81.56 
37 
76.80 
10 
83.90 
63.76 
72.45 
30 
66.66 
09 
S f 4 
83.87 
79.36 
81.55 
43 
76.71 
18 
82.29 
64.67 
72.43 
31 
66.51 
09 
S f 5 
83.25 
79.80 
81.49 
35 
76.63 
12 
82.01 
64.83 
72.41 
34 
66.36 
11 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 10</head><label>10</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" validated="true"><head>Table 11 Optimized feature sets for aspect term extraction task in restaurant domain.</head><label>11</label><figDesc></figDesc><table>Models 
Features 

Word &amp; 
PoS 
Head 
Chunk 
Lemma 
Stop 
Word 
Prefix 
Suffix 
Frequent 
Dependency 
WordNet 
Named 
Char 
Word 
Aspect 
Sentiment 
Orthographic 
context 
Word 
Word 
Length 
aspect 
Relation 
Entity 
n-grams 
cluster 
term 
Orientation 
c: IsCap, 
term 
(1,2,3,4,5)gram 
list 
d: InitDigit 

M.S. Akhtar et al. / Knowledge-Based Systems 125 (2017) 116-135 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" validated="false"><head>Table 12 Optimized feature sets for aspect term extraction task in laptop domain.</head><label>12</label><figDesc></figDesc><table>Models 
Features 

Word &amp; 
PoS 
Head 
Chunk 
Lemma 
Stop 
Word 
Prefix 
Suffix 
Frequent 
Dependency 
WordNet 
Named 
Char 
Word 
Aspect 
Sentiment 
Orthographic 
context 
Word 
Word 
Length 
aspect 
Relation 
Entity 
n-grams 
cluster 
term 
Orientation 
c: IsCap, 
term 
(1,2,3,4,5)gram 
list 
d: InitDigit 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" validated="true"><head>Table 14 Results of PSO based classifier ensemble w.r.t majority ( En Majority ) &amp; weighted ( En Weighted ) techniques.</head><label>14</label><figDesc></figDesc><table>Objectives 
Method 
Restaurant 
Laptop 

Aspect term 
Sentiment 
Aspect term 
Sentiment 

P 
R 
F 
Acc 
P 
R 
F 
Acc 

Accuracy 
En Majority 
-
-
-
79.98 
-
-
-
7 4 . 0 0 
En Weighted 
-
-
-
8 0 . 0 7 
-
-
-
75.22 
F-measure 
En Majority 
86.58 
81.39 
83.90 
-
83.98 
65.75 
73.75 
-
En Weighted 
86.27 
82.01 
84.09 
-
84.70 
66.05 
74.22 
-
Precision &amp; 
En Majority 
87.07 
82.01 
84.46 
-
84.99 
66.67 
74.72 
-
Recall 
En Weighted 
87.09 
82.10 
84.52 
-
85.49 
66.7 
74.93 
-
Proposed Model 
87.09 
82.10 
84.52 
80.07 
85.49 
66.7 
74.93 
75.22 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21" validated="true"><head>Table 18 Comparisons with existing ensemble techniques.</head><label>18</label><figDesc></figDesc><table>Method 
Restaurant 
Laptop 

Aspect term 
Sentiment 
Aspect term 
Sentiment 

P 
R 
F 
Acc 
P 
R 
F 
Acc 

Proposed method 
85.39 
80.95 
84.52 
80.07 
81.99 
65.44 
74.93 
75.22 
Bagging 
67.54 
63.93 
65.69 
73.28 
41.28 
44.04 
42.62 
62.84 
AdaBoost 
67.54 
63.93 
65.69 
70.01 
41.89 
43.76 
42.81 
59.02 
Stacking 
64.55 
62.40 
63.45 
73.98 
39.44 
42.78 
41.05 
63.60 
Voting 
66.04 
62.36 
64.15 
71.64 
44.34 
43.02 
43.67 
62.07 

</table></figure>

			<note place="foot" n="1"> http://nlp.stanford.edu/software/lex-parser.shtml .</note>

			<note place="foot" n="2"> http://www.yelp.com/ . 3 http://snap.stanford.edu/data/other.html . 4 http://sentiwordnet.isti.cnr.it/ .</note>

			<note place="foot" n="5"> http://world-food-and-wine.com/describing-food .</note>

			<note place="foot" n="6"> http://nlp.stanford.edu/software/corenlp.shtml . 7 http://taku910.github.io/crfpp/ . 8 http://chasen.org/ ? taku/software/yamcha/ . 9 http://nlp.stanford.edu/software/classifier.shtml .</note>

			<note place="foot" n="10"> We used WEKA 3.6 for implementation.</note>

			<note place="foot" n="11"> http://www.cs.waikato.ac.nz/ml/weka/ .</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Inf. Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th KDD</title>
		<meeting>the 10th KDD<address><addrLine>Seattle, WAs</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sentiment Analysis and Opinion Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Thumbs up or thumbs down?: Semantic orientation applied to unsupervised classification of reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th ACL</title>
		<meeting>the 40th ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Determining the sentiment of opinions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">1367</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Analysis of different approaches to sentence-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jagtap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pawar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Scient. Eng. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="164" to="170" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Movie review mining and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM International Conference on Information and Knowledge Management, in: CIKM &apos;06</title>
		<meeting>the 15th ACM International Conference on Information and Knowledge Management, in: CIKM &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Aspect extraction through semi-supervised modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="339" to="348" />
		</imprint>
	</monogr>
	<note>ACL &apos;12</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 4: aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semeval-2015 task 12: aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4" to="86" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An unsupervised aspect-sentiment model for online reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="804" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are they different? affect, feeling, emotion, sentiment, and opinion detection in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Munezero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Montero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sutinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pajunen</surname></persName>
		</author>
		<idno type="doi">doi:10.1109/TAFFC.2014.2317187</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="111" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extracting product features and opinions from reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzionir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on HLT/EMNLP</title>
		<meeting>the Conference on HLT/EMNLP</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="339" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building a sentiment summarizer for local service reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blair-Goldensohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Neylon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reynar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WWW Workshop on NLP in the Information Explosion Era</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Old wine or warm beer: target-specic sentiment analysis of adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fahrni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symsposium on Affective Language in Human and Machine, The Society for the Study of Artificial Intelligence and Simulation of Behavior (AISB)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="60" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">IHS R&amp;D belarus: cross-domain extraction of product features using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chernyshevich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="309" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dcu: aspect-based polarity classification for semeval task 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bogdanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tounsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="223" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fine-grained opinion mining with recurrent neural networks and word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1433" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Aspect extraction for opinion mining with a deep convolutional neural network, Knowl.-Based Syst</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Detecting opinion polarities using kernel methods, in: Proceedings of the Workshop on Computational Modelling of People&apos;s Opinions, Personality, and Emotions in Social Media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kaljahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="60" to="69" />
			<pubPlace>Osaka, Japan</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Feature selection in sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CORIA</publisher>
			<biblScope unit="page" from="273" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An approach to feature selection for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koncz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paralic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th IEEE International Conference on Intelligent Engineering Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="357" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Particle swarm optimization for parameter determination and feature selection of support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1817" to="1824" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ensemble of feature sets and classification algorithms for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1138" to="1152" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Customizing sentiment classifiers to new domains: A case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of recent advances in natural language processing (RANLP)</title>
		<meeting>recent advances in natural language processing (RANLP)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Twitter sentiment analysis: a bootstrap ensemble framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on</title>
		<imprint>
			<biblScope unit="page" from="357" to="364" />
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using bilingual knowledge and ensemble techniques for unsupervised chinese sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing, Association for Computational Linguistics</title>
		<meeting>the conference on empirical methods in natural language processing, Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="553" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sentiment classification: the contribution of ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decis. Supp. Syst</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="77" to="93" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
		<title level="m">Swarm Intelligence</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Conditional random fields: probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<idno type="doi">doi:10.1023/A:1022627411411</idno>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Using maximum entropy for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI-99 Workshop on Machine Learning for Information Filtering</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pso-asent: feature selection using particle swarm optimization for aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ekbal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Information Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="220" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Genetic Algorithms in Search, Optimization and Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Boston, MA , USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optimization by simulated annealing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Gelatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="issue">4598</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">DLIREC: aspect term extraction and term polarity classification system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">240</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">WordNet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Class-based N-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computat. Ling</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Predicting the semantic orientation of adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL/EACL</title>
		<meeting>the ACL/EACL</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="174" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Word sense and subjectivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COL-ING/ACL</title>
		<meeting>the COL-ING/ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1065" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A holistic lexicon-based approach to opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 International Conference on Web Search and Data Mining, in: WSDM &apos;08</title>
		<meeting>the 2008 International Conference on Web Search and Data Mining, in: WSDM &apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sentiwordnet 3.0: an enhanced lexical resource for sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baccianella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)</title>
		<meeting>the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
		<title level="m">Swarm Intelligence</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Neural Networks</title>
		<meeting>the IEEE International Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="1942" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A modified particle swarm optimizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE World Congress on Computational Intelligence</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="69" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Tuning &amp; simplifying heuristical optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E H</forename><surname>Pedersen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>University of Southampton</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Simplifying particle swarm optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E H</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Chipperfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="618" to="628" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Good parameters for particle swarm optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E H</forename><surname>Pedersen</surname></persName>
		</author>
		<idno>HL1001</idno>
	</analytic>
	<monogr>
		<title level="j">Hvass Lab</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dlirec: aspect term extraction and term polarity classification system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="235" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Introduction to the Statistical Analysis of Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scolve</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<pubPlace>Houghton Mifflin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Principal Component Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<title level="m">Thirteenth International Conference on Machine Learning</title>
		<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
	<note>Experiments with a new boosting algorithm</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Stacked generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="241" to="259" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">On combining classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<idno type="doi">doi:10.1109/34.667881</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="239" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
