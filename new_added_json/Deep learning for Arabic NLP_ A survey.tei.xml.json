[
     {
          "head": {
               "n": "0",
               "text": "abstract",
               "type": "abstract"
          },
          "paragraphs": [
               "The recent advances in deep learning (DL) have caused breakthroughs in many fields such as computer vision, natural language processing (NLP) and speech processing. Many DL based approaches have been shown to produce state-of-the-art results on various tasks that are of great importance to online social networks (OSN) and social computing such as sentiment analysis (SA) and pharmacovigilance. NLP tasks are becoming very prominent in OSN and DL is offering researchers and practitioners exciting new directions to address these tasks. In this paper, we provide a survey of the published papers on using DL techniques for NLP. We focus on the Arabic language due to its importance, the scarcity of resources on it and the challenges associated with working on it. We notice that DL has yet to receive the attention it deserves from the Arabic NLP (ANLP) community compared with the attention it is getting for other languages despite the vast adoption of social networks in the Arab world. The majority of the early works on using DL for ANLP focused on OCR-related problems while the more recent ones are more diverse with the increasing interest in applying DL to SA, machine translation, diacritization, etc. This survey should serve as a guide for the young and growing ANLP community in order to help bridge the huge gap between ANLP literature and the much richer and more mature English NLP literature."
          ],
          "paper_id": "dd6e7540-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 0,
          "fromPaper": "Deep learning for Arabic NLP: A survey"
     },
     {
          "head": {
               "n": "1.",
               "text": "Introduction",
               "type": "introduction"
          },
          "paragraphs": [
               "Over the past few decades, the amount of user generated content has grown rapidly. This is mainly due to the prevalence of online social networks (OSN). Such surge of available raw data contributed to the rise of many new interesting fields such as social network analysis and big data in addition to increasing the interest in existing fields such as artificial intelligence and natural language processing (NLP).",
               "Social media analytics is closely related to NLP. The use of NLP techniques for processing the contents of OSN have gained a lot of attention from the academic world as well as from the industry. In fact, this myriad research and commercial interests have lead to/helped in the creation of entire events dedicated to this topic, such as Social NLP, 1 in addition to the emergence of a large number of businesses .",
               "NLP techniques can help in addressing many interesting problems in OSN such as geolocation identification, public opinion mining, sentiment/emotion analysis, trend analysis, event extrac-tion, controversy detection, crowd monitoring, public health monitoring, disaster management, etc. .",
               "Recently, the NLP community has witnessed many breakthroughs due to the use of deep learning. Deep learning (DL), a subfield of machine learning (ML), depends on a set of algorithms in order to learn multiple levels of representation with the aim of finding a model for high level abstractions in data. DL tries to mimic the human brain, by constructing an architecture that consists of an input layer and an output layer with many hidden layers (encoders) between them. These hidden layers are responsible for doing complex computations in order to extract features from the raw data in order to obtain a better representation. On the other side, there is the shallow learning, which is a type of learning that consists of at most of three levels, and it is widely used with linear problems . Many techniques are used with DL, such as deep neural networks (DNN), convolutional neural networks (CNN), long shortterm memory (LSTM), recurrent neural networks (RNN), etc., with several success stories across different domains.",
               "In this paper, we survey the published works on applying DL techniques on NLP problems. We focus on the Arabic language due to its importance, the large number of Arabic speakers in OSN, the scarcity of resources on it and the challenges associated with working on it. This survey should serve as a guide for the young and growing Arabic NLP (ANLP) community in order to help bridge the gap between ANLP literature and the much richer and more mature English NLP literature.",
               "The Arabic language is of undeniable importance. Not only it is being used by hundreds of millions of people, it has a quickly increasing online presence (in terms of the users and content). Moreover, Arabic has many unique characteristics that makes the automated handling and understanding of Arabic text challenging and interesting. Giving an overview of the Arabic language and a survey of the young field of Arabic NLP is beyond the scope of this paper. Interested readers are referred to for such coverage.",
               "Among the many characteristics of the Arabic language, there are some characteristics that have significant effect on the general approaches to handle the different NLP problems. Examples include the language's complex nature (derivational, inflectional, etc.). Another example is the existence of many variations of the language such as the Classical Arabic (CA) (which is mainly used in ancient and theological texts but is still understood due to its use in the Holy Quran), the Modern Standard Arabic (MSA) (which is a modernized and simplified version of CA) and Dialectal/colloquial Arabic (DA) (where each region has its own dialect). The most widely-understood variation among Arabic speakers is MSA due to its wide adoption in education, media, and formal communication across the different Arabic speaking countries .",
               "In the rest of this paper, we discuss the papers using DL for ANLP tasks. The coverage is based on the addressed problems. We start with the most common application of DL for ANLP, which is for optical character recognition (OCR) problems."
          ],
          "paper_id": "dd6e7540-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 1,
          "fromPaper": "Deep learning for Arabic NLP: A survey"
     },
     {
          "head": {
               "n": "2.",
               "text": "Optical character recognition (OCR)",
               "type": "introduction"
          },
          "paragraphs": [
               "In this section, we discuss the works employing DL techniques for optical character recognition (OCR) problems in ANLP. We exclude the efforts focusing only on digit recognition including the very popular and highly cited MNIST dataset as well as other similar datasets such as the CMATERdb 3.3.1 dataset. It is worth mentioning that Arabic scripts differ greatly from other languages and, thus, approaches that work well for Latin or Chinese scripts may fail horribly when applied to Arabic script. Reasons behind this include the use of cursive right-to-left style of writing, the fact that the shape of a character depends on its position within the word, the similarity between different characters (as they sometimes differ only by the number or positions of dots), etc. .",
               "As can be inferred from the size of this subsection, DL adoption for OCR of Arabic script has been the most popular among ANLP subfields, however, there is still a lot of room for improvement on OCR problems already addressed by DL. Moreover, there are other OCRrelated problems that have yet to be addressed using DL approaches such as write identification ."
          ],
          "paper_id": "dd6e7540-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 2,
          "fromPaper": "Deep learning for Arabic NLP: A survey"
     },
     {
          "head": {
               "n": "2.1.",
               "text": "Handwritten text",
               "type": "introduction"
          },
          "paragraphs": [
               "We start our discussion with the one of the most frequently addressed OCR problems, which handwritten Arabic text recognition. In one of earliest works, Al-Jawfi introduced an approach for Arabic handwritten text recognition problem. Based on the work of LeCun et al. , Al-Jawfi's approach used LeNet, which is a convolutional neural network (CNN). The approach involves two phases: one for recognizing the body of the characters and one for recognizing the dots. It takes as input a set of 1616 normalized images. They go through two hidden layers and the backpropagation algorithm was used to update the weights in the network. The experiments on this network was carried out using a dataset of 758 segmented images of Arabic characters (550 for training and 208 for testing) handwritten by different people. On average, the mean square error (MSE) for the training set was 0.087 and, for the testing set, it was 0.42. The results are not very encouraging and the author do not compare them with the state-of-the-art results on the same dataset.",
               "In , the authors proposed to combine multidimensional RNN (MDRNN) and connectionist temporal classification (CTC) to design an alphabet-independent offline handwriting recognition system that takes raw pixel data as input. Specifically, they propose to repeatedly compose multidimensional LSTM (MDLSTM) layers with feed-forward layers. They tested their network on the IFN/ENIT dataset , which consists of 32,492 images of Tunisian town names. They used 30,000 images for training and the remaining 2492 images for testing. The reported error rate was 8.57%, which was better than the state-of-the-art at that time.",
               "In , the authors compared MDLSTM learned features with four state-of-the-art handcrafted feature sets. For recognition, each set of features was fed into bidirectional LSTM (BLSTM)-CTC network. For MDLSTM, the authors used the same network as the one used in where the RNNLIB implementation was used. The experiments were carried out using the IFN/ENIT dataset. The results showed that MDLSTM learned features produced the least recognition rates (RR) among all tested feature sets. However, the results also showed that merging the MDLSTM learned features with each one of the four handcrafted feature sets (through plurality voting) lead to improved RR. Finally, merging all five sets of features did not provide significantly better RR.",
               "Building on , the same authors in proposed a framework for evaluating feature sets for offline handwriting recognition. They proposed to train a RNN classifier on each feature and use a weighted vote combination of these classifiers. The goal is to measure the strengths and complementarity of different feature sets. They use their system to evaluate several feature sets (including handcrafted and automatically learned (using MDLSTM networks) feature sets) using an Arabic dataset (IFN/ENIT) and a Latine one (RIMES ).",
               "Another work on Arabic handwritten text recognition problem is that of Porwal et al. , where the authors used deep belief networks (DBN) with one input layer and three hidden layers. The training process had two phases. The first phase is the pre-training phase, which was done using restricted Boltzman machine (RBM). The output of each hidden layer was used as input parameters for the above hidden layer. The second phase was fine tuning, and DBN was used with features based approach. For the experiments, the authors used the applied media analysis (AMA) Arabic Handwritten dataset of parts of Arabic words (PAW) classes. The dataset had 6464 training samples and 848 testing samples distributed among 34 classes. They compared their approach with those of and the results showed that using the proposed DBN approach on the raw images did not yield better results compared to . However, combining the DBN approach with the handcrafted features of gave the best accuracy of 89.4%.",
               "In , the authors discussed combining probabilistic graphical models (PGM) classifiers for Arabic handwritten words recognition. The considered classifiers include hidden Markov models (HMM) and DBN. They experimented with the IFN/ENIT dataset to show the feasibility of the proposed approach.",
               "A large number of papers were published over the past few years by a group of researchers at the University of Gabes and Sfax University, Tunisia, on utilizing different DL networks such as DBN, DNN, CNN, RNN, LSTM, etc., for Arabic handwritten text recognition . Similar to , Elleuch et al. applied DBN on raw images of Arabic scripts. Specifically, they experimented with two datasets: the HACDB dataset for offline Arabic character recognition and the ADAB dataset for online Arabic word recognition . The HACDB dataset contained 6600 images, each normalized 2828 pixels in gray scale. The images represent 66 classes: 58 classes representing the unique ways to write single Arabic characters and 8 classes representing the unique ways to write pairs of Arabic characters. The training part of the dataset contains 5280 images while the testing part contained the remaining 1320 images. For this dataset, experiments showed that the best performance reached a good error rate of 2.1% and it was achieved with two hidden layers, each with 1000 units. According to the authors, this accuracy is comparable to what is reported in the literature. As for the ADAB dataset, it contained 946 different labels representing Tunisian towns names. It has a total of 33,164 sub-words and 174,690 characters written by 166 persons. For this dataset, experiments showed that the best performance reached a very poor error rate of 41% and it was achieved with two hidden layers, each with 400 units. The results for this dataset is way below what is reported in the literature.",
               "In an extension of , the authors of proposed to combine the DBN of with bottleneck features (BNF) . They tested their approach on the LMCA dataset padded with some manually segmented characters and ligatures extracted from the ADAB dataset. The best error rate was 3.84%.",
               "In another extension of , the authors of proposed to use a convolutional DBN (CDBN) combined with support vector machine (SVM) classifier. Similar to , they considered two problems: character recognition, for which they used the HACDB dataset, and word recognition, for which they used the IFN/ENIT dataset. For the HACDB dataset, they showed that the proposed CDBN/SVM approach was better than the DBN approach of with an error rate of 1.82% (compared with 2.1% error rate reported in ). As for the IFN/ENIT dataset, which consists of 26,459 words handwritten by more than 411 different persons representing 946 Tunisian town/village names, the reported error rate was 16.3%.",
               "Another work by the same authors on CDBN is in which the authors proposed to add offline hand-crafted features extracted using the BetaElliptical method. They tested their approach on the ADAB and the LMCA dataset. The latter dataset contains images of digits, characters and words. However, the authors only used the 100,000 images of 56 characters (with 70% used for training and 30% for testing). The reported error rates are 8.2% and 2.49% for the ADAB and LMCA datasets, receptively.",
               "In another work published in the same year as , the authors compared DBN with CNN on the HACDB dataset. For DBN, they used the same architecture as , but reported higher error rate of 3.64% compared with . As for CNN, the authors used two layers for convolution, each followed by a layer for sub-sampling to reduce dimensionality. Then, they had a fully connected layer for classification. The reported error rate for CNN was 14.71%.",
               "In another extension of , the authors of experimented with using the dropout and dropconnect regularization methods to address the overfitting problem of DBN. They reported error rates of 2.73% and 2.27% when using dropout and dropconnect, respectively, which are better than the error rate of 3.64% reported in when No-Drop was used. However, these results are still worse than the error rates reported in .",
               "Building up on , the same authors proposed in to integrate CNN and SVM classifiers (while employing the dropout technique in ). The idea is to use SVM to alter the trainable classifier of the CNN and it was inspired by the work of Niu and Suen on handwritten digit recognition. The CNN part is similar to the one proposed in , except that dropout is applied to the fully connected before feeding the output into an SVM classifier. They experimented on the HACDB and IFN/ENIT datasets. For the HACDB dataset, they reported error rates of 6.59% in and 5.83% in , which are lower than the 14.71% error rate achieved by simple CNN in . As for the IFN/ENIT dataset, which was only used in , the reported error rate was 7.05%.",
               "Other works that tried to integrate SVM with DL was , in which the same authors employed the idea of to mimic DBN architecture in order to build a Deep SVM (DSVM) classifier by simply stacking several multi-class SVM classifiers (with radial basis function (RBF) kernels). They experimented with the HACDB dataset and reported error rates of 8.64% in and 5.68% in .",
               "In another direction by the same group of researchers considered using deep bidirectional LSTM (DBLSTM) networks , which is a special type of RNN. They trained it with CTC and used dropout to avoid overfitting. Using the RNNLIB library of , the authors built a network of 14 layers. Three of these layers are fully connected and hidden and they are composed of 20, 60 and 180 LSTM units. For the output layer, they used a softmax output layer of 46 units. They tested their network on the ADAB dataset and reported 26.28% error rate.",
               "In , the authors built on the MDLSTM network of . They used 28 layers. Three of these layers are fully connected and hidden and they are composed of 2, 10 and 50 LSTM units. For the output layer, they used a softmax output layer of 121 units. The main contribution in was to address the overfitting problem. To do so, the authors used the dropout technique. However, they only applied it on the feed-forward connections before the sub-sample layers which are not fully connected. They tested their approach on the IFN/ENIT dataset to show its effectiveness compared with the original approach of . The error rate of 's approach is 12.09% whereas the error of 's is 16.97%. In an extension of , the authors of further studied applying the dropout technique in different places. Instead of only applying dropout after MDLSTM (as was done in ), they experiment with applying it before or inside MDLSTM. The results on the IFN/ENIT dataset showed reduced error rates of 11.62% and 11.88% for these two cases, respectively.",
               "In , the authors described the systems they submitted to the Arabic handwriting recognition competition OpenHaRT 2013 . These systems were based on an optical model using LSTM network trained to recognize the different forms of the Arabic characters directly from the image, without explicit feature extraction nor segmentation. Large vocabulary selection techniques and ngram language modeling were used to provide a full paragraph recognition, without explicit word segmentation. Several recognition systems were also combined with the ROVER combination algorithm. The best system exceeded 80% of recognition rate.",
               "Later, the same group showed in how to improve the performance of RNN with LSTM cells on unconstrained handwriting recognition using dropout. They show how to carefully use dropout in the network so that it does not affect the recurrent connections, hence the power of RNNs in modeling sequences is preserved. The authors perform extensive experiments to confirm the effectiveness of dropout on deep architectures even when the network mainly consists of recurrent and shared connections. Among the datasets under consideration is the OpenHaRT 2013 dataset.",
               "The unsupervised and transfer learning challenge (UTL) consisted of five datasets from various domains including one with Arabic handwritten ancient manuscripts called AVICENNA. The authors of discuss their DL approach for the UTL challenge datasets. Their approach combined and stacked different one-layer unsupervised learning algorithms, adapted to each one of the five datasets. For the AVICENNA dataset, they applied whitened principle component analysis (PCA) on the raw data. In the second layer, they used denoising auto encoder (DeAE) consisting of 600 hidden units. The last layer simply contained a transductive PCA.",
               "In , the authors proposed a CNN network based on fast wavelet transform (FWT) and Adaboost algorithm for Arabic handwritten character recognition. Based on multi-resolution analysis (MRA) at different levels of abstraction, FWT is used to extract character's features. The authors evaluated their approach on the IESK-arDB dataset , which includes 6000 images, and the obtained accuracy was 93.92%.",
               "Recently, El-Sawy et al. discussed the use of CNN for recognizing Arabic handwritten characters. They used their won dataset consisting of 16,800 images and reported an error rate of 5.1%."
          ],
          "paper_id": "dd6e7540-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 3,
          "fromPaper": "Deep learning for Arabic NLP: A survey"
     },
     {
          "head": {
               "n": "2.2.",
               "text": "Printed text",
               "type": "introduction"
          },
          "paragraphs": [
               "The authors of addressed the problem of recognizing multi-font printed Arabic text in low-resolution images. The experimented with three types of networks inspired by : MDRNN, MDLSTM, and CTC. The dataset they used is taken from the Arabic printed text image (APTI) dataset and it contained ten different fonts and six different font sizes. As for evaluation they used the tasks of the Arabic text recognition competition of ICDAR 2013. The results showed that their system achieved recognition rates of 99% or above on most considered fonts and font sizes."
          ],
          "paper_id": "dd6e7540-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 4,
          "fromPaper": "Deep learning for Arabic NLP: A survey"
     },
     {
          "head": {
               "n": "2.3.",
               "text": "Video-overlaid text",
               "type": "introduction"
          },
          "paragraphs": [
               "The work of Yousfi et al. addressed the problem of detecting and recognition of Arabic text overlaid in videos. In , the authors proposed to use a CNN inspired by a prior work of the same authors . They showed the superiority of the CNN approach compared with two approaches relying on multiexit asymmetric boosting cascade. The developed CNN consists of six layers with the first four layers being used for features extraction and the remaining two layers being used for classification. The authors used an in-house dataset collected from Arabic TV channels. The training set consisted of 30,000 text images and each method used bootstrapping to obtain negative examples. The dataset had two test sets: one has 201 images with 959 annotated text areas while the other has 164 images with 1017 annotated text areas. For the first test set, the CNN approach gave better Fmeasure and lower computation time compared with the other two approaches. However, its detection rate (DR) was lower. As for the second test set, CNN results were better in terms of F-measure and DR.",
               "In , the authors discussed their effort to build the ALIF dataset 7 for detecting Arabic text in TV broadcasts. It consists of 6532 text images extracted from eight Arabic TV channels. The authors conducted experiments in the ALIF dataset using three approaches, one of which is a commercial OCR software (ABBYY Fine Reader 12). The remaining two approaches are RNN based ones inspired by . Specifically, they used BLSTM coupled with CTC component. The difference between the two approaches is that one of them used CNN based features while the other used DBN based features. For the experiments, the authors used 4152 images for training and the remaining images were distributed among three test sets of 900, 1299 and 1022 images. The evaluation metrics used were Character RR (CRR), Word RR (WRR) and Text RR (TRR). In all experiments, the BLSTM-CTC network with CNN based features produced the best results on all evaluation measures followed by the BLSTM-CTC network with DBN based features. The commercial software performed very poorly.",
               "In a follow-up work, the authors of extended the LSTM-CTC network of in order to apply it for recognizing Arabic text embedded in TV broadcasts. In addition to the two features methods of .",
               "Finally, in , the authors proposed to use BLSTM-CTC network, but they integrate with it a large scale Arabic language model (LM). They employed a modified version of the beam search (BS) decoding algorithm that uses both responses from LM and LSTM. To control the decoding's effectiveness and efficiency, the authors introduced hyper-parameters and constraints. The decoding scheme is inspired by 's scheme for speech recognition. As for the LM, the authors consider two approaches: RNN based LMs and those combined with the maximum entropy (MaxEnt) models, in order to produce a character based Arabic LM. The approach is evaluated on the ALIF dataset of where it showed significant improvement over a BLSTM baseline system in addition to a commercial software.",
               "In , the authors addressed the problem of recognizing Arabic text in videos and natural scenes. They proposed to use a CNN-RNN hybrid architecture called convolutional recurrent neural network (CRNN). To evaluate their approach, they used several datasets. For the videos setting, they used the ALIF dataset and the AcTiV dataset . As for the natural scenes setting, they created their own datasets by collecting 500 word images containing Arabic text in different scenarios such as billboards, signs, etc. They called it the IIIT Arabic scene text dataset and they made it publicly available. The results showed the superiority of the proposed approach compared with the existing approaches applied on the datasets under consideration including a commercial system (ABBYY) and an open source tool called Tesseract. "
          ],
          "paper_id": "dd6e7540-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 5,
          "fromPaper": "Deep learning for Arabic NLP: A survey"
     },
     {
          "head": {
               "n": "2.4.",
               "text": "Natural scene text",
               "type": "introduction"
          },
          "paragraphs": [
               "The last work discussed in the previous subsection addressed the problem of detecting and recognizing text in natural scene images and mentioned the IIIT Arabic scene text dataset. In this section, we survey other papers addressing the same problem or problems relevant to it.",
               "The authors of addressed the problem of detecting text in natural scene images, which can be more difficult than text printed on documents or embedded in videos due to the different levels of perspective distortion, illumination, etc. The problem's difficulty is increased when considering multi-script text on complex backgrounds. Following a patch based approach, in , they proposed to combine single layer CNN with a NaiveBayes Nearest Neighbor (NBNN) classifier. For experimentation, they introduced their own MLe2e dataset, which consists of 711 scene images covering four different scripts (not including Arabic). They also experimented with the Video Script Identification Competition (CVSI-2015) dataset , which consists of 10,000 pre-segmented video words equally distributed among the ten scripts it covers (including Arabic). The experiments on both datasets showed the superiority of the proposed approach compared with several state-of-the-art approaches. However, for the CVSI-2015 dataset, the approach of the Google team produced slightly higher results.",
               "In , the same authors extended their work by considering deeper CNN and training them using an Ensemble of Conjoined Networks. In addition to the MLe2e and CVSI-2015 datasets, the authors considered the SIW-13 dataset , which consists of 16,291 pre-segmented text lines in 13 scripts (including Arabic). They also used the ICDAR2013 and ALIF datasets to evaluate the misclassification error of their approach. The results showed that the enhanced approach of outperformed all other approaches (including ) expect that, for the CVSI-2015 dataset, the approach of the Google team produced slightly higher results.",
               "Another work on the same problem is that of , in which the authors proposed a patch based approach that assigns weights to patches using intra-cluster information entropy. Then, they employ what they call bag of CNN words. The authors used the MLe2e and SIW-13 datasets to evaluate their approach and the results showed that the proposed approach yielded competitive results.",
               "In proposed a simple approach of using CNN for recognizing text in scene images. For experiments, they used their own English-Arabic Scene Text (EAST) dataset, which consists of 2450 training images and 250 testing image. The results showed that the proposed system produced encouraging results."
          ],
          "paper_id": "dd6e7540-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 6,
          "fromPaper": "Deep learning for Arabic NLP: A survey"
     },
     {
          "head": {
               "n": "3.",
               "text": "Caption generation",
               "type": "introduction"
          },
          "paragraphs": [
               "The problem of automatically generating a description for a given image is an interesting and practical one. Unfortunately, it is almost untouched in the ANLP community. In , the author addressed this problem for Arabic by associating fragments of the image with root words. The fragments were extracted using a DNN pretrained on the ImageNet dataset . DBN were used to find the most suitable words to be associated with the image and the different root words to be associated with the fragments. Finally, the caption sentences are generating by using dependency tree relations. For evaluation, the author created two datasets. The first one was created by taking 10,000 images from the ImageNet dataset and have human experts add a caption to each image. As for the second dataset, it was created by taking 10,000 images from the Al-Jazeera news website (which has both Arabic and English content). The results were very encouraging as they represent the first approach for Arabic caption generation. Moreover, they were much higher than the simple approach of generating English captions and automatically translating them into Arabic. was based on and it consisted of two hidden layers. The proposed model performed well on an in-house dataset for both multi-speaker and speaker-independent settings.",
               "The authors of describe a distributed NN training algorithm, based on Hessian-free optimization, that scales to deep networks and large datasets. For the state-level minimum Bayes risk (sMBR) criterion, the authors state that this training algorithm is faster than stochastic gradient descent by a factor of 5.5 and yields a 4.4% relative improvement in word error rate on a 50-h broadcast news task. The authors also state that distributed Hessian-free sMBR training yields relative reductions in word error rate of 713% over crossentropy training with stochastic gradient descent on two larger tasks: Switchboard and DARPA RATS noisy Levantine Arabic. Their best Switchboard DBN achieved a 16.4% WER.",
               "In , the authors proposed using feature-rich DNN language models (DNN-LM) on Egyptian Arabic. The inputs to the network are a mixture of words and morphemes along with their features. The authors state that significant word error rate (WER) reductions were achieved compared to the traditional LM, which are based on words.",
               "In , the authors described their efforts invested by the MIT team to tackle the 2016 Arabic multi-genre broadcast (MGB) challenge, which consisted of 1200 h of transcribed audio. In these efforts, the authors experimented with various DNN models including: feed-forward NN, CNN, time-delay NN (TDNN), recurrent LSTM, highway LSTM (H-LSTM), and grid LSTM (G-LSTM). The latter produced the best results with a WER of 18.3%. Previous works from the same group used different DNN and CNN models to build Arabic ASR systems .",
               "Another team participating in the 2016 MGB Challenge was from QCRI. In , the authors describe their team effort, which included using the lattice free maximum mutual information (LF-MMI) modeling framework to generate purely sequence trained acoustic models. Another interesting aspect of this work is using four-gram and RNN with MaxEnt connections (RNNME) LM to do re-scoring. Finally, they used minimum Bayes risk (MBR) decoding criterion for system combination. The proposed approach achieved slightly better WER (14.2%) compared with the MIT team's approach of .",
               "The final team in our discussion to participate in the 2016 MGB challenge using a DL approach was from LIUM lab of the University of Le Mans, France. In , the authors describe this approach which employed GMM-derived (GMMD) to train a DNN, combined with the use of TDNN for acoustic models. The best reported results in indicated that this approach had a WER of 15.7%."
          ],
          "paper_id": "dd6e7540-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 7,
          "fromPaper": "Deep learning for Arabic NLP: A survey"
     },
     {
          "head": {
               "n": "5.",
               "text": "Language modeling",
               "type": "modelling"
          },
          "paragraphs": [
               "In previous sections, we discussed the use of LM in different problems (e.g., used them for OCR and used them for ASR). However, building LM can be studied independently. In , the authors proposed a character-level LM that can work on English as well as MRL such as Arabic. The proposed model applies CNN on input characters before feeding them into LSTM RNN-LM. The results for the Arabic language showed that the proposed LM outperformed various baselines working on word level or morpheme level. The authors have made the implementation of their approach publicly available. "
          ],
          "paper_id": "dd6e7540-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 8,
          "fromPaper": "Deep learning for Arabic NLP: A survey"
     },
     {
          "head": {
               "n": "6.",
               "text": "Automatic machine translation (ATM)",
               "type": "modelling"
          },
          "paragraphs": [
               "In one of the earliest works employing DL for ASR, the author of used RNN to recognize spoken Arabic digits. In this section, we discuss the efforts invested in developing DL based ATM systems for translating text from/to the Arabic language.",
               "We have noticed a scarcity of publications on this subject despite the popularity of DL approaches in ATM systems (especially, the commercial ones such as Google's). There are other relevant problems that have yet to be addressed using DL such as the translation between Arabic dialects .",
               "The authors of proposed to address the Arabic-English machine transliteration problem using DBN, which contains multiple layers of RBM. The proposed approach has three important parts. The first one is the source encoder, which deals with source words by converting them to dimensional binary vectors, then feeding them into first layer in the source encoder, the output of each layer is considered as an input to the next layer. The second part called joint layer. This layer uses the output of the source encoder as an input in order to get a state of hidden neurons, and infer an output state to use as input to the top level of the output encoder. The third part is the target encoder. Within this part, the output vector is decoded by traversing down words through the output encoder. Because RBMs are bidirectional models, the translation process could be from Arabic to English, and vice versa. During the experiments, the authors studied the effects of network structure, by making the number of layers and the size of the bottom layers in the source and target encoders fixed, but the joint layer could be in different layers and different sizes.",
               "Another relevant work is , where the authors explored the use of embeddings obtained from different levels of lexical and morpho-syntactic linguistic analysis. They showed that such an approach improved machine translation evaluation into morphologically rich languages (MRL) such as Arabic.",
               "In another work by the same group , the authors entertained the idea of building a segmenter for one dialect and using it for another dialect. They also proposed to build a single segmenter for all dialects by joining training data for all dialects, thus, eliminating the need for dialect detection as a pre-processing step for segmentation. The authors used the BLSTM-CRF model discussed in the previous paragraph. They considered four dialects and the datasets they used is publicly available. "
          ],
          "paper_id": "dd6e7540-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 9,
          "fromPaper": "Deep learning for Arabic NLP: A survey"
     },
     {
          "head": {
               "n": "9.",
               "text": "Text categorization (TC)",
               "type": "modelling"
          },
          "paragraphs": [
               "TC is a fundamental problem in information retrieval and NLP. However, not many works have proposed to use DL to address it. The only exceptions are .",
               "The authors of developed an approach for Arabic text categorization that is based on three stages. The first stage is preprocessing, where the punctuation marks, auxiliary verbs, pronouns, etc., were removed. Then, the remaining words are represented in root patterns using a letter weight and order scheme. The second stage is clustering. It is done into two phases using fuzzy C-means (FCM) clustering and Markov clustering. The third stage is training DBN for each cluster. They evaluated their approach using two datasets: Al-Jazeera dataset (consisting of 10,000 articles) and the Saudi Press Agency dataset (consisting of 6000 articles). The author reported an F-measure of 91.02%, which is significantly higher than the results reported in the literature."
          ],
          "paper_id": "dd6e7540-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 10,
          "fromPaper": "Deep learning for Arabic NLP: A survey"
     },
     {
          "head": {
               "n": "7.",
               "text": "Dialect detection",
               "type": "modelling"
          },
          "paragraphs": [
               "In , the authors discuss discriminating between similar languages (DSL) shared tasks, which included a task for identifying Arabic dialects. The authors note that high-order character n-grams were the most successful feature, and the best classification approaches included traditional supervised learning methods such as SVM, logistic regression, and language models, while DL approaches did not perform very well.",
               "In , the authors describe their character-level NN for the Arabic dialects identification task of the DSL challenge . Given a sequence of characters, their model embeds each character in vector space, runs the sequence through multiple convolutions with different filter widths, and pools the convolutional representations to obtain a hidden vector representation of the text that is used for predicting the language or dialect. The implementation of their approach is publicly available 15 and the obtained F-measure is 48.3%.",
               "Another deep learning approach for the dialect identification task of the DSL challenge is . The author used CNN and LSTM networks and obtained 43.29% weighted F-measure using CNN approach using default network parameters."
          ],
          "paper_id": "dd6e7540-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 11,
          "fromPaper": "Deep learning for Arabic NLP: A survey"
     },
     {
          "head": {
               "n": "8.",
               "text": "Dialectal Arabic (DA) segmentation",
               "type": "modelling"
          },
          "paragraphs": [
               "Segmenting text written in DA can be a very challenging problem due to the lack of rules, standards and resources on DA. In , the authors built on the work of Yao and Huang and proposed to use a character-level BLSTM network combined with the conditional random field (CRF) algorithm to build a segmenter for the Egyptian dialect. Without relying on any additional resources, the proposed system produced results comparable to the state-of-theart tools in text segmentation such as MADAMIRA and Farasa.",
               "In , the authors focused on building a word embeddings using a 3.4 billion words corpus from a collected 10 billion words web-crawled corpus. They used a CNN trained on top of pretrained Arabic word embeddings for SA to evaluate the quality of these word embeddings. The experiment results show that their scheme outperforms existing methods on several publicly available datasets .",
               "Relying on word embeddings as the main source of features for SA, the authors of presented their system which consists of the following steps. First, they compile a large Arabic corpus from various sources to learn word representations. Second, they train and generate word vectors (embeddings) from the corpus using the word2vec tool . Third, they use the embeddings in their feature representation for training several binary classifiers to detect subjectivity and sentiment in both standard and dialectal Arabic. The implementation of this approach is publicly available. The authors compare their results with other methods in literature. Their approach, which does not employ hand-crafted features, achieved a slightly better accuracy than the top hand-crafted methods.",
               "The authors of investigated the results of applying DL for sentence level Arabic SA. Four types of DL algorithms were used: DBN, DAE, DNN, and recursive auto encoder (RAE). The input features for the first three models were extracted based on the ArSenL lexicon , while the features for the fourth model were the raw words indices, which are drawn from a known vocabulary obtained from a separate and independent training set. For evaluation, the authors used the Linguistic Data Consortium Arabic Tree Bank (LDC ATB) dataset, which consists of 1180 sentence, split into 80% training and 20% testing. The results showed the superiority of RAE compared with the three models under consideration in addition to state-of-the-art classifiers applied on the same dataset.",
               "In a follow up work by the same group , the authors addressed some limitations of using RAE for Arabic SA, such as its poor handling of Arabic's morphological complexity. So, they proposed a recursive deep learning model for opinion mining in Arabic (AROMA), which starts by morphologically tokenizing the input before using semantic and sentiment embedding models. Finally, the structure of automatically generated syntactic parse trees is used to determine the order of the model's recursion. AROMA was evaluated using several datasets and the results showed that it outperformed baseline RAE as well as other well-known ML approaches for Arabic SA.",
               "Baly et al. proposed to use recursive neural tensor networks (RNTN) for SA of Arabic text. In order to use RNTN, the authors created the first Arabic Sentiment Treebank (ArSenTB). ArSenTB is enriched with orthographic and morphologic information. The authors made use of other tools such as word2vec and MADAMIRA . The input for the RNTN The proposed approach outperforms well-known classifiers such as SVM, RAE and LSTM.",
               "The same group in showed how to adapt state-of-theart English SA systems in order to use it for Arabic SA. Then, they compared this approach with the RNTN approach they proposed in . The first approach used surface, syntactic and semantic features fed into a SVM classifier. As for the second approach, they used their approach from. For evaluation, they used the Arabic Sentiment Twitter Data (ASTD) .",
               "In , the authors present their Twitter dataset consisting of opinions on health services. After discussing the data collection and filtration process, the authors discussed the pre-processing and annotation processes. For classification, they conduct several experiments using various machine leaning algorithms including DNN and CNN. DL approach showed promising results, but they were outperformed by other classifiers such as SVM.",
               "The authors of focused on SA of tweets datasets that are highly imbalanced. They compared several classifiers fed word embedding features. For evaluation, they used the Syria Tweets dataset. In a follow-up work , the same authors proposed to use CNN and LSTM networks with word embeddings features. The proposed systems were evaluated on free datasets and the results showed that the LSTM model is the best.",
               "In a very recent work, the authors of addressed the aspect based SA (ABSA) problem. They used the Arabic subset of the SemEval-2016 Task 5 , which consists of 2291 hotel reviews. There were mainly three tasks associated with this dataset: aspect category identification, aspect opinion target expression (OTE) extraction, and aspect sentiment polarity identification. For these different tasks, the authors extracted lexical, morphological, syntactic, and semantic features and used them to train a SVM classifier. They compared this classifier with a RNN trained on the same features in addition to word embedding features. The results showed that the SVM classifier produced more accurate results, however, it took much longer time to do so.",
               "SemEval 2017 witnessed more papers employing DL approaches for Arabic tasks. In , the authors addressed three subtasks of . Specifically, they addressed Subtask A (message polarity classification) using an enhanced version of the sentiment analyzer developed by the same authors . They also addressed Subtask B (topic-based message polarity classification) and D (Tweet quantification), for which they proposed to use three classifiers and combine their outcomes using voting. These classifiers are: CNN (trained on word2vec word embeddings), MLP and logistic regression (LR). The results showed that the proposed approaches for the three subtasks under consideration outperformed all of their competitors.",
               "Other works using DL approaches to address the SemEval-2017 Task 4 include . In the former, the authors addressed all five subtasks of the task in question. They used three CRNN, whose inputs were out-/in-domain embeddings and sequences of words polarities. The outputs of the three networks were concatenated and fed into a fully-connected MLP network. The results of the proposed system were very promising.",
               "Like , Baly et al. addressed all five subtasks of the task in question. For Subtask A, they employed state-of-the-art approaches for English tweets to analyze Arabic tweets and showed that the results for such an approach were solid. As for subtasks B-E, they introduce a topic based approach and showed that it was ranked first in subtasks C and E, and second in subtask D. One of the systems proposed by uses the RAE of ."
          ],
          "paper_id": "dd6e7540-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 12,
          "fromPaper": "Deep learning for Arabic NLP: A survey"
     },
     {
          "head": {
               "n": "11.",
               "text": "Question answering (QA)",
               "type": "modelling"
          },
          "paragraphs": [
               "In , the authors addressed the question ranking problem in Arbic community QA (cQA) forums. They used the Farasa toolkit to build a UIMA 20 based processing pipeline that involves a TreeKernel (TK) based ranker. Then, they used LSTM networks to choose the question fragments to be used in the ranker. They complement the features they compute with word embeddings features computed using the word2vec tool . They evaluated their approach on the CQA-MD corpus, a medical Arabic corpus that is part of the SemEval 2016 Task 3-D . The results show strong performance by the proposed pipeline, which is further enhanced by the use of LSTM networks."
          ],
          "paper_id": "dd6e7540-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 13,
          "fromPaper": "Deep learning for Arabic NLP: A survey"
     },
     {
          "head": {
               "n": "12.",
               "text": "Automatic diacritization",
               "type": "modelling"
          },
          "paragraphs": [
               "In , a deep RNN (specifically, DBLSTM) is used to for automatic diacritization of Arabic text. This approach requires no preprocessing steps such as lexical, morphological and syntactical analysis. However, in order to achieve better results, post processing error correction techniques are employed. The network is built by placing multiple RNN hidden layers on top of each other, where the outputs of Layer L1 in the stack is considered as an input to the above layer (L2). Since it is bidirectional, this means that each hidden layer gets its input from both the forward and backward layers. RNN has been trained in two different ways; one-to-one network and one-to-many network. One-to-one makes sure to encode every possible diacritization of each single letter, and the input and the output sequences have the same length. On the other hand, within the one-to-many network, the input and the output sequences have different lengths (the output is longer than the input). The results showed that one-to-one outperforms one-to-many. So, the authors continued their experiments using one-to-one network. The experiments made using one-to-one network as an improvement of the performance of RNN to get better accuracy. The experiments tested the impact of the following: weight noise distortion, network size, data size, and influence of the post-processing step. The authors tested their system on 11 books and reported average diacritic and word error rates of 2.09% and 5.82%, respectively. The experiments were conducted using the RNNLIB library .",
               "Another work on diacritization is , where the authors focus on the use of RNN to build a language-independent system for automatic diacritization. They experimented with different hidden layer types ranging from a single feed-forward layer to DBLSTM layers. They add a linear projection after the input layer and use softmax at the output layer. For the experiments, they use the dataset of which consists of hundreds of thousands of words and millions of letters. On this dataset, the MaxEnt approach of (which utilizes language-dependent resources such as segmenter and part of speech (POS) tagger) produced an error rate of 5.1% whereas the approach of produced an error rate of 4.85%.",
               "In , the authors addressed the problem of automatically restoring diacritics. The presented the confused subset resolution (CSR) along with Arabic part-of-speech (PoS) tagging using DNN. The proposed system was evaluated on several dataset and the results showed its superiority over state-of-the-art systems."
          ],
          "paper_id": "dd6e7540-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 14,
          "fromPaper": "Deep learning for Arabic NLP: A survey"
     },
     {
          "head": {
               "n": "13.",
               "text": "Distributed word representations",
               "type": "modelling"
          },
          "paragraphs": [
               "Across the previous sections, we have mentioned many works employing different distributed word representations techniques such as word2vec . The use of such techniques has been shown to produce interesting results especially with DL based approaches .",
               "In , the authors presented Polyglot, a word embedding technique for multilingual NLP. The authors apply their technique on more than 100 languages (including Arabic) based on the Wikipedia pages of each language. In , the authors evaluated their word embeddings on part of speech (POS) tagging of few languages (not including Arabic). In , they showed how to use Polyglot to address the named entity recognition (NER) problem.",
               "Another effort in the same direction is that of Zahran et al. , in which the authors consider different techniques to build vectorized space representations for Arabic. Specifically, the authors built three models: continuous bag of word (CBOW), Skipgram (SKIP-G), and GloVe . They published these mode for public use. The authors studied the impact of these representations on two NLP tasks: Query Expansion for Information Retrieval and Short Answer Grading, and the results were encouraging.",
               "The authors of evaluated several Arabic word embedding techniques using their own benchmark, 22 instead of relying on translated benchmarks like previous works. They consider several NLP tasks such as TC and NER."
          ],
          "paper_id": "dd6e7540-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 15,
          "fromPaper": "Deep learning for Arabic NLP: A survey"
     },
     {
          "head": {
               "n": "14.",
               "text": "Conclusion",
               "type": "conclusion"
          },
          "paragraphs": [
               "In this survey, we surveyed the literature related to applying DL for ANLP tasks. Despite the huge impact DL has had on the general NLP community, this is yet to happen for ANLP. We expect this to change as more researchers become aware and convinced of the power of DL."
          ],
          "paper_id": "dd6e7540-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 16,
          "fromPaper": "Deep learning for Arabic NLP: A survey"
     }
]