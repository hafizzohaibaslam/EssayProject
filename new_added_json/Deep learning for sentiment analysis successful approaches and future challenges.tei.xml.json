[
     {
          "head": {
               "n": "0",
               "text": "abstract",
               "type": "abstract"
          },
          "paragraphs": [
               "Sentiment analysis (also known as opinion mining) is an active research area in natural language processing. It aims at identifying, extracting and organizing sentiments from user generated texts in social networks, blogs or product reviews. A lot of studies in literature exploit machine learning approaches to solve sentiment analysis tasks from different perspectives in the past 15 years. Since the performance of a machine learner heavily depends on the choices of data representation, many studies devote to building powerful feature extractor with domain expert and careful engineering. Recently, deep learning approaches emerge as powerful computational models that discover intricate semantic representations of texts automatically from data without feature engineering. These approaches have improved the state-of-the-art in many sentiment analysis tasks including sentiment classification of sentences/documents, sentiment extraction and sentiment lexicon learning. In this paper, we provide an overview of the successful deep learning approaches for sentiment analysis tasks, lay out the remaining challenges and provide some suggestions to address these challenges."
          ],
          "paper_id": "dd788760-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 0,
          "fromPaper": "Deep learning for sentiment analysis: successful approaches and future challenges"
     },
     {
          "head": {
               "text": "INTRODUCTION",
               "type": "introduction"
          },
          "paragraphs": [
               "S entiment analysis and opinion mining are the field that analyze people's opinions, sentiments, emotions from user generated texts. It is one of the most active research areas in natural language processing, 4,5 and is also widely studied in data mining, web mining, and social media analytics as sentiments are key influencers of human behaviors. With the rapid growth of social media such as Twitter, Facebook, and online review sites such as IMDB, Amazon, Yelp, sentiment analysis draws growing attentions from both research and industry communities.",
               "According to the definition by Liu et al., sentiment (or an opinion) is a quintuple, (e, a, s, h, t), where e is the name of an entity, a is the aspect of e, s is the sentiment on aspect a of entity, e, h is the opinion holder, and t is the time when the opinion is expressed by h. In this definition, the sentiment s can be a positive, negative, or neutral sentiment, or a numeric rating score expressing the strength/intensity of the sentiment (e.g., 1-5 stars) in review sites like Yelp and Amazon. The entity can be a product, service, topic organization, or event. Let us take a toy example to better explain the definition of 'sentiment.' Supposing an Amazon user called Tom posted a review 'The photos from my Samsung camera are not that great, but the battery life is great.' at June In this example, there are two sentiment quintuples, namely and . Based on the definition of 'sentiment,' the objective of sentiment analysis aims at discovering all the sentiment quintuples in a document.",
               "Sentiment analysis tasks are derived from the five components of the sentiment quintuple. For example, the task of document level sentiment classification 8 targets at the third component (sentiment such as positive, negative, and neutral) while ignoring the other aspects. The task of fine-grained opinion extraction focuses on the first four components of the quintuple. In the past 15 years, machine learning driven methods almost dominate sentiment analysis tasks. As feature representation greatly affects the performance of a machine learner, a lot of studies in literature focus on effective features by hand with domain expertise and careful engineering.",
               "11,12 But this can be avoided by representation learning algorithms, which automatically discover discriminative and explanatory text representations from data."
          ],
          "paper_id": "dd788760-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 1,
          "fromPaper": "Deep learning for sentiment analysis: successful approaches and future challenges"
     },
     {
          "paragraphs": [
               "Deep learning is a kind of representation learning approach. It learns multiple levels of representation with nonlinear neural networks, each of which transforms the representation at one level into a representation at a higher and more abstract level. The learned representations can be naturally used as features and applied for detection or classification tasks.",
               "In this study, we introduce successful deep learning algorithms for sentiment analysis. The notation of 'deep learning' in this article stands for learning continuous and real-valued text representation/ feature automatically from data, mostly with neural network approaches. We first describe the methods to learn continuous word representation, also called word embedding, as words are the basic computational units of natural language. These word embeddings can be used as inputs to subsequent sentiment analysis tasks, so that we describe how word embeddings are investigated for different sentiment analysis tasks. In particular, we describe semantic compositional methods that compute representations of longer expressions (e.g., sentence or document) for sentence/document level sentiment classification task, 15,16 followed by neural sequential models for fine-grained opinion extraction. As sentiment lexicon is an important resource for many sentiment analysis systems, we also describe neural methods to build large-scale sentiment lexicons. We finally conclude this study and provide some future directions.",
               "A straight-forward way is to encode a word w i as a one-hot vector. It has the same length as the size of the vocabulary, and only one dimension is 1, with all others being 0. However, the one-hot word representation only encodes the indices of words in a vocabulary, while failing to capture rich relational structure of the lexicon. One common approach to discover the similarities between words is to learn word clusters. Each word is associated with a discrete class, and words in the same class are similar in some respect. This leads to a one-hot representation over a smaller vocabulary size.",
               "Instead of characterizing the similarity with a discrete variable based on clustering results which corresponds to a soft or hard partition of the set of words, many researchers target at learning a continuous and real-valued vector for each word, also known as word embedding. Existing embedding learning algorithms are typically based on the distributional hypothesis, which states that words in similar contexts have similar meanings. To this end, many matrix factorization methods can be viewed as modeling word representations. For example, Latent Semantic Indexing (LSI) can be regarded as learning a linear embedding with a reconstruction objective, which uses a matrix of 'term-document' co-occurrence statistics, e.g., each row stands for a word or term and each column corresponds to an individual document in the corpus. Hyperspace Analogue to Language 23 utilizes a matrix of 'term-term' co-occurrence statistics, where both rows and columns correspond to words and the entries stand for the number of times a given word occurs in the context of another word. Hellinger PCA 24 is also investigated to learn word embeddings over 'term-term' cooccurrence statistics. As standard matrix factorization methods do not incorporate task-specific information, it is not clear whether they are useful enough for a target goal. Supervised Semantic Indexing "
          ],
          "paper_id": "dd788760-9018-11ea-9ac1-1bc38575703c",
          "head": {
               "type": "introduction"
          },
          "paragraphNo": 2,
          "fromPaper": "Deep learning for sentiment analysis: successful approaches and future challenges"
     },
     {
          "head": {
               "text": "WORD REPRESENTATION",
               "type": "introduction"
          },
          "paragraphs": [
               "Word representation aims at representing aspects of word meaning. For example, the representation of 'cellphone' may capture the facts that cellphones are electronic products, that they include battery and screen, that they can be used to chat with others, and so on. tackles this problem and takes the supervised information of a targeted task (e.g., information retrieval) into consideration. They learn the embedding model from click-through data with a margin ranking loss.",
               "With the revival of interest in deep learning and neural network, 10,14,26 a surge of studies learn word embeddings with neural network. A pioneered work in this field is given by Bengio et al., which introduces a neural probabilistic language model that learns simultaneously a continuous representation for words and a probability function for word sequences based on these word representations. Given a word w i and its preceding context words, the algorithm first maps each context word to its continuous vector with a shared lookup table. Afterward, context word vectors are fed to a feed-forward neural network with softmax as output layer to predict the conditional probability of next word w i . The parameters of neural network and lookup table are jointly estimated with back propagation. Following 27 work, a lot of approaches are proposed to speed-up training processing or capturing richer semantic information. Bengio et al. introduce a neural architecture by concatenating the vectors of context words and current word, and use importance sampling to effectively optimize the model with observed 'positive sample' and sampled 'negative samples.' Morin and Bengio 28 develop hierarchical softmax to decompose the conditional probability with a hierarchical binary tree.   There are also many algorithms developed for capturing richer semantic information, including global document information, and 'bad' are mapped into close vectors in the embedding space. This is meaningful for some tasks such as pos-tagging as the two words have similar usages and grammatical roles, but it becomes a disaster for sentiment analysis as they have the opposite sentiment polarity. In order to learn word embeddings tailored for sentiment analysis tasks, some studies encode sentiment of texts in continuous word representation. Maas et al. introduce a probabilistic topic model by inferring the polarity of a sentence based on the embedding of each word it contains. Labutov and Lipson re-embed an existing word embedding with logistic regression by regarding sentiment supervision of sentences as a regularization item. 46 extend the C&W model and develop three neural networks to learn sentimentspecific word embedding from Twitter. The tweets containing positive and negative emoticons are used as training data, regarding positive and negative emoticon signals as sentiment indicators. The learned word embeddings are applied for Twitter sentiment classification, and perform comparable performance with the state-of-the-art hand-crafted features on SemEval 2013 dataset. They also build a system named 'Coooolll' and participate in the Twitter sentiment classification evaluation in word morphemes, dependency-based contexts,"
          ],
          "paper_id": "dd788760-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 3,
          "fromPaper": "Deep learning for sentiment analysis: successful approaches and future challenges"
     },
     {
          "paragraphs": [
               "The system yields the second place among 45 participants. Holding a similar idea, Tang et al. extend SkipGram method and leverage sentiment of texts for word embedding learning. Brief illustrations of two neural models for learning sentiment-specific word embeddings are given in .",
               "word-word co-occurrence, sense of ambiguous words, 40 semantic lexical information in WordNet, and hierarchical relations between words."
          ],
          "paper_id": "dd788760-9018-11ea-9ac1-1bc38575703c",
          "head": {
               "type": "introduction"
          },
          "paragraphNo": 4,
          "fromPaper": "Deep learning for sentiment analysis: successful approaches and future challenges"
     },
     {
          "paragraphs": [
               "The aforementioned neural network algorithms typically only use the contexts of words to learn word embeddings. As a result, the words with similar contexts but opposite sentiment polarity like 'good'"
          ],
          "paper_id": "dd788760-9018-11ea-9ac1-1bc38575703c",
          "head": {
               "type": "introduction"
          },
          "paragraphNo": 5,
          "fromPaper": "Deep learning for sentiment analysis: successful approaches and future challenges"
     },
     {
          "head": {
               "text": "SENTIMENT CLASSIFICATION",
               "type": "introduction"
          },
          "paragraphs": [
               "Sentiment classification is a fundamental and extensively studied area in sentiment analysis.",
               "1,2 It targets at determining the sentiment polarity (positive or  negative) of a sentence (or a document) based on its textual content. In the literature, existing studies for sentiment classification are dominated by two mainstream directions: lexicon-based approach and corpus-based approach.",
               "Lexicon-based approaches 48,49 mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). A representative lexicon-based method is given by Turney, which consists of three steps. He first extract phrases if their postags conform to the predefined patterns. Afterwards the sentiment polarity of each extracted phrase is estimated through point-wised mutual information (PMI), which measures the degree of statistical dependence between two terms. In Turney's work, the PMI score is calculated by feeding queries to a search engine and collecting the number of hits. Finally, he averages the polarity of all phrases in a review as its sentiment polarity. Ding and Liu is achieved by SVM with bag-of-words feature. Following Pang et al.'s work, many studies focus on designing or learning effective features to obtain a better classification performance. On movie and product reviews, Wang and Manning 54 present NBSVM, which trades-off between Naive Bayes and NB-feature enhanced SVM. Paltoglou and Thelwall apply negation words like 'not,' 'never,' 'cannot,' and contrary words like 'but' to enhance the performance of lexicon-based method. Taboada et al. integrate intensifications and negation words with the sentiment lexicons annotated with their polarity and sentiment strength. Thelwall et al. develop SentiStrength using sentiment lexicons and linguistic rules to detect the sentiment strength of tweets. Reckman et al. develop an entirely rule-based system for Twitter sentiment classification. They use lexicalized hand-written rules, each of which is a pattern that matches words or sequences of words.",
               "Corpus-based methods treat sentiment classification as a special case of text categorization problem.",
               "8 They mostly build the sentiment classifier from sentences with annotated sentiment polarity. The sentiment supervision can be manually annotated, or automatically collected by sentiment signals like emoticons in tweets or human ratings in reviews."
          ],
          "paper_id": "dd788760-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 6,
          "fromPaper": "Deep learning for sentiment analysis: successful approaches and future challenges"
     },
     {
          "paragraphs": [
               "Pang et al. pioneer to treat the sentiment classification of reviews as a special case of text categorization problem and first investigate machine learning methods. A brief illustration is given in . They employ Naive Bayes, Maximum Entropy, and Support Vector Machines (SVM) with a diverse set of features. In their experiments, the best performance utilize dependency trees, polarity-shifting rules and conditional random fields (CRFs) with hidden variables to compute the document feature. On Twitter, Jiang et al. use lexicon features and syntactic and POS tagging features. NRC-Canada 11 develop a state-of-the-art Twitter sentiment classifier in SemEval 2013 by using a variety of sentiment lexicons and hand-crafted features.",
               "Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering. With the rapid growing of deep learning (representation learning ), many recent studies focus on learning the low-dimensional, dense, and real-valued vector as text features for sentiment analysis without any feature engineering. Existing deep learning methods for sentiment classification typically include two stages. In the first stage, they learn word embeddings from text corpus. In the second stage, word embeddings are applied to producing the representations of sentences/documents with semantic composition.",
               "58 Embedding learning algorithms have been described in previous Section. Existing composition learning approaches are typically based on the principle of compositionality, 59 which states that the meaning of a longer expression (e.g., a sentence or a document) comes from the meanings of its constituents and the rules used to combine them. Specifically, Bespalov et al. initialize the word embeddings by latent semantic analysis and further represent each document as the linear weighted of ngram vectors for sentiment classification. Glorot et al. use Stacked Denoising Autoencoder in an unsupervised fashion based on reconstruction, and apply it for domain adaptation. Autoencoder is a kind of neural network that is optimized by reconstructing the input itself. "
          ],
          "paper_id": "dd788760-9018-11ea-9ac1-1bc38575703c",
          "head": {
               "type": "introduction"
          },
          "paragraphNo": 7,
          "fromPaper": "Deep learning for sentiment analysis: successful approaches and future challenges"
     },
     {
          "head": {
               "text": "Focus Article wires.wiley.com/widm",
               "type": "introduction"
          },
          "paragraphs": [
               "Denoising Autoencoding randomly masks the values of inputs and tries to reconstruct the noisy inputs. Socher et al. introduce a family of recursive deep neural models including Recursive Autoencoder (RAE), Matrix-Vector Recursive Neural Network (MV-RNN), and Recursive Neural Tensor Network (RNTN) to learn the composition of variable-length phrases based on the representation of its children. Specifically, RAE first learns the structure of a sentence with greedy unsupervised reconstruction, and further conduct compositionality over the learned tree structure. In RAE, each word is encoded as a vector and the calculator is matrix multiplication plus nonlinear hyperbolic tangent function. In MV-RNN, each word is also associated with a matrix representation, and the tree structure is obtained from an external parser (like Stanford parser). In RNTN, they use neural tensor network as the compositional functions to better capture the interactions between elements. An example of RNN is given in  develop adaptive RNN that uses more than one composition functions and adaptively select them depending on the input vectors. Li extend RNN by tuning feature weight to control how much one specific unit contributes to the higher-level representation.",
               "Another powerful neural network for semantic composition is convolutional neural network (CNN). A brief illustration is given in (a). For example, Kalchbrenner et al. develop dynamic CNNs (DCNN) and introduce the dynamic k-max pooling for learning the sentence representation. Kim and Johnson and Zhang 72 also investigate CNNs, and achieve several state-of-the-art performances on some benchmark datasets for sentiment classification.",
               "Beyond sentence level compositionality, a few studies move eyes on document level semantic composition. 76,77 Two recent studies exploit the semantic relationship between sentences for document level sentiment analysis. Tang et al. compose sentence representations to document representation in a sequential way, without using external discourse parser. Bhatia et al., however, use RST discourse parser and integrate the parsed results with RNN for document level sentiment analysis."
          ],
          "paper_id": "dd788760-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 8,
          "fromPaper": "Deep learning for sentiment analysis: successful approaches and future challenges"
     },
     {
          "head": {
               "text": "OPINION EXTRACTION",
               "type": "introduction"
          },
          "paragraphs": [
               "Given a piece of text (e.g., a sentence or a document), fine-grained opinion extraction targets at finding the elements of a sentiment quintuple including the opinion holder, the entity/aspect, the sentiment of an affective expression, and so on. This task is typically regarded as a sequential labeling problem. For example, the sentence 'the screen of apple is so amazing' should be tagged with",
               "An illustration of the approach is given in . The method proceeds a word at a time from the beginning to the end of a sentence. For each index, the input is word embedding, which will be fed to one or more hidden layers to get more abstractive and discriminative representations of data. Each hidden unit is computed based on its history and the current input. The method can be naturally extended to a bi-directional way, where the 'history' of a hidden unit comes from its surrounding contexts without constraining to the preceding words. The last hidden layer of a position is regarded as the representation of the corresponding word, and is used for classifying the tag label of the word. This method outperforms strong CRF baselines on a benchmark opinion extraction dataset. Paulus et al. investigate an analogous tree structured RNN for fine grained sentiment analysis task.",
               "For fine-grained sentiment analysis, deep learning (neural network) approaches also achieve some promising results recently. For example, Vo and Zhang show that rich features including word embedding perform well on target-dependent sentiment classification. Zhang et al. use word embeddings and integrate them into neural CRF for open-domain sentiment analysis. Liu et al. conduct empirical studies for fine-grained sequential labeling task, and show that LSTM recurrent neural network performs better than feature-based CRF on benchmark datasets."
          ],
          "paper_id": "dd788760-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 9,
          "fromPaper": "Deep learning for sentiment analysis: successful approaches and future challenges"
     },
     {
          "head": {
               "text": "BUILDING SENTIMENT LEXICON",
               "type": "introduction"
          },
          "paragraphs": [
               "A sentiment lexicon is a list of words and phrases such as 'excellent,' 'awful,' and 'coooolll,' each of which is assigned with a positive or negative score reflecting its sentiment polarity and strength. Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually, through thesaurus-based method and corpus-based method. As the manual method is time-consuming, it is mainly combined with automatic methods as the final check. We describe thesaurus-based and corpusbased method.",
               "Thesaurus-based method mainly utilizes semantic relationships (e.g., synonyms, antonyms, hypernyms, etc.) between tokens from an external thesaurus like WordNet. Under this direction, majority of existing studies regard word as basic unit, 6 yet some researchers target at the synset 85 in WordNet. Kim and Hovy 86 use synonym and antonym relations from WordNet to build sentiment lexicon. The hypothesis is that the synonyms of a positive word should have a positive polarity, and vice versa for antonym relation. They manually label a small size of adjective and verb words as sentiment seeds, and then apply a bootstrapping method to expand the seed list. Esuli and Sebastiani 87 take the gloss information of a word in WordNet into consideration. They manually label some sentiment seeds, and use a semi-supervised method to classify the polarity of a word based on its gloss in WordNet.  et al. propose learning connotation lexicon, which lists words with connotative polarity, i.e., words with positive connotation (e.g., 'award' and 'promotion') and words with negative connotation (e.g., 'cancer' and 'war'). Feng et al. go a step further and focus on the words that are objective on the surface like 'intelligence,' 'human,' 'cheesecake,' and so forth. Besides detecting the sentiment of each word, many researchers focus on identifying the polarity of a WordNet synset (also referred to as sense). Baccianella et al. release the well-known SentiWordNet, in which each synset is associated with three numerical scores, describing how objective, positive, and negative the terms contained in the synset are. Each score in SentiWordNet is in range (0.0, 1.0) and the summation is 1.0. Esuli et al. use pagerank to rank senses of WordNet in terms of how strongly they are positive or negative. Su et al. use a semi-supervised framework based on mincut to recognize the subjectivity of a word sense in WordNet.",
               "As the thesaurus like WordNet cannot well cover the growing colloquial sentiment expressions on the web, many researches employ corpus-based methods that induce a sentiment lexicon from text corpora. pioneer this field by extracting the polarity of adjective words. The idea is that words conjoined with 'and' favor to have the same polarity, and words conjoined by 'but' favor to have opposite polarity labels. They start with a list of sentiment seeds, and then identify more subjective adjectives with pre-defined conjunction patterns. Qiu et al. propose a semi-supervised method dubbed double propagation for opinion word expansion and target extraction based on dependency relations between sentiment words and aspect words. An enhanced double propagation approach is given by Liu et al., and the method shows strong performances on multiple datasets. Velikovich et al. represent words and phrases with their syntactic contexts within a window size from the web documents, and use graph propagation for label inference. Chen et al. utilize the Urban Dictionary and extract the target-dependent sentiment expressions from Twitter. use pointwise mutual information between each phrase and hashtag/emoticon seed words, such as '#good,' and '#bad,' ':),' and ':(.' Severyn and Moschitti as features for word-level sentiment classification, (2) a seed expansion algorithm that expands a small list of sentiment seeds to collect training data for building the word-level classifier. The framework is illustrated in . The generated sentiment lexicon is evaluated via applied as features for Twitter sentiment classification. The lexicon shows superior performances over traditional sentiment lexicon like MPQA 100 and large-scale sentiment lexicons like Sentiment140."
          ],
          "paper_id": "dd788760-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 10,
          "fromPaper": "Deep learning for sentiment analysis: successful approaches and future challenges"
     },
     {
          "head": {
               "text": "SOME SUGGESTIONS ON IMPLEMENTATION",
               "type": "introduction"
          },
          "paragraphs": [
               "In this part, we briefly talk about how to implement deep learning (neural network) approaches for the beginners in this area. Let us take the hybrid prediction model in as an example. There are two options when we implement the model. The first choice is to calculate the gradient of each parameter in terms of the loss function by hand, and use that to update the value of each parameter. However, it is not scalable because we need to calculate the gradients again even if we just slightly modify the neural architecture. In order to pay more attentions to developing powerful neural architecture rather than gradient calculating, we use another choice with layer-wise implementation. For example, we implement lookup table, linear layer, tanh layer, softmax layer, and so on, and we link them together to build up the final model. Once the basic neural layers are completed, we do not need to care about them anymore and we can test variations of neural architecture easily. The implementations by us can be found at http://ir.hit.edu.cn/~dytang. One could also use GPU supported toolkits including Theano (deeplearning.net/software/theano/), Torch (torch.ch), and Caffe (caffe.berkeleyvision.org/)."
          ],
          "paper_id": "dd788760-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 11,
          "fromPaper": "Deep learning for sentiment analysis: successful approaches and future challenges"
     },
     {
          "paragraphs": [
               "build sentiment lexicons using scores from an SVM model learned on a distant supervised corpora.",
               "Tang et al. cast sentiment lexicon learning as a phrase-level classification task and use deep learning strategy. The method consists of two part: (1) an embedding learning algorithm to effectively learn the continuous representation of phrases, which are used In this study, we introduce successful deep learning approaches for several sentiment analysis tasks involving word embedding learning, sentiment classification, opinion extraction, and sentiment lexicon learning.",
               "We lay out the future challenges of deep learning for aforementioned sentiment analysis tasks in this part. As deep learning is an emerging topic over machine learning and artificial intelligence in both research and industry community and is currently evolving very quickly, so the challenges might soon be outdated. For learning better word representations for sentiment analysis tasks, 18,46 incorporate sentence level sentiment signals as supervisions. In this line of research, different levels of sentiment signals may be also investigated such as external lexical level sentiment information. In addition, it has been show that the approached introduced by 18,46 do not surpass context-based embeddings for document level classification task, 101,102 which remains a potential challenge about how to use coarse-and fine-grained supervisions. Another interesting direction is how to interpret the meaning of a word embedding because the dense representations are uninterpretable. A recent study given by Faruqui et al. showed that transforming word embeddings into sparse vectors could yield promising results.",
               "For semantic compositionality, most of existing studies focus on sentence level. Toward this goal, a potential challenge is how to learn sentence structure together with the composed semantic representation as structure prediction is a big challenge in both natural language processing and machine learning communities. Some studies also claim that the internal connections in CNN form an automatic structure of natural language. Another direction is how to interpret the effects of neural networks with visualization. provide several methods to visualize how neural models like LSTM are able to compose meanings in sentiment analysis task.",
               "For building sentiment lexicon, the deep learning approach given by Tang et al. cannot infer the sentiment polarity of the phrases not covered by the existing vocabulary. How to generate new sentiment words/phrases from new corpus is a remained as a challenge. A self-studied life-long framework might be a practicable solution."
          ],
          "paper_id": "dd788760-9018-11ea-9ac1-1bc38575703c",
          "head": {
               "type": "introduction"
          },
          "paragraphNo": 12,
          "fromPaper": "Deep learning for sentiment analysis: successful approaches and future challenges"
     },
     {
          "head": {
               "text": "Focus Article",
               "type": "introduction"
          },
          "paragraphs": [
               "wires.wiley.com/widm 3. Feldman R. Techniques and applications for sentiment analysis. Commun ACM 2013, 56:82-89."
          ],
          "paper_id": "dd788760-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 13,
          "fromPaper": "Deep learning for sentiment analysis: successful approaches and future challenges"
     },
     {
          "head": {
               "text": "Conference on Empirical Methods in Natural Language",
               "type": "introduction"
          },
          "paragraphs": [
               "Processing (EMNLP), Doha, Qatar, 2014, 720-728. 18. Tang D, Wei F, Qin B, Zhou M, Liu T. Building largescale twitter-specific sentiment lexicon: a representation learning approach. In: Proceedings of COLING, Dublin, Ireland, 2014, 172-182. "
          ],
          "paper_id": "dd788760-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 14,
          "fromPaper": "Deep learning for sentiment analysis: successful approaches and future challenges"
     },
     {
          "head": {
               "n": "2010",
               "text": "Annual Conference of the North American",
               "type": "introduction"
          },
          "paragraphs": [
               "Chapter of the Association for Computational Linguistics, June 2010, 786-794. Association for Computational Linguistics."
          ],
          "paper_id": "dd788760-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 15,
          "fromPaper": "Deep learning for sentiment analysis: successful approaches and future challenges"
     },
     {
          "head": {
               "n": "69.",
               "text": "Xu L, Liu K, Lai S, Chen Y, Zhao J. Mining opinion words and opinion targets in a two-stage framework.",
               "type": "introduction"
          },
          "paragraphs": [
               "In: ACL "
          ],
          "paper_id": "dd788760-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 16,
          "fromPaper": "Deep learning for sentiment analysis: successful approaches and future challenges"
     }
]