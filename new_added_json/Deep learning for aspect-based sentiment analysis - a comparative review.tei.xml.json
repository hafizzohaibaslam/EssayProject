[
     {
          "head": {
               "n": "0",
               "text": "abstract",
               "type": "abstract"
          },
          "paragraphs": [
               "The increasing volume of user-generated content on the web has made sentiment analysis an important tool for the extraction of information about the human emotional state. A current research focus for sentiment analysis is the improvement of granularity at aspect level, representing two distinct aims: aspect extraction and sentiment classification of product reviews and sentiment classification of target-dependent tweets. Deep learning approaches have emerged as a prospect for achieving these aims with their ability to capture both syntactic and semantic features of text without requirements for high-level feature engineering , as is the case in earlier methods. In this article, we aim to provide a comparative review of deep learning for aspect-based sentiment analysis to place different approaches in context."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 0,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "1.",
               "text": "Introduction",
               "type": "introduction"
          },
          "paragraphs": [
               "The evolution of web technologies has enabled new means of communication through user-generated content, in the form of blogs, social networks, forums, website reviews, e-commerce websites, etc. . Following this exponential growth, there has been strong interest from individuals and organisations in data mining technologies to exploit this source of subjective information. One of the most prolific research areas in computer sciences is sentiment analysis, which aims to identify and extract user opinions .",
               "In their seminal work on Aspect-based Sentiment Analysis (ABSA), argued that the study of sentiment analysis is possible at three levels -document, sentence and entity or aspect. A focus on the document or sentence level presumes that only one topic is expressed in the document or sentence, which is not the case in many situations. A more thorough analysis, therefore, requires investigation at entity and aspect level to identify entities and related aspects and classify sentiments associated with these entities and aspects. Examples of entities include products, services, topics, issues, persons, organizations or events, which usually have several aspects ( Jimnez-Zafra, Martn-Valdivia, . For example, a laptop consists of a CPU, screen and keyboard; each also represents an aspect. Furthermore, as an entity is the hierarchy of all aspects, * Correspondence author. Penatiyana Withanage Chandana Prasad Charles Sturt University, Sydney Campus, Sydney, Australia.",
               "E-mail addresses: cwithana@studygroup.com (P. Prasad), aalsadoon@studygroup.com (A. Maag), amaag@studygroup.com it is also a general aspect. For the purpose of this paper, ABSA signifies sentiment analysis at entity or aspect level.",
               "This kind of fine-grained analysis has generally relied on machine learning techniques, which, although effective, require large, domain specific datasets and manual training data ( ). Furthermore, an aspect may be represented by different words requiring more than one classification algorithm . More recently, experimental work with machine learning methods has shown promise, with Poria, reporting higher accuracy using deep convolutional neural networks, a feature of deep learning (DL), named for its 'deep' multilayer processing technique that uses successive module layers to build on prior output using a backpropagation algorithm . In each layer, input is converted to numerical representations, which are subsequently classified. Thus, an increasingly higher level of abstraction is achieved . A range of algorithms (i.e. deep neural networks (DNN), recurrent neural networks (RNN), convolutional neural networks (CNN), recursive neural networks (RecNN), etc.) facilitate analysis in different fields with deep neural networks particularly suited to fine-grained work due to the significant number of layers of connected processors, activated either by sensors from the environment or by the weighted computations from preceding neurons ). An increase in the level of depth leads to higher capability for selective and invariant representation (i.e. extricating different objects) ( . Applied to Natural Language Processing (NLP) tasks, the advantage of DL lies in its independence from expert knowledge and linguistic resources ) as well as in its superior performance, demonstrated in the areas of 'name-entity recognition' , 'semantic role labelling' and 'Parts-Of-Speech (POS) tagging' ( .",
               "Early approaches to DL investigated linguistic features, grammatical relations, machine learning classifiers and topic modelling to identify aspects and polarities . More recently, DL methods have been successfully applied to NLP, which makes it interesting to investigate how DL has performed when set fine-grained tasks such as ABSA.",
               "To the best of our knowledge, this work is the first of its type to investigate application of DL methods to ABSA tasks. Recent surveys on have not yet covered the areas of ABSA in-depth, even in the work of , , Young, Hazarika, Poria, , and L. Zhang, . Deep learning methods are also absent from surveys on ABSA methods, evident from the work of , and . This paper, rather than repeating established findings from previous surveys, aims to present and compare more recent developments in DL approaches in general and for ABSA in particular. This review is specifically designed for students and researchers in the field of natural language processing, who would like to investigate deep neural networks as well as recent trends in research in ABSA.",
               "The remainder of the paper is organized as follows: Section 2 defines the tasks of ABSA and evaluation measures; Section 3 and 4 analyse DL models for ABSA, investigating in particular how DL affects the interpretation, architecture and performance of ABSA tasks; Section 5 discusses challenges in terms of ABSA and sentiment analysis; the conclusion in section 6 summarises the current landscapes of ABSA and deep learning methods. The sentence has two opinion targets: sushi & service. The category of \"sushi\" is \"Food\", with the attribute being \"Quality\" and polarity \"Positive\". The category is \"Service\", with an attribute of \"General\" and polarity of \"Positive\". focuses only on explicit targets while ACD is concerned with both explicit and implicit aspects."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 1,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "2.",
               "text": "Aspect-Based Sentiment Analysis (ABSA)",
               "type": "introduction"
          },
          "paragraphs": [
               "2.1. The three tasks of ABSA have assigned three important subtasks to ABSA: (i) Opinion target extraction (OTE), (ii) Aspect category detection (ACD) and (iii) Sentiment Polarity (SP), whereby OTE is concerned with the extraction of aspect terms (I.e. entity or attribute), ACD with identification of associated entities and attributes and (iii) and SP with the clarification of the sentiment polarity of the aspects. represents the three tasks of ABSA: The aim of OTE is to extract the opinion target (also referred as \"aspect term\" 1 _bookmark0) from sentences -in this case \"sushi\", or service. For ACD, given the predefined categories, the task is to identify the entity -the aspect of \"sushi\" as \"food\" and an attribute denoting \"quality\". SP identifies the sentiment of a target aspect -\"positive\" or \"negative\". It should be noted that the two latter tasks correlate strongly with each other as only through the combination of \"great\" and \"sushi\", can both, aspect category and polarity be recognised.",
               "In terms of aspects, identified two types, explicit and implicit, depending on whether or not the aspect words were explicitly stated. provides an example of an implicit opinion target in the statement \"My HP is very heavy\". It is clear that polarity and aspect can still be inferred. This implies that OTE ABSA is mainly applied to customer reviews from websites and e-commerce platforms such as Amazon, Yelp, Taobao and others. These are likely to be product or service reviews and it may be assumed that in each of these only one entity is mentioned but one or more aspects . In recent years, systems have been developed for domains such as electronic product reviews (laptop, camera, and phone) and hospitality reviews (restaurant, hotels). A number of benchmark datasets have been made available, including the customer review dataset by and a number of datasets released by 'International Workshop on Semantic Evaluation' on laptop, camera, restaurant and hotel reviews ( .",
               "Another line of research for ABSA is targeted (or targetdependent) sentiment analysis , which classifies opinion polarities of a certain target entity mentioned in sentences under scrutiny (normally a tweet). A number of benchmark datasets have been developed for this type such as the Twitter dataset by Dong, Wei, Tan, Tang, Zhou, and . below provides a list of publicly available data sets."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 2,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "2.3.",
               "text": "Previous approaches to ABSA tasks",
               "type": "modelling"
          },
          "paragraphs": [
               "Earlier approaches to identification of OTE and ACD (for example, ) were based on frequency of nouns and noun phrases in the text, with the assumption that aspect words were more likely to be repeated. The limitation of this approach is the dependency on the frequency of certain word categories (nouns/noun phrases), which may work well if the text contains high-frequency terms, but may fail if terms are infrequent ( ) . XML tag, in which two attributes (\"from and \"to\") that indicate its start and end offset in the text < sentence id = \"81\" > < text > Lightweight and the screen is beautiful! < /text > < aspectTerms > < aspectTerm term = \"screen\" polarity = \"positive\" from = \"20\" to = \"26\"/ > < /aspectTerms > < /sentence > XML tag of {E#A, polarity} < sentence id = \"1004293:0\"> < text > Judging from previous posts this used to be a good place, but not any longer. < /text > < Opinions > < Opinion target = \"place\" category = \"RESTAURANT#GENERAL\" polarity = \"negative\" from = \"51\" to = \"56\"/ > < /Opinions XML tag of {E#A, polarity} < sentence id = \"1661043:4\"> < text > Decor is charming. < /text > < Opinions > < Opinion target = \"Decor\" category = \"AMBIENCE#GENERAL\" polarity = \"positive\" from = \"0\" to = \"5\"/ > < /Opinions > < /sentence > MMAX format < markables xmlns = \"www.eml.org/NameSpaces/OpinionExpression\"> < markable id = \"markable_38\" span = \"word_118..word_119\" referent = \"empty\" annotation_type = \"holder\" mmax_level = \"opinionexpression\" isreference = \"true\" / > < markable id = \"markable_40\" span = \"word_123\" annotation_type = \"opinionexpression\" opinionholder = \"markable_38\" mmax_level = \"opinionexpression\" opiniontarget = \"markable_39\" strength = \"average\" polarity = \"positive\" opinionmodifier = \"empty\" / > < markable id = \"markable_37\" span = \"word_126\" annotation_type = \"target\" mmax_level = \"opinionexpression\" isreference = \"false\" / > < markable id = \"markable_39\" span = \"word_124\" referent = \"markable_37\" annotation_type = \"target\" mmax_level = \"opinionexpression\" isreference = \"true\" / > < /markables > Financial news headlines: 529 samples; financial microblogs: 774 annotated posts JSON nodes with sentiment score ranged from -1 to 1, \"target\" indicates opinion target, and \"aspect\" indicates aspect categories according to different level \"1\": { \"sentence\": \"Royal Mail chairman Donald Brydon set to step down\", \"info\": [ { \"snippets\": \"['set to step down']\", \"target\": \"Royal Mail\", \"sentiment_score\": \"-0.374\", \"aspects\": \" Others extracted OTE and ACD by exploiting opinion and target relations. Poria, and focused on rule-based linguistic patterns, including stop words and negation, etc. The assumption here was that it is easier to detect sentiment than aspect words. The authors proposed a set of opinion rules to first identify a sentiment word, and then use grammatical relations to build the syntactic structure of sentences and to detect the aspect. The final step consists of refinement where infrequent words are added and irrelevant aspects are removed. The lexical relation between sentiment words and aspects is the key element in this method, which is able to identify low-frequency aspects ( . However, a drawback is reliance on grammatical accuracy of the sentence and the requirement for manipulation .",
               "Topic modelling has been widely used to perform ACD tasks, with the most popular model being Latent Dirichlet Allocation (LDA) as implemented in , Alam, Ryu, and , Garca-Pablos, Cuadros, and , and Weichselbraun, Gindl, Fischer, . The basis of LDA is the introduction of a latent variable \"topic\" between the variables \"document\" and \"word\", whereby each document contains a random mix of topics, and each topic is constructed through relevant words. While this approach is appropriate to detect aspects at the document level, these may be too broad to capture fine-grained aspects ( . Furthermore, it was also observed that in the majority of studies, such as that by , the topics are unlabelled and require manual evaluation.",
               "For all three tasks, supervised learning approaches, characterised by the use of classifiers built from linguistic resources, predominated ( . A substantial number of studies in SemEval 2014-2016 chose classifiers such as Maximum Entropy (ME), Conditional Random Field (CRF) and Support Vector Machine (SVM) for aspect detection and polarity. Top performers include CRF models in , , and Brun, . SVM models were found in and and ME in . Supervised machine learning approaches were also used at aspect level in sentiment analysis of movie reviews , and Naive Bayes in ). A recent study by Akhtar, presented a cascaded framework based on two steps: first base learning algorithms as classifiers ME, CRF, SVM followed by an ensemble of feature selection and classifier using particle swarm optimization. While the machine learning is simple and quite efficient, it shows certain weaknesses, including the requirement for large datasets, reliance on manual training data, and non-replicable results for other domains . Furthermore, aspects can be represented by different words, which means one classification algorithm is insufficient ( ) ."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 3,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "2.4.",
               "text": "Evaluation measures of ABSA tasks",
               "type": "experiment"
          },
          "paragraphs": [
               "International Workshops on Semantic Evaluation are promoting the development of aspect-level sentiment analysis ( ) providing controlled evaluation methodology and shared datasets for all participants. For the measurement of the efficiency of a classifying model, four main measurements were proposed: Precision (P), Recall (R), F-score (F1) and Accuracy (Acc)."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 4,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "text": "P = T P T P + F P",
               "type": "experiment"
          },
          "paragraphs": [
               "(1)",
               "TP (true positives) and TN (true negatives) are the respective labels and non-labels that are assigned by the system (rather than by humans); FP (false positives) are those labels assigned by the system but not by human annotators, FN (false negatives) are those labels that human annotators assigned and which were not detected by the system.",
               "Precision measures the percentage of labels correctly assigned by the system. Recall measures the percentage of labels found by the system. Accuracy and F-score represent true results (TF and TN).",
               "For OTE and ACD tasks, the F-score is frequently used as the tasks are similar to information retrieval and, to evaluate SP, accuracy is applied."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 5,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "3.",
               "text": "Deep Learning for ABSA",
               "type": "experiment"
          },
          "paragraphs": [
               "Deep learning (DL) is a machine learning method based on learning data representation through algorithms -artificial neural and belief networks -based on multiple layers of modules where input is analysed and classified, with output from one layer fed into the next layer as input. This process is known as backpropagation ( , whereby activation initiates the backward computation of the gradient of an objective function . Irrelevant of the type of input (i.e. sound, image or text), it is converted to numerical vectors, then clustered into meaningful classification. As each successive layer is corresponding to an increased level of abstraction, DL can be said to represent \"nested hierarchies\" of simpler concepts . Another feature is that its depth level can be seen as similar to multi-stage programming, in which each layer is a computer's memory state after executing a set of instructions ( . By increasing the depth level, the system capacity to selectively and invariantly represent is enhanced .",
               "Deep neural networks (DNNs) are good examples of DL and are the focus of this paper. Deep neural networks are types of artificial neural networks (algorithms) which include a significant number of layers of \"neurons\" or connected processors, activated either by sensors from the environment or by the weighted computations from previous neurons . For DL, as for machine learning approaches in general, datasets are often divided into three components: training, validation and test datasets, conforming to general machine learning principles. mapped out the training process as a conversion of input into vector scores, regardless of type of input (i.e. images). Initially, an error score appears which needs to be reduced by training the algorithm to more closely conform to the set parameters (weights) for the target word (or image). Adjustments are subsequently made by the machine to reduce the error. The error-adjustment trigger is a 'gradient vector', which responds to the manipulation of the parameters, which needs to counter-balance the error. An example sentence with labels in IOB format with the opinion target/aspect term as \"onion rings\"."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 6,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "text": "Words:",
               "type": "experiment"
          },
          "paragraphs": [
               "The ",
               "The activation function is a non-linear function, either the sigmoid function ( Eq. 6 ), the hyperbolic tangent function ( Eq. 7 ), or the rectified linear function ( Eq. 8 )."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 7,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "3.1.",
               "text": "Multiple layers of DNN",
               "type": "experiment"
          },
          "paragraphs": [
               "For layers of neurons, given layer l with m neurons, each with n -dimensional input vector xR n with n -dimensional associated weight matrix WR mn and a bias scalar bR n and the activation function s (either sigmoid, tanh or ReLU ), the computation of l can be written as:",
               "Deep neural network (DNN) approaches to NLP are distinguished by (i) dense word embeddings; (ii) multiple hidden layers between the input and output; and (iii) output units .",
               "Word embeddings are d-dimensional space representations of words, encoded as dense numerical vectors ). These vectors, argue, establish the likelihood of a word appearing within a specific word matrix (i.e. with associated words). One of the first word embedding models was that of who proposed a neural probabilistic language model with shared lookup table. Thus, given a word and its preceding words, the model looks up its continuous vector, and then feeds the information into a feed-forward neural network to predict the probable function of the next word. In an attempt to reduce feature engineering, many DNN based studies have used word embeddings as the only feature (such as . In recent DNN models, word embeddings are typically pre-trained but not task-specific data so that the learning word vectors can capture general syntactical and semantic information ( Chen, Xu, He, & Wang, 2017 ; P. . There are different models for word embeddings, such as Word2Vec ) that encode contextual information using continuous BagOf-Words (CBOW) and skip-gram models. Word embeddings are discussed further in Section 3.3.1.",
               "The second feature -hidden layers -can be constructed in different forms and architectures, i.e. feed-forward networks and recurrent or recursive networks . Each hidden layer is composed of multiple neurons, stacked together to compute non-linear outputs ( . Generally, the higher layers evolve through training to exploit the complex compositional nonlinear functions of the lower layers and, hence, capture more abstract representations than the lower layers .",
               "The computation of hidden features starts with neurons, which take n input to produce a single output. Considering the inputs x 1 , x 2 ,R with n associated parameters (or weights) w 1 , w 2 ,R and a bias scalar bR , the activation of the neu-",
               "The third feature -output units -represents the distributed probability over all labels or classes. Supposing the last layer is z and there are K labels/classes, the probability for the label i can be obtained using the softmax function as set out below:",
               "For OTE, categories can be represented as similar to sequence tagging of IBO labels (\"B\" is the start of the aspect term, \"I\" is the continuation of the aspect term and \"O\" is not an aspect term) ( ).",
               "For ACD, for a given category or attribute, the label can be represented as a binary T = {category, non-category}. For sentiment polarity, categories might be the set of 4 way polarities as in SemEval tasks T = {positive, negative, neutral, conflict} or simple binary polarities, such as T = {positive, negative}. For all three tasks, the output units of the DNN model can return the probability to assign a given label to each input, whereby the label with the highest probability represents the result of the prediction.",
               "To summarise the above discussion, suggested that DNNs with distributed representation are able to generalise new combinations of learnt features beyond what has been learned in the training phase. Therefore, in contrast to standard machine learning, DNN models attempt to automatically learn good features or representations ( Rojas-Barahona, 2016 ). Unlike traditional methods, DNN models also do not require much feature engineering, and if the right model is chosen, it has more robust extraction and representation capacities ( Araque, Corcuera-Platas, .",
               "In the sections below, major DNN models applied to ABSA tasks will be reviewed, including convolutional neural networks (CNN), recurrent neural networks (RNN), recursive neural networks (RecNN), and hybrid models in Section 3.2 -3.5. In each section, a review of architecture will be provided, following by the application of models to ABSA tasks. "
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 8,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "3.2.",
               "text": "Initialization of input vectors",
               "type": "experiment"
          },
          "paragraphs": [
               "where R () is the regularisation term, and g ( w t , w t ? 1 , , w t ? n + 1 ;) can be estimated by the softmax function as p ( w t | w t ? 1 , , Before reaching the first hidden layer of the DNN, the input layer is encoded with a distributed representation, or word embeddings, which represent each word as a low-dimensional, realvalued and continuous vector to encode its semantic and syntactic properties ( Tang, Wei, et al., 2016 )."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 9,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "3.2.1.",
               "text": "Word embeddings vectors",
               "type": "experiment"
          },
          "paragraphs": [
               "One of the first word embedding models is by Bengio, Schwenk, Sencal, Morin, & Gauvain (2006) who proposed a neural probabilistic language model with a shared lookup table. Given a word and its previous words, the model can look up its continuous vector, feed the vector to a feed-forward neural network and predict the probability function for its next word. Assuming a sequence of T words w 1 , w 2 , , w T with n previous words fed into the model, the model can predict the probability p of the words u t based on w t ? n + 1 ). developed the word2vec with two different neural network models for creating word embeddings for training on large corpora: a bag-of-words based model (CBOW) that obtains word context from sequential word context; and the skip-gram model that predicts the word embeddings from neighbouring words .",
               "The CBOW for the target as the word w t at time step t , the model receives a window of n words around w t , and the loss function J can be written as:",
               "In contrast, the skip-gram model uses the centre word w t to predict the neighbouring words w t + j . In this case, the objective function is: T finding the model parameterthat maximises the objective funclog p ( w t+ j | w t ) tion J :",
               "In addition to word2vec framework, a number of software have been developed for training word embeddings such as GloVe developed by Stanford University or fastText developed by Facebook. Pre-trained word vectors have also been developed such as SENNA (based on the Wikipedia corpus); Google (based on the Google News corpus); Amazon (based on the Amazon corpus); GloVe (based on Wikipedia and Twitter); SSWE (based on Twitter with inclusion of emoticons) There are a number of ways to initialize word embeddings, including random initialization (i.e setting the embedding vectors to random values), or pre-trained (i.e. tuning the vectors so that similar words will obtain similar vectors) . The recent high performing models typically opt for pre-trained word embeddings and fine-tune them to better initialize the model. As discussed in , the random approach can lead to stochastic gradient descent in the local minima, and if the pretrained word beddings are employed from readily available resource without tuning, this may not exploit automatic feature learning capacity of DNNs. Experiments from studies such as , , show that the model will be beneficial with the initialization of pre-trained work embeddings and fine-tune them in training, for example, only using pre-trained word embeddings contributed a gain of 6-9% in aspect term extraction ) or 2% in sentiment polarity ( Wu, Gu, Sun, & Gu, 2016 ).   , for the task of customer review with less formal texts than Wikipedia and Google news corpus, a word embeddings scheme that contains more opinion specific words such as Amazon have better performance. Meanwhile, in Twitter target ABSA, showed that a sentiment-specific word embeddings (SSWE) have better performance."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 10,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "3.2.2.",
               "text": "Featuring vectors",
               "type": "experiment"
          },
          "paragraphs": [
               "As mentioned above, in contrast to previous approaches, deep learning rarely relies on feature engineering, parser, or positional information, but solely on language input ( ). However, in order to generate more salient performance, a number of feature vectors are fed into the DNN, together with the word embeddings. The most common features are summarised below."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 11,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "3.2.3.",
               "text": "Part-Of-Speech (POS) and chunk tags",
               "type": "experiment"
          },
          "paragraphs": [
               "One observation from , there is high probability that the aspect terms are nouns or noun chunks, which reveals the importance of POS features in OTE. The number of classifications for POS tagging varies (i.e. 6 tags according to Stanford  Generally, k tags, representing k parts of speech, can be encoded as k -dimensional binary vectors and then concatenated with the word embeddings vectors before being fed to the neural network models. Experiments have shown that POS tagging and word chunks complement word embeddings play a major role in aspect extraction, contributing from 1% ( ) to 4% gain ."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 12,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "3.2.4.",
               "text": "Commonsense knowledge",
               "type": "experiment"
          },
          "paragraphs": [
               "Another feature suggested by and to improve both aspect extraction and sentiment classification is common-sense knowledge through SenticNet. This base consists of over 50,0 0 0 concepts with associated affective properties ( Y. which are represented by real-value scores consisting of 5 sentics: pleasantness, attention, sensitivity, aptitude, and polarity, which can imply semantic links to aspect and sentiment ) . An example given by Y. is the concept \"cupcake\" has the property \"KindOf-food\" that can be related to 'restaurant' or 'food quality', but also emotions, e.g., \"Arise-joy\" that supports sentiment classification.",
               "By including them as 5 feature vectors for each concept, those studies have shown improvement. suggested the Sentic LSTM significantly outperformed a baseline LSTM. observed that while sentics did not contribute to aspect term extraction, the usage of sentic vectors contributed to 4% gain in the model for sentiment analysis and considerably reduced the training time.",
               "the final layer to obtain optimum performance ( . The most extensively used classifiers in recent years include Support Vector Machine (SVM) and Conditional Random Fields (CRF) classifiers, with examples in ABSA tasks as CRF in , Xu, Lin, Wang, , , or SMV in Akhtar, , and . All these are discriminative models, which learn the most useful features of the input to predict the output, and are trained with different loss functions ( ).",
               "The SVM model is a classifier that outputs the identity of different classes based on a linear function. The incorporation of the SVM with the neural network model can be implemented selecting the label with the highest score of y expressed as:",
               "Thus, for the highest scoring label k = argmin i y i and the correct labe? k = arg min i ? y i , the SVM loss function is:"
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 13,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "3.3.",
               "text": "Training process of DNNs",
               "type": "experiment"
          },
          "paragraphs": [
               "A neural network is trained through a backpropagation process in which the gradients of all parameters are computed backward and updated with stochastic gradient descent .",
               "Let x = x 1 , x 2 , , x n be the input, and y = y 1 , y 2 , , y n be the output from the machine learning algorithm with the actual labels by = ? y 1 , ? y 2 ,, ? y n , the goal of the algorithm is to estimate a function y = f ( x ) that matches the inputs with their correct label. The loss function is employed during the training phrase to calculate a numerical score L that is loss when predicting output y with respect t? y . In this sense, the parameters of the function (weight matrix WR mn and a bias scalar b ) are to be set to minimise the loss L . The loss function for the whole sample is calculated",
               "Comparing the softmax layer with SVM, one advantage of the latter is that it is useful under conditions of hard decision rule (i.e. when it is not necessary to estimate the probability of each label) . Nevertheless, applied to classification task such as sentiment analysis, its performance maybe hindered by its \"sparse\" and \"discrete\" features, which makes it difficult to transfer information regarding relationships and coherence of chunks or sentences .",
               "CRF assumes that the output y is connected by undirected edges in an undirected graph . In this sense, the CRF represents the score of a given label sequence (or the clique potential) as a conditional probability that is proportional to the input sequence ( T. as:",
               "where Z x is the normalization, S ( y, x ) the set of cliques of the undirected graph where the outputs are connected and ? s ( y s , x s ) is the clique potential. The loss function is calculated as:",
               "with respect to the parameteras the average loss:",
               "i =1",
               "In addition, while minimizing the cost, the model maybe overfitting. Thus, the algorithm combines another function R () to measure the complexity. Therefore, the goal of the function then is For the CRF model, the labels of each consecutive point can influence others , which overcomes the disadvantage of softmax which features independent labels . Thus, it can be inferred that CRFs can take advantage of the entire sentence sequence to estimate probability for the sentence labelling making CRF a frequent final classification layer of bidirectional RNNs ( T. ; P. .",
               "to set theto minimize the loss value while keeping a low complexity R ():",
               "Following the softmax function as above, the categorical crossentropy loss is used as the loss function:",
               "Because the scores y i are not negative with the sum of one, the cross-entropy loss produces not only the label prediction but also the distribution.",
               "In some cases, after the features have been obtained from the neural network models, non-neural classifiers are incorporated as"
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 14,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "3.4.1.",
               "text": "Architecture",
               "type": "experiment"
          },
          "paragraphs": [
               "CNN has become a popular DL model amongst NLP researchers, since the pioneering works of and who advocated the success of CNN in a number of NLP tasks, including sentiment analysis. The main strength of the CNN is its ability to extract the most important n-gram features from the input to create an \"informative latent semantic representation\" for undertaking further classification tasks .",
               "The basic single layer CNN for sentence modelling may consist of 4 layers as below, according to .",
               "The representation of each layer is: Input layer : representing the sentence of length n as where x iR d be the d -dimensional word vector corresponding to the i-th word in the sentence and is the concatenation operator. Convolutional layer : this layer generates the new feature c i with the filter wR hk , using the window of h words from i to i + h ? 1 as to capture the most important feature (n-gram) for each feature map; and second, the max-pooling layer can produce a fixedlength output regardless of the size of the filter window h .",
               "where bR is the bias term and s is a non-linear activation function, such as sigmoid , hyperbolic tangent ( tanh ), or rectified linear ( ReLU ) functions.",
               "So for the sentence, as the possible windows are { x 1: h ; x 2: h + 1; x n ? h + 1: n }, leading to the feature map as: ",
               "So the softmax layer outputs a probability distribution overall output labels or classes.",
               "The CNN has two important implications from the convolutional and max-pooling layer: first, the convolutional layer is able"
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 15,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "3.4.2.",
               "text": "Application in consumer review domain",
               "type": "relatedwork"
          },
          "paragraphs": [
               "The motivation for utilising the CNN model in ABSA tasks is the assumption that key words may contain the aspect term, and indicate a category or determine polarity, regardless of their position. The CNN is capable of learning to find those features with its architecture and is, thus, able to extract local patterns from data regardless of their location. This is very useful for identifying fixed-length phrases . Another advantage is that the CNN is a non-linear model which is expected to better fit the data than linear models such as the CRF and does not require extensive hand-crafted features such as fixed language rules ).",
               "The CNN model has been applied to all tasks of ABSA, mainly in consumer review domain ).",
               "For the OTE task, a prime example of successful studies is Poria, who adapted CNN architecture from sentence representation to word-based prediction. With the assumption that the tag of each word is dependent on each neighbouring words, they formed a local feature window of five words around each word in a sentence. A deep CNN of seven layers, in- Note: PM indicates that the dataset was primarily collected by authors. 3-way represents the three polarities of positive, negative, neutral. cluding one input layer, two convolution layers, two max-pool layers, and a fully connected layer with softmax output, is then applied to each window of words with the prediction to the centre of the window. Experiments show that the deep CNN model, even without any feature engineering or linguistic patterns, still outperformed state-of-the-art models. For other ABSA tasks, CNN is also a promising approach. Toh and Su (2016) achieved the best performance in SemEval 2016 in ACD with the assembling of two different machine learning systems. As they considered ACD as a multi-class classification problem, they followed a binary relevance approach. Particularly, they used multiple binary classifiers trained on a single layer feedforward neural network then combined the probabilities output from a deep CNN to predict if the text consists of an aspect category. Compared with other features, CNN features contributed the most to performance.  responding aspect. If that is the case, the sentiment classifier predicts sentiment polarity as positive or negative. Apart from the advantages of reduced feature engineering compared to traditional ML methods, the cascaded model also showed that the CNN presented a remarkable reduction of elapsed time, compared to SVM.",
               "Oppositely, an example of multitasking CNN is the work of that proposed a CNN approach to undertake both ACD and SP. Similar to , they considered aspect extraction as a multi-label classification problem but approached this through a probability distribution threshold . Assuming a sentence S contains K aspect categories, the probability for the with W being the weighted matrix function between x t and h t ? 1 , and s is a non-linear activation function, such as tanh or ReLU . Therefore, the output can be computed as:",
               "sentence to contain an aspect category k is defined as",
               "The thresholdis selected to maximise the F1 score, and the aspect category is selected to satisfy p ( k | S ).",
               "To determine the sentiment towards an aspect, they concatenated an aspect vector with every word embedding and applied a CNN over it. The model also has demonstrated convincing results in the multilingual settings of Spanish, Dutch, and Turkish, showing the strength of DNN as language and domain independence. Another work, , proposed a multitask CNN, which contains aspect mappers and a sentiment classifier sharing word embedding layer whereas other parameters are kept specific in each task. Although this is a promising approach, the experiment showed that multitask CNN performed just slightly better than cascaded CNN."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 16,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "3.5.",
               "text": "Recurrent Neural Network Models (RNN)",
               "type": "modelling"
          },
          "paragraphs": [
               "Recurrent Neural Networks have become popular in sentiment analysis tasks. The basic of RNN models is that a fixed-size vector represents one sequence (i.e. sentence or document) by feeding each token into a recurrent unit, so it can capture the inherent sequential nature of language (i.e. one word develop its semantic meaning thanks to its previous word) ). Compared to the CNN models, RNN models have flexible computation steps that the output from RNN is dependent on the previous computations, making it capable of capturing context dependencies in language as well as capable to model various text lengths ).",
               "The RNN model has two important features compared to the feed-forward neural network. First, unlike the CNN has different parameters at each layer, the parameters in RNN are the same in each steps, which then reduces the number of parameters needed to learn ( L. . Second, as the output of one state depends on the previous state, RNN can be said to have the memory of previous computations, making it more superior in processing sequential information compared to the CNN.",
               "However, the simple RNN has a major weakness in terms of the vanishing gradient problems (the gradient comes close to zero) or exploding gradient (the gradient is extremely high) . As discussed earlier, because the basic role of the gradient is to tune the parameters to improve the gradient, extremes make it difficult to decide in which direction to tweak the parameters, while an exploding gradient causes an unstable learning process ( .",
               "However, the simple RNN has limitations caused by the gradient. It may vanish (coming close to zero) or explode (being extremely high). This occurs during the backpropagation process, making it difficult to train and fine-tune the parameters . This limitation has been improved with the introduction of networks such as long short-term memory (LSTM) ) and gated recurrent units (GRU) ( ). below compares the demonstration and computation of hidden networks between LSTM and GRU. The basis of LSTM is a memory cell that controls the read, write and reset operations of its internal state through output, input and forget gates. At one time t , with the current input x t and output from the previous state h t ? 1 , the forget gate will decide which information to keep and which to offload, subsequently updating the memory cell. GRU consists of two gates -the reset and the update gate and handles the flow of information, similar to LSTM without the memory unit."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 17,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "3.5.1.",
               "text": "Computation of RNN models",
               "type": "modelling"
          },
          "paragraphs": [
               "The simple RNN model is based on the Elman network ) with direct cycles in their hidden connection ). This model proposes that the hidden state is dependent on the input and past hidden state, with the same function and the same set of parameters being used at every time step. shows a basic RNN with a three-layer network of input, hidden state and output. At time t, given x t as the input to the network, the hidden state h t is calculated as;"
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 18,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "3.5.2.",
               "text": "Bidirectional RNN",
               "type": "modelling"
          },
          "paragraphs": [
               "The three models presented above focus on using past words to predict the next word. In practice, many studies would like to make predictions based on the future words, and thus, the bidirectional RNN models are proposed, with the incorporation a forward and a backward layer in order to learn information from preceding and following tokens ). As shown in  ",
               "based on the future hidden state h t+1 and current input x t . The Comparison of LSTM and GRU. Equations and figures from Chung, Gulcehre, .",
               "forward and backward context representations h t and h t are then concatenated into a long vector at the timestep t as:"
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 19,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "3.5.3.",
               "text": "Attention mechanism and memory networks",
               "type": "modelling"
          },
          "paragraphs": [
               "In ABSA, as the aim is to classify sentiment with respect to target aspect terms in the text, it is important for the method to model the interaction between the aspects and the whole sentence. The traditional encoder-decoder framework such as RNN has a potential problem in that the encoder may encode irrelevant information, especially when the input is very informationrich ( P. Chen, Sun, Bing, & Yang, 2017 ; Y. Wang, Huang, Zhao, & Zhu, 2016 ). One possible solution is to employ an attention mechanism, which allows the model to learn which part of the text to focus on. The general idea of the attention mechanism is to compute an attention weight from each lower level then aggregate the weighted vectors for higher level representation . below shows the global attention model on a bidirectional LSTM, following the decoder-encoder model by Bahdanau, in neural machine translation.",
               "In this model, given the input sentence S = { x 1 , x 2 , x T }, at time step t , the output y t is dependent on the decoder state s t and the ",
               "where c t is the context vector and c t is dependent on the set of H t = { h 1 , h 2 , h T }. Given the attention weights denoted ast = {t 1 ,t 2 , ,tT }, the context vector is computed as:",
               "To compute the attention weight, the model utilises an alignment process, which first computes the attention energies e ti from s t ? 1 and h i using a feed-forward neural network a as:",
               "Variants of attention mechanisms can be computed according to a different function a , such as the additive attention ( Bahdanau et al., 2014 ; Y. as:",
               "where W, U are weighted matrix and v a is the weight vector (or aspect embedding vector).",
               "After that, the weightti can be computed using the softmax",
               "In the case of ABSA, this also implies that during the decoding period the decoder is conditioned on a \"context\" vector. This mechanism is most suitable to be applied for the task of sentiment classification, given the aspect terms or aspect categories. It is expected that the models with attention mechanism can focus on the important parts of the sentence in terms of aspects.",
               "Another mechanism that can be applied to resolve the issue of irrelevant information is using external memory such as the Memory Networks model (MemNet) . shows an extension of the attention mechanism with external memory in a MemNet by .",
               "Initially, the sentence is modelled as the composition of n words {w 1 , w 2 , , w i , , w n } with the aspect word as w i . Simultaneously , the context word vectors {c 1 , c 2 , , c i ? 1 , c i + 1 , , c n } are stacked as the external memory slices { m 1 , m 2 , , m i ? 1 , m i + 1 , , m n }. Then, in the first computation layer (hop 1), the aspect vector v a is selected from the external memory. The output is a continuous vector v computed from the weighted sum of the memory slides as",
               "wherei is the weight of m i to be calculated and i w mi = 1. The scoring function aims at measuring the semantic similarity of each memory slice m i to the aspect vector v a as:",
               "where W a is the weighted matrix and b a is the bias. This is followed by computation of the weight of m i using the softmax function as:",
               "After the first hop, the attention layer and the linear transformation of the aspect vector are totalled. The sum is stored in the external memory for further information retrieval. This means that the decoder can encode context embeddings from context words and aspect embeddings from aspect words. Thus, the application of attention and memory network mechanisms is useful for sentiment classification of the whole sentence."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 20,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "3.5.4.",
               "text": "Application in the consumer review domain",
               "type": "relatedwork"
          },
          "paragraphs": [
               "Compared to other neural network models, RNNs and bidirectional RNNs have had a huge presence in the word-level and sentence-level classification in consumer review domain ).",
               "Many RNN-based models took advantage of the bidirectional RNN to recode past and future contexts. An approach from used hierarchical bidirectional RNN in ACD in highly skewed data of hotel review domain and obtained superior result over LSTM. The model is composed of six layers including four layers of bidirectional RNNs , one fully connected layer and one softmax layer. Each layer constitutes a hierarchy of classifier. They proposed a mini-batch approach whereby the input dataset is separated randomly into a few words to compute seed classifications; then the remaining words are placed into seed classes to find the highest similarity on average. With the similar task of ACD, also used an RNN to model the context of each word as well as the background context. Using continuous vectors to calculate the probabilities of generating different words, they offered an alternative solution for topic models, which was more effective. Jebbara and Cimiano (2016) employed bidirectional GRU for OTE and aspect-specific sentiment extraction. As a first step, a bidirectional GRU is used to extract aspects from a text as a sequence labelling of IOB. In a second step, a bidirectional GRU extracted aspect regarding its context and predicted its sentiment label. Other features include pre-trained semantic word embedding, semantic knowledge extracted from Word-Net and SenticNet.",
               "One of the most successful attempts is to combine RNN with the CRF classification layer, so that the model not only captures the long-term dependency of the entire sentences, but also utilises the dependency of each label on each other. proposed an application of recurrent neural network (RNN) in OTE with linguistic features of POS, word chucks, which showed better performance than a feature-rich CRF-based system. Inspired by the system of NER by , T. proposed a bidirectional LSTM-CRF in classifying numbers of targets in the sentence, but the model also achieved state-of-the-art performance in OTE. Overcoming the limitation of a fixed window size in CNN model, this network captured long-term dependencies of context information. The result of the bidirectional LSTM is two fixed-size vectors, which were then concatenated at the fully connected layer. For the IOB tagging, the authors use a CRF layer at last . A similar model by also showed its effectiveness in Vietnamese.",
               "Another promising direction is to utilise attention and memory networks. Approaching the OTE task, proposed an extended memory framework for LSTM while proposed a LSTM model in cross-domain aspect term extraction, with the combination of rule-based methods that generated auxiliary label sequence for each sentence. Another study by also incorporated attention in tasks of OTE and ACD with their Truncated History-Attention (THA) and Selective Transformation Network (STN) built on two LSTMs.",
               "Meanwhile, W. proposed a Coupled MultiLayer Attention Model (CMLA) based on GRU for co-extracting of aspect and opinion terms. Therefore, learning can be done by encoding/decoding the dual propagations of aspects and opinion terms, and not restricted to grammatical relations . This framework reduces engineering features compared to the CRF and the co-extraction is a worth-noting feature.",
               "Y. and Y. proposed a solution with attention weight, in which aspect embeddings are used to decide attention weights for sentiment classification, in addition with sentence representation. Therefore, the model can have different concentration on different parts when different aspects are given (for example in . Another work by Cheng et al. Note: PM indicates that the dataset was primarily collected by the authors. 3-way refers to the three polarities of positive, negative, neutral.",
               "(2017) applied attention with bidirectional GRU model to attend the aspect information for one given aspect and extract sentiment for that given aspect. Their work achieved state-of-the-art performance on benchmark datasets. Also in a similar task, proposed an aspect target sequence model (ATSM) to incorporate adaptive embeddings at word, character and radical level in dealing with multiple-word aspect issues in Chinese.",
               "On the other hand, Tang, Qin, et al. (2016) adopted a memory network (MemNet) solution, which is based on multiple-hop attention. They included a multiple-attention computation layer on the memory network, which improved lookup for most informational regions. Memory networks also feature in R. who uses compositing strategies to represent context and features for each word. In deep hops, their model outperforms state-of-the-art approaches of SVM with less feature engineering. proposed Dyadic Memory Networks (DyMemNN), which incorporates composition techniques that model the dyadic interactions between aspect and words in a document. Their model also achieved competitive performance in OTE and ACD.",
               "An interesting work by proposed adding attention layers in their bidirectional LSTM. They proposed two models to achieve target-specific sentiment classification: Target Dependent L STM (TDL STM) directly uses the hidden outputs of a bidirectional LSTM sentence encoder in panning the target mentions, while Target Connection L STM (TCL STM) extends TDL STM by concatenating each input word vector with a target vector . However, they failed to achieve competitive results possibly due to the small training corpus. "
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 21,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "3.6.",
               "text": "Recursive Neural Network Model (RecNN)",
               "type": "modelling"
          },
          "paragraphs": [
               "3.5.5. Application in targeted sentiment analysis RNN models are also applied in Twitter domain and new comments rather than CNN model, as explained above on the limitation of CNN model in capturing long-term dependencies. Unlike the customer review domain, the Twitter domain is challenged with the limited length, informal contexts and the use of emoticons ( . However, studies using RNN models have showed competitive performance in this task."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 22,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "3.6.1.",
               "text": "Architecture",
               "type": "modelling"
          },
          "paragraphs": [
               "Recursive neural network (RecNN) models are linguistically motivated in that they explore tree structures (e.g., syntactic structures) and aim to learn elegantly compositional semantics. Arguably, natural language demonstrates a natural recursive structure, placing words and phrases in a hierarchical manner. Thus, tree-structured models can better make use of such syntactic interpretations of sentence structure . Generally, in a recursive neural network, the vector representation of each node in the tree structure is calculated from the representation of all its children us-  . Grammatical relations determine aspects and opinions: 'fish burger' and 'tastes' are obvious aspect terms, with the respective opinions of 'best' and 'fresh'. Considering tastes as an aspect term, fresh can be extracted as an opinion term through a direct relation. Considering 'fish burger' as an aspect term, 'tastes' can be extracted as another aspect term through the indirection relation. Taken from W. .  ing a weight matrix W which is shared across the whole network ( . For example, giving c 1 and c 2 as ndimensional vector representation of nodes, their parent will also be an n -dimensional vector calculated using a non-linear function such as tanh:",
               "So in general, a hidden vector for any node n associated with a word vector x n can be computed as:",
               "where K n denotes the set of children of node n, r nk denotes the dependency relation between node n and its child node k , and hk is the hidden vector of the child node k .",
               "The tree structures used for RNNs include constituency tree and dependency tree. In a constituency tree, the words is represented at leaf nodes, a phrase is represented at internal nodes the root node represents the whole sentence ( . Meanwhile, in a dependency tree, each node including represents a word, connecting with other nodes with dependency connections ). Demonstration of the constituency tree and dependency tree is presented in . ral network frameworks can resolve this issue, they have not overcome the limitation of RecNN caused by the requirement for a predefined tree structure to encode sentences, which limits the scope of its application ( Rojas-Barahona, 2016 ).",
               "Approaching the task of target-dependent Twitter sentiment analysis, proposed Adaptive Recursive Neural Network that propagates the sentiments of words to target depending on the context and syntactic relationship. This work can be considered as similar to sentiment polarity of aspect term as their annotated dataset contains only one target per tweet.",
               "Nevertheless, contrary to previous studies, argued that simply averaging the attention vector in RNN models might not solve the issues of multiple targets within a text. Their proposed model of using dependency trees overcomes this issue and achieves competitive performance in Twitter target sentiment analysis."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 23,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "3.6.2.",
               "text": "Application",
               "type": "modelling"
          },
          "paragraphs": [
               "Despite the popularity of Recursive Neural Networks (RecNN) in various NLP tasks, its application to ABSA is rather limited .",
               "Regarding to consumer domain review, a study by proposed recursive tensor neural networks in extracting both target and sentiment that is more robust than two single models, and also allows for the representation of multiple aspects within the text.",
               "Two more recent works involving RecNN in the customer review domain include those of and , both aimed at exploiting the aspects through the dependency and constituent trees of the sentence. While just focus on OTE using dependency and constituent trees, W. expanded the recursive neural network models by with a novel framework of RecNN & CRF to co-extract the aspect and opinion terms. This framework consists of a dependency-tree RecNN sentence representation, which feeds input to the CRF for target and opinion coextraction.",
               "RecNN models developed by and aimed to improve the error-prone two-step approaches ( , whereby error 1 leads to error 2. However, while recursive neuCoping with both advantages and disadvantages of the previously discussed models, many studies attempted to apply hybrid solutions in customer review domains, such as Xue, Zhou, , , Chen, Xu, Yang, and . noted that the aspect terms and aspect category are closely related, so they proposed a multi-task framework of BiLSTM for OTE and CNN for ACD. The main benefits of this framework is the mutual information sharing of two tasks, in which the CNN can also utilize extra information learned in the BiLSTM to improve its informative features, while the predicted tag from the BiLSTM can also receive the most salient n-gram features via convolutional operations.",
               "Similarly, also combined LSTM and CNN together for sentiment classification but used LST for generating context embedding and CNN for detecting features. proposed a dependency-tree based convolutional stacked neural network (DTBCSNN) for aspect term extraction, in which the convolution is included in the sentence's dependency parse trees to capture syntactic and semantic features. This can overcome the practical limitations of sequential models (RNNs) which cannot capture the tree-based dependency information. The proposed model does not need any handcraft features and flexible to include extra linguistic patterns. below provides an overview of hybrid solutions."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 24,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "4.",
               "text": "Comparison of performance on benchmark datasets",
               "type": "experiment"
          },
          "paragraphs": [
               "The above discussion has provided insights into the different approaches chosen by researchers for NLP tasks. It is evident that outcomes depend not only on model choice but on the seman-  ) due to their ability to extract local patterns (i.e. the most important n-gram) of a sentence to produce fixed size input. However, this is true only as long as classification of key phrases of limited length is required . Furthermore, CNN models demand large sets of training data and require a significant amount of fine-tuned parameters . Further issues arise from the fixed size of the hidden layer which prompts manipulation of input sentence length (CNN models add padding to short sentences and reduce long sentences), making capture of broader contextual information and sentence dependen- . Although this limitation can to some extent be overcome by a text window approach, whereby local feature windows of neighbouring words form around each word such as demonstrated by , no information can be captured outside the window. As pointed out by Tu, Lu, Liu, Liu, and , this has important implications for the application of CNN to languages with morphologically-rich texts such as Russian and Mandarin. In such cases, a model capable of recognising long-term dependencies such as Recurrent Neural Networks (RNN) or Recursive Neural Networks (RecNN) is called for.",
               "RNNs are powerful because they combine two properties: (i) Distributed hidden states that allows them to efficiently store information from past computations; and (ii) Non-linear dynamics that better fit the non-linear nature of data ). Significant research suggests that RNN is superior to CNN, citing the example of the LSTM model which does not require large training datasets ) and can achieve comparative performance to CNNs with fewer parameters . Therefore, in terms of ABSA tasks, RNNs may perform better than CNNs if the classification is dependent on the semantic relationship of whole sentences.",
               "In the case of RecNNs, a simple architecture and the ability to learn tree-structures of sentences and new words are distinct advantages . However, they are heavily dependent on parsers have not yet shown consistent performance in sentence classification . Further research is clearly required.",
               "Thus, these different models were designed with different objectives in terms of sentence modelling, particularly when analysing CNNs and RNNs. While CNNs try to extract the most important n-grams, RNNs try to create a composition with unbounded context ( . presents the summary of model comparison.",
               "To provide insights into the large number of proposed methods for ABSA, the below session will classify all methods according to three ABSA tasks: aspect term (or opinionated target) extraction (OTE), methods focusing on aspect category detection (ACD), and methods focusing on aspect-specific sentiment polarity (SP).",
               "For each task, the comparison is presented with a table outlining the attempted DNN methods together with the bestperforming methods from SemEval ABSA. Each table contains the method, its domain, the performance as reported by the studies. The performance is reported in the form of Precision, Recall, F1, and Accuracy. It should be noted that: (i) some papers did not provide all the measures; (ii) when multiple models are proposed, the best model will be reported; (iii) due to the difference in experimental settings, the methods should not be compared using the scores; (iv) some researchers aimed to resolve two or three tasks and so will appear more than once in the tables."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 25,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "4.1.",
               "text": "Opinion target extraction",
               "type": "experiment"
          },
          "paragraphs": [
               "As the majority of studies in this task used data from SemEval 2014 with two domains in English, namely laptop and restaurant, evaluates performance for a range of models with respect to this dataset. It is apparent that the majority of approaches were implemented according to RNN and its variants as LSTM or GRU, with high performance in both domains -F1 of over 75 in the laptop domain and over 80 in the restaurant domain. The current best model appears to be the CNN system by for both domains, showing that a window-approach in CNN can extract relevant opinion targets, and can overcome the issue of long-term dependency. It is also interesting that the attention mechanism can boost performance of RNN-based systems (for examples . Fewer attempts were made to apply RecNN with lower performance, suggesting that processing words sequentially may be more informative than a tree structure. compares the performance of models across different languages within the SemEval 2016 dataset for the restaurant domain. It is apparent that the performance of models in English is better than for other languages, followed by French and Spanish. It is also interesting to see that the LSTM models ) and hybrid models ( show higher performance than the best models in the SemEval competition. There is, therefore, some evidence that the LSTM-based models are more effective in multilingual environments, due to their ability to record past and future contexts of words.",
               "It should be emphasised that in this dataset most of the sentences consist only of one target term, and most target terms are expressed by a single word. Therefore, the CNNs can extract the target efficiently. However, the comparison also shows that when RNNs are incorporated with other components such as attention and MemNet, they have comparable power. Such combinations can overcome the weakness of RNNs in capturing key phrases."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 26,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "4.2.",
               "text": "Aspect category detection",
               "type": "experiment"
          },
          "paragraphs": [
               "Similar to the OTE task, the DNN model has outperformed the best performing supervised machine learning models , with their performance in ACD reaching F1 60-70 in English, and over 50 for other languages. In this task, more datasets and more Performance in opinion target extraction using the SemEval 2014 dataset (restaurant and laptop domains). The best outcomes are highlighted in blue.",
               "languages have been used, from which can be inferred that the performance in English is much higher than for other languages. In term of the model, CNN seems to have the best performance for this task, with winning models in SemEval ABSA 2016 by and and an outperforming model by . Nevertheless, because of the limited neural network studies in this ACD task, it is difficult to conclude which model achieves the best performance."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 27,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "4.3.",
               "text": "Sentiment polarity of aspect-based consumer reviews",
               "type": "relatedwork"
          },
          "paragraphs": [
               "The task of aspect level sentiment polarity is more challenging than general task of sentiment analysis because the model needs to incorporate the impacts of context words towards the target or aspect. A general approach for using DNN in this task is through representing context, generating a target representation, and then identifying the important sentiment words for the target. In polarity classification, although many deep learning techniques have been proposed, there has not yet been an attempt that uses the RecNN model . Similar to the task of OTE, the RNNs have Performance in opinion target extraction with SemEval 2016 dataset in restaurant domain. Shaded cells highlight the best models in the SemEval competition. demonstrated their competitive performance, in terms of capturing long-term dependency in sentences and general semantic classification. Furthermore, the best performers are the RNNs that incorporate attention or memory networks. This shows that with an attention weight aggregated from a lower level, the models can learn how to concentrate on different parts of the sentence to classify target and opinion words and the link between them."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 28,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "4.4.",
               "text": "Sentiment polarity of targeted text",
               "type": "relatedwork"
          },
          "paragraphs": [
               "ABSA tasks with CNN models but achieved lower outcomes than with the SVM approach. found that a purely window-based neural network produces outcomes that are comparable to an LSTM-RNN approach, and concluded that local context rather than long-term dependencies were important for aspect extraction. A study by for Arabic hotel reviews demonstrated that the SVM approach outperforms other deep RNN approaches for all ABSA tasks. All this illustrates that there are still significant challenges in terms of the application of DL methods to sentiment analysis in general and to ABSA in particular. shows various performance indicators of models based on a Twitter dataset by . Compared to the performance indicators reported in Section 4.3 , it is interesting to observe that the accuracy of this domain is lower than in the customer review domain, which is largely due to the characteristics of tweets -short, highly expressive, high use of sarcasms, and less grammatical correctness than review texts . It also shows that the CNN model has not yet been utilized, possibly because of its weaknesses in processing this type of data. Overall, the performance of RNN and RecNN are similar, with accuracy ranging from 69 to 72. While the CNN and RNN may work better in a grammatically correct context, overall this indicates that for the identification of sentiment polarity of targeted text, the tree structure and parser represent a promising approach."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 29,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "5.",
               "text": "Challenges",
               "type": "relatedwork"
          },
          "paragraphs": [
               "From the above discussion, it is clear that DL methods are still in their infancy. There are cases where the performance of DL methods is not as effective as expected. An example comes from Xu, Liu, who attempted to approach three One major challenge for ABSA is the current focus on consumer reviews, which raises the issue of domain adaptation, namely whether the trained parameters in one domain can be applied to another domain. It is apparent that the sentiment of a word can only be determined given its domain or context. For example, \"small\" contains a positive sentiment in the electronics domain in \"the phone is small and convenient\" but it has negative sentiment in a restaurant review when it states, \"the portion is small\". Given numerous domains, domain adaptation is clearly important to exploit the knowledge from one domain and increase the effectiveness of the analysis .",
               "With respect to ABSA tasks, it is clear from the above findings that while one method may perform well in one domain, there is no guarantee of similar performance others. For example, reported a much lower score in cross-domain performance compared to in-domain. Most noticeably, the performance of models varies significantly between domains. Evidence comes from the SemEval 2014 dataset, where performance in the restau- Performance of DNN models in aspect category detection tasks from rant domain is reported to be higher than in the laptop domain, for all studies and tasks ( .",
               "One possible explanation is that the prevalence of aspect phrases is higher within the laptop domain (i.e. 36.55% versus 24.56%), making it more difficult to predict than in the case of single-word aspects . Furthermore, consumer reviews in general are highly product-oriented, which means that most of the aspects or opinions are expressed with nouns or noun phrases, while in reality, aspects and opinion can be represented in different formats, and the co-existence of opinionated texts and non-opinionated texts is frequent . In unsupervised machine learning approaches, the issue can be resolved by incorporating a domain-specific lexicon , which could also be considered for DNN models. have attempted to overcome the issue through domain adaptation, simplified by domain similarity metrics to guide the selection of appropriate training data. Another way to overcome the domain adaption is to pre-train the word embeddings in a large similar corpora , which has shown promise as discussed in session 3.1.1. Similarly, suggested that the domain adaptation will be more effective if the word embeddings is created from an opinion-based corpus rather than a general purpose one (such as Wikipedia). They proposed NeuroSent, a tool for calculating the linguistic overlaps between different domains for conjecturing sentiment polarity."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 30,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "5.2.",
               "text": "Multilingual application",
               "type": "relatedwork"
          },
          "paragraphs": [
               "Lo, argued that multilingual sentiment analysis has certain challenges, including word sense ambiguity, language-specific structure and translation errors. have illustrated this in the case of Chinese, where each sub-word may encode semantics. Thus, the verb 'shine' contains 'sun' and 'moon' as sub-elements. This is radically different from English where only character N-grams (i.e. \"pre\", \"sub\") contain semantics. Therefore, it requires a higher effort to encode and decode the former type of language.",
               "Despite the fact that DNN models require less language-specific features ( , this review has highlighted that ABSA has not yet achieved its potential in a multilingual environment. The first issue stems from the fact that there are insufficient resources for many languages to construct NLP models. This is particularly the case for low-resource languages, which lack of large monolingual or parallel corpora such as Hindi or Tegulu. Is observed that word vectors in those type of languages obtain lower quality than others ( . It is also clear that there are yet no benchmark ABSA datasets on different languages. Apart from SemEval 2016, there is merely a small number of product review datasets in Chinese ( , and in Vietnamese ."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 31,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "text": "Table 14",
               "type": "relatedwork"
          },
          "paragraphs": [
               "Performance in opinion target extraction with SemEval 2014 dataset (restaurant and laptop domains). Cells in shading indicate the best model in SemEval competition."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 32,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "text": "Table 15",
               "type": "relatedwork"
          },
          "paragraphs": [
               "Performance on Twitter dataset by . It is, therefore, hardly surprising that there are few successful attempts at using DL methods on ABSA with different languages, with the exception of and for French, Spanish, Russian, Dutch and Turkish. From SemEval 2016, it is evident that the performance of models varies between languages, with higher scores recorded in English and Chinese but lower ones in French, Spanish, Dutch, and Russian ( . have suggested incorporation of different embeddings trained on a range of corpora in different languages. successfully incorporated radical, character and word embeddings into Chinese to overcome the issue of multiword aspect representation. Therefore, it is expected that training the models to associate with different surface forms could help to reduce the performance differences."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 33,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "5.3.",
               "text": "Technical requirements",
               "type": "relatedwork"
          },
          "paragraphs": [
               "Best performing ABSA systems generally use manually labelled data and language specific resources for training on a particular domain and language . Particularly the DL based systems require a significant amount of labelled data for training . For example, one major issue in Tang, is the failure to produce consistent results, possibly due to a small training corpus ( .",
               "Another issue is related to computational resources and time. Despite the improvement in technology that reduces computational time in training for DNN models, reported by and , when compared to conventional machine learning, time is still a current issue for DNN models. For example, reported an acceptable time span of four hours to process over 30 0 0 sentences."
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 34,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     },
     {
          "head": {
               "n": "5.4.",
               "text": "Linguistic complications",
               "type": "relatedwork"
          },
          "paragraphs": [
               "As discussed in , there are still challenges in terms of language that have not yet been satisfactorily addressed in current studies. They include the issue of comparative sentences, where it is difficult to detect whether any aspect is preferred. Similar problems exist with conditional sentences (irrealis phenomenon), where it is difficult to extract sentiment from an unknown/unreal situation. Also highly complex is analysis from sentences that contain negation and valence-shifting, where the polarity can be flipped, and sentiment value can be decreased or increased.",
               "It is also challenging to extract implicit aspects, which can only be read between the lines ( . The same text may be read differently in a different situation/ A classic example comes from \"go read the book\" expresses positive sentiments in the case of a book review, but implies a negative sentiment as a film review. One consideration is to undertake \"co-reference\", reflecting aspects with pronouns or synonymous phrases; however, not much research exists as yet .",
               "NLP methods also need to catch up with the evolution of usergenerated content, which is quite different from standard text. It is characterized by its \"noisiness\" from highly expressive tokens such as emoticons, flooding (repetition of some characters such as \"loooool\") as well as misspellings, grammatical errors, abbreviations and more use of sarcasm, irony, humour and metaphor, particularly for twitters . This makes it more difficult to train with tools that were originally trained from a standard text .",
               "The difficulties increase with regard to other languages, with Chinese with words that are ambiguous in terms of semantics and syntax , in Hindi and Arabic through the issue of multi-dialects and lately also for Arabizi -Arabic words with Latin characters . A promising approach suggested by is to evolve to the more concept-centric approach of the knowledge base. Recent works in SenticNet and SSWE suggest that the incorporation of the knowledge base and recent language evolution is promising.",
               "the relationship between aspect and opinion, improved performance can be obtained by joint extraction and classification of aspect, category and sentiment. However, many robust studies opt to perform only aspect extraction or categorization, and those who jointly perform aspect detection and sentiment analysis, have not yet achieved optimal performance. Therefore, there is the need for a combined approach that can undertake both tasks and create more pervasive sentiment analysis at aspect level. Research would further benefit from a more concept-centric approach to connect knowledge bases with deep learning methods. "
          ],
          "paper_id": "dd75a130-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 35,
          "fromPaper": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review"
     }
]