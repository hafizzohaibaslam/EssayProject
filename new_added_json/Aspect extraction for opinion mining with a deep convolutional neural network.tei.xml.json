[
     {
          "head": {
               "n": "0",
               "text": "abstract",
               "type": "abstract"
          },
          "paragraphs": [
               "In this paper, we present the first deep learning approach to aspect extraction in opinion mining. Aspect extraction is a subtask of sentiment analysis that consists in identifying opinion targets in opinionated text, i.e., in detecting the specific aspects of a product or service the opinion holder is either praising or complaining about. We used a 7-layer deep convolutional neural network to tag each word in opinionated sentences as either aspect or non-aspect word. We also developed a set of linguistic patterns for the same purpose and combined them with the neural network. The resulting ensemble classifier, coupled with a word-embedding model for sentiment analysis, allowed our approach to obtain significantly better accuracy than state-of-the-art methods."
          ],
          "paper_id": "dd5c25c0-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 0,
          "fromPaper": "Aspect extraction for opinion mining with a deep convolutional neural network"
     },
     {
          "head": {
               "n": "1.",
               "text": "Introduction",
               "type": "introduction"
          },
          "paragraphs": [
               "The opportunity to capture the opinion of the general public about social events, political movements, company strategies, marketing campaigns, and product preferences has raised increasing interest of both the scientific community (because of the exciting open challenges) and the business world (because of the remarkable benefits for marketing and financial market prediction). Today, sentiment analysis research has its applications in several different scenarios. There are a good number of companies, both large-and small-scale, that focus on the analysis of opinions and sentiments as part of their mission .",
               "Opinion mining techniques can be used for the creation and automated upkeep of review and opinion aggregation websites, in which opinions are continuously gathered from the Web and not restricted to just product reviews, but also to broader topics such as political issues and brand perception. Sentiment analysis also has a great potential as a sub-component technology for other systems. It can enhance the capabilities of customer relationship management and recommendation systems; for example, allowing users to find out which features customers are particularly interested in or to exclude items that have received overtly negative feedback from recommendation lists. Similarly, it can be used in social communication for troll filtering and to enhance anti-spam systems. Business intelligence is also one of the main factors behind corporate interest in the field of sentiment analysis .",
               "In opinion mining, different levels of analysis granularity have been proposed, each one having its own advantages and drawbacks . Aspect-based opinion mining focuses on the relations between aspects and document polarity. An aspect, also known as an opinion target, is a concept in which the opinion is expressed in the given document. For example, in the sentence, \"The screen of my phone is really nice and its resolution is superb\" for a phone review contains positive polarity, i.e., the author likes the phone. However, more specifically, the positive opinion is about its screen and resolution ; these concepts are thus called opinion targets, or aspects, of this opinion. The task of identifying the aspects in a given opinionated text is called aspect extraction.",
               "There are two types of aspects defined in aspect-based opinion mining: explicit aspects and implicit aspects. Explicit aspects are words in the opinionated document that explicitly denote the opinion target. For instance, in the above example, the opinion targets screen and resolution are explicitly mentioned in the text. In contrast, an implicit aspect is a concept that represents the opinion target of an opinionated document but which is not specified explicitly in the text. One can infer that the sentence, \"This camera is sleek and very affordable\" implicitly contains a positive opinion of the aspects appearance and price of the entity camera . These same aspects would be explicit in an equivalent sentence: \"The appearance of this camera is sleek and its price is very affordable.\" Most of the previous works in aspect term extraction have either used conditional random fields (CRFs) or linguistic patterns . Both of these approaches have their own limitations: CRF is a linear model, so it needs a large number of features to work well; linguistic patterns need to be crafted by hand, and they crucially depend on the grammatical accuracy of the sentences.",
               "In this paper, we overcome both limitations by using a convolutional neural network (CNN), a non-linear supervised classifier that can more easily fit the data. Previously, used such a network to solve a range of tasks (not for aspect extraction), on which it outperformed other state-of-the-art NLP methods. In addition, we use linguistic patterns to further improve the performance of the method, though in this case the above-mentioned issues inherent in linguistic patterns affect the framework. This paper is the first one to introduce the application of a deep learning approach to the task of aspect extraction. Our experimental results show that a deep CNN is more efficient for aspect extraction than existing approaches. We also introduced specific linguistic patterns and combined a linguistic pattern approach with a deep learning approach for the aspect extraction task. semi-supervised model, which allows the user to set must-link and cannot-link constraints. A must-link constraint means that two terms must be in the same topic, while a cannot-link constraint means that two terms cannot be in the same topic. Poria et al. integrated common-sense computing in the calculation of word distributions in the LDA algorithm, thus enabling the shift from syntax to semantics in aspect-based sentiment analysis. proposed two semi-supervised models for product aspect extraction based on the use of seeding aspects. In the category of supervised methods, employed seed words to guide topic models to learn topics of specific interest to a user, while and employed seeding words to extract related product aspects from product reviews.",
               "On the other hand, recent approaches using deep CNNs showed significant performance improvement over the stateof-the-art methods on a range of natural language processing (NLP) tasks. Collobert et al. fed word embeddings into a CNN to solve standard NLP problems such as named entity recognition (NER), part-of-speech (POS) tagging and semantic role labeling."
          ],
          "paper_id": "dd5c25c0-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 1,
          "fromPaper": "Aspect extraction for opinion mining with a deep convolutional neural network"
     },
     {
          "head": {
               "n": "2.",
               "text": "Related work",
               "type": "relatedwork"
          },
          "paragraphs": [
               "Aspect extraction from opinions was first studied by Hu and Liu . They introduced the distinction between explicit and implicit aspects. However, the authors only dealt with explicit aspects and used a set of rules based on statistical observations. Hu and Liu's method was later improved by Popescu and Etzioni and by Blair-Goldensohn et al. . Popescu and Etzioni assumed the product class is known in advance. Their algorithm detects whether a noun or noun phrase is a product feature by computing the point-wise mutual information between the noun phrase and the product class.",
               "Scaffidi et al. presented a method that uses language model to identify product features. They assumed that product features are more frequent in product reviews than in a general natural language text. However, their method seems to have low precision since retrieved aspects are affected by noise. Some methods treated the aspect term extraction as sequence labeling and used CRF for that. Such methods have performed very well on the datasets even in cross-domain experiments .",
               "Topic modeling has been widely used as a basis to perform extraction and grouping of aspects . Two models were considered: pLSA and LDA . Both models introduce a latent variable \"topic\" between the observable variables \"document\" and \"word\" to analyze the semantic topic distribution of documents. In topic models, each document is represented as a random mixture over latent topics, where each topic is characterized by a distribution over words. Such methods have been gaining popularity in social media analysis like emerging political topic detection in Twitter . The LDA model defines a Dirichlet probabilistic generative process for document-topic distribution; in each document, a latent aspect is chosen according to a multinomial distribution, A deep neural network (DNN) can be viewed as a composite of simple, unsupervised models such as restricted Boltzmann machines (RBMs), where each RBM's hidden layer serves as the visible layer for the next RBM. An RBM is a bipartite graph comprising of two layers of neurons: a visible and a hidden layer; connections between neurons in the same layer are not allowed.",
               "To train such a multi-layer system, one needs to compute the gradient of the total energy function E with respect to weights in all the layers. To learn these weights and maximize the global energy function, the approximate maximum likelihood contrastive divergence approach can be used. This method employs each training sample to initialize the visible layer. Next, it uses the Gibbs sampling algorithm to update the hidden layer and then reconstruct the visible layer consecutively, until convergence occurs . As an example, consider a logistic regression model to learn the binary hidden neurons. Each visible neuron is assumed to be a sample from a normal distribution . The continuous stath j of the hidden neuron j , with bias b j , is a weighted sum over all continuous visible neurons v :",
               "where w ij is the weight of connection from the visible neuron v i to the hidden neuron j . The binary state h j of the hidden neuron can be defined by a sigmoid activation function:",
               "controlled by a Dirichlet prior. Then, given an aspect, a word is extracted according to another multinomial distribution, conSimilarly, at the next iteration, the continuous state of each visible neuron v i is reconstructed. Here, we determine the state of the visible neuron i , with bias c i , as a random sample from the normal distribution where the mean is a weighted sum over all binary hidden neurons:",
               "trolled by another Dirichlet prior. Among existing works em-",
               "ploying these models are the extraction of global aspects ( such as the brand of a product) and local aspects (such as the property of a product ), the extraction of key phrases , the rating of multi-aspects , and the summarization of aspects and sentiments . employed the maximum entropy method to train a switch variable based on POS tags of words and used it to separate aspect and sentiment words. Mcauliffe and Blei added user feedback to LDA as a response-variable related to each document. Lu and Zhai proposed a semi-supervised model. DF-LDA also represents a",
               "where w ij is the weight of connection from the visible neuron i to the hidden one j . This continuous state is a random sample from a normal distribution N (v i ,) , whereis the variance of all visible neurons. Unlike hidden neurons, in a Gaussian RBM the visible ones can take continuous values.",
               "Then, the weights are updated as the difference between the original data v data and reconstructed visible layer v recon :",
               "whereis the learning rate and v i h j is the expected frequency with which the visible neuron i and the hidden neuron j are active together, when the visible vectors are sampled from the training set and the hidden neurons are calculated according to (1) -(3) , after some k iterations.",
               "Finally, the energy of a DNN can be determined from the final layer (the one before the output layer) as: time the score for all paths that end in a given tag . Let y k t denote all paths that end with the tag k at the token t . Then, using recursion, we obtain",
               "For the sake of brevity, we shall not delve into details of the recursive procedure, which can be found in . The next equation gives the log-add for all the paths to the token T :",
               "To extend the deep neural network to a deep CNN, one simply partitions the hidden layer into Z groups. Each of the Z groups is associated with an n xn y filter, where n x is the height of the kernel and n y is the width of the kernel. Assume that the input has dimensions L xL y , which in our case is given by L x words in the sentence and L y features, such as word embedding, of each word. Then the convolution will result in a hidden layer of Z groups, each p,y i Using these equations, we can maximize the likelihood of (11) over all training pairs. For inference, we need to find the best tag path using the Viterbi algorithm; e.g., we need to find the best tag path that minimizes the sentence score (10) ."
          ],
          "paper_id": "dd5c25c0-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 2,
          "fromPaper": "Aspect extraction for opinion mining with a deep convolutional neural network"
     },
     {
          "head": {
               "n": "5.",
               "text": "Our network architecture",
               "type": "relatedwork"
          },
          "paragraphs": [
               "The learned weights of these kernels are shared among all hidden neurons in a particular group. The energy function of the layer l is now a sum over the energy of individual blocks:",
               "r,s"
          ],
          "paper_id": "dd5c25c0-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 3,
          "fromPaper": "Aspect extraction for opinion mining with a deep convolutional neural network"
     },
     {
          "head": {
               "n": "4.",
               "text": "Training CNN for sequential data",
               "type": "relatedwork"
          },
          "paragraphs": [
               "We used a special training algorithm suitable for sequential data, proposed by Collobert et al. . We will summarize it here, mainly following .",
               "The algorithm trains the neural network by back-propagation in order to maximize the likelihood over training sentences. Consider the network parameter. We say that h y is the output score for the likelihood of an input x to have the tag y . Then, the probability to assign the label y to x is calculated as",
               "j exp (h j )",
               "Define the logadd operation as logadd",
               "i i then for a training example, the log-likelihood becomes",
               "i",
               "In aspect term extraction, the terms can be organized as chunks and are also often surrounded by opinion terms. Hence, it is important to consider sentence structure on a whole in order to obtain additional clues. Let it be given that there are T tokens in a sentence and y is the tag sequence while h t, i is the network score for the t -th tag having i -th tag. We introduce A i, j transition score from moving tag i to tag j . Then, the score tag for the sentence s to have the tag path y is defined by",
               "The features of an aspect term depend on its surrounding words. Thus, we used a window of 5 words around each word in a sentence, i.e.,2 words. We formed the local features of that window and considered them to be features of the middle word. Then, the feature vector was fed to a CNN.",
               "The network contained one input layer, two convolution layers, two max-pool layers, and a fully connected layer with softmax output. The first convolution layer consisted of 100 feature maps with filter size 2. The second convolution layer had 50 feature maps with filter size 3. The stride in each convolution layer is 1 as we wanted to tag each word. A max-pooling layer followed each convolution layer. The pool size we use in the max-pool layers was 2. We used regularization with dropout on the penultimate layer with a constraint on L2-norms of the weight vectors, with 30 epochs. The output of each convolution layer was computed using a non-linear function; in our case we used the hyperbolic tangent.",
               "As features, we used word embeddings trained on two different corpora. We also used some additional features and rules to boost the accuracy; see Section 7 . The CNN produces local features around each word in a sentence and then combines these features into a global feature vector. Since the kernel size for the two convolution layers was different, the dimensionality L xL y mentioned in Section 3 was 3300 and 2300, respectively. The input layer was 65300, where 65 was the maximum number of words in a sentence, and 300 the dimensionality of the word embeddings used, per each word.",
               "The process was performed for each word in a sentence. Unlike traditional max-likelihood leaning scheme, we trained the system using propagation after convolving all tokens in the sentence. Namely, we stored the weights, biases, and features for each token after convolution and only back-propagated the error in order to correct them once all tokens were processed using the training scheme from in Section 4 .",
               "If a training instance s had n words, then we represented the input vector for that instance as s "
          ],
          "paper_id": "dd5c25c0-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 4,
          "fromPaper": "Aspect extraction for opinion mining with a deep convolutional neural network"
     },
     {
          "head": {
               "n": "6.",
               "text": "Datasets used",
               "type": "experiment"
          },
          "paragraphs": [
               "This formula represents the tag path probability over all possible paths. Now, from (8) we can write the log-likelihood",
               "In this section, we present the data used in our experiments.",
               "p, j"
          ],
          "paper_id": "dd5c25c0-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 5,
          "fromPaper": "Aspect extraction for opinion mining with a deep convolutional neural network"
     },
     {
          "head": {
               "n": "6.1.",
               "text": "Word embeddings",
               "type": "experiment"
          },
          "paragraphs": [
               "The number of tag paths has exponential growth. However, using dynamic programming techniques, one can compute in polynomial Word embeddings are distributed representations of text, which encode semantic and syntactic properties of words. Usually they Characteristics of the dataset developed by Qiu et al. and comparison of our approach with the state of the art on it. Popescu stands for and Prof-dep for ; P stands for precision and R for recall. "
          ],
          "paper_id": "dd5c25c0-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 6,
          "fromPaper": "Aspect extraction for opinion mining with a deep convolutional neural network"
     },
     {
          "head": {
               "n": "7.",
               "text": "Features and rules used",
               "type": "experiment"
          },
          "paragraphs": [
               "are dense, low-dimensional vectors. In this section, we describe two word embedding datasets that we used in our experiments.",
               "Here we present the features, the representation of the text, and linguistic rules used in our experiments."
          ],
          "paper_id": "dd5c25c0-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 7,
          "fromPaper": "Aspect extraction for opinion mining with a deep convolutional neural network"
     },
     {
          "head": {
               "n": "6.1.1.",
               "text": "Google embeddings",
               "type": "experiment"
          },
          "paragraphs": [
               "Mikolov et al. presented two different neural network models for creating word embeddings. The models were log-linear in nature, trained on large corpora. One of them is a bag-of-words based model called CBOW; it uses word context in order to obtain the word embeddings. The other one is called skip-gram model; it predicts the word embeddings of surrounding words given the cur-"
          ],
          "paper_id": "dd5c25c0-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 8,
          "fromPaper": "Aspect extraction for opinion mining with a deep convolutional neural network"
     },
     {
          "head": {
               "n": "7.1.",
               "text": "Features",
               "type": "experiment"
          },
          "paragraphs": [
               "We used the following the features: rent word. Those authors made a dataset called word2vec publicly available. These 300-dimensional vectors were trained on a 100-billion-word corpus from Google News using the CBOW architecture."
          ],
          "paper_id": "dd5c25c0-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 9,
          "fromPaper": "Aspect extraction for opinion mining with a deep convolutional neural network"
     },
     {
          "head": {
               "n": "6.1.2.",
               "text": "Our Amazon embeddings",
               "type": "experiment"
          },
          "paragraphs": [
               "We trained the CBOW architecture proposed by Mikolov et al. on a large Amazon product review dataset developed by . This dataset consists of 34,686,770 reviews (4.7 billion words) of 2,441,053 Amazon products from June 1995 to March 2013. We kept the word embeddings 300-dimensional. The model is available at http://sentic.net/ AmazonWE.zip . Due to the nature of the text used to train this model, this includes opinionated/affective information, which is not present in ordinary texts such as the Google News corpus.",
               "? Word embeddings We used the word embeddings described in Section 6.1 as features for the network. This way, each word was encoded as 300-dimensional vector, which was fed to the network."
          ],
          "paper_id": "dd5c25c0-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 10,
          "fromPaper": "Aspect extraction for opinion mining with a deep convolutional neural network"
     },
     {
          "head": {
               "text": "? Part of speech tags",
               "type": "experiment"
          },
          "paragraphs": [
               "Most of the aspect terms are either nouns or noun chunk. This justifies the importance of POS features. We used the POS tag of the word as its additional feature. We used 6 basic parts of speech (noun, verb, adjective, adverb, preposition, conjunction) encoded as a 6-dimensional binary vector. We used Stanford Tagger as a POS tagger. These two features vectors were concatenated and fed to CNN. So, for each word the final feature vector is 306 dimensional."
          ],
          "paper_id": "dd5c25c0-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 11,
          "fromPaper": "Aspect extraction for opinion mining with a deep convolutional neural network"
     },
     {
          "head": {
               "n": "6.2.",
               "text": "Evaluation corpora",
               "type": "experiment"
          },
          "paragraphs": [
               "In some of our experiments, we used a set of linguistic patterns (LPs) that leverage on SenticNet and its extensions , a concept-level knowledge base for sentiment analysis built by means of sentic computing . The five LPs used are listed below.",
               "For training and evaluation of the proposed approach, we used two corpora:",
               "? Aspect-based sentiment analysis dataset developed by Qiu et al. ; see , and ? SemEval 2014 dataset. The dataset consists of training and test sets from two domains, Laptop and Restaurant; see .",
               "Rule 1 Let a noun h be a subject of a word t , which has an adverbial or adjective modifier present in a large sentiment lexicon, SenticNet. Then mark h as an aspect. Rule 2 Except when the sentence has an auxiliary verb, such as is, was, would, should, could , etc., we apply:",
               "The annotations in both corpora were encoded according to IOB2, a widely used coding scheme for representing sequences. In this encoding, the first word of each chunk starts with a \"B-Type\" tag, \"I-Type\" is the continuation of the chunk and \"O\" is used to tag a word which is out of the chunk. In our case, we are interested to determine whether a word or chunk is an aspect, so we only have \"B-A\", \"I-A\" and \"O\" tags for the words. Here is an example of IOB2 tags: Rule 2.1 If the verb t is modified by an adjective or adverb or is in adverbial clause modifier relation with another token, then mark h as an aspect. E.g., in \"The battery lasts little\", battery is the subject of lasts , which is modified by an adjective modifier little , so battery is marked as an aspect. Rule 2.2 If t has a direct object, a noun n , not found in SenticNet, then mark n an aspect, as, e.g., in \"I like the lens of this camera\".",
               "Rule 3 If a noun h is a complement of a couplar verb, then mark h as an explicit aspect. E.g., in \"The camera is nice\", camera is marked as an aspect. Random features vs. Google embeddings vs. Amazon embeddings on the SemEval 2014 dataset.  Feature analysis for the CNN classifier."
          ],
          "paper_id": "dd5c25c0-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 12,
          "fromPaper": "Aspect extraction for opinion mining with a deep convolutional neural network"
     },
     {
          "head": {
               "text": "Table 6",
               "type": "experiment"
          },
          "paragraphs": [
               "Comparison with the state of the art. ZW stands for ; LP stands for linguistic patterns. Rule 4 If a term marked as an aspect by the CNN or the other rules is in a noun-noun compound relationship with another word, then instead form one aspect term composed of both of them. E.g., if in \"battery life\", \"battery\" or \"life\" is marked as an aspect, then the whole expression is marked as an aspect. Rule 5 The above rules 1-4 improve recall by discovering more aspect terms. However, to improve precision, we apply some heuristics: e.g., we remove stop-words such as of, the, a , etc., even if they were marked as aspect terms by the CNN or the other rules. We used the Stanford parser to determine syntactic relations in the sentences."
          ],
          "paper_id": "dd5c25c0-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 13,
          "fromPaper": "Aspect extraction for opinion mining with a deep convolutional neural network"
     },
     {
          "head": {
               "text": "Domain",
               "type": "experiment"
          },
          "paragraphs": [
               "We combined the LPs with the CNN as follows: both LPs and CNN-based classifier are run on the text; then all terms marked by any of the two classifiers are reported as aspect terms, except for those unmarked by the last rule. shows that our approach outperforms the state-of-theart methods by Popescu and Etzioni and Dependency Based Propagation by 5%-10%, respectively. The paired t -tests show that all our improvements were statistically significant at the confidence level of 95%. shows the accuracy of our aspect term extraction framework in laptop and restaurant domains. The framework gave better accuracy on restaurant domain reviews, because of the lower variety of aspect available terms than in laptop domain. However, in both cases recall was lower than precision. shows improvement in terms of both precision and recall when the POS feature is used."
          ],
          "paper_id": "dd5c25c0-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 14,
          "fromPaper": "Aspect extraction for opinion mining with a deep convolutional neural network"
     },
     {
          "head": {
               "n": "8.",
               "text": "Experimental results",
               "type": "experiment"
          },
          "paragraphs": [
               "Pre-trained word embeddings performed better than randomized features (each word's vector initialized randomly); see .",
               "Amazon embeddings performed better than Google word2vec",
               "embeddings. This supports our claim that the former contains opinion-specific information, which helped it to outperform the accuracy of Google embeddings trained on more formal texts-the Google news corpus.",
               "Because of this, in the sequel we only show the performance using Amazon embeddings, which we denote simply as WE (word embeddings).",
               "In both domains, CNN suffered from low recall, i.e., it missed some valid aspect terms. Linguistic analysis of the syntactic structure of the sentences substantially helped to overcome some drawbacks of machine learning-based analysis. Our experiments showed good improvement in both precision and recall when the linguistic patterns ( Section 7.2 ) were used together with CNN; see .",
               "As to the linguistic patterns, the removal of stop-words, Rule 1, and Rule 3 were most beneficial. shows a visualization for . and shows the comparison between the proposed method and the state of the art on the SemEval dataset.",
               "One can see that about 36.55% aspect terms present in the laptop domain corpus are phrase and restaurant corpus consists of 24.56% aspect terms. The performance of detecting aspect phrases are lower than single word aspect tokens in both domains. This shows that the sequential tagging is indeed a tough task to do. Lack of sufficient training data for aspect phrases is also one of the reasons to get lower accuracy in this case. In particular, we got 79.20% and 83.55% F-score to detect aspect phrases in laptop and restaurant domain respectively. We observed some cases where only 1 term in an aspect phrase is detected as aspect term. In those cases Rule 4 of the linguistic patterns helped to correctly detect the aspect phrases. We also carried out experiments on the aspect dataset originally developed by . This is to date the largest comprehensive aspect-based sentiment analysis dataset. , left part, shows the details of this dataset.",
               "The best accuracy on this dataset was obtained when word embedding features were used together with the POS features. This shows that while the word embedding features are most useful, the POS feature also plays a major role in aspect extraction ( .",
               "As on the SemEval dataset, linguistic patterns together with CNN increased the overall accuracy. However, linguistic patterns have performed much better on this dataset than on the SemEval dataset. This supports the observation made previously that on this dataset linguistic patterns are more useful. One of the  compares the proposed method with the state of the art. We believe that there are two key reasons for our framework to outperform state-of-the-art approaches. First, a deep CNN, which is non-linear in nature, better fits the data than linear models such as CRF. Second, the pre-trained word embedding features help our framework to outperform state-of-the-art methods that do not use word embeddings. The main advantage of our framework is that it does not need any feature engineering. This minimizes development cost and time."
          ],
          "paper_id": "dd5c25c0-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 15,
          "fromPaper": "Aspect extraction for opinion mining with a deep convolutional neural network"
     },
     {
          "head": {
               "n": "9.",
               "text": "Conclusion",
               "type": "conclusion"
          },
          "paragraphs": [
               "possible reasons for this is that most of the sentences in this dataset are grammatically correct and contain only one aspect term. Here we combined the linguistic patterns and a CNN to achieve even better results than the approach of by Qiu et al. based only on linguistic patterns. Our experimental results showed that this ensemble algorithm (CNN+LP) can better understand the semantics of the text than 's pure LP-based algorithm, and thus extracts more salient aspect terms. and shows the performance and comparisons of different frameworks.",
               "We have introduced the first deep learning-based approach to aspect extraction. As expected, this approach gave a significant improvement in performance over state-of-the-art approaches. We proposed a specific deep CNN architecture that comprises seven layers: the input layer, consisting of word embedding features for each word in the sentence; two convolution layers, each followed by a max-pooling layer; a fully connected layer; and, finally, the output layer, which contained one neuron per each word.",
               "We also developed a set of heuristic linguistic patterns and integrated them with the deep learning classifier. In the future, we plan to extend and refine these patterns. "
          ],
          "paper_id": "dd5c25c0-9018-11ea-9ac1-1bc38575703c",
          "paragraphNo": 16,
          "fromPaper": "Aspect extraction for opinion mining with a deep convolutional neural network"
     }
]