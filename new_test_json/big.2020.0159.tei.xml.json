[
     {
          "head": {
               "n": "0",
               "text": "abstract",
               "type": "abstract"
          },
          "paragraphs": [
               "Time series forecasting has become a very intensive field of research, which is even increasing in recent years. Deep neural networks have proved to be powerful and are achieving high accuracy in many application fields. For these reasons, they are one of the most widely used methods of machine learning to solve problems dealing with big data nowadays. In this work, the time series forecasting problem is initially formulated along with its mathematical fundamentals. Then, the most common deep learning architectures that are currently being successfully applied to predict time series are described, highlighting their advantages and limitations. Particular attention is given to feed forward networks, recurrent neural networks (including Elman, long-short term memory, gated recurrent units, and bidirectional networks), and convolutional neural networks. Practical aspects, such as the setting of values for hyper-parameters and the choice of the most suitable frameworks, for the successful application of deep learning to time series are also provided and discussed. Several fruitful research fields in which the architectures analyzed have obtained a good performance are reviewed. As a result, research gaps have been identified in the literature for several domains of application, thus expecting to inspire new and better forms of knowledge."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "paragraphNo": 0,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "head": {
               "text": "Introduction",
               "type": "introduction"
          },
          "paragraphs": [
               "The interest in processing huge amounts of data has experienced a rapid increase during the past decade due to the massive deployment of smart sensors or the social media platforms, 2 which generate data on a continuous basis.",
               "3 However, this situation poses new challenges, such as storing these data in disks or making available the required computational resources.",
               "Big data analytics emerges, in this context, as an essential process focused on efficiently collecting, organizing, and analyzing big data with the aim of discovering patterns and extracting valuable information. In most organizations, this helps to identify new opportunities and making smarter moves, which leads to more efficient operations and higher profits.",
               "tions in deep learning can be done in parallel by, for instance, powerful graphic processing units (GPUs). That way, scalable distributed models are easier to be built and they provide better accuracy at a much higher speed. Higher depth allows for more complex non-linear functions but, in turn, with higher computational costs.",
               "Downloaded by Auckland University of from www.liebertpub.com at 04/27/21. For personal use only."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "paragraphNo": 1,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "paragraphs": [
               "From all the learning paradigms that are currently being used in big data, deep learning highlights because of its outstanding performance as the scale of data increases. Most of the layer computaDeep learning can be applied to numerous research fields. Applications to both supervised and unsupervised problems can be abundantly found in the literature.",
               "8 Pattern recognition and classification were the first and most relevant uses of deep learning, achieving great success in speech recognition, text mining, or image analysis. Nevertheless, the application to regression problems is becoming quite popular nowadays mainly due to the development of deep-learning architectures particularly conceived to deal with data indexed over time. Such is the case of time series and, more specifically, time series forecasting."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "head": {
               "type": "introduction"
          },
          "paragraphNo": 2,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "head": {
               "n": "4",
               "text": "TORRES ET AL",
               "type": "introduction"
          },
          "paragraphs": [
               "A time series is a set of measures collected at even intervals of time and ordered chronologically. Given this definition, it is hard to find physical or chemical phenomena without variables that evolve over time. For this reason, the proposal of time series forecasting approaches is fruitful and can be found in almost all scientific disciplines.",
               "Statistical approaches have been used from the 1970s onward, especially those based on the Box-Jenkins methodology.",
               "11 With the appearance of machine learning and its powerful regression methods, 12 many models were proposed as outperforming the former, which have remained as baseline methods in most research works. However, methods based on deep learning are currently achieving superior results and much effort is being put into developing new architecture.",
               "For all that has been mentioned earlier, the primary motivation behind this survey is to provide a comprehensive understanding of deep-learning fundamentals for researchers interested in the field of time series forecasting. Further, it overviews several applications in which these techniques have been proven successful and, as a result, research gaps have been identified in the literature and are expected to inspire new and better forms of knowledge.",
               "Although other surveys discussing deep-learning properties have been published during the past years, the majority of them provided a general overview of both theory and applications to time series forecasting. Thus, Zhang et al. reviewed emerging researches of deeplearning models, including their mathematical formulation, for big data feature learning. Another remarkable work can be found in Ref., in which the authors introduced the time series classification problem and provided an open-source framework with implemented algorithms and the University of East Anglia/University of California in Riverside repository. Recently, Mayer and Jacobsen published a survey about scalable deep learning on distributed infrastructures, in which the focus was placed on techniques and tools, along with a smart discussion about the existing challenges in this field. plying deep learning to forecast time series. Applications section overviews the most relevant papers, sorted by fields, in which deep learning has been applied to forecast time series. Finally, the lessons learned and the conclusions drawn are discussed in the Conclusions section."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "paragraphNo": 3,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "head": {
               "text": "Problem Definition",
               "type": "modelling"
          },
          "paragraphs": [
               "This section provides the time series definition (Time Series Definition section), along with a description of the main time series components (Time Series Components section). The mathematical formulation for the time series forecasting problem is introduced in the Mathematical Formulation section. Final remarks about the length of the time series can be found in Shortand Long-Time Series Forecasting section.",
               "Time series definition A time series is defined as a sequence of values, chronologically ordered, and observed over time. Although the time is a variable measured on a continuous basis, the values in a time series are sampled at constant intervals (fixed sampling frequency).",
               "This definition holds true for many applications, but not every time series can be modeled in this way, due to some of the following reasons:",
               "1. Missing data in time series is a very common problem due to the reliability of data collection.",
               "To deal with these values, there are a lot of strategies but those based on imputing the missing information and on omitting the entire record are the most widely used. 2. Outlying data is also an issue that appears very frequently in time series. Methods based on robust statistics must be chosen to remove these values or, simply, to incorporate them into the model.",
               "The rest of the article is structured as follows. The forecasting problem and mathematical formulation for time series can be found in the Problem Definition section. Deep-Learning Architectures section introduces the deep-learning architectures typically used in the context of time series forecasting. Practical Aspects section provides information about several practical aspects (including implementation, hyper-parameter tuning, or hardware resources) that must be considered when apSome of these issues can be handled natively by the used model, but if the data are collected irregularly, this should be accounted for in the model. In this survey, the time series preprocessing is out of scope, but please refer to this work for detailed information."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "paragraphNo": 4,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "paragraphs": [
               "Time series components Time series are usually characterized by three components: trend, seasonality, and irregular components, also known as residuals.",
               "20 Such components are described later:",
               "1. Trend. It is the general movement that the time series exhibits during the observation period, without considering seasonality and irregularities. In some texts, this component is also known as long-term variation. Although there are different kinds of trends in time series, the most popular are linear, exponential, or parabolic ones. 2. Seasonality. This component identifies variations that occur at specific regular intervals and may provide useful information when time periods exhibit similar patterns. It integrates the effects reasonably stable along with the time, magnitude, and direction. Seasonality can be caused by several factors such as climate or economical cycles, or even festivities."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "head": {
               "type": "modelling"
          },
          "paragraphNo": 5,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "head": {
               "n": "3.",
               "text": "Residuals. Once the trend and cyclic oscillations",
               "type": "modelling"
          },
          "paragraphs": [
               "have been calculated and removed, some residual values remain. These values can be, sometimes, high enough to mask the trend and the seasonality. In this case, the term outlier is used to refer these residuals, and robust statistics are usually applied to cope with them. These fluctuations can be of diverse origin, which makes the prediction almost impossible. However, if by any chance, this origin can be detected or modeled, they can be thought of precursors in trend changes.",
               "Time series can be graphically represented. In particular, the x-axis identifies the time, whereas the y-axis identifies the values recorded at punctual time stamps (x t ). This representation allows the visual detection of the most highlighting features of a series, such as oscillations amplitude, existing seasons, and cycles or the existence of anomalous data or outliers. depicts an time series, x t , using an additive model with linear seasonality with constant frequency and amplitude over time, represented by the function sin(x); linear trend where changes over time are consistently made by the same amount, represented by the function 0:0213x; and residuals, represented by random numbers in the interval [0, 0:1].",
               "A time series is an aggregate of these three components. Real-world time series present a meaningful irregular component and are not stationary (mean and variance are not constant over time), turning this component into the most challenging one to model. For this reason, to make accurate predictions for them is extremely difficult, and many forecasting classical methods try to decompose the target time series into these three components and make predictions for all of them separately.",
               "The effectiveness of one technique or another is assessed according to its capability of forecasting this"
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "paragraphNo": 6,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "head": {
               "text": "Mathematical formulation",
               "type": "modelling"
          },
          "paragraphs": [
               "Time series models can be either univariate (one timedependent variable) or multivariate (more than one time-dependent variables). Although models may dramatically differ between a univariate and a multivariate system, the majority of the deep-learning models can handle indistinctly with both of them.",
               "On the one hand, let y = y(t ? L), . . . , y(t ? 1), y(t), y(t ? 1), . . . , y(t ? h) be a given univariate time series with L values in the historical data, where each y(t ? i), for i = 0, . . . , L, represents the recorded value of the variable y at time t ? i. The forecasting process consists of estimating the value of y(t ? 1), denoted by ^ y(t ? 1), with the aim of minimizing the error, which is typically represented as a function of y(t ? 1) ? ^ y(t ? 1). This prediction can be made also when the horizon of prediction, h, is greater than one, that is, when the objective is to predict the h next values after y(t), that is, y(t ? i), with i = 1, . . . , h. In this situation, the best prediction is reached when the function + Downloaded by Auckland University of from www.liebertpub.com at 04/27/21. For personal use only.",
               "On the other hand, multivariate time series can be expressed as follows, in the matrix form:",
               ". . . where y i (t ? m) identifies the set of time series, with i = f1, 2, . . . , ng, being m = f0, 1, . . . , Lg the historical data and current sample and m = f ? 1, ? 2, . . . , ? hg the future h values. Usually, there is one target time series (the one to be predicted) and the remaining ones are denoted as independent time series."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "paragraphNo": 7,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "head": {
               "text": "Short-and long time series forecasting",
               "type": "modelling"
          },
          "paragraphs": [
               "Another key issue is the length of the time series. Depending on the number of samples, long-or short time series can be defined. It is well known that the Box-Jenkins' models do not work well for long time series mainly due to the time-consuming process of parameters optimization and to the inclusion of information, which is no longer useful to model the current samples.",
               "been published in recent years. These models make use of clusters of machines or GPUs to overcome the limitations described in the previous paragraphs.",
               "Deep-learning models can deal with time series in a scalable way and provide accurate forecasts. Ensemble learning can also be useful to forecast big data time series or even methods based on well-established methods such as nearest neighbours or pattern sequence similarity. How to deal with these issues is highly related to the purpose of the model. Flexible nonparametric models could be used, but this still assumes that the model structure will work over the whole period of the data, which is not always true. A better approach consists of allowing the model to vary over time. This can be done by either adjusting a parametric model with time-varying parameters or adjusting a nonparametric model with a time-based kernel. But if the goal is only to forecast a few observations, it is simpler to fit a model with the most recent samples and transforming the long time series into a short one."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "paragraphNo": 8,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "head": {
               "text": "Deep-Learning Architectures",
               "type": "modelling"
          },
          "paragraphs": [
               "This section provides a theoretical tour of deep learning for time series prediction in big data environments. First, a description of the most used architectures in the literature to predict time series is made. Then, a state-of-the-art analysis is carried out, where the deep-learning works and frameworks to deal with big data are described."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "paragraphNo": 9,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "paragraphs": [
               "Although a preliminary approach to use a distributed ARIMA model has been recently published, it remains challenging to deal with such time series with classical forecasting methods. However, a number of machine-learning algorithms adapted to deal with ultra-long time series, or big data time series, have Deep feed forward neural network Deep feed forward neural networks (DFFNN), also called multi-layer perceptron, arose due to the inability of single-layer neural networks to learn certain functions. The architecture of a DFFNN is composed of an input layer, an output layer, and different hidden layers, as shown in . In addition, each hidden layer has a certain number of neurons to be determined.",
               "The relationships between the neurons of two consecutive layers are modeled by weights, which are calculated during the training phase of the network. In TORRES ET AL particular, the weights are computed by minimizing a cost function by means of gradient descent optimization methods. Then, the back-propagation algorithm is used to calculate the gradient of the cost function. Once the weights are computed, the values of the output neurons of the network are obtained by using a feed-forward process defined by the following equation:",
               "where  In time series forecasting, the rectified linear unit function is commonly used as activation function for all layers, except for the output layer to obtain the predicted values, which generally uses the hyperbolic tangent function (tanh).",
               "For all network architectures, the values of some hyper-parameters have to be chosen in advance. These hyper-parameters, such as the number of layers and the number of neurons, define the network architecture, and other hyper-parameters, such as the learning rate, the momentum, and number of iterations or minibatch size, among others, have a great influence on the convergence of the gradient descend methods. The optimal choice of these hyper-parameters is important, as these values greatly influence the prediction results obtained by the network. The hyper-parameters will be discussed in more detail in the Hyper-Parameter Optimization section. and one output), and many to many (many inputs and outputs). The most common RNNs are many to one for classification problems or many to many for machine translation or time series forecasting for instance. In addition, for the case of a time series, the length of the input data sequence is usually different from the size of the output data sequence that usually is the number of samples to be predicted. A basic RNN architecture to address the forecasting of time series is shown in . x i and ^ x i are the actual and predicted values of the time series at time i, and h is the number of samples to be predicted, called prediction horizon.",
               "The most widely used RNNs for time series forecasting are briefly described later.",
               "Downloaded by Auckland University of from www.liebertpub.com at 04/27/21. For personal use only."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "head": {
               "type": "modelling"
          },
          "paragraphNo": 10,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "head": {
               "text": "Recurrent neural network",
               "type": "modelling"
          },
          "paragraphs": [
               "Recurrent neural networks (RNNs) are specifically designed to deal with sequential data such as sequences of words in problems related to machine translation, audio data in speech recognition, or time series in forecasting problems. All these problems present a common characteristic, which is that the data have a temporal dependency between them. Traditional feed-forward neural networks cannot take into account these dependencies, and RNNs arise precisely to address this problem.",
               "Elman RNN. The Elman network (ENN) was the first RNN and it incorporated the t state of a hidden unit to make predictions in data sequences. The ENN consists of a classical one-layer feed-forward network but the hidden layer is connected to a new layer, called context layer, using fixed weights equal to one, as shown in . The main function of the neurons of this Therefore, the input data in the architecture of a RNN are both past and current data. There are different types of architectures, depending on the number of data inputs and outputs in the network, such as one to one (one input and one output), one to many (one input and many outputs), many to one (many inputs context layer is to save a copy of the values of activation of the neurons of the hidden layer. Then, the model is defined by:",
               "where a t are the values of the neurons in the t state in the hidden layer, x t is the current input, a t ? 1 is the information saved in the context hidden units, W a , U a , and b a are the weights and the bias, and g is the activation function.",
               "where W u , W f , and W o , and b u , b f , and b o are the weights and biases that govern the behavior of the G u , G f , and G o gates, respectively, and W c and b c are the weights and bias of the ~ c t memory cell candidate. shows a picture of how a hidden unit works in an LSTM recurrent network. Theand + operators mean an element-wise vectors multiplication and sum.",
               "Long short-term memory. Standard basic RNNs suffer the vanishing gradient problem, which consists of the gradient decreasing as the number of layers increases. Indeed, for deep RNNs with a high number of layers, the gradient practically becomes null, preventing the learning of the network. For this reason, these networks have a short-term memory and do not obtain good results when dealing with long sequences that require memorizing all the information contained in the complete sequence. Long short-term memory (LSTM) recurrent networks emerge to solve the vanishing gradient problem. For this purpose, LSTM uses three gates to keep longstanding relevant information and discard irrelevant information. These gates are G f forget gate, G u update gate, and G o output gate. G f decides what information should be thrown away or saved. A value close to 0 means that the past information is forgotten whereas a value close to 1 means that it remains. G u decides what new information ~ c t to use to update the c t memory state. Thus, c t is updated by using both G Gated recurrent units. Recurrent networks with gated recurrent units (GRU) are long-term memory networks such as LSTMs but they emerged in 2014 f and G u . Finally, G o decides which is the output value that will be the input of the next hidden unit.",
               "The information of the a t ? 1 previous hidden unit and the information of the x t current input is passed through the r sigmoid activation function to compute all the gate values and through the tanh activation function to compute the ~ c t new information, which will be used to update. The equations defining an LSTM unit are:",
               "as a simplification of LSTMs due to the high computational cost of the LSTM networks. GRU is one of the most commonly used versions that researchers have converged on and found to be robust and useful for many different problems. The use of gates in RNNs has made it possible to improve capturing of very long-range dependencies, making RNNs much more effective. The LSTM is more powerful and more effective since it has three gates instead of two, but the GRU is a simpler model and it is computationally faster as it only has two gates, G u update gate and G r relevance gate as shown in . The G u gate will decide whether the c t memory state is or is not updated by using the ~ c t memory state candidate. The G r gate determines how relevant c t ? 1 is to compute the next candidate for c t , that is, ~ c t . A GRU is defined by the following equations:",
               "Downloaded by Auckland University of from www.liebertpub.com at 04/27/21. For personal use only.",
               "FIG. 5. Hidden unit in an LSTM. LSTM, long short-term memory. ",
               "where W u and W r , and b u and b r are the weights and the bias that govern the behavior of the G u and G r gates, respectively, and W c and b c are the weights and bias of the ~ c t memory cell candidate. Bidirectional RNN. There are some problems, in the field of natural language processing (NLP) for instance, where to predict a value of a data sequence in a given instant of time, information from the sequence both before and after that instant is needed. Bidirectional recurrent neural networks (BRNNs) address this issue to solve this kind of problems. The main disadvantage of the BRNNs is that the entire data sequence is needed before the prediction can be made.",
               "Standard networks compute the activation values for hidden units by using a unidirectional feed-forward process. However, in a BRNN, the prediction uses information from the past as well as information from the present and the future as input, using both forward and backward processing.",
               "Thus, the prediction at time t, ^ x t , is obtained by using a g activation function applied to the corresponding weights with both the forward and backward activation at time t. That is:",
               "Deep recurrent neural network. A deep recurrent neural network (DRNN) can be considered as an RNN with more than one layer, also called stacked RNN. The hidden units can be standard RNN, GRU or LSTM units, and it can be unidirectional or bidirectional as described in previous sections. illustrates the architecture of a DRNN with three layers.",
               "In general, a DRNN works quite well for time series forecasting, but its performance deteriorates when using very long data sequences as input. To address this issue, attention mechanisms can be incorporated into the model, being one of the most powerful ideas in deep learning. An attention model allows a neural ",
               "where W x and b x are the weights and bias and a  network to pay attention to only part of an input data sequence while it is generating the output. This attention is modeled by using weights, which are computed by a single-layer feed-forward neural network. Convolutional neural networks Convolutional neural networks (CNN) were presented in Ref. by Fukushima and are one of the most common architectures in image processing and computer vision. The CNNs have three kinds of layers: convolution, pooling, and fully connected. The main task of the convolution layers is the learning of the features from data input. For that, filters of a predefined size are applied to the data by using the convolution operation between matrices. The convolution is the sum of all element-wise products. The pooling reduces the size of input, speeding up the computing and preventing overfitting. The most popular pooling methods are average and max pooling, which summarize the values by using the mean or maximum value, respectively. Once the features have been extracted by the convolutional layers, the forecasting is carried out by using fully connected layers, also called dense layers, as in DFFNN. The input data for these last fully connected layers are the flattened features resulting of the convolutional and pooling layers. depicts the overall architecture of a CNN.",
               "Recently, a variant of CNN, called temporal convolutional networks (TCNs), has emerged for data sequence, competing directly with DRNNs in terms of execution times and memory requirements.",
               "The TCNs have the same architecture as a DFFNN but the values of activations for each layer are computed by using earlier values from the previous layer. Dilated convolution is used to select which values of the neurons from the previous layer will contribute to",
               "where d is the dilation factor parameter, and f is a filter of size K. shows the architecture of a TCNN when applying a dilated convolution by using a filter of size 3 and dilation factors of 1, 2, and 4 for each layer, respectively.",
               "Moreover, it is necessary to use generic residual modules in addition to convolutional layers when deeper and larger TCN are used to achieve further stabilization. These generic residual blocks consist of adding the input of data to the output before applying the activation function. Then, the TCN model can be defined as follows: ",
               "where F d ( ? ) is the dilated convolution of d factor defined in Equation (16), a l t is the value of the neuron of the l-th layer at time t, W l a and b l a are the weights and bias corresponding to the l-th layer, and g is the activation function."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "paragraphNo": 11,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "head": {
               "text": "Practical aspects Implementation",
               "type": "modelling"
          },
          "paragraphs": [
               "The implementation of a multilayer perceptron is relatively simple. However, deep-learning models are more complex, and their implementation requires a high level of technical expertise and a considerable time investment to implement. For this reason, the profile of the deep-learning expert has become one of the most demanded nowadays. To make easier implementations and reduce the time needed to design and train a model, some companies have focused their work on developing frameworks that allow for the implementation, training and use of deep learning models.",
               "The main idea of the deep-learning frameworks is to provide an interface that allows for the implementation of models without having to pay too much attention to the mathematical complexity behind them. There are several frameworks available in the literature. The choice of one or another will depend on several important factors, such as the type of architecture that can be implemented, support for distributed programming environments, or whether it can run on GPUs. In this sense, summarizes the most widely used frameworks in the literature, where the term all includes the DFFNN, CNN, TCN, RNN, LSTM, GRU, or BRNN architectures, and CPU is a central processing unit. shows that the predominant programming language for developing deep-learning models is Python. In addition, most of the frameworks support distributed execution and the use of GPU's. Although the described frameworks facilitate the development of the models, some of them require too many lines of code to obtain a complete implementation. For this reason, high-level libraries based on the core of the frameworks have been developed, making programming even easier. Some examples of high-level libraries can be Keras, Sonnet, 51 Swift, or Gluon, 52 among others. The main advantage of using a highlevel-library is that the syntax can be reused for another base framework, in addition to facilitating its implementation. However, the lack of flexibility is the main disadvantage."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "paragraphNo": 12,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "head": {
               "text": "Hyper-parameter optimization",
               "type": "modelling"
          },
          "paragraphs": [
               "The combination of frameworks and high-levellibraries greatly facilitates the implementation of models. However, there is an important study gap: the model optimization. This optimization will determine the quality of the model, and it must be performed based on the adjustment of its hyper-parameters. In deep learning, there are two types of hyper-parameters: model parameters and optimization parameters. The model parameters must be adjusted in the model definition to obtain optimal performance. The optimization parameters are adjusted during the training phase of the model by using the dataset. Some of the most relevant hyper-parameters are described and categorized by network architecture in . The number of hyper-parameters will depend on the network architecture to be used. In addition, the value of each one will be influenced by the characteristics of the problem and the data. This makes the task of optimizing a model a challenge for the research community. Moreover, and taking into account the parameters described in , an immense number of possible combinations can be deduced. For this reason, various metaheuristics and optimization strategies are used. According to the literature, there are several strategies to optimize a set of hyper-parameters for deep-learning models, as shown in . Thus, the hyper-parameter optimization methods can be classified into four major blocks:",
               "1. Trial-error: This optimization method is based on varying each of the hyper-parameters manually. Therefore, this method implies a high time investment, having a relatively low computational cost and a low search space, because it requires the action of a user to modify the values manually each time a run is finished. Since in deep learning there are a large number of hyper-parameters and the values they can set are infinite, it is not advisable to use this optimization method. 2. Grid: The grid method explores the different possible combinations for a set of established hyperparameters. This method covers a high search space, although it has a high computational cost associated with it, which makes this method unviable to apply in deep learning, let alone in big data environments. 3. Random: Random search allows to cover a high search space, because infinite combinations of hyper-parameters can be generated. Within this group we can differentiate between totally random or guided search strategies, such as those based on metaheuristics. Examples of this type of searches are the genetic algorithms, 68,69 particle swarm optimization, or neuroevolution of augmenting topologies 71 algorithms, among others. The wide search range, added to the medium cost involved in this search strategy, makes it one of the best methods for optimizing deep-learning models. In addition, new hyper-parameters optimization metaheuristics are being published, such as the bioinspired model in the propagation of COVID-19 presented by the authors in Ref.  few are designed specifically for the optimization of deep-learning model hyper-parameters, being also compatible with the frameworks and high-level libraries described in . summarizes a set of libraries for the optimization of hyper-parameters in deeplearning models, classifying them by search strategies, support to distributed computing, programming language, and compatible framework from Table 1. Note that it is not known whether HPOLib supports distributed computing or in which frameworks it works."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "paragraphNo": 13,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "head": {
               "text": "Hardware performance",
               "type": "modelling"
          },
          "paragraphs": [
               "One of the most important decisions a researcher must make is to determine the physical resources needed to ensure that deep-learning algorithms will find accurate models. Hence, this section overviews different hardware infrastructures typically used for deep-learning contexts, given its increasing demand for better and more sophisticated hardware. Although a CPU can be used to execute deep-learning algorithms, the intensive computational requirements usually make the CPU physical resources insufficient (scalar architecture). For this reason, three different hardware architectures are typically used for mining information with deep-learning frameworks: GPU, tensor processing unit (TPU), and intelligence processing unit (IPU).",
               "A GPU is a co-processor allocated in a CPU that is specifically designed to handle graphics in computing environments. The GPUs can have hundreds or even thousands of more cores than a CPU, but running at lower speeds. The GPUs achieve high data parallelism with single instructions, multiple data architecture and play an important role in the current artificial intelligence domain, with a wide variety of applications.",
               "The first generation of TPUs was introduced in 2016, at the Google I/O Conference and they were specifically designed to run already trained neural networks. The TPUs are custom application-specific integrated circuits built specifically for machine learning. Compared with GPUs (frequently used for the same tasks since 2016), TPUs are implicitly designed for a larger volume of reduced precision calculation (e.g., from 8 bits of precision) and lack of hardware for rasterization/ texture mapping. The term was coined for a specific chip designed for Google's TensorFlow framework. Generally speaking, TPUs have less accuracy compared with the computations performed on a normal CPU or GPU, but it is sufficient for the calculations they have to perform (an individual TPU can process more than 100 millions of pictures per day). Moreover, TPUs are highly optimized for large batches and CNNs and have the highest training throughput."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "paragraphNo": 14,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "paragraphs": [
               "The IPU is completely different from today's CPU and GPU processors. It is a highly flexible, easy-to-use, parallel processor that has been designed from the ground up to deliver state-of-the-art performance on current machinelearning models. But more importantly, the IPU has been designed to allow new and emerging machine intelligence workloads to be realized. The IPU delivers much better arithmetic efficiency on small batch sizes for both training and inference, which results in faster model convergence in training, models that generalize better, the ability to parallelize over many more IPU processors to reduce training time for a given batch size, and also delivers much higher throughput at lower latencies for inference. Another interesting feature is its lower power consumption compared with GPUs or TPUs (up to 20% less). summarizes the properties of the processing units explored in this section. Note that the performance is measured in flops and the cost in USD.",
               "Downloaded by Auckland University of from www.liebertpub.com at 04/27/21. For personal use only. Note that for TPUs cloud services are available for a price starting at 4.50 USD per hour (retrieved in March 2020)."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "head": {
               "type": "modelling"
          },
          "paragraphNo": 15,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "head": {
               "text": "Applications",
               "type": "modelling"
          },
          "paragraphs": [
               "To motivate the relevance of the time series prediction problem, an analysis of the state of the art has been carried out by classifying the deep-learning research works by application domain (such as energy and fuels, image and video, finance, environment, industry, or health) and the most widespread network architectures used (ENN, LSTM, GRU, BRNN, DFFNN, CNN, or TCN). A summary on the works reviewed can be found in . An overview of the items for each application domain is made in the following paragraphs, to highlight the goals reached for each method and field: tion of LSTM to forecast oil production. Hybrid architectures have been also used in this research field, for example, to forecast the price of carbon, the price of energy in electricity markets, 101 energy consumption, or solar power generation. 1. Energy and fuels: With the increasing use of renewable energies, accurate estimates are needed to improve power system planning and operating. Many techniques have been used to make predictions, including deep learning. Reviewing the literature in the past few years, it can be concluded that the vast majority of deep-learning architectures are suitable to this application area. Misc --"
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "paragraphNo": 16,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "head": {
               "n": "16",
               "text": "TORRES ET AL",
               "type": "modelling"
          },
          "paragraphs": [
               "3. Financial: Financial analysis has been a challenging issue for decades. Therefore, there are many research works related to this application area, as described in In addition, various architectures such as CNN, or LSTM have been used. Some authors make a comparison between some of these architectures, analyzing which one offers better results."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "paragraphNo": 17,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "head": {
               "n": "6.",
               "text": "Health: The use of deep-learning architectures",
               "type": "modelling"
          },
          "paragraphs": [
               "in the area of health is common in the past years. However, time series prediction using deep-learning models is not very widespread as time series are generally short in this field, along with the high computational cost involved in recurrent network training. The authors of Ref. "
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "paragraphNo": 18,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "paragraphs": [
               "Although these studies are widespread, the complexity of the problem requires the search for new methodologies and architectures. 4. Environmental: Environmental data analysis is one of the most popular areas for the scientific community. Many of these works are also based on the application of deep-learning techniques to forecast time series. The authors in Ref. 150 applied CNN and LSTM to forecast wind speed or temperature by using meteorological data from Beijing, China. Other authors focused on a single specific variable. For instance, the authors used TCN, GRU, ENN, BRNN, and LSTM architectures to forecast information related to wind in. Water quality and demand were also predicted by using An application of LSTMbased neural networks for correlated time series prediction was also proposed by Further, carbon dioxide emissions, flood, 143 conducted a comprehensive study of time series prediction models in health care diagnosis and prognosis with a focus on cardiovascular disease. Instead, it is usual to apply convolution-based architectures or implement hybrid models. For example, the authors used CNN to accelerate the computation for magnetic resonance fingerprinting in Ref. CNN was also used to monitoring the sleep stage in Ref. for detecting premature problems as ventricular contractions or to forecast the Sepsis. In Ref. 179 the authors used a backpropagation network to forecast the incidence rate of pneumonia. Other network architecture such as LSTM can be used to forecast the status of critical patients according to their vital functions.",
               "171 A recent study conducted by the authors in Ref. uses some deep-learning architectures to forecast COVID-19 cases. 7. Miscellaneous: In recent years, the TCN has been one of the most widely checked general purpose architectures for time series forecasting. or NH 3 concentration for swine house 199 were also predicted by using deep-learning techniques, in particular ENN. 5. Industry: In the industry sector, deep-learning techniques are also being used to carry out tasks of different kinds. For instance, TCN and BRNN can be used to traffic flow forecasting. The LSTM can be used for multiple purposes, such as process planning, 160 construction equipment recognition or to improve the performance of organizations. The authors in Ref. used a DFFNN to forecast bath and metal height features in the electrolysis process. The ENN and GRU networks have been also used, for example, to forecast the useful life or degradation of the materials.",
               "However, any of the other network architectures can be applied to time series of miscellaneous application domains not classified in Namely, readers interested in cybersecurity can find a detailed description in Refs. From the previous analysis of , two main conclusions can be drawn. First, there exist several methods that have not been applied yet to particular application fields. Second, the existence of these gaps encourages the conduction of research in such lines. Deep-learning techniques are also widely applied to architecture, as can be seen in the indepth study conducted by the authors in Ref. It can be concluded that almost all network architectures have been used, given the wide variety of problems existing in this area."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "head": {
               "type": "modelling"
          },
          "paragraphNo": 19,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "head": {
               "text": "Conclusions",
               "type": "conclusion"
          },
          "paragraphs": [
               "Deep learning has proven to be one of the most powerful machine-learning techniques for solving complex problems dealing with big data. Most of the data mainly generated through smart devices are time series nowadays, and their prediction is one of the most frequent and current problems in almost all research areas. Thus, these two topics have been jointly analyzed in this survey to provide an overview of deep-learning techniques applied to time series forecasting. First, the most used deep-learning architectures for time series data in the past years have been described, with special emphasis on important practical aspects that can have a great influence on the reported results. In particular, it has placed focus on the search for hyper-parameters, the frameworks for deployment of the different architectures, and the existing hardware to lighten the hard training of the proposed network architectures. Second, a study of the deep neural networks used to predict time series in different application domains has been carried out in this survey, with the aim of providing a good comparative framework to be used in future works and to show which architectures have not been sufficiently tested in some applications."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "paragraphNo": 20,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "head": {
               "text": "Author Disclosure Statement",
               "type": "conclusion"
          },
          "paragraphs": [
               "No competing financial interests exist."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "paragraphNo": 21,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     },
     {
          "head": {
               "text": "Funding Information",
               "type": "conclusion"
          },
          "paragraphs": [
               "This work was supported by the Spanish Ministry of Science, Innovation and Universities under project TIN2017-88209-C2-1-R. Also, this work has been partially supported by the General Directorate of Scientific Research and Technological Development (DGRSDT, Algeria), under the PRFU project (ref: C00 L07UN060120200003)."
          ],
          "paper_id": "0574dc30-b55e-11eb-b26e-8fc6bc11dbc2",
          "paragraphNo": 22,
          "fromPaper": "Deep Learning for Time Series Forecasting: A Survey"
     }
]