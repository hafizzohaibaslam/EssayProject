<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-07-16T23:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Can Chinese Web Pages be Classified with English Data Source?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>April 21-25. 2008</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Rong</forename><surname>Xue</surname></persName>
							<email>grxue@apex.sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Jiang</surname></persName>
							<email>yunjiang@apex.sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
							<email>qyang@cs.ust.hk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Jiao Tong University</orgName>
								<address>
									<addrLine>800 Dongchuan Road</addrLine>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Clearway Bay</addrLine>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Can Chinese Web Pages be Classified with English Data Source?</title>
					</analytic>
					<monogr>
						<title level="m">WWW 2008 / Alternate Track: WWW in China -Mining the Chinese Web</title>
						<meeting> <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">April 21-25. 2008</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>Categories and Subject Descriptors H33 [Information Storage and Retrieval]: Information Search and Retrieval</keywords>
			</textClass>
			<abstract>
				<p>As the World Wide Web in China grows rapidly, mining knowledge in Chinese Web pages becomes more and more important. Mining Web information usually relies on the machine learning techniques which require a large amount of labeled data to train credible models. Although the number of Chinese Web pages increases quite fast, it still lacks Chi-nese labeled data. However, there are relatively sufficient English labeled Web pages. These labeled data, though in different linguistic representations, share a substantial amount of semantic information with Chinese ones, and can be utilized to help classify Chinese Web pages. In this paper , we propose an information bottleneck based approach to address this cross-language classification problem. Our algorithm first translates all the Chinese Web pages to En-glish. Then, all the Web pages, including Chinese and En-glish ones, are encoded through an information bottleneck which can allow only limited information to pass. Therefore, in order to retain as much useful information as possible, the common part between Chinese and English Web pages is inclined to be encoded to the same code (i.e. class label), which makes the cross-language classification accurate. We evaluated our approach using the Web pages collected from Open Directory Project (ODP). The experimental results show that our method significantly improves several existing supervised and semi-supervised classifiers.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Categories and Subject Descriptors</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval</head><p>in China now exceeds 160 millions and the Chinese Web pages are numbered in billions <ref type="bibr">1</ref> . As the most commonly used language only second to English, Chinese is expected to enjoy such a rocketing increase in scale. Because the Web pages written in Chinese is becoming a major information source on the Internet, more research efforts are now devoted to organizing and mining the Chinese Web pages via Web mining techniques, such as Chinese blog mining <ref type="bibr" target="#b18">[20]</ref> and query log analysis <ref type="bibr" target="#b17">[19]</ref>.</p><p>A potential problem in mining the Chinese Web pages is the lack of sufficient labeled data. As we know, classification requires a large amount of labeled training data. Generally speaking, the more labeled training data one can obtain, the better the classification accuracy and robustness are. Fortunately, due to many reasons, there exists a lot of labeled Web-page information in English, in particular in the machine learning community. Examples of these resources are Reuters-21578 <ref type="bibr" target="#b14">[16]</ref>, 20 Newsgroups <ref type="bibr" target="#b13">[15]</ref>, and Open Document Project <ref type="bibr" target="#b20">[22]</ref>. It is thus useful and intriguing to fully utilize the labeled documents in English to help classify the Web pages in Chinese. This problem is called cross-language Web-page classification. In this paper, we address this important problem using a novel information theory based technique.</p><p>Although the training and test documents are in different languages, one can use a translation tool to help translate the test data sets in the English language, before a classifier trained on English pages can be applied. While such a method may be feasible, we observe that a simple-minded application of this method may result in serious problems because of the following reasons:</p><p>? Second, due to the errors introduced in the translation process, there may be different kinds of errors in the translated text. For example, some errors may result from Chinese phrase segmentation, others are due to ambiguities introduced by a dictionary. This translation noise problem must be addressed effectively.</p><p>WWW 2008, <ref type="bibr">April 21-25, 2008</ref>, Beijing, China.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WWW 2008 / Alternate Track: WWW in China -Mining the Chinese Web April 21-25, 2008 ¡¤ Beijing, China</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English Web Pages</head><p>common part coding output to be extracted and used for classification, despite their differences. We show experiments on real English and Chinese Web documents. Our method is shown to improve other classification algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>information bottleneck</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chinese Web Pages</head><p>Figure 1: The model of our information bottleneck cross-language classifier.</p><p>? Finally, the feature spaces of the English and Chinese Web pages may be different, resulting in a situation where the training and test data may have different feature sets. We therefore must be innovative in our solution to this problem by carefully extracting the common-semantic parts of the two data sets, and use these parts as a bridge to propagate the class labels.</p><p>The rest of our paper is organized as follows: In Section 2 we briefly discuss the related work. Following the basic concepts reviewed in Section 3, we introduce the information bottleneck theory in Section 4. Section 5 describes our proposed method in details. The experiments and results are presented in Section 6. In the end, we conclude this paper with future work discussion in Section 7. The detailed proofs to the lemmas and theorems in this paper will be given in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>In this section, we review several prior researches mostly related to our work, including traditional classification, crosslanguage classification and information theoretic learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Traditional Classification</head><p>To solve the above problems, we develop a novel approach for classifying the Web pages in Chinese using the training documents in English. Our key observation is that despite the above listed problems, linguistically, the pages in Chinese and English may share the same semantic information, although they are in different representation forms, i.e., Chinese characters and English words, respectively. This is reasonable, because people with good command of both English and Chinese can convey the same information in both languages. Also, it is noted that the same meaning might be expressed in different ways due to the cultural and linguistic differences.</p><p>Based on the observations, we propose to tackle the crosslanguage classification problem using information bottleneck theory <ref type="bibr" target="#b28">[29]</ref>. <ref type="figure">Figure 1</ref> illustrates our idea intuitively. The training and translated target texts are encoded together, allowing all the information to be put through a "bottleneck" and represented by a limited number of codewords (i.e. labels in the classification problem). Information bottleneck based approach can maintain most of the common information and disregard the irrelevant information. Thus we can approximate the ideal situation where similar training and translated test pages, which is the common part, are encoded into the same codewords.</p><p>In the experimentation, we collect the Web pages in English and Chinese from Open Directory Project for the data sets. Five binary and three multi-class tasks have been set up. Our method gives significant improvement against several existing classifiers, and converges very well.</p><p>In summary, in this paper we make the following contributions:</p><p>The traditional classification formulation is built on the foundation of statistical learning theory. Two schemes are generally considered, where one is supervised classification and the other is semi-supervised classification. Supervised classification focuses on the case where the labeled data are sufficient, and where the learning objective is to estimate a function that maps examples to class labels using the labeled training instances. Examples of supervised classification algorithms include decision trees <ref type="bibr" target="#b23">[25]</ref>, K nearest neighbor methods <ref type="bibr" target="#b4">[6]</ref>, naive Bayes classifiers <ref type="bibr" target="#b15">[17]</ref>, support vector machines <ref type="bibr" target="#b3">[5]</ref>, and so on.</p><p>Semi-supervised classification <ref type="bibr" target="#b31">[32]</ref> addresses the problem that the labeled data are too few to build a good classifier. It makes use of a large amount of unlabeled data, together with a small amount of the labeled data to enhance the classifiers. Many semi-supervised learning techniques have been proposed, e.g., co-training <ref type="bibr" target="#b2">[4]</ref>, EM-based methods <ref type="bibr" target="#b19">[21]</ref>, transductive learning <ref type="bibr" target="#b11">[13]</ref> etc.</p><p>As we claimed, there are two main difficulties in the crosslanguage classification, namely errors by translation and bias by topic drifts, which traditional classifiers cannot handle well. Our proposed method tries to tackle these difficulties via the information bottleneck technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross-Language Text Classification</head><p>? In addressing the cross-language Web page classification problem, we observe that there is a common part of Chinese and English documents and develop a novel method for addressing the problem of topic drifts in Chinese and English documents, thus improving the classification performance.</p><p>? We propose to handle noisy features and different features problem in cross-language Web-page classification by the information bottleneck technique. This method allows the common part in the two languages</p><p>There are several research works addressing the crosslanguage classification problem. <ref type="bibr">Bel et al.</ref> [3] studied EnglishSpanish cross-language classification problem. Two scenarios are considered in their work. One scenario assumes to have training documents in both languages. The other scenario is to learn a model from the text in one language and classify the data in another language by translation. In our work, we focus on the second scenario. <ref type="bibr" target="#b24">[26]</ref> gave good empirical results on English-Italian cross-language text categorization using an EM-based learning method. Note that to avoid trivial partitions, it applies feature selection before each iteration. <ref type="bibr" target="#b21">[23]</ref> employed a general probabilistic EnglishCzech dictionary to translate Czech text into English and then classified Czech documents using the classifier built on English training data. Other cross-language text classification research include <ref type="bibr" target="#b9">[11]</ref> (English-Spanish), <ref type="bibr" target="#b16">[18]</ref> (EnglishJapanese) etc, to be mentioned. In addition to text catego-rization, there are some other specific cross-language applications: Named Entity Recognition <ref type="bibr" target="#b29">[30]</ref>, Question Answering <ref type="bibr" target="#b6">[8]</ref>, etc.</p><p>However, most existing algorithms are based on traditional supervised or semi-supervised classification techniques. As we stated, there are two difficulties in the cross-language classification, the translation error and topic drift, which lead to difference in distributions between the Web pages in two languages. Since traditional supervised classification techniques assume identical distribution for training and test data and in the cross-language setting this assumption is hardly met, most existing algorithms will not cope with the cross-language text classification well. In this work, our method tries to handle the Chinese-English cross-language categorization problem by information bottleneck. We will show that our algorithm can better alleviate the impact of translation error and topic drift, and improve the (English to Chinese) cross-language classification performance against existing methods. In classification, X is the set of instances, and?Xand? and?X is the prediction labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Information Theoretic Learning</head><p>Another related research is information theory based learning. Information theory is widely used in machine learning, e.g. decision tree <ref type="bibr" target="#b23">[25]</ref>, feature selection <ref type="bibr" target="#b30">[31]</ref>, etc.</p><p>The information bottleneck theory (IB) was first proposed by Tishby et al. <ref type="bibr" target="#b28">[29]</ref>. They constructed a model that uses information theory to solve the clustering problem. In their work, a rate distortion function is introduced as a loss function. They also presented a converging iterative algorithm for this self-consistent determination problem. After that, a lot of interesting works have been conducted, c.f. <ref type="bibr" target="#b26">[27]</ref>. As known, IB is an information theoretic formulation for clustering problem while maximum likelihood of mixture models is a standard statistical method to clustering. Interestingly, Slonim and Weiss <ref type="bibr" target="#b27">[28]</ref> have proved that under a certain mapping, these two approaches are strongly related. Moreover, when input data is large enough, they are statistically equivalent.</p><p>Several extensions to information bottleneck method have been investigated recently. <ref type="bibr" target="#b7">[9]</ref> proposed a word clustering method which minimizes the loss in mutual information between words and class-labels, before and after clustering. Using similar strategy, mutual information based <ref type="bibr" target="#b8">[10]</ref> and Bregman divergence based <ref type="bibr" target="#b0">[2]</ref> co-clustering were proposed.</p><p>In contrast to these works, we focus on solving the crosslanguage classification problem via an information theoretic approach. More specifically, the IB technique is used to mine the common part of the pages in different languages for classification.</p><p>contained in a piece of data, i.e. its minimum average message length in bits. This means the best possible lossless data compression is limited to this measure.</p><p>Let X and Y be random variable sets with a joint distribution p(X, Y ) and marginal distributions p(X) and p(Y ). The mutual information I(X; Y ) is defined as</p><formula xml:id="formula_0">Z Z I(X; Y ) = p(x, y) log p(x, y) p(x)p(y) dxdy. (2) x y</formula><p>The mutual information is a measure of the dependency between random variables. It is always non-negative, and it is zero if and only if the variables are statistically independent.</p><p>Higher mutual information values indicate more certainty that one random variable depends on another. The mutual information is related to the Kullback-Leibler (KL) divergence or relative entropy measures, defined for two probability mass functions p(x) and q(x),</p><formula xml:id="formula_1">Z D(p||q) = p(x) log p(x) q(x) dx,<label>(3)</label></formula><p>x where q(x) is a reference distribution. The KL-divergence can be considered as a kind of a distance between the two probability distributions, although it is not a real distance measure because it is not symmetric. In addition, KLdivergence is always non-negative due to the Gibbs' inequality <ref type="bibr" target="#b5">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">INFORMATION BOTTLENECK</head><p>In this section, we present the basic concepts for information bottleneck, and show how it can be applied to crosslanguage classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Basic Concepts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PRELIMINARY</head><p>In this section, some preliminary knowledge in the information theory is briefly introduced, including information entropy, mutual information and Kullback-Leibler divergence <ref type="bibr" target="#b12">[14]</ref>. For more details, please refer to <ref type="bibr" target="#b5">[7]</ref>.</p><p>The term entropy is used to measure the uncertainty associated with a random variable X. Formally,</p><formula xml:id="formula_2">Z H(X) = ? p(x) log(p(x))dx,<label>(1)</label></formula><p>x where x enumerates each value X may take. In the communication system, the entropy quantifies the information</p><p>The information bottleneck (IB) method is a distributional learning algorithm proposed by Tishby et al. <ref type="bibr" target="#b28">[29]</ref>. In this theory, the clustering and classification problems can be treated as a coding process. Let X be the signals to be encoded, and?Xand? and?X be the set of codewords. In classification, the codewords?Xcodewords? codewords?X is defined as class labels, and then classifying X can be seen as using?Xusing? using?X to encode X. Usually?XUsually? Usually?X is not able to contain as much information as X. Therefore, when the signals X is encoded to codewords?Xcodewords? codewords?X, part of the information contained by X will be lost. This can be imaged as the information in X passing through a bottleneck?X bottleneck? bottleneck?X, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>, and thus?Xthus? thus?X is called information bottleneck.</p><p>A good clustering should guarantee the encoding effectiveness (the lower rate the better) as well as meaningful information, which can be formulated as a trade-off objective I( ? X; X)?¦ÂI( ? X; Y ) <ref type="bibr" target="#b28">[29]</ref>, where Y is the feature set with respect to X. Note that the coding rate I( ? X; X) in classification is no so important as in clustering problems, since classification focuses on prediction accuracy. Therefore, in this task, we need to mainly concentrate on improving the classification accuracy. So that, ?I( ? X; Y ) is optimized instead of I( ? X; X) ? ¦ÂI( ? X; Y ), as the coding rate I( ? X; X) is ignored. Moreover, in order to make the optimization easier, in this work, we optimize I(X; Y ) ? I( ? X; Y ) instead of ?I( ? X; Y ). We will show the optimization details in the next section. Note that, I(X; Y ) is a constant, when X and Y are fixed, and thus optimizing I(X; Y ) ? I( ? X; Y ) is equivalent to optimizing ?I( ? X; Y ). The meaning of objective function I(X; Y ) ? I( ? X; Y ) can be also understood in another way. In classification, the instances are described by the features, and thus there should be mutual information between data instances and their features, i.e. I(X; Y ) &gt; 0. In addition, the category information is also described by the features, and thus I( ? X; Y ) &gt; 0. Therefore, the information in X and?Xand? and?X is contained in forms of I(X; Y ) and I( ? X; Y ). Note that I(X; Y ) is always greater than or equal to I( ? X; Y ), which will be derived in Lemma 1 in Section 5. <ref type="bibr">I(X; Y )</ref>. Therefore, in the information bottleneck classification setting, the quality of the categorization should be judged by the loss in mutual information between the original instances and categorized instances. The following remark lays our the objective function formally. To summarize, it is noted that cross-language Web pages are carrying a common part of semantic information. According to the information bottleneck theory, this part of information is expected to help encode similar Web pages in different languages since it is highly relevant information to class labels and also is contained in the Web pages in both languages. Therefore, it is clear that the information bottleneck technique is suitable to address the cross-language classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">I(X; Y ) ? I( ? X; Y ) &gt; 0 means there is some loss in mutual information after categorization. To sum up, a good categorization should keep the mutual information between data and features, and minimize the information loss. In other words, I( ? X; Y ) should be close to</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CROSS-LANGUAGE CLASSIFIER VIA INFORMATION BOTTLENECK</head><p>In this section, the problem is carefully formulated and an objective function is proposed to build a classification model. The classification algorithm (IB) is then presented. Also, the convergence and time complexity are theoretically analyzed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Problem Formulation</head><p>Remark 1. In the information bottleneck (IB) classification setting, a qualified hypothesis h : X ¡ú ? X approximately satisfies:</p><formula xml:id="formula_3">h = arg min h * " I(X; Y ) ? I( ? X; Y ) " .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Applying to Cross-Language Classifier</head><p>Before the formal problem formulation, we would like to present a brief idea to address the cross-language classification problem by information bottleneck. Suppose we have the union set X of English and Chinese Web pages, Y is the set of words contained in X, and?Xand? and?X stands for the class labels.</p><p>It is observed that there is a common part of English and Chinese web pages, which share the similar semantic information. This observation inspired our work. If the texts are put through the "bottleneck", the labeled data (English Web pages) will, to some extent, guide the classification on Chinese pages by encoding the common part into the same codewords. It is because that during the information theoretic compression, IB tends to encode the similar pages into the same code (label) in that it can reduce the code length without loss of much information. If one Chinese page is in the common part, i.e. similar to a labeled English page, this page will be classified into the same category as that of the English one.</p><p>Let Xe be the set of the Web pages in English with class labels and Xc be the set of the Web pages in Chinese without labels. Usually, it is assumed that the English Web pages Xe and Chinese Web pages Xc share some common information with each other, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. The feature space for Xe is the English words Ye and for Xc is the Chinese words Yc. The objective is, we are trying to classify the pages in Xc into C, the predefined class label set, which is the same for the pages in Xe. To sum up, labeled training documents are available only in one language, and we want to estimate a hypothesis h : Xc ¡ú C which classifies documents written in another language. This is called crosslanguage text classification.</p><p>In contrast to traditional text categorization, where the training and test pages are in the same language, crosslanguage text classification requires that the training and test data should be unified into one single feature space. Otherwise, it is not possible for existing machine learning techniques to get the results. The most common way is to translate the pages in one language into the other one. However, it is noticed that this common approach will bring the error and bias for further classification. Linguistically speaking, machine translation technique is far from satisfactory. What is worse, the simple translation preprocessing does not give the accurate information. The topics of original data may drift under translation. Empirically, these claims were justified. The experimental details are presented in Section 6.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Objective Function</head><p>As stated in Section 4.2, we propose to address the problems via the information bottleneck technique. The test Web page set is translated to English, denoted as X T c . Let X = Xe ¡È X T c , as the original signal for the bottleneck. The class label set is the output of the bottleneck?Xbottleneck? bottleneck?X. Y is referred to the features of all the documents. To fully utilize the common part for classification, we defined the objective function as</p><formula xml:id="formula_4">I(X; Y ) ? I( ? X; Y ),<label>(5)</label></formula><p>which is exactly the objective function in Remark 1. Note that I(X; Y ) ? I( ? X; Y ) is always non-negative, which will be derived by Lemma 1. Based on Remark 1, the objective function value should be minimized, since we aim to draw I( ? X; Y ) close to I(X; Y ). Before proposing the optimization approach for minimizing the objective function, we first define some notations used in subsequent analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 2 gives a straightforward explanation for crosslanguage text classification via the information bottleneck technique. If one Chinese page is more similar to the English pages of one class?xclass? class?x, i.e. the distance D(p(Y |x)||?p||?p(Y |?x|?x)) is the smallest, assigning this page t?</head><p>x will lead to a lower value of the objective function. Then it is desirable during the optimization process, which means the common part between English and Chinese Web pages work as stated in Section 4.2.</p><p>Also, Lemma 2 provides an alternative way to reduce the objective function value. From Equation <ref type="formula">(9)</ref> <ref type="figure">(Y |?x|?x)</ref>) for each instance x, the objective function will decrease monotonically. Thus, based on Lemma 2, the information bottleneck cross-language text classification (IB) is derived as in Algorithm 1. Definition 1. We use?Xuse? use?X = {?x}{?x} to denote a categorization of X for the hypothesis h, where?xwhere? where?x = {x |h(x ) = h(x)}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>, we know that minimizing D(p(Y |x)||?p||?p(Y |?x|?x)) for a single instance x could reduce the global objective function D(p(X, Y )||?p||?p(X, Y )). As a result, if we iteratively optimize the corresponding D(p(Y |x)||?p||?p</head><p>Clearly, | ? X| is equal to |C|, since h maps the instances in X to the class-labels in C.  In the following, we will transform the objective function in Equation (5) into another representation by KLdivergence <ref type="bibr" target="#b12">[14]</ref>. and Equation (6). 4: for t = 1, . . . , N do 5:</p><formula xml:id="formula_5">X T c = T (Xc). Let X = Xe ¡È X T ? p(x, y) = p(? x, y)p(x|?xx|?x) = p(? x, y) p(x) p(? x) ,<label>(6)</label></formula><p>for each x ¡Ê X T c do 6: h (t) (x) = arg mi? x¡Ê?Lemmax¡Ê? x¡Ê?Lemma 1. For a fixed categorizatio? X, we can write the objective function in Equation (5) as</p><formula xml:id="formula_6">I(X; Y ) ? I( ? X; Y ) = D(p(X, Y )||?p||?p(X, Y )),<label>(7)</label></formula><p>where D(¡¤||¡¤) is the KL-divergence defined as Equation <ref type="formula" target="#formula_1">(3)</ref>.</p><formula xml:id="formula_7">X D(p(Y |x)||?p||?p (t?1) (Y |?x|?x)) 7:</formula><p>end for 8:</p><p>for each x ¡Ê Xe do 9:</p><formula xml:id="formula_8">h (t) (x) = h (t?1) (x) 10:</formula><p>end for 11:</p><p>Update?pUpdate? Update?p (t) based on h (t) and Equation (6). 12: end for 13: Return h (N) as the final hypothesis h f .</p><p>Note that, based on the non-negativity of KL-divergence, the objective function I(X; Y ) ? I( ? X; Y ) should always be non-negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Optimization</head><p>In Algorithm 1, in each iteration, the algorithm keeps the prediction labels for the English Web pages Xe unchanged since their true labels are already known, while choosing the best category?Xcategory? category?X for each data instance x in X T From Equation <ref type="formula" target="#formula_6">(7)</ref>, it is found that the loss in mutual information in the objective function equals to the KL-divergence between p(X, Y ) and?pand? and?p(X, Y ). To minimize the objective function in Equation (5), we need only to find a categorizatio? X which minimizes the KL-divergence value</p><formula xml:id="formula_9">c to minimize the function D(p(Y |x)||?p||?p (t?1) (Y |?x|?x)).</formula><p>As we have discussed above, this process is able to decrease the objective function in Equation <ref type="formula" target="#formula_6">(7)</ref>. The whole algorithm is illustrated in <ref type="figure" target="#fig_4">Figure  4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Convergence</head><formula xml:id="formula_10">D(p(X, Y )||?p||?p(X, Y )) .<label>(8)</label></formula><p>However, the objective function in Equation <ref type="formula" target="#formula_6">(7)</ref> is in the joint probability form that is difficult to be optimized. Now, we are to rewrite it into a conditional probability form, which will facilitate our algorithm to reduce the objective function value.</p><p>Since our algorithm IB is iterative, it is necessary to discuss its property of convergence. The following theorem shows that the objective function in our algorithm monotonically decreases, which establishes that the algorithm converges eventually.</p><p>Lemma 2. The objective function in Equation <ref type="formula" target="#formula_6">(7)</ref> can be expressed by a conditional probability form as Theorem 1. The objective function in Equation <ref type="formula" target="#formula_6">(7)</ref> monotonically decreases in each iteration of Algorithm IB.</p><formula xml:id="formula_11">X X D(p(X, Y )||?p||?p (t) (X, Y )) ¡Ý D(p(X, Y )||?p||?p (t+1) (X, Y )). (10) D(p(X, Y )||?p||?p(X, Y )) = p(x)D(p(Y |x)||?p||?p(Y |?x|?x)). (9) ? x¡Ê?Xx¡Ê? x¡Ê?X x¡Ê?xx¡Ê?x</formula><p>Note that, although the algorithm is able to minimize the objective function value in Equation <ref type="formula" target="#formula_4">(5)</ref>    <ref type="table" target="#tab_1">English Web Pages  Arts  1,942  186,307  Business  6,503  203,569  Computers  1,907  102,571  Games  296  39,269  Health  518  47,607  Home  203  23,117  Kids and Teens  292  27,323  News  359  96,510  Recreation  681  77,901  Reference  2,338  48,231  Science  914  75,434  Shopping  488  86,736  Society  1,481  185,466  Sports  321  71,065  Total  18,243  1,271,106</ref> find a locally minimal one. Finding the global optimal solution is NP-hard. From Theorem 1, we can straightforwardly derived that the algorithm IB converges in a finite number of iterations, since the hypothesis space is finite. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Computational Complexity</head><p>Regarding the computational cost for IB, suppose the nonzeros in p(X, Y ) is N . In each iteration, IB needs to calculate h <ref type="bibr">(t)</ref> in O(|C| ¡¤ N ) and update?pupdate? update?p</p><formula xml:id="formula_12">(t) (Y | ? X) in O(|C| ¡¤ |Y |)</formula><p>. Therefore, the time complexity of IB is O(|C| ¡¤ (|Y | + N )) as a result. Usually, |C| is not large and could be considered as a constant, while |Y | is usually not larger than N . Thus, the time complexity of IB is O(N ) in general. Thus, our algorithm IB has good scalability, and is capable for large data sets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTS</head><p>In this section, we evaluate our cross-language classification algorithm based on information bottleneck, and compare our algorithm with several state-of-art supervised and semi-supervised classifiers.</p><p>beled Web pages <ref type="bibr">(1,</ref><ref type="bibr">271,</ref><ref type="bibr">106</ref>) is much more abundant than Chinese ones (18,243).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Data Preparation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data Sets</head><p>We conduct our evaluation on the Web pages crawled from the Open Directory Project (ODP) <ref type="bibr" target="#b20">[22]</ref> during August 2006. Each Web page in ODP was classified by human experts into 17 top level categories (Arts, Business, Computers, Games, Health, Home, Kids and Teens, News, Recreation, Reference, Science, Shopping, Society, Sports, Regional, Adult and World). We removed the Regional category because the Web pages in the Regional category are also in other categories. The Web pages in the Adult have not been crawled by our crawler, because most of them are banned by our internet service provider, and thus the Adult category is not included in our data collection. Moreover, the Web pages in the World category are in the languages other than English. We selected all the Chinese pages from the World category as Chinese test data. For Chinese Web pages, there are also 14 top categories each of which can be mapped to a top category in the English ODP. Therefore, we have 14 categories for both Chinese and English Web pages in this experiments. <ref type="figure">Figure 1</ref> represents the detailed description for our data collection. From the table, it can be seen that, the number of English labeled Web pages is much larger than that of Chinese ones in ODP, which indicates English laData preprocessing has been applied to the raw data. First, all the Chinese Web pages were translated by Google Translator <ref type="bibr">[1]</ref>. Then, we converted all the letters to lower cases, and stemmed the words using the Porter's stemmer <ref type="bibr" target="#b22">[24]</ref>. After that, stop words were removed. In order to reduce the size of the feature space, we used a simple feature selection method, document frequency (DF) thresholding <ref type="bibr" target="#b30">[31]</ref>, to cut down the number of features, and speed up the classification. Based on <ref type="bibr" target="#b30">[31]</ref>, DF thresholding, which has comparable performance with information gain (IG) or CHI, is suggested since it is simplest with lowest cost in computation. In our experiments, we set the DF threshold to 3. After feature selection, the vocabulary size becomes 512,896.</p><p>In order to evaluate our cross-language classifier, we set up eight cross-language classification tasks. Five of them are binary classification tasks, and others are for multiple-class classification. <ref type="table" target="#tab_3">Table 2</ref> presents the detailed composition for each classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Cross-Language Topic Drift</head><p>We extracted the most frequent features in the Chinese and English Web pages for each of the 14 ODP categories, and found the frequent features in the Chinese and English Web pages are quite different, although they share some classification algorithms to show the advantages of our algorithm.</p><p>We take the supervised classification algorithms to be the baseline methods. Naive Bayes classifiers (NBC) <ref type="bibr" target="#b15">[17]</ref> and support vector machines (SVM) <ref type="bibr" target="#b3">[5]</ref> are evaluated in the experiments. They are trained on Xe and tested on X T 500 400 c . Transductive support vector machines (TSVM) <ref type="bibr" target="#b11">[13]</ref> is also introduced as comparison semi-supervised learning methods, which take both labeled Xe and unlabeled X c for testing. For implementation details, TF-IDF is used for feature weighting when training support vector machines (SVM) <ref type="bibr" target="#b3">[5]</ref> and transductive support vector machines (TSVM) <ref type="bibr" target="#b11">[13]</ref>. TF is used for feature weighting when training naive Bayes classifier (NBC) <ref type="bibr" target="#b15">[17]</ref> and our information bottleneck based cross-language classification algorithm (IB).</p><p>SVM and TSVM are implemented by SVM light <ref type="bibr" target="#b10">[12]</ref> with default parameters (linear kernel). For more details about SVM and TSVM, please refer to <ref type="bibr" target="#b3">[5]</ref> and <ref type="bibr" target="#b11">[13]</ref>. NBC and IB are implemented by ourselves. The initial categorizations for IB are given by NBC.</p><p>part. <ref type="table" target="#tab_4">Table 3</ref> presents the top 5 frequent features in the Chinese and English Web pages for each of the 14 ODP categories. We believe there is topic drift between Chinese and English Web pages. For example, there are several Chinese words which are frequently appears in the Chinese Web pages, such as "qiyuan", "mufurong", "pingqiu" and so on. In the Games, "qiyuan", one of the famous online game in China, is the most frequent keyword, while it hardly appears in the English Web pages. This is due to the difference in culture between the Chinese and western societies. "pingqiu", which means "draw" in English, hardly appears in English Web pages, because the translator fails to provide a mapping between "draw" and it. The observations demonstrate the claim we made previously that there are two main obstacles for the cross-language classification: one is the difference in topic focus between the two languages; the other is the translation error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Classification Performance</head><p>We now present the classification performance for each comparison methods, and show advantages of our information bottleneck cross-language classifier IB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Evaluation Metrics</head><p>The metrics used in this experiments are macro-average precision, recall and F1-measure. Let f be the function which maps from document d to its true class label c = f (d), and h be the function which maps from document d to its prediction label c = h(d) given by the classifiers. The macroaverage precision P and recall R are defined as <ref type="figure" target="#fig_7">Figure 5</ref> shows the instance-feature co-occurrence distribution on the Games vs News data set. In this figure, documents 1 to 484 are from Xe, while documents 485 to 853 are from Xc. Within a data set, Xe or Xc, the documents are ordered by their categories (Games or News). The words are sorted by ng(w)/nn(w), where ng(w) and nn(w) represent the number of word positions w appears in Games and News documents, respectively. From <ref type="figure" target="#fig_7">Figure 5</ref>, it can be found that the distributions of English and Chinese Web pages are somewhat different, however the figure also shows large commonness exists between the two data sets. The density divergence between two data sets in the figure makes the cross-language classification difficult, because most classification techniques rely on the basic assumption that the training data should be drawn from the same distribution as the test data. However, the common part between the two data sets can help increase the feasibility of the classification.</p><formula xml:id="formula_13">X P = 1 |C| |{d|d ¡Ê Xc ¡Ä h(d) = f (d) = c}| |{d|d ¡Ê Xc ¡Ä h(d) = c}| , (11) c¡ÊC X R = 1 |C| |{d|d ¡Ê Xc ¡Ä h(d) = f (d) = c}| |{d|d ¡Ê Xc ¡Ä f (d) = c}| . (12) c¡ÊC</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Density Analysis</head><p>F1-measure is a harmonic mean of precision and recall defined as follows</p><formula xml:id="formula_14">F1 = 2P R P + R .<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparison Methods</head><p>In this experiments, we compare our information bottleneck cross-language classifier (IB) with several state-of-art <ref type="table" target="#tab_5">Table 4</ref> presents the performance on each binary classification data set given by NBC, SVM, TSVM and our algorithm IB. The implementation details of the algorithms have already been presented in the last subsection. The evaluation metrics are macro-average precision, recall and F1-measure, of which we have just given the definitions. From the table, we can see that IB significantly improves the other three methods. Although SVM and TSVM is slightly better than IB on the Arts vs Computers data set, IB is still comparable. But, on some of the other data sets, e.g. Computers vs Sports and Reference vs Shopping, both SVM and TSVM fail, while IB is much better than the two discriminative methods. In addition, NBC is always worse than IB, but never fails a lot. In average, IB gives the best performance in all the three evaluation metrics. <ref type="table">Table 5</ref> presents the performance on each multiple-class classification data set given by NBC and our algorithm IB.    SVM and TSVM are not included here because they are designed for binary classification, and cannot cope well with the multiple-class classification problem. In this table, IB still gives significant improvements against the baseline method NBC. Therefore, we believe our algorithm IB is not only effective for English-Chinese cross-language classification, but also extensible for multiple-class classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Iterations</head><p>Figure 6: The F1-measure curves after each iterations on three data sets Arts vs Computers, Recreation vs Science and 3 Categories respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Convergence</head><p>Since our algorithm IB is an iterative algorithm, an important issue for IB is the convergence property. Theorem 1 has already proven the convergence of IB theoretically. Now, let us empirically show the convergence property of IB. <ref type="figure">Figure  6</ref> shows the test error rate curves as functions for each iteration on three data sets, Arts vs Computers, Recreation vs Science and 3 Categories. From the figure, it can be seen that IB always achieves almost convergence points within 10 iterations. This indicates that IB converges very fast. We believe that 10 iterations is empirically enough for IB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS AND FUTURE WORK</head><p>The tremendous growth of the World Wide Web in China has raised the need for classifying and organizing Chinese Web space via classification techniques. In this paper we put forward a technique for the Chinese Web mining task to exploit the abundant labelled information in English. In particular, we have developed a novel method known as the information bottleneck technique to address the topic drift and different feature-space problems across two languages. Our method brings out a common part between the Chinese and English Web pages, which can be used to encode similar pages in different languages into the same codewords (class labels). An iterative algorithm is presented to optimize the objective function and therefore solve this problem. The experimental results show that our method can effectively improve existing methods in general, including five binary and three multi-class problems.</p><p>To extend our work, we wish to modify our method to achieve a global optimal value. It is also interesting to conduct more experiments in other language pair (e.g. French vs English, which does not suffer the word segmentation problem). Moreover, our method has the potential to be effective for the cross-language information retrieval problem.  we have</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 : The information bottleneck. Here, X is the signals to be encoded, and?Xand? and?X is the codewords.</head><label>2</label><figDesc>Figure 2: The information bottleneck. Here, X is the signals to be encoded, and?Xand? and?X is the codewords. In classification, X is the set of instances, and?Xand? and?X is the prediction labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 : The cross-language classification problem. Here, Xe is the set of the Web pages in English and Xc is the set of the Web pages in Chinese. It is assumed that there is some common part between English and Chinese Web pages.</head><label>3</label><figDesc>Figure 3: The cross-language classification problem. Here, Xe is the set of the Web pages in English and Xc is the set of the Web pages in Chinese. It is assumed that there is some common part between English and Chinese Web pages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Definition 2 .</head><label>2</label><figDesc>The joint probability distribution of X and Y under the categorizatio? X is denoted by?pby? by?p(X, Y ), where Algorithm 1 The Cross-Language Text Classification (IB) Algorithm Input: English Web pages Xe; Chinese Web pages Xc; an existing translator T ; the number of iterations N . Output: the final hypothesis h f : Xc ¡È Xe ¡ú C. 1: Translate Xc into English:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where x ¡Ê ? x, and p(x|?xx|?x) = p(x) p(? x) since x totally depends o?o? x. c . 2: Train an initial hypothesis h (0) based on Xe by super- vised learning method (e.g. naive Bayes classifiers [17]). 3: Initialize the probability distributio? p (0) based on h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 : The scheme of the IB-based cross-language classification algorithm.</head><label>4</label><figDesc>Figure 4: The scheme of the IB-based cross-language classification algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The instance-feature co-occurrence density for the Games vs News data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>5 : Macro-average precision, recall and F1- measure for each classifier on each multiple-class classification data set.</head><label>5</label><figDesc>The training documents for NBC and IB are Xe and the test data for them are X 0.80 ?measure F 1 0.75 T c . Note that, SVM and TSVM are not included here, because they are designed for binary classifi- cation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>=</head><label></label><figDesc>D(? p (t+1) (Y |?x|?x)||?p||?p (t) (Y |?x|?x)) ¡Ý 0 . X X X Thus, D(p(X, Y )||?p||?p(X, Y )) = p(x, y) log p(x, y) ? p(x, y) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>?</head><label></label><figDesc>x¡Ê?Xx¡Ê? x¡Ê?X y¡ÊY x¡Ê?xx¡Ê?x D(p(X, Y )||?p||?p (t) (X, Y )) ? ¡Ý X X X Since p(x) p(y|x) ? x:h (t+1) x¡Ê?xx¡Ê?x y¡ÊY " log p(y|x) + log 1 ? p (t+1) (y|?xy|?x) ? p(x, y) = p(? x, y)p(x|?xx|?x) = p(? x, y) p(x) p(? x) X X X = p(x) p(y|x) log p(y|x) ? p (t+1) (y|?xy|?x) = p(x)p(y|?xy|?x) = p(x)? p(y|?xy|?x) , ? x:h (t+1) x¡Ê?xx¡Ê?x y¡ÊY =D(p(X, Y )||?p||?p (t+1) (X, Y )) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Translator Basic Classifier N times</head><label></label><figDesc>, it is only able to Optimize the objective function and give new labels for Chinese Web Pages</figDesc><table>Unlabeled 
Chinese Web 
Pages 

Unlabeled 
Chinese Web 
Pages in 
English 
New Labels for 
Chinese Web 
pages 

Labeled 
English Web 
Pages 

Output the labels 
for Chinese Web 
pages 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>The descriptions for all the categories in 
ODP, including Chinese and English ones. Note 
that, all the Chinese Web pages as well as their 
category labels were translated into English using 
Google Translator. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The composition for each data sets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The most frequent (stemmed) features in the Chinese and English Web pages for each ODP category. 
All the Chinese Web pages have already been translated into English using Google Translator. 

Data Set 
Precision 
Recall 
F 1 -Measure 
NBC SVM TSVM 
IB 
NBC SVM TSVM 
IB 
NBC SVM TSVM 
IB 
Games vs News 
0.749 0.737 
0.747 
0.798 
0.747 0.739 
0.779 
0.817 
0.748 0.738 
0.762 
0.807 
Arts vs Computers 
0.731 0.783 
0.768 
0.767 
0.728 0.801 
0.785 
0.782 
0.730 0.792 
0.776 
0.774 
Recreation vs Science 
0.836 0.874 
0.883 
0.903 
0.832 0.877 
0.891 
0.906 
0.834 0.876 
0.887 
0.905 
Computers vs Sports 
0.783 0.669 
0.611 
0.844 
0.800 0.840 
0.759 
0.873 
0.792 0.745 
0.677 
0.858 
Reference vs Shopping 0.911 0.743 
0.650 
0.929 
0.827 0.858 
0.766 
0.859 
0.867 0.797 
0.703 
0.893 
Average 
0.802 0.761 
0.732 
0.848 0.787 0.823 
0.796 
0.847 0.794 0.790 
0.761 
0.847 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 : Macro-average precision, recall and F1-measure for each classifier on each binary classification data set. The training documents for NBC, SVM and IB are Xe and the test data for them are X T c . TSVM is trained on labeled Xe and unlabeled X T c , and tested on X T c .</head><label>4</label><figDesc></figDesc><table>Data Set 
Precision 
Recall 
F 1 -Measure 
NBC 
IB 
NBC 
IB 
NBC 
IB 
3 Categories 0.647 
0.661 
0.630 
0.665 
0.638 
0.663 
4 Categories 0.445 
0.592 
0.570 
0.648 
0.500 
0.619 
5 Categories 0.550 
0.608 
0.488 
0.582 
0.517 
0.627 
Average 
0.547 0.620 0.563 0.632 0.552 0.636 

1.00 

0.95 

Arts vs Computers 
Recreation vs Science 
3 Categories 

0.90 

0.85 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> According to the report by CNNIC in January 2007: http://www.cnnic.net.cn/uploadfiles/pdf/2007/2/13/95522.pdf ACM 978-1-60558-085-2/08/04.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGEMENTS</head><p>Qiang Yang would like to thank the support of Hong Kong RGC Grant 621307. We also thank the anonymous reviewers for their great helpful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D(p(X, Y )||?p||?p(X, Y )) =</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>p(x)p(y|x) log p(x)p(y|x) p(x)? p(y|?xy|?x)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. PROOF TO THEOREM 1</head><p>Proof. Based on Lemma 2, we have</p><p>From the Steps 6 and 9 in Algorithm 1,</p><p>Thus,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>In this appendix, we provide the detailed proof to Lemmas 1 and 2, and Theorem 1.</p><p>Proof.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A generalized maximum entropy approach to bregman co-clustering and matrix approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merugu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="509" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-Lingual Text Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings ECDL</title>
		<imprint>
			<biblScope unit="volume">200</biblScope>
			<biblScope unit="page" from="126" to="139" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Annual Conference on Computational Learning Theory</title>
		<meeting>the Eleventh Annual Conference on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Annual Workshop on Computational Learning Theory</title>
		<meeting>the Fifth Annual Workshop on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nearest neighbor pattern classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="21" to="27" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>Wiley-Interscience</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Question Classification in English-Chinese Cross-Language Question Answering: An Integrated Genetic Algorithm and Machine Learning Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 IEEE International Conference on Information Reuse and Integration</title>
		<meeting>the 2007 IEEE International Conference on Information Reuse and Integration</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="203" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Enhanced word clustering for hierarchical text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Information-theoretic co-clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cross language Text Categorization by acquiring Multilingual Domain Models from Comparable Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gliozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACL Workshop on Building and Using Parallel Texts (in conjunction of ACL-05)</title>
		<meeting>of the ACL Workshop on Building and Using Parallel Texts (in conjunction of ACL-05)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Svm Light</surname></persName>
		</author>
		<ptr target="http://svmlight.joachims.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth International Conference on Machine Learning</title>
		<meeting>the Sixteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Newsweeder: Learning to filter netnews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Machine Learning</title>
		<meeting>the Twelfth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<ptr target="http://www.daviddlewis.com/" />
		<title level="m">Reuters-21578 test collection</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Representation and learning in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<pubPlace>Amherst, MA, USA</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Advanced learning algorithms for cross-language patent retrieval and classification. Information Processing and Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1183" to="1199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic search engine performance evaluation with click-through data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1133" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploring in the weblog space by detecting informative and affective articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="281" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Text classification from labeled and unlabeled documents using em</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="103" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<ptr target="http://www.dmoz.com/" />
	</analytic>
	<monogr>
		<title level="j">ODP. Open directory project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-language text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Haji?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="645" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An algorithm for suffix stripping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<publisher>Program</publisher>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4.5: Programs for Machine Learning</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An em based training algorithm for cross-language text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rigutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005</title>
		<meeting>the 2005</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">ACM International Conference on Web Intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="529" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The Information Bottleneck: Theory and Applications. Unpublished doctoral dissertation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<pubPlace>Jerusalem, Israel</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Hebrew University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Maximum likelihood and the information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The information bottleneck method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-seventh Annual Allerton Conference on Communication, Control and Computing</title>
		<meeting>the Thirty-seventh Annual Allerton Conference on Communication, Control and Computing</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="368" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two-pass named entity classification for cross language question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NTCIR-6 Workshop Meeting</title>
		<meeting>NTCIR-6 Workshop Meeting</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="168" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A comparative study on feature selection in text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Fourteenth International Conference on Machine Learning</title>
		<meeting>Fourteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno>1530</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
