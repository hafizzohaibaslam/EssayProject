<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-07-16T23:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Budget Allocation for Maximizing Influence of Advertisements</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Hatano</surname></persName>
							<email>hatano@nii.ac.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuro</forename><surname>Fukunaga</surname></persName>
							<email>takuro@nii.ac.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<addrLine>Japan JST</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ERATO</orgName>
								<orgName type="institution" key="instit2">Kawarabayashi Large Graph Project</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Budget Allocation for Maximizing Influence of Advertisements</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The budget allocation problem is an optimization problem arising from advertising planning. In the problem, an advertiser has limited budgets to allocate across media, and seeks to optimize the allocation such that the largest fraction of customers can be influenced. It is known that this problem admits a (1 1/e)-approximation algorithm. However, no previous studies on this problem considered adjusting the allocation adaptively based upon the effect of the past campaigns, which is a usual strategy in the real setting. Our main contribution in this paper is to analyze adaptive strategies for the budget allocation problem. We define a greedy strategy, referred to as the insensitive policy, and then give a provable performance guarantee. This result is obtained by extending the adaptive submodularity, which is a concept studied in the context of active learning and stochastic optimization, to the functions over an integer lattice.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Suppose an advertiser wishes to maximize the influence on customers, but has limited budgets to allocate across media (e.g., webpages, television, or newspapers). The main question, called the budget allocation problem, is how to select media considering budget constraints such that the largest fraction of customers can be influenced; that is, how can a budget achieve the maximum reach?</p><p>The main difficulty behind the budget allocation problem is the complex dynamic of influence from media to customers. This dynamic has been investigated in the framework of the influence maximization problem, which was first introduced by <ref type="bibr" target="#b1">Domingos and Richardson [2001;</ref><ref type="bibr" target="#b1">2002]</ref>. A seminal work by Kempe, Kleinberg, and Tardos <ref type="bibr">[2003]</ref> formulated the influence maximization problem in the framework of submodularity. The submodularity concept represents a certain diminishing marginal return property in discrete settings. <ref type="bibr">Kempe et al.</ref> showed that the expected number of customers influenced by media is represented by a submodular set function. On the basis of this observation, they proved an approximation guarantee of a polynomial-time greedy algorithm for the influence maximization problem.</p><p>These studies on the influence maximization problem motivated the work of Alon, <ref type="bibr" target="#b0">Gamzu, and Tennenholtz [2012]</ref>, who formulated the budget allocation problem in the bipartite influence model as another combinatorial optimization problem and provided a provable approximation algorithm. There was difficulty in expressing their problem setting using submodular functions because submodularity is usually defined for combinations of objects whereas budget allocations are assignments of budgets to media. However, Soma et al. <ref type="bibr">[2014]</ref> showed that the problem setting of Alon et al. can also be expressed in the framework of submodularity. They utilized submodularity functions over an integer lattice, which are more general than submodular set functions.</p><p>Despite these developments, the previous studies on the budget allocation problem have a crucial limitation. In their settings, advertisers have to assign their entire budget at once at the beginning of the process. However, in reality, advertisers routinely adjust their strategy when they see changes in the dynamic or when something unexpected happens. For example, in the US presidential campaign of 2012, both Obama and Romney spent half a billion dollars for TV ads <ref type="bibr">[The Washington Post, 2012]</ref>. In particular, they invested huge amounts of money in "swing" states. For these states, both campaigns changed their strategy for TV ads every day, according to their polls (i.e., either gaining momentum or not). In this case, momentum changed frequently, and the dynamic was a deciding factor in their strategy. Hence, both the campaigns changed their strategy adaptively every day. In this paper, we are motivated by this observation. We aim to consider adaptive strategies to address the budget allocation problem.</p><p>Adaptivity has been already considered in the framework of submodularity. <ref type="bibr" target="#b1">Golovin and Krause [2011b]</ref> defined a concept of adaptive submodularity, and showed that a greedy adaptive algorithm has a theoretical approximation guarantee if the objective function is adaptive monotone submodular. After their initial work, numerous studies further investigated algorithms for optimization problems with adaptive submodular functions <ref type="bibr" target="#b1">[Golovin and Krause, 2011a;</ref><ref type="bibr" target="#b1">Gabillon et al., 2013;</ref><ref type="bibr" target="#b1">2014;</ref><ref type="bibr">Gotovos et al., 2015]</ref>, as well as their applications <ref type="bibr" target="#b1">[Golovin et al., 2010;</ref><ref type="bibr" target="#b1">Chen and Krause, 2013;</ref><ref type="bibr" target="#b1">Chen et al., 2014;</ref><ref type="bibr" target="#b1">Deshpande et al., 2014;</ref><ref type="bibr" target="#b1">Krause et al., 2014;</ref><ref type="bibr" target="#b1">Chen et al., 2015]</ref>. Because the model of Golovin and Krause contains the adaptive setting of <ref type="bibr" target="#b1">Kempe et al. [2003]</ref>, adaptive strategies have been already analyzed in the influence maxi-mization problem. However, adaptive strategies for the budget allocation problem have not been captured by their model. Thus, we need to formulate a new concept of adaptive submodularity that can model the budget allocation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>In this paper, we consider adaptive strategies in the budget allocation problem in the bipartite influence model introduced by Alon et al. <ref type="bibr">[2012]</ref>. To this end, we define adaptive submodularity of functions over integer lattices, which is a new concept that extends both the adaptive submodularity given by <ref type="bibr" target="#b1">Golovin and Krause [2011b]</ref> and submodularity over integer lattice used in Soma et al. <ref type="bibr">[2014]</ref> (see Section 3.3). This concept captures the objective function in the adaptive version of the budget allocation problem. Hence we obtain a good adaptive strategy for the budget allocation problem by designing a strategy for maximizing an adaptive submodular function over an integer lattice.</p><p>In many variants of the submodular maximization problems, the greedy algorithms achieve good performance both in practice and in theory. Thus we analyze the performance of greedy adaptive algorithms for maximizing adaptive monotone submodular functions over integer lattices. For our problem, a greedy strategy repeats allocating a certain amount of budget to a medium so that the increase of the influence per allocated budget is maximized. In our setting, the strategy is given a new feedback when it allocates a unit amount of budget. It is natural to update the strategy each time a new feedback is given. We call such a strategy sensitive greedy strategy. On the other hand, an insensitive greedy strategy ignores feedbacks until a certain proportion of a budget has been allocated to a media. Surprisingly, we can show both theoretically and empirically that several typical sensitive greedy strategies are inferior even to the non-adaptive algorithms. Our proposal algorithms are sort of insensitive greedy strategies.</p><p>More specifically, we present the following two variations of the insensitive greedy algorithms. rithms are better than any non-adaptive algorithms by more than 58%; see at the end of Section 4.</p><p>Let us explain why the former variation of our insensitive greedy algorithm violates the budget constraints. Our budget constraint corresponds to knapsack constraints in the submodular function maximization. For the non-adaptive setting of maximizing submodular functions subject to the knapsack constraints, the (1 1/e)-approximation is achieved only by combining a greedy algorithm with a partial enumeration of solutions in the initial step. However, in the adaptive setting, this partial enumeration is not permitted; hence, it is difficult to achieve (1 1/e)-approximation. Therefore, we propose violating the budget constraints by a factor of at most two. The similar approach was adopted in <ref type="bibr" target="#b1">Golovin and Krause [2011b]</ref> for the adaptive maximization of submodular set functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Organization</head><p>The rest of this paper is organized as follows. Section 2 introduces the budget allocation problem in the bipartite influence model and the submodular functions over an integer lattice. Section 3 formulates our problem setting and defines adaptive submodularity over an integer lattice. Section 4 analyzes the adaptive greedy algorithms. Section 5 compares performance of the algorithms through computational experiments. Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Budget allocation problem and submodular functions</head><p>In this section, we introduce the budget allocation problem proposed by Alon et al. <ref type="bibr">[2012]</ref>, and slightly extended by Some et al. <ref type="bibr">[2014]</ref>. We also explain a relationship with the submodular functions over an integer lattice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bipartite influence model of the budget allocation problem</head><p>? An algorithm outputs a budget allocation that achieves (1 1/e)-approximation. That is, its expected objective value is at least (1 1/e) times that achieved by arbitrary adaptive algorithms. The allocation may violate the budget constraints by a factor of at most two; however, its expected cost is at most the given budget upper limit. (see Theorem 3)</p><p>Let Z + and R + be the sets of non-negative integers and real numbers, respectively. For a finite set V , let Z V + denote the set of non-negative integer vectors, where each component is indexed by an element in V . For vectors x, y 2 Z V ? Another algorithm outputs a budget allocation of an approximation ratio (e 1)/(2e). The allocation is guaranteed to satisfy the budget constraints. (see Theorem 4) V Alon et al. <ref type="bibr">[2012]</ref> showed that a non-adaptive greedy algorithm achieves (1 1/e)-approximation for the budget allocation problem. We note that our guarantee on the first insensitive policy is superior to the one obtained by Alon et al. although their approximation ratios match. This is because our algorithm is compared with adaptive algorithms whereas Alon et al. compared only non-adaptive algorithms. So if the optimal adaptive algorithm is strictly better than the nonadaptive one (which is quite often the case), then our guarantee is better. Indeed, there is an instance in which our algo-</p><formula xml:id="formula_0">+ , we write x ? y if x(v) ? y(v) for all v 2 V . For an integer i 2 Z + , let [i] denote {0, 1, . . . , i}.</formula><p>We consider a bipartite graph (V, U ; E), where V is the set of media, U is the set of customers, and E is the set of edges between V and U . We are given a budget k 2 R + , a cost function c : V ? Z + ! R + , and a vector b 2 Z + representing the numbers of slots of each media. In addition, for each edge vu 2 E that joins nodes v 2 V and u 2 U , we are given a probability function</p><formula xml:id="formula_1">q vu : [b(v)] ! [0, 1].</formula><p>We assume that a media v has b(v) slots in total. It costs P j2 <ref type="bibr">[i]</ref> c(v, j) to buy i slots of v. We allocate a total budget of k to the media. For notational convenience, we let c(x) denote</p><formula xml:id="formula_2">P v2V P j2[x(v)] c(v, j) for x 2 Z V + .</formula><p>The allocation can be represented by a vector x 2 Z V + such that x ? b and c(x) ? k; if x(v) = i, it represents that we buy i slots of media v.</p><p>Let N (v) denote the set of neighbors of a node v in the bipartite graph. If x(v) slots of media v are bought, v attempts to influence each customer u 2 N (v) x(v) times. For i 2 [b(v)] and u 2 N (v), q vu (i) represents the probability that the i-th trial of media v to influence customer u succeeds. Here, we assume that each trial is independent. g(x) is defined as the expected number of influenced customers when the budget allocation is x. That is, over an integer lattice, which is summarized in the theorem below. For v 2 V , let v denote the vector in Z V + such that v (v) = 1 and v (u) = 0 for each u 2 V \ {v}. Theorem 2 ( <ref type="bibr">[Soma et al., 2014]</ref></p><formula xml:id="formula_3">). If f : Z V + ! R + is mono- tone submodular, then it satisfies f (x _ k v ) f (x) f (y _ k v ) f (y) for any k 2 Z + , v 2 V , and x, y 2 Z V + 0 with x ? y. x(v) 1 g(x) := X @ 1 Y Y A . (1) u2U v2N (u) i=1 (1 q vu (i))</formula><p>We note that the monotone submodular function f over an integer lattice does not always satisfy the component-wise convexity represented by</p><formula xml:id="formula_4">f (x + v ) f (x) f (x + 2 v ) f (x + v ) for x 2 Z V</formula><p>The budget allocation problem in the bipartite influence model seeks an allocation <ref type="bibr">et al., 2012]</ref>, Alon et al. called this model by the source-side influence model to distinguish from another model they called the target-side influence model. Since we consider only the source-side influence model, we simply call it by the bipartite influence model. In the original definition, the costs for buying slots are not considered, i.e., c(v, i) = 1 for all v 2 V and i 2 [b(v)]. Thus our definition is more general than the original one.  3 Adaptive submodularity over an integer lattice</p><formula xml:id="formula_5">x 2 Z V + that maximizes g(x) sub- ject to x ? b and c(x) ? k. Remark 1. In [Alon</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Submodular functions over an integer lattice</head><formula xml:id="formula_6">For two vectors x, y 2 Z V + , let x _ y denote the vector in Z V + defined by (x _ y)(v) = max{x(v), y(v)} for v 2 V , and x ^ y denote the vector in Z V + defined by (x ^ y)(v) = min{x(v), y(v)} for v 2 V . Let f : Z V + ! R + be a</formula><formula xml:id="formula_7">f (x) + f (y) f (x ^ y) + f (x _ y) for all x, y 2 Z V + . (2) f is considered monotone if f (x) ? f (y) for any x, y 2 Z V + with x ? y.</formula><p>For a finite set V , a set function h : 2 V ! R + is called submodular if</p><p>Our main contribution in this paper is to analyze the adaptive strategies in the budget allocation problem. In this section, we first define the adaptive setting of the budget allocation problem in the bipartite influence model. Then, we extend it to the submodular maximization problem.</p><formula xml:id="formula_8">h(X) + h(Y ) h(X \ Y ) + h(X [ Y ) for all X, Y 2 2 V .</formula><p>(3) When x and y are restricted to vectors over a Boolean lattice (i.e., 2 3.1 Adaptive setting of the bipartite influence model V ), the condition (2) is equivalent to (3) . Hence the submodularity over an integer lattice includes the concept of the submodularity for set functions.</p><p>Soma et al. <ref type="bibr">[2014]</ref> showed that the budget allocation problem in the bipartite influence model can be captured by the submodular functions over an integer lattice. More concretely, they proved the following theorem. Theorem 1 ( <ref type="bibr">[Soma et al., 2014]</ref>). The function g defined by (1) is monotone submodular over an integer lattice.</p><p>For the set functions, it is known that h satisfies condition (3) if and only if</p><formula xml:id="formula_9">h(X [ {v}) h(X) h(Y [ {v}) h(Y ) for any X, Y 2 2</formula><p>In the adaptive setting, the inputs of the problem are same as the non-adaptive setting of the bipartite influence model; namely, a bipartite graph</p><formula xml:id="formula_10">(V, U ; E), b 2 Z V + , k 2 R + , c : V ? Z + ! R + , and q vu : [b(v)] ! [0, 1] for each vu 2 E. A budget allocation x 2 Z V V with X ? Y and v 2 V \ Y .</formula><p>This property is known as the decreasing marginal gain property of submodular set functions. <ref type="bibr">Soma et al. also</ref> showed that this property is extended to the monotone submodular functions + is initialized to the all-zero vector. Then, we allocate the total budget k to the media sequentially. If we buy one slot of a media v 2 V when x(v) = i, x(v) is increased to i + 1, and the media v activates each customer u 2 N (v) in probability q vu (i + 1). In the adaptive setting, we can observe which customers in N (v) got influenced by this trial immediately after increasing x(v) from i to i + 1, and we can change the behavior in the subsequent steps based on the observation. Thus, our aim is to find a good policy, which describes how we behave for each observation.</p><p>This setting is natural in the marketing. When an advertising campaign is committed, the advertiser can observe how many customers are influenced (e.g., buy a product, subscribe to a service), and profile of the influenced customers can be easily obtained in most cases. The advertiser changes the strategies based on the observation. Particularly, this applies well to the internet advertising, wherein real-time marketing is widely used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adaptive setting of the maximization problem over an integer lattice</head><p>For the sake of generality, we discuss the adaptive policies in the budget allocation problem in the framework of the submodular functions. We extend the budget allocation problem to a maximization problem of a stochastic objective function over an integer lattice subject to a knapsack constraint.</p><p>We then define the adaptive monotonicity and the adaptive submodularity of a stochastic objective function, and prove that the objective function defined from the budget allocation problem satisfies these properties. First, let us introduce the problem setting in the general framework. Let R be a set of random variables. The range of variables in R is denoted by S. We call the value of a variable r 2 R by the state of r. For r 2 R and s 2 S, let p r (s) denote the probability that the variable r is in the state s. In the problem, the states of the random variables are not observed in advance. Initially, the available information is the probabilities p r (s) for all r 2 R and s 2 S.</p><p>We represent the states of all random variables in R by a function : R ! S, which we refer to as full realization. The objective function depends on the full realization. Let f : Z In the budget allocation problem with the bipartite influence model, there is a random variable for each pair of vu 2 E and i 2 {1, . . . , b(v)}. Hence let R denote {(vu, i) : vu 2 E, i 2 {1, . . . , b(v)}} by abusing the notation. The state of each pair (vu, i) 2 R represents that the i-th trial of v to influence u succeeds or not. Let S := {&gt;, ?}, where &gt; and ? respectively denote "success" and "failure." Recall that we are given a probability function q vu : Z + ! [0, 1] for each vu 2 E in the budget allocation problem. The state of (vu, i) is &gt; in probability q vu (i). Let : R ! S be a full realization. The probability that occurs is</p><formula xml:id="formula_11">p() := Y q vu (i) Y (1 q v 0 u 0 (i 0 )). (vu,i)2R (vu,i)=&gt; (v 0 u 0 ,i 0 )2R (v 0 u 0 ,i 0 )=?</formula><p>When the budget allocation is x 2 Z V + , we can observe the states of (vu, i) for all vu 2 E and i 2 {1, . . . , x(v)}; i.e.,</p><formula xml:id="formula_12">O ,x = {(vu, i) : vu 2 E, i 2 {1, . . . , x(v)}}. Notice that O ,x</formula><note type="other">is monotone with respect to x. The number of influenced customers when the budget allocation is x 2 Z V + is represented by</note><formula xml:id="formula_13">g (x) = |{u 2 U : 9(vu, i) 2 O ,x , (vu, i) = &gt;}| . (4)</formula><p>Maximizing the number of influenced customers adaptively is equivalent to maximizing g .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptive monotonicity and adaptive</head><p>submodularity over an integer lattice In the problem, the solution x is initialized by x(v) := 0 for all v 2 V , and the policy repeats increasing a component of x by one while satisfying the constraints; i.e., it is prohibited to decrease x, and x must always satisfy c(x) ? k and x ? b for a given cost function c : V ? Z + ! R + , a budget k 2 R + , and the numbers of slots b 2 Z such that for each ? 2 R, (?) is the state of ? if it is already observed, and (?) = ? if it is not yet observed. We refer to such a function as (partial) realization. The domain of a partial realization , denoted by dom( ), indicates {? 2 R : (?) 6 = ?}. In other words, if represents the observation when the full realization is : R ! S and the solution is</p><formula xml:id="formula_14">x 2 Z V + , then dom( ) = O ,x . Let ? V + . When x(v)</formula><p>is increased from i 1 to i, the policy observes the states of the variables in O ,x \ O ,x 0 , where x (resp., x and denote the set of all partial realizations and the set of all full realizations, respectively. We say that a realization extends another realization</p><formula xml:id="formula_15">0 if 0 (?) = (?) for each ? 2 dom( 0 ). If extends 0 0</formula><p>) denotes the vector after (resp., before) the increase. The behavior of the policy in the subsequent steps depends on the observation.</p><p>Here, let ? be a policy. We denote by x ,? the vector output by the policy ? when the states of the random variables in R are represented by : R ! S. Let f avg (?) = E[f (x ,? )], where the expectation depends on the randomness of the variables in R. Hence f avg (?) denotes the expected value of the objective function obtained by running the policy ?. We may consider a randomized policy as ?, and, in this case, the expectation also depends on the randomness of ?. We measure the performance of ? by f avg (?).</p><p>, we use the notation ? 0 . Recall that a full realization happens in probability p(). If a realization 2 ? is not full, we assume that happens in probability P {p() : 2 , ? }. We denote this probability by p( ).</p><formula xml:id="formula_16">For a vector x 2 Z V + , let ? x denote { 2 ? : 9 2 , dom( ) = O ,x }. For x 2 Z V + , 2 ? x , v 2 V , and i 2 [b(v)], define (v, i | x, ) := E [f (x _ i v ) f (x) | 2 , ? ] .</formula><p>In other words, (v, i | x, ) is the expected gain we obtain by increasing x(v) to i, conditioned that the current realization is and the current solution is x. Definition 1 (adaptive monotonicity). f := {f : 2 } is adaptive monotone (with respect to distribution p(), 2 ) if (v, i | x, ) 0 holds for any v 2 V , i 2 [b(v)], x 2 Z increasing y(v) to i. Then, we have </p><formula xml:id="formula_17">f avg (?@? 0 )f avg (?) ? = X</formula><p>x with p( ) &gt; 0. Definition 2 (adaptive submodularity). f := {f : 2 } is adaptive submodular (with respect to distribution p(), 2</p><formula xml:id="formula_18">) if (v, i | x, ) ? (v, i | y, ) holds for any v 2 V , i 2 [b(v)], x, y 2 Z 2 E[f (?@? 0 ) f (?)]p() ? = p( ) X V E[f (x_i v )f (x)]p( | )</formula><p>+ with x y, and 2 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adaptive greedy policies</head><p>Recall that x ,? denotes the vector output by a policy ? for a full realization . For x 2 Z + , 2</p><p>x and a policy ?, let</p><formula xml:id="formula_19">(? | x, ) := E [f (x _ x ,? ) f (x) | 2 , ? ] .</formula><p>As mentioned in Section 1, integer lattice setting introduces two types of greedy policies: sensitive and insensitive policies. In this section, we present two variations of insensitive policies. One achieves approximation factor (1 1/e), which matches the best ratio for the non-adaptive setting. However, it may violate the knapsack constraint c(x) ? k. We show that the vector x output by the policy always satisfies c(x) ? 2k, and E[c(x)] ? k. The other policy always outputs a feasible solution. However, its approximation factor is (e 1)/(2e).</p><p>First, let us prove several preparatory lemmas. For two policies ? and ? In other words, (? | x, ) is the expected gain we obtain when, after selecting vector x and observing , we run policy ? ignoring the information given from . Note that the expectation depends on the randomness of realizations, and is conditioned on being observed after selecting x. When ? is a randomized policy, the expectation also depends on the randomness of ?. </p><formula xml:id="formula_20">2 Z V . + . Then, we run ? 0 (? ? | x, ) ? E[c(x ,? ? ) | ? ] max (v,i)2V ?Z+ (v, i | x, ) P j2[i] c(v, j)</formula><p>from a fresh start to obtain x ? 0 2 Z </p><formula xml:id="formula_21">(v, i | x, ). Therefore, f avg (?@? 0 ) f avg (?) is expressed by P -portion of ? increases x 0 (v) to i. The contribution of this operation to (? ? | x, ) is at most (v, i | x, ) by the adaptive submodularity. Therefore, (? ? | x, ) ? P (v,i)2V ?Z+ w(v, i) i | x, ). Note that w(v, i) 2 [0, 1] for each (v, i) 2 V ? Z + , and E[c(x ,? ? ) | ? ] = P (v,i)2V ?Z+ w(v, i) P j2[i] c(v, j). Therefore, P x2Z V + 2 ? x w(x, ) i | x, ).</formula><p>Since f is adaptive monotone, (v, i | x, ) 0 holds for any x and with w(x, ) &gt; 0. </p><formula xml:id="formula_22">Hence f avg (?@? 0 ) f avg (?). Suppose that (v, i | x, ) &lt; 0 for some (v, i) 2 V ? Z + , x 2 Z (? ? | x, ) ? X (v,i)2V ?Z+ w(v, i) i | x, ) = X w(v, i)(</formula><formula xml:id="formula_23">c(v, j)) (v, i | x, ) P j2[i] c(v, j) ? E[c(x ,? ? ) | ? ] max (v 0 ,i 0 )2V ?Z+ (v 0 , i 0 | x, ) P j2[i 0 ] c(v 0 , j) . 0 terminates after</formula><p>For a deterministic policy ? and i 2 <ref type="bibr">[k]</ref>, let ? i denote the truncation of ? defined as follows. Fixing a full realization , we define how ? i behaves for . Let x be the temporal solution kept by ? during its run for . Consider the moment ? when c(x) exceeds i. Suppose that ? is increasing x(v) at moment ?. Let ? 0 be the latest moment before ? at which ? increases a component of x other than x(v). If there is no such moment, ? 0 denotes the moment at which ? begins to run. Similarly, let ? 1 be the earliest moment after ? at which ? increases a component of x other than x(v). If there is no such moment, ? 1 denotes the moment at which ? terminates. Suppose that c(x) = i 0 at ? 0 , and x(v) = j 1 and c(x) = i 1 at ? 1 . Until ? 0 , ? i behaves as ?. Then, in probability (i i 0 )/(i 1 i 0 ), ? i increases x(v) to j 1 and terminates. Otherwise, ? i terminates without increasing x(v). Note that the truncation ? i outputs x such that E[c(x)] ? i for any full realization , where the expectation is over only the randomness of ? i .</p><p>We now define a policy ? as follows. We assume without loss of generality that c(b(v) v ) ? k holds for all v 2 V in the rest of this section. Starting from</p><formula xml:id="formula_24">x ? 0, ? chooses (v, i) 2 V ? Z + that maximizes (v, i | x, )/( P j2[i] c(v, j))</formula><p>and increases x(v) to i, where is the current realization. ? repeats this procedure and terminates when c(x) k holds. Our first proposal algorithm is its truncation ? k . We describe the details of ? k in Policy 1. Notice that the behavior of ? k depends on the observation in Step 8, and hence it is an adaptive policy.</p><p>is feasible, and it always outputs a vector x such that c(x) ? 2k.</p><p>Proof. Let j 2 {1, . . . , k}. We give an lower bound on f avg (? j ) f avg (? j1 ). Suppose that ? j has a solution x j 2 Z + and a realization j 2 ? xj when its last iteration is beginning, and (v j , i j ) is chosen in Step 4 of the last iteration. Let j 0 := c(x j ) and C := c(x j _ i j vj ) j 0 . The expected increase of the objective function in the last iteration of ? j is</p><formula xml:id="formula_25">(v j , i j | x j , j )(j j 0 )/C. If j 1 &gt; j 0</formula><p>, then ? j1 behaves in the same way as ? j until it enters the last iteration, which updates a solution x j to x j _ i j vj with probability (j 1 j 0 )/C. Hence the expected increase of the objective function in the last iteration is (v j , i j | x j , j )(j1j 0 )/C in this case. If j 1 = j 0 , ? j1 is the policy that does not execute the last iteration of ? j . In either case, the difference of the expected objective values achieved by ? j and ? j1 is</p><formula xml:id="formula_26">(v j , i j | x j , j ) C (v j , i j | x j , j ) P i2[ij ] c(v j , i) .<label>(5)</label></formula><p>Notice that we are fixing x j and j in this discussion, whereas x j and j depends on the randomness of the variables in R.</p><p>Taking the expectation of <ref type="formula" target="#formula_26">(5)</ref> over all full realizations, we have</p><formula xml:id="formula_27"># f avg (? j ) f avg (? j1 ) E " (v j , i j | x j , j ) P i2[ij ] c(v j , i)</formula><p>.</p><p>Policy 1 Bicriteria (1 1/e)-Approximation Policy Input: a finite set V , an adaptive monotone submodular function {f : Z Next, we give an upper bound on f avg (? j1 @? ? ) f avg (? j1 ). We again discuss with fixing x j and j . Suppose that ? j1 terminates with the realization ? for each ? 2 R 3: while c(x) &lt; k do and y satisfy 0 ? j and y x j . Hence, the adaptive submodularity of f indicates (? ? | y, 0 ) ? (? ? | x j , j ). By taking the expectation over all full realizations, we have 4:</p><formula xml:id="formula_29">(v, i) arg max i | x, )/ P j2[i] c(v, j) where the maximization is over all (v, i) 2 V ? Z + with i ? b(v) f avg (? j1 @? ? ) f avg (? j1 ) ? E[ ? | x j , j )].<label>(7)</label></formula><p>By Lemma 2, we have 5:</p><formula xml:id="formula_30">C P i j=x(v)+1 c(v, j) (? ? | x j , j ) 6:</formula><p>If c(x) + C &gt; k, output x and terminate in probability</p><formula xml:id="formula_31">1 (k c(x))/C ? E[c(x ,? ? ) | ? j ] max . 7: x(v) max{x(v), i} (v,i)2V ?Z+ (v, i | x j , j ) P i 0 2[i] c(v, i 0 ) 8:</formula><p>(?) the state of ? for each observed random variable ? 2 R 9: end while 10: output x <ref type="formula">(8)</ref> The maximum in the right-hand side of <ref type="formula">(8)</ref> is attained by (v j , i j ) by the definition. E[c(x ,? ? )] ? k for any full realization because ? ? is feasible. Hence, from <ref type="formula" target="#formula_29">(7)</ref> and <ref type="formula">(8)</ref>, we have</p><p>We consider a policy feasible if it outputs a vector x with E[c(x)] ? k for any full realization, where the expectation is over the inner randomness of the policy. The following theorem presents the (1 1/e)-approximation guarantee of Policy 1.</p><formula xml:id="formula_32"># f avg (? j1 @? ? ) f avg (? j1 ) ? k ， E " (v j , i j | x j , j ) P i 0 2[ij ] c(v j , i 0 )</formula><p>. Theorem 3. Let ? k be the policy presented in Policy 1. If f is adaptive monotone submodular, then</p><formula xml:id="formula_33">f avg (? k ) (1 1/e)f avg (? ? ) holds for any feasible policy ? ? . Moreover, ? k (9) Define ? j := f avg (? ? ) f avg (? j ). Note that ? j1 ? j = f avg (? j ) f avg (? j1 ). Since f is adaptive mono- tone, Lemma 1 implies that f avg (? ? ) ? f avg (? j1 @? ? ). Hence, f avg (? j1 @? ? ) f avg (? j1 ) ? j1</formula><p>. Therefore, (6) and <ref type="formula">(9)</ref> </p><formula xml:id="formula_34">indicate that ? j1 ? j ? j1 /k holds for all j = 1, . . . , k. This implies ? k ? ? 0 /e, which is equivalent to (1 1/e)f avg (? ? ) ? f avg (? k )</formula><p>. By its construction, ? k satisfies E[c(x ,? k )] ? k for any full realization , and hence ? k is feasible. Let x be the vector when the last iteration is beginning, and suppose that the last iteration increases x(v).</p><formula xml:id="formula_35">Then c(x ,? k ) ? c(x) + c(b(v) v )</formula><p>holds. c(x) &lt; k holds since otherwise ? k terminates before the last iteration, and we are assuming c(b(v) v ) ? k. Therefore, c(x ,? k ) ? 2k holds for any full realization .</p><p>Let ? 1 denote the policy that behaves as ? with the exception that ? 1 does not execute the last iteration of ?. In addition, let ? 2 be the policy that always outputs</p><formula xml:id="formula_36">b(v 0 ) v 0 , where v 0 maximizes f avg (b(v 0 ) v 0 ). We first prove f avg (?) ? f avg (? 1 ) + f avg (? 2 ). Clearly, f avg (? 1 ) = E[f (x 0 )]. Let y 2 Z V Remark 2. Each iteration of Policy 1 chooses a pair (v, i) that maximizes (v, i | x, )/ P j2[i] c(v, j). Since increas- ing x(v) to i costs P i j=x(v)+1 c(v, j), one may feel that (v, i) should maximize (v, i | x, )/ P i j=x(v)+1 c(v, j). In fact, we can prove the same guarantee even if (v, i) maximizes (v, i | x, )/ P i j=x(v)+1 c(v, j) because Lemma 2 still holds even after P j2[i] c(v, j) is replaced by P i j=x(v)+1 c(v, j).</formula><p>Policy 1 can be modified so that it always outputs a vector x with x ? b and c(x) ? k whereas the approximation guarantee is reduced to (e 1)/(2e). See Algorithm 2 for its detail.</p><p>+ denote the zero-vector, and denote the realization such that (?) = ? for all ? 2 R. The adaptive submodularity of</p><formula xml:id="formula_37">f indicates E[f (x 0 _ i v )] E[f (x 0 )] ? (v, i | y, ). Since f avg (? 2 ) = f avg (b(v 0 ) v 0 ) (v, i | y, )</formula><p>, we obtain the inequality.</p><p>For each full realization, the objective value of a vector output by ? 0 is at least f (? 1 ) in probability 1/2; otherwise, this is at least f (? 2 ). Therefore, Note that this does not consider the time for observing the states of random variables, which corresponds to checking whether each customer is influenced or not in the budget allocation problem.</p><formula xml:id="formula_38">f avg (? 0 ) (f avg (? 1 ) + f avg (? 2 ))/2 f avg (?)/2 (e 1)/(2e) ， f avg (? ? ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Policy 2 (e 1)/(2e)-Approximation Policy</head><p>Input: a finite set V , an adaptive monotone submodular function {f :</p><formula xml:id="formula_39">Z V + ! R + } 2 , k 2 R + , b 2 Z V + , and c : V ? Z + ! R + Output: x 2 Z V + such that c(x) ? k, and x ? b 1: x(v) 0 for each v 2 V 2: (?) ? for each ? 2 R 3: v 0 arg max v2V f avg (b(v) v ) 4: In probability 1/2, set x(v 0 ) b(v 0 )</formula><p>, and (?) the state of ? for each observed random variable ? 2 R 5: while 9(v, i) 2 V ? Z + such that (v, i | x, ) &gt; 0 and x _ i v is feasible do Remark 3. Asadpour and Nazerzadeh <ref type="bibr">[2009]</ref> showed that an adaptive algorithm is better than any non-adaptive algorithm by a factor e/(e1) &gt; 1.58 for the stochastic maximum k-cover problem, which is a special case of the budget allocation problem in the bipartite influence model. Their proof gave an instance of the stochastic maximum k-cover problem for which an adaptive policy achieves an objective value L and any non-adaptive solution does not achieve an objective value better than (1 1/e)L. In fact, the adaptive policy in their proof coincides with Policy 1. This indicates that our adaptive policies improve the objective value by at least 58% than arbitrary non-adaptive algorithms for those instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>(v, i)</p><formula xml:id="formula_40">arg max i | x, )/ P j2[i] c(v, j) where the maximization is over (v, i) 2 V ? Z + such that x _ i v is feasible</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>x(v) i 8:</p><p>(?) the state of ? for each observed random variable ? 2 R 9: end while 10: output x We implemented three adaptive policies: Policies 1 and 2, and a sensitive greedy policy defined as follows. Suppose that the policy maintains a vector x 2 Z </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Bipartite influence model</head><p>Proof. Define ? as described in the paragraph before Policy 1. Since f avg (?) f avg (? k ), Theorem 3 indicates that f avg (?) (1 1/e)f avg (? ? ) holds. Let x 0 be the vector kept by the policy ? when the last iteration begins, and suppose that ? increases x 0 (v) to i in the last iteration. Note that</p><formula xml:id="formula_41">f avg (?) = E[f (x 0 _ i v )].</formula><p>We run the algorithms for instances of the bipartite influence model. As a bipartite graph, we prepared a synthetic graph (V, U ; E) over a media set V and a customer set U . The degree distribution on V follows the power law, and |V | = 100 and |U | = 10, 000. We randomly chose b(v) from {20, 21, ， ， ， , 30} for each v 2 V . We prepared two  <ref type="figure">Figure 1</ref>: Experimental results on the bipartite influence model types of the probabilities q vu , vu 2 E: In the normal distribution, q vu (i) is given by exp((i 15) 2 /50)/ p 50? for each i 2 {1, . . . , 30} and vu 2 E; In the power law distribution, q vu (i) is given by exp(0.2(i 30))/10 for each i 2 {1, . . . , 30} and vu 2 E.</p><p>We compute budget allocations over 500 instances by the policies, and compare their objective values by f avg (?) for a policy ?. By preliminary experiments, we verified that 500 instances are enough to compare the average objective values. <ref type="figure">Figure 1</ref> indicates the average objective values when the budget k is set to a value in {20, 40, . . . , 200}. For setting the probabilities q vu of each edge vu, the normal distribution is used in (a), and the power law distribution is used in (b).</p><p>Although the theoretical performance guarantee of Policy 2 (in Theorem 4) is inferior to Policy 1 (in Theorem 3), we observe from the experimental results that performances of Policies 1 and 2 are almost same in all instances. Recall that Policy 2 always outputs a feasible allocation whereas Policies 1 does not. Moreover, they are clearly superior to the other two algorithms. In the normal distribution instances, the nonadaptive algorithm is sometimes worse even than the sensitive algorithm. In the power law distribution instances, the nonadaptive algorithm outperforms the sensitive algorithm, but it is clearly worse than Policies 1 and 2. tempt can be found in Demaine et al. <ref type="bibr">[2014]</ref>, but our model is different from theirs. <ref type="bibr" target="#b0">Alon et al. [2012]</ref> also mentioned that their bipartite influence model can be naturally extended to general graphs, but they do not seem to consider the multiple influence levels on all nodes.</p><p>For the experiments, we prepared a graph that represents user-user following information in Twitter <ref type="bibr">[KONECT, 2014]</ref>. Each node represents a user, and an arc from a node i to another node j represents that the user corresponding to i is followed by the user corresponding to j. The graph consists of 23370 nodes and 33101 arcs. In this graph, we choose 500 nodes that have largest out-degrees, and consider allocating budgets to these nodes. The parameters in the instances are set as follows: b(v) = 15 for all chosen nodes v, and the objective of the problem is defined as the maximization of the number of nodes influenced at least once. Budget k is set to a value in {20, 40, . . . , 200}, and the objective values are averaged over 500 instances for each k. <ref type="figure" target="#fig_5">Figure 2</ref> shows the results. Performance of Policies 1 and 2 are nearly equal, the non-adaptive policy is behind them, and the sensitive policy is clearly worse than the others. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">General influence model</head><p>Adaptive submodularity over an integer lattice is a useful notion, and it has numerous applications other than the budget allocation in the bipartite influence model. Taking advantage of this feature, we extend the budget allocation problem from the bipartite influence model to another influence model defined over general directed graphs. This general influence model can be captured by an adaptive monotone submodular function over an integer lattice, and hence all algorithmic results proposed in this paper can be applied to it. Here, we report empirical performance of the adaptive strategies and the non-adaptive (1 1/e)-approximation algorithm in this general influence model. We do not describe the detail of the model due to the space limitation. The non-adaptive setting of this model is obtained by introducing multiple influence levels into the independence cascade model studied by <ref type="bibr" target="#b1">Kempe et al. [2003]</ref> in a context of influence maximization. We note that a similar atIn this paper, we analyzed adaptive greedy policies for the budget allocation problem. Our contributions are based on the new concept of the adaptive submodularity; we extended the adaptive submodularity defined for the set functions to the functions over integer lattices. We believe that this new concept has other applications than the budget allocation problem. Indeed, we have already studied its applications to data summarization and sensor management. We will report them in the full version of the present paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>+</head><label></label><figDesc>and v 2 V . Therefore, in contrast with submodular set functions, submodular functions over an integer lattice are not considered convex in the direction of v . By Theorem 1, the maximization problem of f extends the budget allocation problem in the bipartite influence model. In the submodular maximization problem, we are given a mono- tone submodular function f : Z V + ! R + , and seek to find a solution x 2 Z V + that maximizes f (x). When the prob- lem demands a knapsack constraint, b 2 Z V + , k 2 Z + , and c : V ? Z + ! R + are given as inputs, and a solution x 2 Z V +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>function defined on the integer lattice where each component corre- sponds to an element in V . The function f is submodular (over an integer lattice) if must satisfy x ? b and c(x) ? k. The constraint is called a cardinality constraint if c(v, i) = 1 for all v 2 V and i 2 Z + . Soma et al. proposed a (1 1/e)-approximation algorithm for the submodular maximization problem with the knapsack constraints. If f is the function g defined by (1), the problem coincides with the budget allocation problem in the bipartite influence model. Hence the result of Soma et al. extends the algorithm given by Alon et al. [2012].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>V</head><label></label><figDesc>+ ! R + denote the objective function when the real- ization is : R ! S. Our goal is to find a policy that adap- tively computes a solution x 2 Z V + maximizing f (x) under given constraints. If a policy sets a solution to x 2 Z In this subsection, we define the adaptive monotonicity and the adaptive submodularity over an integer lattice. When the full realization is : R ! S and a policy sets the solution to x 2 Z V V + , it observes the state of random variables in a subset O ,x of S. Note that O ,x depends on the full realization and the solution x. O ,x is monotone with respect to x; namely, if x, y 2 Z ? V + , it observes the states of all random variables in O ,x . Let ? represent the fact that the state of a random variable is not yet observed, and define S ? := S [ {?}. We represent the observation available to the policy by a function : R ! S + satisfy x ? y, then O ,x ? O ,y for any : R ! S. The value of f (x) depends only on the states of variables in O ,x .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>It is not hard to observe that the objective function g := {g : 2 } defined by (4) in the bipartite influence model is adaptive monotone submodular. = p( ) i | x, ) &lt; 0. V ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Lemma 2 .</head><label>2</label><figDesc>Let f be an adaptive monotone submodular func- tion. Let x 2 Z V + , and 2 ? x be a realization with p( ) &gt; 0, and let ? ? be an arbitrary policy. Then, 0 , their concatenate ?@? 0 is defined as fol- lows. First, we run ? to obtain x ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>V+</head><label></label><figDesc>, ignoring the informa- tion from the observation during the run of ?. ?@? 0 outputs x ? _ x ? 0 . Proof. We define a policy ? as follows. Let x 0 Lemma 1. Function f := {f | 2 } is adaptive mono- tone if and only if f avg (?) ? f avg (? 0 @?) for all policies ? and ? 0 . be a solution vector kept by ?. Starting from x 0 ? 0, ? increases x 0 (v) from 0 to x(v) for all v 2 V , and terminates if the observed state of a random variable ? differs from (?). If it does not terminate after observing all random variables, it proceeds to run ? ? , while ignoring all information obtained up to this point. Note that ? proceeds to run ? ? Proof. Note that ?@? 0 always outputs the same solutions as ? 0 @? for any full realizations. Thus, f avg (?@? 0 ) = f avg (? 0 @?). Suppose that f is adaptive monotone. Assume that a vector x 2 Z in probability p( ). For each (v, i) 2 V ? Z + , let w(v, i) be the probability that ? terminates with x 0 (v) = i under the condition that the full realization extends . Note that if this event happens, the ? ? V + and a realization 2 ? x appears in probability w(x, ) as a solution and a realization kept by the ? 0 -portion of ?@? 0 in a certain moment. If ?@? 0 increases x(v) to i for , then the increase of the objective function is expected to be</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>.</head><label></label><figDesc>We define policies ? and ? 0 as follows. Let y denote the vector kept by the policies. Both policies first increase y(u) from 0 to x(u) for each u 2 V . If the observed state of some random variable ? 2 R differs from (?), both policies terminate. If they succeed to increase all u 2 V , ? terminates, and ? (v,i)2V ?Z+ j2[i]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>+</head><label></label><figDesc>! R + } 2 , k 2 R + , b 2 Z V + , and c : V ? Z + ! R + Output: x 2 Z and outputs a solution y. The ? -portion of ? j1 @? increases the ob- jective value by (? ? | y, 0 ) in expectation. Notice that 0 V + such that c(x) ? 2k and x ? b 1: x(v) 0 for each v 2 V 2: (?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Let us briefly discuss the running time of Policies 1 and 2. Suppose that (v, i | x, ) can be computed in O(T ) time for any v 2 V , i 2 [b(v)], x 2 Z V + , and 2 ? x . Then, both policies decide the next behavior in O(T P v2V b(v)) time in each iteration. In the budget allocation problem with the bi- partite influence model, we have T = O(|U | max v2V b(v)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>V</head><label></label><figDesc>+ and a realization 2 ? x Theorem 4. Let ? 0 denote Policy 2. If f := {f | 2 } is adaptive monotone submodular, ? 0 always outputs a vector x with x ? b and c(x) ? k, and it achieves f avg (? 0 ) (e 1)/(2e) ， f avg (? ? ) for any feasible policy ? when a certain iteration begins. In this iteration, the policy computes (v, i) := arg max i | x, )/( P j2[i] c(v, j)) and increases x(v) by one, where the maximization is taken over all (v, i) 2 V ? Z + such that x _ i v is feasible. In addition to the adaptive policies, we implemented a non- adaptive greedy (1 1/e)-approximation algorithm [Soma et al., 2014]. ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Experimental results on the general influence model with a Twitter graph</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optimizing budget allocation among channels and influencers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st International Conference on World Wide Web (WWW)</title>
		<imprint>
			<publisher>Arash Asadpour and Hamid Nazerzadeh</publisher>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="381" to="388" />
		</imprint>
	</monogr>
	<note>Maximizing Stochastic Monotone Submodular Functions. ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Approximation algorithms for stochastic boolean function evaluation and stochastic submodular set cover</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause ; Yuxin Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krause ; Chen</surname></persName>
		</author>
		<ptr target="http://www.washingtonpost.com/wp-srv/special/politics/track-presidential-campaign-ads-2012/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010 (NIPS)</title>
		<meeting><address><addrLine>Daniel Golovin, and Sarah J. Converse; AI Magazine</addrLine></address></meeting>
		<imprint>
			<publisher>Andreas Krause</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">4450</biblScope>
			<biblScope unit="page" from="351" to="359" />
		</imprint>
	</monogr>
	<note>31th International Conference on Machine Learning (ICML). The Washington Post, 2012] The Washington Post</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
