<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-07-17T00:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reinforcement Learning: A Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="1996">1996</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Pack</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaelbling</forename></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
							<email>mlittman@cs.brown.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smith</forename><surname>Hall</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Brown University Providence</orgName>
								<address>
									<postBox>Box 1910</postBox>
									<postCode>02912-1910</postCode>
									<region>RI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reinforcement Learning: A Survey</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Journal of Artiicial Intelligence Research</title>
						<imprint>
							<biblScope unit="volume">4</biblScope>
							<biblScope unit="page" from="237" to="285"/>
							<date type="published" when="1996">1996</date>
						</imprint>
					</monogr>
					<note type="submission">Submitted 9995; published 5596</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper surveys the of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the and a broad selection of current w ork are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but diiers considerably in the details and in the use of the word The paper discusses central issues of reinforcement learning, including trading oo exploration and exploitation, establishing the foundations of the via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reinforcement learning dates back to the early days of cybernetics and work in statistics, psychology, neuroscience, and computer science. In the last to ten years, it has attracted rapidly increasing interest in the machine learning and artiicial intelligence communities. Its promise is beguiling|a way of programming agents by reward and punishment without needing to specify how the task is to be achieved. But there are formidable computational obstacles to fulllling the promise.</p><p>This paper surveys the historical basis of reinforcement learning and some of the current work from a computer science perspective. We give a high-level overview of the and a taste of some speciic approaches. It is, of course, impossible to mention all of the important work in the this should not be taken to be an exhaustive account.</p><p>Reinforcement learning is the problem faced by an agent that must learn behavior through trial-and-error interactions with a dynamic environment. The work described here has a strong family resemblance to eponymous work in psychology, but diiers considerably in the details and in the use of the word It is appropriately thought o f a s a class of problems, rather than as a set of techniques.</p><p>There are two main strategies for solving reinforcement-learning problems. The is to search in the space of behaviors in order to one that performs well in the environment. This approach has been taken by w ork in genetic algorithms and genetic programming, as well as some more novel search techniques <ref type="bibr" target="#b100">Schmidhuber, 1996</ref>. The second is to use statistical techniques and dynamic programming methods to estimate the utility of taking actions in states of the world. This paper is devoted almost entirely to the second set of techniques because they take advantage of the special structure of reinforcement-learning problems that is not available in optimization problems in general. It is not yet clear which set of approaches is best in which circumstances.</p><p>The rest of this section is devoted to establishing notation and describing the basic reinforcement-learning model. Section 2 explains the trade-oo between exploration and exploitation and presents some solutions to the most basic case of reinforcement-learning problems, in which w e w ant to maximize the immediate reward. Section 3 considers the more general problem in which rewards can be delayed in time from the actions that were crucial to gaining them. Section 4 considers some classic model-free algorithms for reinforcement learning from delayed reward: adaptive heuristic critic, TD and Q-learning. Section 5 demonstrates a continuum of algorithms that are sensitive to the amount of computation an agent can perform between actual steps of action in the environment. Generalization|the cornerstone of mainstream machine learning research|has the potential of considerably aiding reinforcement learning, as described in Section 6. Section 7 considers the problems that arise when the agent does not have complete perceptual access to the state of the environment. Section 8 catalogs some of reinforcement learning's successful applications. Finally, Section 9 concludes with some speculations about important open problems and the future of reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Reinforcement-Learning Model</head><p>In the standard reinforcement-learning model, an agent is connected to its environment via perception and action, as depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. On each step of interaction the agent receives as input, i, some indication of the current state, s, of the environment; the agent then chooses an action, a, to generate as output. The action changes the state of the environment, and the value of this state transition is communicated to the agent through a scalar reinforcement signal, r. The agent's behavior, B, should choose actions that tend to increase the long-run sum of values of the reinforcement signal. It can learn to do this over time by systematic trial and error, guided by a wide variety of algorithms that are the subject of later sections of this paper. Formally, the model consists of a discrete set of environment states, S; a discrete set of agent actions, A; and a set of scalar reinforcement signals; typically f0; 1g, or the real numbers.</p><p>The also includes an input function I, which determines how the agent views the environment state; we will assume that it is the identity function that is, the agent perceives the exact state of the environment until we consider partial observability in Section 7.</p><p>An intuitive w ay to understand the relation between the agent and its environment i s with the following example dialogue. . . .</p><p>. . .</p><p>The agent's job is to a policy , mapping states to actions, that maximizes some long-run measure of reinforcement. We expect, in general, that the environment will be non-deterministic; that is, that taking the same action in the same state on two diierent occasions may result in diierent next states anddor diierent reinforcement v alues. This happens in our example above: from state 65, applying action 2 produces diiering reinforcements and diiering states on two occasions. However, we assume the environment i s stationary; that is, that the probabilities of making state transitions or receiving speciic reinforcement signals do not change over time. <ref type="bibr">1</ref> Reinforcement learning diiers from the more widely studied problem of supervised learning in several ways. The most important diierence is that there is no presentation of inputtoutput pairs. Instead, after choosing an action the agent is told the immediate reward and the subsequent state, but is not told which action would have been in its best long-term interests. It is necessary for the agent to gather useful experience about the possible system states, actions, transitions and rewards actively to act optimally. Another diierence from supervised learning is that on-line performance is important: the evaluation of the system is often concurrent with learning.</p><p>Some aspects of reinforcement learning are closely related to search and planning issues in artiicial intelligence. AI search algorithms generate a satisfactory trajectory through a graph of states. Planning operates in a similar manner, but typically within a construct with more complexity than a graph, in which states are represented by compositions of logical expressions instead of atomic symbols. These AI algorithms are less general than the reinforcement-learning methods, in that they require a predeened model of state transitions, and with a few exceptions assume determinism. On the other hand, reinforcement learning, at least in the kind of discrete cases for which theory has been developed, assumes that the entire state space can be enumerated and stored in memory|an assumption to which conventional search algorithms are not tied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Models of Optimal Behavior</head><p>Before we can start thinking about algorithms for learning to behave optimally, w e h a ve to decide what our model of optimality will be. In particular, we h a ve to specify how the agent should take the future into account in the decisions it makes about how to behave now. There are three models that have been the subject of the majority o f w ork in this area.</p><p>The nite-horizon model is the easiest to think about; at a given moment in time, the agent should optimize its expected reward for the next h steps: it need not worry about what will happen after that. In this and subsequent expressions, r t represents the scalar reward received t steps into the future. This model can be used in two w ays. In the the agent will have a non-stationary policy; that is, one that changes over time. On its step it will take what is termed a h-step optimal action. This is deened to be the best action available given that it has h steps remaining in which to act and gain reinforcement. On the next step it will take a h , 1-step optimal action, and so on, until it takes a 1-step optimal action and terminates. In the second, the agent does receding-horizon control, in which i t a l w ays takes the h-step optimal action. The agent always acts according to the same policy, but the value of h limits how far ahead it looks in choosing its actions. The model is not always appropriate. In many cases we m a y not know the precise length of the agent's life in advance. The innnite-horizon discounted model takes the long-run reward of the agent i n to account, but rewards that are received in the future are geometrically discounted according to discount factor , where 0 1:</p><formula xml:id="formula_0">E 1 X t=0 t r t :</formula><p>We can interpret in several ways. It can be seen as an interest rate, a probability of living another step, or as a mathematical trick to bound the innnite sum. The model is conceptually similar to receding-horizon control, but the discounted model is more mathematically tractable than the model. This is a dominant reason for the wide attention this model has received.</p><p>Another optimality criterion is the average-reward m o del, in which the agent is supposed to take actions that optimize its long-run average reward:</p><formula xml:id="formula_1">lim h X h!1 E 1 h t=0 r t :</formula><p>Such a policy is referred to as a gain optimal policy; it can be seen as the limiting case of the innnite-horizon discounted model as the discount factor approaches 1 Bertsekas, 1995.</p><p>One problem with this criterion is that there is no way to distinguish between two policies, one of which gains a large amount o f r e w ard in the initial phases and the other of which does not. Reward gained on any initial preex of the agent's life is overshadowed by the long-run average performance. It is possible to generalize this model so that it takes into account both the long run average and the amount of initial reward than can be gained.</p><p>In the generalized, bias optimal model, a policy is preferred if it maximizes the long-run average and ties are broken by the initial extra reward. <ref type="figure" target="#fig_3">Figure 2</ref> contrasts these models of optimality b y providing an environment in which changing the model of optimality c hanges the optimal policy. In this example, circles represent the states of the environment and arrows are state transitions. There is only a single action choice from every state except the start state, which is in the upper left and marked with an incoming arrow. All rewards are zero except where marked. Under a model with h = 5, the three actions yield rewards of +6:0, +0:0, and +0:0, so the action should be chosen; under an innnite-horizon discounted model with = 0 :9, the three choices yield +16:2, +59:0, and +58:5 so the second action should be chosen; and under the average reward model, the third action should be chosen since it leads to an average reward of +11. If we c hange h to 1000 and to 0.2, then the second action is optimal for the model and the for the innnite-horizon discounted model; however, the average reward model will always prefer the best long-term average. Since the choice of optimality model and parameters matters so much, it is important t o c hoose it carefully in any application.</p><p>The model is appropriate when the agent's lifetime is known; one important aspect of this model is that as the length of the remaining lifetime decreases, the agent's policy may c hange. A system with a hard deadline would be appropriately modeled this way. The relative usefulness of innnite-horizon discounted and bias-optimal models is still under debate. Bias-optimality has the advantage of not requiring a discount parameter; however, algorithms for bias-optimal policies are not yet as well-understood as those for optimal innnite-horizon discounted policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Measuring Learning Performance</head><p>The criteria given in the previous section can be used to assess the policies learned by a given algorithm. We w ould also like to be able to evaluate the quality of learning itself. There are several incompatible measures in use.</p><p>Eventual convergence to optimal. Many algorithms come with a provable guarantee of asymptotic convergence to optimal behavior <ref type="bibr" target="#b129">Watkins &amp; Dayan, 1992</ref>. This is reassuring, but useless in practical terms. An agent that quickly reaches a plateau at 99 of optimality m a y, in many applications, be preferable to an agent that has a guarantee of eventual optimality but a sluggish early learning rate.</p><p>Speed of convergence to optimality. Optimality is usually an asymptotic result, and so convergence speed is an ill-deened measure. More practical is the speed o f convergence t o n e ar-optimality. This measure begs the deenition of how near to optimality is suucient. A related measure is level of performance after a given time, which similarly requires that someone deene the given time. It should be noted that here we h a ve another diierence between reinforcement learning and conventional supervised learning. In the latter, expected future predictive accuracy or statistical eeciency are the prime concerns. For example, in the well-known PAC framework Valiant, 1984, there is a learning period during which mistakes do not count, then a performance period during which they do. The framework provides bounds on the necessary length of the learning period in order to have a probabilistic guarantee on the subsequent performance. That is usually an inappropriate view for an agent with a long existence in a complex environment. In spite of the mismatch b e t ween embedded reinforcement learning and the trainntest perspective, Fiechter 1994 provides a PAC analysis for Q-learning described in Section 4.2 that sheds some light on the connection between the two views. Measures related to speed of learning have an additional weakness. An algorithm that merely tries to achieve optimality as fast as possible may incur unnecessarily large penalties during the learning period. A less aggressive strategy taking longer to achieve optimality, but gaining greater total reinforcement during its learning might be preferable.</p><p>Regret. A more appropriate measure, then, is the expected decrease in reward gained due to executing the learning algorithm instead of behaving optimally from the very beginning. This measure is known as regret <ref type="bibr">Berry &amp; Fristedt, 1985</ref>. It penalizes mistakes wherever they occur during the run. Unfortunately, results concerning the regret of algorithms are quite hard to obtain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Reinforcement Learning and Adaptive Control</head><p>Adaptive control <ref type="bibr" target="#b18">Burghes &amp; Graham, 1980;</ref><ref type="bibr" target="#b112">Stengel, 1986</ref> is also concerned with algorithms for improving a sequence of decisions from experience. Adaptive control is a much more mature discipline that concerns itself with dynamic systems in which states and actions are vectors and system dynamics are smooth: linear or locally linearizable around a desired trajectory. A v ery common formulation of cost functions in adaptive control are quadratic penalties on deviation from desired state and action vectors. Most importantly, although the dynamic model of the system is not known in advance, and must be estimated from data, the structure of the dynamic model is leaving model estimation as a parameter estimation problem. These assumptions permit deep, elegant and powerful mathematical analysis, which in turn lead to robust, practical, and widely deployed adaptive control algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Exploitation versus Exploration: The Single-State Case</head><p>One major diierence between reinforcement learning and supervised learning is that a reinforcement-learner must explicitly explore its environment. In order to highlight the problems of exploration, we treat a very simple case in this section. The fundamental issues and approaches described here will, in many cases, transfer to the more complex instances of reinforcement learning discussed later in the paper.</p><p>The simplest possible reinforcement-learning problem is known as the k-armed bandit problem, which has been the subject of a great deal of study in the statistics and applied mathematics literature <ref type="bibr">Berry &amp; Fristedt, 1985</ref>. The agent is in a room with a collection of k gambling machines each called a bandit" in colloquial English. The agent i s permitted a number of pulls, h. A n y arm may be pulled on each turn. The machines do not require a deposit to play; the only cost is in wasting a pull playing a suboptimal machine. When arm i is pulled, machine i pays oo 1 or 0, according to some underlying probability parameter p i , where payoos are independent e v ents and the p i s are unknown.</p><p>What should the agent's strategy be?</p><p>This problem illustrates the fundamental tradeoo between exploitation and exploration. The agent might believe that a particular arm has a fairly high payoo probability; should it choose that arm all the time, or should it choose another one that it has less information about, but seems to be worse? Answers to these questions depend on how long the agent is expected to play the game; the longer the game lasts, the worse the consequences of prematurely converging on a sub-optimal arm, and the more the agent should explore.</p><p>There is a wide variety of solutions to this problem. We will consider a representative selection of them, but for a deeper discussion and a number of important theoretical results, see the book by Berry and Fristedt 1985. We use the term to indicate the agent's choice of arm to pull. This eases the transition into delayed reinforcement models in Section 3. It is very important to note that bandit problems our deenition of a reinforcement-learning environment with a single state with only self transitions.</p><p>Section 2.1 discusses three solutions to the basic one-state bandit problem that have formal correctness results. Although they can be extended to problems with real-valued rewards, they do not apply directly to the general multi-state delayed-reinforcement case. Section 2.2 presents three techniques that are not formally justiied, but that have had wide use in practice, and can be applied with similar lack of guarantee to the general case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Formally Justiied Techniques</head><p>There is a fairly well-developed formal theory of exploration for very simple problems. Although it is instructive, the methods it provides do not scale well to more complex problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Dynamic-Programming Approach</head><p>If the agent is going to be acting for a total of h steps, it can use basic Bayesian reasoning to solve for an optimal strategy <ref type="bibr">Berry &amp; Fristedt, 1985</ref>. This requires an assumed prior joint distribution for the parameters fp i g, the most natural of which is that each p i is independently uniformly distributed between 0 and 1. We compute a mapping from belief states summaries of the agent's experiences during this run to actions. Here, a belief state can be represented as a tabulation of action choices and payoos: fn 1 ; w 1 ; n 2 ; w 2 ; : : : ; n k ; w k g denotes a state of play in which each arm i has been pulled n i times with w i payoos. We write V n 1 ; w 1 ; : : : ; n k ; w k as the expected payoo remaining, given that a total of h pulls are available, and we use the remaining pulls optimally. If P i n i = h, then there are no remaining pulls, and V n 1 ; w 1 ; : : : ; n k ; w k = 0. This is the basis of a recursive deenition. If we know the V value for all belief states with t pulls remaining, we can compute the V value of any belief state with t + 1 pulls remaining: V n 1 ; w 1 ; : : : ; n k ; w k = max i E " Future payoo if agent takes action i, then acts optimally for remaining pulls ! = max i i V n 1 ; w i ; : : : ; n i + 1 ; w i + 1 ; : : : ; n k ; w k + 1 , i V n 1 ; w i ; : : : ; n i + 1 ; w i ; : : : ; n k ; w k where i is the posterior subjective probability of action i paying oo given n i , w i and our prior probability. For the uniform priors, which result in a beta distribution, i = w i + 1 =n i + 2.</p><p>The expense of in the table of V values in this way for all attainable belief states is linear in the number of belief states times actions, and thus exponential in the horizon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Gittins Allocation Indices</head><p>Gittins gives an index" method for the optimal choice of action at each step in k-armed bandit problems <ref type="bibr" target="#b40">Gittins, 1989</ref>  Because of the guarantee of optimal exploration and the simplicity of the technique given the table of index values, this approach holds a great deal of promise for use in more complex applications. This method proved useful in an application to robotic manipulation with immediate reward <ref type="bibr" target="#b97">Salganicoo &amp; Ungar, 1995</ref>. Unfortunately, no one has yet been able to an analog of index values for delayed reinforcement problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Learning Automata</head><p>A branch of the theory of adaptive control is devoted to learning automata, surveyed by Narendra and Thathachar 1989, which w ere originally described explicitly as state automata. The Tsetlin automaton shown in <ref type="figure" target="#fig_4">Figure 3</ref> provides an example that solves a 2-armed bandit arbitrarily near optimally as N approaches innnity.</p><p>It is inconvenient to describe algorithms as automata, so a move w as made to describe the internal state of the agent as a probability distribution according to which actions would be chosen. The probabilities of taking diierent actions would be adjusted according to their previous successes and failures.</p><p>An example, which stands among a set of algorithms independently developed in the mathematical psychology literature <ref type="bibr" target="#b45">Hilgard &amp; Bower, 1975</ref>, is the linear reward-inaction algorithm. Let p i be the agent's probability of taking action i.</p><p>When action a i succeeds, p i := p i + 1 , p i p j := p j , p j for j 6 = i When action a i fails, p j remains unchanged for all j.</p><p>This algorithm converges with probability 1 t o a v ector containing a single 1 and the rest 0's choosing a particular action with probability 1. Unfortunately, it does not always converge to the correct action; but the probability that it converges to the wrong one can be made arbitrarily small by making small <ref type="bibr" target="#b85">Narendra &amp; Thathachar, 1974</ref>. There is no literature on the regret of this algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ad-Hoc Techniques</head><p>In reinforcement-learning practice, some simple, ad hoc strategies have been popular. They are rarely, i f e v er, the best choice for the models of optimality w e h a ve used, but they may be viewed as reasonable, computationally tractable, heuristics. Thrun 1992 has surveyed a v ariety of these techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Greedy Strategies</head><p>The strategy that comes to mind is to always choose the action with the highest estimated payoo. The is that early unlucky sampling might indicate that the best action's reward is less than the reward obtained from a suboptimal action. The suboptimal action will always be picked, leaving the true optimal action starved of data and its superiority never discovered. An agent m ust explore to ameliorate this outcome.</p><p>A useful heuristic is optimism in the face of uncertainty in which actions are selected greedily, but strongly optimistic prior beliefs are put on their payoos so that strong negative evidence is needed to eliminate an action from consideration. This still has a measurable danger of starving an optimal but unlucky action, but the risk of this can be made arbitrarily small. Techniques like this have been used in several reinforcement learning algorithms including the interval exploration method Kaelbling, 1993b described shortly, the exploration bonus in Dyna Sutton, 1990, curiosity-driven exploration <ref type="bibr" target="#b101">Schmidhuber, 1991a</ref>, and the exploration mechanism in prioritized sweeping Moore &amp; Atkeson, 1993.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Randomized Strategies</head><p>Another simple exploration strategy is to take the action with the best estimated expected reward by default, but with probability p, c hoose an action at random. Some versions of this strategy start with a large value of p to encourage initial exploration, which is slowly decreased.</p><p>An objection to the simple strategy is that when it experiments with a non-greedy action it is no more likely to try a promising alternative than a clearly hopeless alternative. A slightly more sophisticated strategy is <ref type="bibr">Boltzmann exploration.</ref> In this case, the expected reward for taking action a, ERa is used to choose an action probabilistically according to</p><formula xml:id="formula_2">the distribution Pa = e ERa=T P a 0 2A e ERa 0 =T :</formula><p>The temperature parameter T can be decreased over time to decrease exploration. This method works well if the best action is well separated from the others, but suuers somewhat when the values of the actions are close. It may also converge unnecessarily slowly unless the temperature schedule is manually tuned with great care.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Interval-based Techniques</head><p>Exploration is often more eecient when it is based on second-order information about the certainty o r v ariance of the estimated values of actions. Kaelbling's interval estimation algorithm 1993b stores statistics for each action a i : w i is the number of successes and n i the number of trials. An action is chosen by computing the upper bound of a 100 1, conndence interval on the success probability of each action and choosing the action with the highest upper bound. Smaller values of the parameter encourage greater exploration.</p><p>When payoos are boolean, the normal approximation to the binomial distribution can be used to construct the conndence interval though the binomial should be used for small n. Other payoo distributions can be handled using their associated statistics or with nonparametric methods. The method works very well in empirical trials. It is also related to a certain class of statistical techniques known as experiment design methods <ref type="bibr" target="#b16">Box &amp; Draper, 1987</ref>, which are used for comparing multiple treatments for example, fertilizers or drugs to determine which treatment if any is best in as small a set of experiments as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">More General Problems</head><p>When there are multiple states, but reinforcement is still immediate, then any of the above solutions can be replicated, once for each state. However, when generalization is required, these solutions must be integrated with generalization methods see section 6; this is straightforward for the simple ad-hoc methods, but it is not understood how to maintain theoretical guarantees.</p><p>Many of these techniques focus on converging to some regime in which exploratory actions are taken rarely or never; this is appropriate when the environment is stationary. However, when the environment is non-stationary, exploration must continue to take place, in order to notice changes in the world. Again, the more ad-hoc techniques can be modiied to deal with this in a plausible manner keep temperature parameters from going to 0; decay the statistics in interval estimation, but none of the theoretically guaranteed methods can be applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Delayed Reward</head><p>In the general case of the reinforcement learning problem, the agent's actions determine not only its immediate reward, but also at least probabilistically the next state of the environment. Such e n vironments can be thought o f a s n e t works of bandit problems, but the agent m ust take i n to account the next state as well as the immediate reward when it decides which action to take. The model of long-run optimality the agent is using determines exactly how it should take the value of the future into account. The agent will have t o b e able to learn from delayed reinforcement: it may take a long sequence of actions, receiving insigniicant reinforcement, then arrive at a state with high reinforcement. The agent must be able to learn which of its actions are desirable based on reward that can take place arbitrarily far in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Markov Decision Processes</head><p>Problems with delayed reinforcement are well modeled as Markov decision processes MDPs. An MDP consists of a set of states S, a set of actions A, a reward function R : S A ! ! , and a state transition function T : S A!S, where a memb e r o f S is a probability distribution over the set S i.e. it maps states to probabilities. We write Ts; a; s 0 for the probability of making a transition from state s to state s 0 using action a.</p><p>The state transition function probabilistically speciies the next state of the environment a s a function of its current state and the agent's action. The reward function speciies expected instantaneous reward as a function of the current state and action. The model is Markov if the state transitions are independent o f a n y previous environment states or agent actions. There are many good references to <ref type="bibr">MDP models Bellman, 1957;</ref><ref type="bibr" target="#b12">Bertsekas, 1987;</ref><ref type="bibr" target="#b48">Howard, 1960;</ref><ref type="bibr" target="#b89">Puterman, 1994</ref>.</p><p>Although general MDPs m a y h a ve innnite even uncountable state and action spaces, we will only discuss methods for solving and problems. In section 6, we discuss methods for solving problems with continuous input and output spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Finding a Policy Given a Model</head><p>Before we consider algorithms for learning to behave i n MDP environments, we will explore techniques for determining the optimal policy given a correct model. These dynamic programming techniques will serve as the foundation and inspiration for the learning algorithms to follow. We restrict our attention mainly to optimal policies for the innnite-horizon discounted model, but most of these algorithms have analogs for the horizon and average-case models as well. We rely on the result that, for the innnite-horizon discounted model, there exists an optimal deterministic stationary policy Bellman, 1957.</p><p>We will speak of the optimal value of a state|it is the expected innnite discounted sum of reward that the agent will gain if it starts in that state and executes the optimal policy.</p><p>Using as a complete decision policy, it is written ! It is not obvious when to stop the value iteration algorithm. One important result bounds the performance of the current greedy policy as a function of the Bellman residual of the current v alue function <ref type="bibr" target="#b134">Williams &amp; Baird, 1993b</ref>. It says that if the maximum diierence between two successive v alue functions is less than , then the value of the greedy policy, the policy obtained by c hoosing, in every state, the action that maximizes the estimated discounted reward, using the current estimate of the value function diiers from the value function of the optimal policy by no more than 2 , a t a n y state. This provides an eeective stopping criterion for the algorithm. Puterman 1994 discusses another stopping criterion, based on the span semi-norm, which m a y result in earlier termination. Another important result is that the greedy policy is guaranteed to be optimal in some number of steps even though the value function may not have converged <ref type="bibr" target="#b12">Bertsekas, 1987</ref>. And in practice, the greedy policy is often optimal long before the value function has converged.</p><p>Value iteration is very The assignments to V need not be done in strict order as shown above, but instead can occur asynchronously in parallel provided that the value of every state gets updated innnitely often on an innnite run. These issues are treated extensively by Bertsekas 1989, who also proves convergence results.</p><p>Updates based on Equation 1 are known as full backups since they make use of information from all possible successor states. It can be shown that updates of the form Qs; a : = Qs; a + r + max a 0 Qs 0 ; a 0 , Qs; a can also be used as long as each pairing of a and s is updated innnitely often, s 0 is sampled from the distribution Ts; a; s 0 , r is sampled with mean Rs; a and bounded variance, and the learning rate is decreased slowly. This type of sample backup Singh, 1993 is critical to the operation of the model-free methods discussed in the next section.</p><p>The computational complexity of the value-iteration algorithm with full backups, per iteration, is quadratic in the number of states and linear in the number of actions. Commonly, the transition probabilities Ts; a; s 0 are sparse. If there are on average a constant number of next states with non-zero probability then the cost per iteration is linear in the number of states and linear in the number of actions. The number of iterations required to reach the optimal value function is polynomial in the number of states and the magnitude of the largest reward if the discount factor is held constant. However, in the worst case the number of iterations grows polynomially in 1=1 , , so the convergence rate slows considerably as the discount factor approaches 1 Littman, Dean, &amp; Kaelbling, 1995b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Policy Iteration</head><p>The policy iteration algorithm manipulates the policy directly, rather than it indirectly via the optimal value function. It operates as follows: The value function of a policy is just the expected innnite discounted reward that will be gained, at each state, by executing that policy. It can be determined by solving a set of linear equations. Once we know the value of each state under the current policy, w e consider whether the value could be improved by c hanging the action taken. If it can, we c hange the policy to take the new action whenever it is in that situation. This step is guaranteed to strictly improve the performance of the policy. When no improvements are possible, then the policy is guaranteed to be optimal.</p><formula xml:id="formula_3">choose</formula><p>Since there are at most jAj jSj distinct policies, and the sequence of policies improves at each step, this algorithm terminates in at most an exponential number of iterations Puterman, 1994. However, it is an important open question how many iterations policy iteration takes in the worst case. It is known that the running time is pseudopolynomial and that for any discount factor, there is a polynomial bound in the total size of the MDP Littman et al., 1995b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Enhancement to Value Iteration and Policy Iteration</head><p>In practice, value iteration is much faster per iteration, but policy iteration takes fewer iterations. Arguments have been put forth to the eeect that each approach is better for large problems. Puterman's modiied p olicy iteration algorithm <ref type="bibr" target="#b90">Puterman &amp; Shin, 1978</ref> provides a method for trading iteration time for iteration improvement in a smoother way. The basic idea is that the expensive part of policy iteration is solving for the exact value of V . Instead of an exact value for V , w e can perform a few steps of a modiied value-iteration step where the policy is held over successive iterations. This can be shown to produce an approximation to V that converges linearly in . In practice, this can result in substantial speedups.</p><p>Several standard numerical-analysis techniques that speed the convergence of dynamic programming can be used to accelerate value and policy iteration. Multigrid methods can be used to quickly seed a good initial approximation to a high resolution value function by initially performing value iteration at a coarser resolution R ude, 1993. State aggregation works by collapsing groups of states to a single meta-state solving the abstracted problem <ref type="bibr" target="#b14">Bertsekas &amp; Casta~ non, 1989.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Computational Complexity</head><p>Value iteration works by producing successive approximations of the optimal value function.</p><p>Each iteration can be performed in OjAjjSj 2 steps, or faster if there is sparsity in the transition function. However, the number of iterations required can grow exponentially in the discount factor Condon, 1992; as the discount factor approaches 1, the decisions must be based on results that happen farther and farther into the future. In practice, policy iteration converges in fewer iterations than value iteration, although the per-iteration costs of OjAjjSj 2 + jSj 3 can be prohibitive. There is no known tight w orst-case bound available for policy iteration <ref type="bibr" target="#b65">Littman et al., 1995b</ref>. Modiied policy iteration <ref type="bibr" target="#b90">Puterman &amp; Shin, 1978</ref> seeks a trade-oo between cheap and eeective iterations and is preferred by some practictioners Rust, 1996.</p><p>Linear programming Schrijver, 1986 is an extremely general problem, and MDPs can be solved by general-purpose linear-programming packages <ref type="bibr" target="#b35">Derman, 1970;</ref><ref type="bibr" target="#b34">D'Epenoux, 1963;</ref><ref type="bibr" target="#b46">Hooman &amp; Karp, 1966</ref>. An advantage of this approach is that commercial-quality linear-programming packages are available, although the time and space requirements can still be quite high. From a theoretic perspective, linear programming is the only known algorithm that can solve MDPs in polynomial time, although the theoretically eecient algorithms have not been shown to be eecient in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning an Optimal Policy: Model-free Methods</head><p>In the previous section we reviewed methods for obtaining an optimal policy for an MDP assuming that we already had a model. The model consists of knowledge of the state transition probability function Ts; a; s 0 and the reinforcement function Rs; a. Reinforcement learning is primarily concerned with how to obtain the optimal policy when such a model is not known in advance. The agent m ust interact with its environment directly to obtain information which, by means of an appropriate algorithm, can be processed to produce an optimal policy.</p><p>At this point, there are two w ays to proceed.</p><p>Model-free: Learn a controller without learning a model. Model-based: Learn a model, and use it to derive a controller.</p><p>Which approach is better? This is a matter of some debate in the reinforcement-learning community. A n umber of algorithms have been proposed on both sides. This question also appears in other such as adaptive control, where the dichotomy i s b e t ween direct and indirect adaptive control. This section examines model-free learning, and Section 5 examines model-based methods.</p><p>The biggest problem facing a reinforcement-learning agent i s temporal credit assignment. How d o w e know whether the action just taken is a good one, when it might h a ve farreaching eeects? One strategy is to wait until the and reward the actions taken if the result was good and punish them if the result was bad. In ongoing tasks, it is diicult to know what the is, and this might require a great deal of memory. Instead, we will use insights from value iteration to adjust the estimated value of a state based on the immediate reward and the estimated value of the next state. This class of algorithms is known as temporal diierence methods Sutton, 1988. We will consider two diierent temporal-diierence learning strategies for the discounted innnite-horizon model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Adaptive Heuristic Critic and TD</head><p>The adaptive heuristic critic algorithm is an adaptive v ersion of policy iteration <ref type="bibr" target="#b8">Barto, Sutton, &amp; Anderson, 1983</ref> in which the value-function computation is no longer implemented by solving a set of linear equations, but is instead computed by an algorithm called TD 0. A block diagram for this approach is given in <ref type="figure" target="#fig_6">Figure 4</ref>. It consists of two components: a critic labeled AHC, and a reinforcement-learning component labeled RL. The reinforcement-learning component can be an instance of any o f t h e k-armed bandit algorithms, modiied to deal with multiple states and non-stationary rewards. But instead of acting to maximize instantaneous reward, it will be acting to maximize the heuristic value, v, that is computed by the critic. The critic uses the real external reinforcement signal to learn to map states to their expected discounted values given that the policy being executed is the one currently instantiated in the RL component.</p><p>We can see the analogy with modiied policy iteration if we imagine these components working in alternation. Whenever a state s is visited, its estimated value is updated to be closer to r + Vs 0 , since r is the instantaneous reward received and V s 0 is the estimated value of the actually occurring next state. This is analogous to the sample-backup rule from value iteration|the only diierence is that the sample is drawn from the real world rather than by simulating a known model. The key idea is that r + Vs 0 is a sample of the value of V s, and it is more likely to be correct because it incorporates the real r. If the learning rate is adjusted properly it must be slowly decreased and the policy is held TD 0 is guaranteed to converge to the optimal value function.</p><p>The TD 0 rule as presented above is really an instance of a more general class of algorithms called TD , with = 0 . TD 0 looks only one step ahead when adjusting value estimates; although it will eventually arrive at the correct answer, it can take quite a while to do so. The general TD rule is similar to the TD 0 rule given above, The eligibility of a state s is the degree to which it has been visited in the recent past; when a reinforcement is received, it is used to update all the states that have been recently visited, according to their eligibility. When = 0 this is equivalent t o TD 0. When = 1 , it is roughly equivalent to updating all the states according to the number of times they were visited by the end of a run. Note that we can update the eligibility online as follows:</p><formula xml:id="formula_4">es : = + 1 if s = current state otherwise .</formula><p>It is computationally more expensive to execute the general TD , though it often converges considerably faster for large <ref type="bibr" target="#b29">Dayan, 1992;</ref>. There has been some recent w ork on making the updates more eecient <ref type="bibr" target="#b23">Cichosz &amp; Mulawka, 1995</ref> and on changing the deenition to make TD more consistent with the certainty-equivalent method <ref type="bibr" target="#b107">Singh &amp; Sutton, 1996</ref>, which is discussed in Section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Q-learning</head><p>The work of the two components of AHC can be accomplished in a uniied manner by Watkins' Q-learning algorithm <ref type="bibr" target="#b128">Watkins, 1989;</ref><ref type="bibr" target="#b129">Watkins &amp; Dayan, 1992</ref> Because the Q function makes the action explicit, we can estimate the Q values online using a method essentially the same as TD 0, but also use them to deene the policy, because an action can be chosen just by taking the one with the maximum Q value for the current state.</p><p>The Q-learning rule is</p><p>Qs; a : = Qs; a + r + max a 0 Qs 0 ; a 0 , Qs; a ; where hs; a; r; s 0 i is an experience tuple as described earlier. If each action is executed in each state an innnite number of times on an innnite run and is decayed appropriately, the Q values will converge with probability 1 t o Q <ref type="bibr" target="#b128">Watkins, 1989;</ref><ref type="bibr" target="#b125">Tsitsiklis, 1994;</ref><ref type="bibr" target="#b49">Jaakkola, Jordan, &amp; Singh, 1994</ref>. Q-learning can also be extended to update states that occurred more than one step previously, a s i n TD P eng &amp; Williams, 1994.</p><p>When the Q values are nearly converged to their optimal values, it is appropriate for the agent to act greedily, taking, in each situation, the action with the highest Q value.</p><p>During learning, however, there is a diicult exploitation versus exploration trade-oo to be made. There are no good, formally justiied approaches to this problem in the general case; standard practice is to adopt one of the ad hoc methods discussed in section 2.2.</p><p>AHC architectures seem to be more diicult to work with than Q-learning on a practical level. It can be hard to get the relative learning rates right in AHC so that the two components converge together. In addition, Q-learning is exploration insensitive: that is, that the Q values will converge to the optimal values, independent o f h o w the agent behaves while the data is being collected as long as all state-action pairs are tried often enough. This means that, although the exploration-exploitation issue must be addressed in Q-learning, the details of the exploration strategy will not aaect the convergence of the learning algorithm. For these reasons, Q-learning is the most popular and seems to be the most eeective model-free algorithm for learning from delayed reinforcement. It does not, however, address any of the issues involved in generalizing over large state anddor action spaces. In addition, it may converge quite slowly to a good policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model-free Learning With Average Reward</head><p>As described, Q-learning can be applied to discounted innnite-horizon MDPs. It can also be applied to undiscounted problems as long as the optimal policy is guaranteed to reach a reward-free absorbing state and the state is periodically reset.</p><p>Schwartz 1993 examined the problem of adapting Q-learning to an average-reward framework. Although his R-learning algorithm seems to exhibit convergence problems for some MDPs, several researchers have found the average-reward criterion closer to the true problem they wish to solve than a discounted criterion and therefore prefer R-learning to Q-learning <ref type="bibr" target="#b68">Mahadevan, 1994.</ref> With that in mind, researchers have studied the problem of learning optimal averagereward policies. Mahadevan 1996 surveyed model-based average-reward algorithms from a reinforcement-learning perspective and found several diiculties with existing algorithms. In particular, he showed that existing reinforcement-learning algorithms for average reward and some dynamic programming algorithms do not always produce bias-optimal policies. Jaakkola, Jordan and Singh 1995 described an average-reward learning algorithm with guaranteed convergence properties. It uses a Monte-Carlo component to estimate the expected future reward for each state as the agent m o ves through the environment. In addition, Bertsekas presents a Q-learning-like algorithm for average-case reward in his new textbook 1995. Although this recent w ork provides a much needed theoretical foundation to this area of reinforcement learning, many important problems remain unsolved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Computing Optimal Policies by Learning Models</head><p>The previous section showed how it is possible to learn an optimal policy without knowing the models Ts; a; s 0 o r Rs; a and without even learning those models en route. Although many of these methods are guaranteed to optimal policies eventually and use very little computation time per experience, they make extremely ineecient use of the data they gather and therefore often require a great deal of experience to achieve good performance. In this section we still begin by assuming that we don't know the models in advance, but we examine algorithms that do operate by learning these models. These algorithms are especially important in applications in which computation is considered to be cheap and real-world experience costly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Certainty Equivalent Methods</head><p>We begin with the most conceptually straightforward method: learn the T and R functions by exploring the environment and keeping statistics about the results of each action; next, compute an optimal policy using one of the methods of Section 3. This method is known as certainty equivlance <ref type="bibr">Kumar &amp; Varaiya, 1986.</ref> There are some serious objections to this method:</p><p>It makes an arbitrary division between the learning phase and the acting phase. How should it gather data about the environment initially? Random exploration might be dangerous, and in some environments is an immensely ineecient method of gathering data, requiring exponentially more data Whitehead, 1991 than a system that interleaves experience gathering with policy-building more tightly <ref type="bibr" target="#b55">Koenig &amp; Simmons, 1993</ref>. See <ref type="figure" target="#fig_7">Figure 5</ref> for an example.</p><p>The possibility o f c hanges in the environment is also problematic. Breaking up an agent's life into a pure learning and a pure acting phase has a considerable risk that the optimal controller based on early life becomes, without detection, a suboptimal controller if the environment c hanges. A v ariation on this idea is certainty equivalence, in which the model is learned continually through the agent's lifetime and, at each step, the current model is used to compute an optimal policy and value function. This method makes very eeective use of available data, but still ignores the question of exploration and is extremely computationally demanding, even for fairly small state spaces. Fortunately, there are a number of other model-based algorithms that are more practical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dyna</head><p>Sutton's Dyna architecture 1990, 1991 exploits a middle ground, yielding strategies that are both more eeective than model-free learning and more computationally eecient than  Choose an action a 0 to perform in state s 0 , based on the Q values but perhaps modiied by an exploration strategy.</p><p>The Dyna algorithm requires about k times the computation of Q-learning per instance, but this is typically vastly less than for the naive model-based method. A reasonable value of k can be determined based on the relative speeds of computation and of taking action. <ref type="figure" target="#fig_8">Figure 6</ref> shows a grid world in which in each cell the agent has four actions N, S, E, W and transitions are made deterministically to an adjacent cell, unless there is a block, in which case no movement occurs. As we will see in <ref type="table">Table 1</ref>, Dyna requires an order of magnitude fewer steps of experience than does Q-learning to arrive at an optimal policy. Dyna requires about six times more computational eeort, however. Steps before Backups before convergence convergence Q-learning 531,000 531,000 Dyna 62,000 3,055,000 prioritized sweeping 28,000 1,010,000 <ref type="table">Table 1</ref>: The performance of three algorithms described in the text. All methods used the exploration heuristic of in the face of uncertainty": any state not previously visited was assumed by default to be a goal state. Q-learning used its optimal learning rate parameter for a deterministic maze: = 1. Dyna and prioritized sweeping were permitted to take k = 200 backups per transition. For prioritized sweeping, the priority queue often emptied before all backups were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Prioritized Sweeping Queue-Dyna</head><p>Although Dyna is a great improvement on previous methods, it suuers from being relatively undirected. It is particularly unhelpful when the goal has just been reached or when the agent is stuck in a dead end; it continues to update random state-action pairs, rather than concentrating on the parts of the state space. These problems are addressed by prioritized sweeping <ref type="bibr" target="#b82">Moore &amp; Atkeson, 1993 and</ref><ref type="bibr">Queue-Dyna Peng &amp; Williams, 1993</ref>, which are two independently-developed but very similar techniques. We will describe prioritized sweeping in some detail. The algorithm is similar to Dyna, except that updates are no longer chosen at random and values are now associated with states as in value iteration instead of state-action pairs as in Q-learning. To make appropriate choices, we m ust store additional information in the model. Each state remembers its predecessors: the states that have a non-zero transition probability to it under some action. In addition, each state has a priority, initially set to zero.</p><p>Instead of updating k random state-action pairs, prioritized sweeping updates k states with the highest priority. F or each high-priority state s, i t w orks as follows:</p><p>Remember the current v alue of the state: V old = V s. Compute the value change = jV old , V sj.</p><p>Use to modify the priorities of the predecessors of s.</p><p>If we h a ve updated the V value for state s 0 and it has changed by amount , then the immediate predecessors of s 0 are informed of this event. Any state s for which there exists an action a such that ^ Ts; a; s 0 6 = 0 has its priority promoted to ^ Ts; a; s 0 , unless its priority already exceeded that value.</p><p>The global behavior of this algorithm is that when a real-world transition is the agent happens upon a goal state, for instance, then lots of computation is directed to propagate this new information back to relevant predecessor states. When the realworld transition is the actual result is very similar to the predicted result, then computation continues in the most deserving part of the space.</p><p>Running prioritized sweeping on the problem in <ref type="figure" target="#fig_8">Figure 6</ref>, we see a large improvement over Dyna. The optimal policy is reached in about half the number of steps of experience and one-third the computation as Dyna required and therefore about 20 times fewer steps and twice the computational eeort of Q-learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Other Model-Based Methods</head><p>Methods proposed for solving MDPs given a model can be used in the context of modelbased methods as well.</p><p>RTDP real-time dynamic programming Barto, Bradtke, &amp; Singh, 1995 is another model-based method that uses Q-learning to concentrate computational eeort on the areas of the state-space that the agent is most likely to occupy. It is speciic to problems in which the agent is trying to achieve a particular goal state and the reward everywhere else is 0. By taking into account the start state, it can a short path from the start to the goal, without necessarily visiting the rest of the state space.</p><p>The Plexus planning system Dean, <ref type="bibr" target="#b33">Kaelbling, Kirman, &amp; Nicholson, 1993;</ref><ref type="bibr" target="#b54">Kirman, 1994</ref> exploits a similar intuition. It starts by making an approximate version of the MDP which i s m uch smaller than the original one. The approximate MDP contains a set of states, called the envelope, that includes the agent's current state and the goal state, if there is one. States that are not in the envelope are summarized by a single state. The planning process is an alternation between an optimal policy on the approximate MDP and adding useful states to the envelope. Action may take place in parallel with planning, in which case irrelevant states are also pruned out of the envelope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Generalization</head><p>All of the previous discussion has tacitly assumed that it is possible to enumerate the state and action spaces and store tables of values over them. Except in very small environments, this means impractical memory requirements. It also makes ineecient use of experience. In a large, smooth state space we generally expect similar states to have similar values and similar optimal actions. Surely, therefore, there should be some more compact representation than a table. Most problems will have continuous or large discrete state spaces; some will have large or continuous action spaces. The problem of learning in large spaces is addressed through generalization techniques, which allow compact storage of learned information and transfer of knowledge between states and actions.</p><p>The large literature of generalization techniques from inductive concept learning can be applied to reinforcement learning. However, techniques often need to be tailored to speciic details of the problem. In the following sections, we explore the application of standard function-approximation techniques, adaptive resolution models, and hierarchical methods to the problem of reinforcement learning.</p><p>The reinforcement-learning architectures and algorithms discussed above h a ve included the storage of a variety of mappings, including S ! A policies, S ! value functions, S A ! ! Q functions and rewards, S A ! S deterministic transitions, and S A S ! 0; 11 transition probabilities. Some of these mappings, such as transitions and immediate rewards, can be learned using straightforward supervised learning, and can be handled using any of the wide variety of function-approximation techniques for supervised learning that support noisy training examples. Popular techniques include various neuralnetwork methods <ref type="bibr" target="#b93">Rumelhart &amp; McClelland, 1986</ref><ref type="bibr">, fuzzy logic Berenji, 1991</ref><ref type="bibr" target="#b57">Lee, 1991</ref><ref type="bibr">. CMAC Albus, 1981</ref>, and local memory-based methods Moore, Atkeson, &amp; Schaal, 1995, such as generalizations of nearest neighbor methods. Other mappings, especially the policy mapping, typically need specialized algorithms because training sets of input-output pairs are not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Generalization over Input</head><p>A reinforcement-learning agent's current state plays a central role in its selection of rewardmaximizing actions. Viewing the agent as a state-free black b o x, a description of the current state is its input. Depending on the agent architecture, its output is either an action selection, or an evaluation of the current state that can be used to select an action. The problem of deciding how the diierent aspects of an input aaect the value of the output is sometimes called the credit-assignment" problem. This section examines approaches to generating actions or evaluations as a function of a description of the agent's current state. The group of techniques covered here is specialized to the case when reward is not delayed; the second group is more generally applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Immediate Reward</head><p>When the agent's actions do not innuence state transitions, the resulting problem becomes one of choosing actions to maximize immediate reward as a function of the agent's current state. These problems bear a resemblance to the bandit problems discussed in Section 2 except that the agent should condition its action selection on the current state. For this reason, this class of problems has been described as associative reinforcement learning.</p><p>The algorithms in this section address the problem of learning from immediate boolean reinforcement where the state is vector valued and the action is a boolean vector. Such algorithms can and have been used in the context of a delayed reinforcement, for instance, as the RL component in the AHC architecture described in Section 4.1. They can also be generalized to real-valued reward through reward c omparison methods <ref type="bibr" target="#b114">Sutton, 1984.</ref> CRBP The complementary reinforcement backpropagation algorithm <ref type="bibr" target="#b0">Ackley &amp; Littman, 1990</ref> crbp consists of a feed-forward network mapping an encoding of the state to an encoding of the action. The action is determined probabilistically from the activation of the output units: if output unit i has activation y i , then bit i of the action vector has value 1 with probability y i , and 0 otherwise. Any neural-network supervised training procedure can be used to adapt the network as follows. If the result of generating action a is r = 1 , then the network is trained with input-output pair hs; ai. If the result is r = 0, then the network is trained with input-output pair hs; ai, where a = 1 , a 1 ; : : : ; 1 , a n .</p><p>The idea behind this training rule is that whenever an action fails to generate reward, crbp will try to generate an action that is diierent from the current c hoice. Although it seems like the algorithm might oscillate between an action and its complement, that does not happen. One step of training a network will only change the action slightly and since the output probabilities will tend to move t o ward 0.5, this makes action selection more random and increases search. The hope is that the random distribution will generate an action that works better, and then that action will be reinforced.</p><p>ARC The associative reinforcement comparison arc algorithm <ref type="bibr" target="#b114">Sutton, 1984</ref> is an instance of the ahc architecture for the case of boolean actions, consisting of two feed-forward networks. One learns the value of situations, the other learns a policy. These can be simple linear networks or can have hidden units.</p><p>In the simplest case, the entire system learns only to optimize immediate reward. First, let us consider the behavior of the network that learns the policy, a mapping from a vector describing s t o a 0 o r 1 . I f t h e output unit has activation y i , then a, the action generated, will be 1 if y + 0, where is normal noise, and 0 otherwise.</p><p>The adjustment for the output unit is, in the simplest case, e = ra , 1=2 ;</p><p>where the factor is the reward received for taking the most recent action and the second encodes which action was taken. The actions are encoded as 0 and 1, so a ,1=2 always has the same magnitude; if the reward and the action have the same sign, then action 1 will be made more likely, otherwise action 0 will be.</p><p>As described, the network will tend to seek actions that given positive reward. To extend this approach to maximize reward, we can compare the reward to some baseline, b. This changes the adjustment t o e = r , ba , 1=2 ;</p><p>where b is the output of the second network. The second network is trained in a standard supervised mode to estimate r as a function of the input state s.</p><p>Variations of this approach h a ve been used in a variety of applications <ref type="bibr" target="#b3">Anderson, 1986;</ref><ref type="bibr" target="#b8">Barto et al., 1983;</ref><ref type="bibr" target="#b60">Lin, 1993b;</ref><ref type="bibr" target="#b114">Sutton, 1984.</ref> REINFORCE Algorithms <ref type="bibr" target="#b131">Williams 1987</ref><ref type="bibr" target="#b132">Williams , 1992</ref> studied the problem of choosing actions to maximize immedate reward. He identiied a broad class of update rules that perform gradient descent on the expected reward and showed how t o i n tegrate these rules with backpropagation. This class, called reinforce algorithms, includes linear reward-inaction Section 2.1.3 as a special case.</p><p>The generic reinforce update for a parameter w ij can be written</p><formula xml:id="formula_5">w ij = ij r , b ij @ @w ij lng j</formula><p>where ij is a non-negative factor, r the current reinforcement, b ij a reinforcement baseline, and g i is the probability density function used to randomly generate actions based on unit activations. Both ij and b ij can take on diierent v alues for each w ij , h o wever, when ij is constant throughout the system, the expected update is exactly in the direction of the expected reward gradient. Otherwise, the update is in the same half space as the gradient but not necessarily in the direction of steepest increase.</p><p>Williams points out that the choice of baseline, b ij , can have a profound eeect on the convergence speed of the algorithm.</p><p>Logic-Based Methods Another strategy for generalization in reinforcement learning is to reduce the learning problem to an associative problem of learning boolean functions. A boolean function has a vector of boolean inputs and a single boolean output. Taking inspiration from mainstream machine learning work, Kaelbling developed two algorithms for learning boolean functions from reinforcement: one uses the bias of k-DNF to drive the generalization process <ref type="bibr" target="#b53">Kaelbling, 1994b</ref>; the other searches the space of syntactic descriptions of functions using a simple generate-and-test method <ref type="bibr" target="#b52">Kaelbling, 1994a</ref>.</p><p>The restriction to a single boolean output makes these techniques diicult to apply. I n very benign learning situations, it is possible to extend this approach to use a collection of learners to independently learn the individual bits that make up a complex output. In general, however, that approach suuers from the problem of very unreliable reinforcement: if a single learner generates an inappropriate output bit, all of the learners receive a l o w reinforcement v alue. The cascade method Kaelbling, 1993b allows a collection of learners to be trained collectively to generate appropriate joint outputs; it is considerably more reliable, but can require additional computational eeort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Delayed Reward</head><p>Another method to allow reinforcement-learning techniques to be applied in large state spaces is modeled on value iteration and Q-learning. Here, a function approximator is used to represent the value function by mapping a state description to a value.</p><p>Many reseachers have experimented with this approach: Boyan and Moore 1995 used local memory-based methods in conjunction with value iteration; Lin 1991 used backpropagation networks for Q-learning; Watkins 1989 used CMAC for Q-learning; <ref type="bibr" target="#b118">Tesauro 1992</ref><ref type="bibr" target="#b120">Tesauro , 1995</ref> used backpropagation for learning the value function in backgammon described in Section 8.1; Zhang and Dietterich 1995 used backpropagation and TD to learn good strategies for job-shop scheduling.</p><p>Although there have been some positive examples, in general there are unfortunate interactions between function approximation and the learning rules. In discrete environments there is a guarantee that any operation that updates the value function according to the Bellman equations can only reduce the error between the current v alue function and the optimal value function. This guarantee no longer holds when generalization is used. These issues are discussed by B o yan and Moore 1995, who give some simple examples of value function errors growing arbitrarily large when generalization is used with value iteration. Their solution to this, applicable only to certain classes of problems, discourages such divergence by only permitting updates whose estimated values can be shown to be near-optimal via a battery of Monte-Carlo experiments.</p><p>Thrun and Schwartz 1993 theorize that function approximation of value functions is also dangerous because the errors in value functions due to generalization can become compounded by the operator in the deenition of the value function.</p><p>Several recent results <ref type="bibr" target="#b42">Gordon, 1995;</ref><ref type="bibr" target="#b126">Tsitsiklis &amp; Van Roy, 1996</ref> show h o w the appropriate choice of function approximator can guarantee convergence, though not necessarily to the optimal values. Baird's residual gradient technique Baird, 1995 provides guaranteed convergence to locally optimal solutions.</p><p>Perhaps the gloominess of these counter-examples is misplaced. Boyan and Moore 1995 report that their counter-examples can be made to work with problem-speciic hand-tuning despite the unreliability o f u n tuned algorithms that provably converge in discrete domains. Sutton 1996 shows how modiied versions of Boyan and Moore's examples can converge</p><p>successfully. An open question is whether general principles, ideally supported by theory, can help us understand when value function approximation will succeed. In Sutton's com-parative experiments with Boyan and Moore's counter-examples, he changes four aspects of the experiments:</p><p>1. Small changes to the task speciications. 2. A very diierent kind of function approximator <ref type="bibr">CMAC Albus, 1975</ref> that has weak generalization. 3. A diierent learning algorithm: SARSA Rummery &amp; Niranjan, 1994 instead of value iteration. 4. A diierent training regime. Boyan and Moore sampled states uniformly in state space, whereas Sutton's method sampled along empirical trajectories. There are intuitive reasons to believe that the fourth factor is particularly important, but more careful research is needed.</p><p>Adaptive Resolution Models In many cases, what we w ould like to do is partition the environment i n to regions of states that can be considered the same for the purposes of learning and generating actions. Without detailed prior knowledge of the environment, it is very diicult to know what granularity or placement of partitions is appropriate. This problem is overcome in methods that use adaptive resolution; during the course of learning, a partition is constructed that is appropriate to the environment.</p><p>Decision Trees In environments that are characterized by a set of boolean or discretevalued variables, it is possible to learn compact decision trees for representing Q values. The G-learning algorithm <ref type="bibr" target="#b20">Chapman &amp; Kaelbling, 1991</ref>, works as follows. It starts by assuming that no partitioning is necessary and tries to learn Q values for the entire environment a s if it were one state. In parallel with this process, it gathers statistics based on individual input bits; it asks the question whether there is some bit b in the state description such that the Q values for states in which b = 1 are signiicantly diierent from Q values for states in which b = 0. If such a bit is found, it is used to split the decision tree. Then, the process is repeated in each of the leaves. This method was able to learn very small representations of the Q function in the presence of an overwhelming number of irrelevant, noisy state attributes. It outperformed Q-learning with backpropagation in a simple videogame environment and was used by McCallum 1995 in conjunction with other techniques for dealing with partial observability to learn behaviors in a complex driving-simulator. It cannot, however, acquire partitions in which attributes are only signiicant in combination such as those needed to solve parity problems.</p><p>Variable Resolution Dynamic Programming <ref type="bibr">The VRDP algorithm Moore, 1991</ref> enables conventional dynamic programming to be performed in real-valued multivariate state-spaces where straightforward discretization would fall prey to the curse of dimensionality. A kd-tree similar to a decision tree is used to partition state space into coarse regions. The coarse regions are reened into detailed regions, but only in parts of the state space which are predicted to be important. This notion of importance is obtained by running trajectories" through state space. This algorithm proved eeective o n a n umber of problems for which full high-resolution arrays would have been impractical. It has the disadvantage of requiring a guess at an initially valid trajectory through state-space. PartiGame Algorithm Moore's PartiGame algorithm Moore, 1994 is another solution to the problem of learning to achieve goal conngurations in deterministic high-dimensional continuous spaces by learning an adaptive-resolution model. It also divides the environment into cells; but in each cell, the actions available consist of aiming at the neighboring cells this aiming is accomplished by a local controller, which m ust be provided as part of the problem statement. The graph of cell transitions is solved for shortest paths in an online incremental manner, but a minimax criterion is used to detect when a group of cells is too coarse to prevent m o vement b e t ween obstacles or to avoid limit cycles. The ooending cells are split to higher resolution. Eventually, the environment is divided up just enough to choose appropriate actions for achieving the goal, but no unnecessary distinctions are made. An important feature is that, as well as reducing memory and computational requirements, it also structures exploration of state space in a multi-resolution manner. Given a failure, the agent will initially try something very diierent to rectify the failure, and only resort to small local changes when all the qualitatively diierent strategies have been exhausted. <ref type="figure" target="#fig_10">Figure 7a</ref> show s a t wo-dimensional continuous maze. <ref type="figure" target="#fig_10">Figure 7b</ref> shows the performance of a robot using the PartiGame algorithm during the very trial. <ref type="figure" target="#fig_10">Figure 7c</ref> shows the second trial, started from a slightly diierent position. This is a very fast algorithm, learning policies in spaces of up to nine dimensions in less than a minute. The restriction of the current implementation to deterministic environments limits its applicability, h o wever. McCallum 1995 suggests some related tree-structured methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Generalization over Actions</head><p>The networks described in Section 6.1.1 generalize over state descriptions presented as inputs. They also produce outputs in a discrete, factored representation and thus could be seen as generalizing over actions as well.</p><p>In cases such as this when actions are described combinatorially, it is important t o generalize over actions to avoid keeping separate statistics for the huge number of actions that can be chosen. In continuous action spaces, the need for generalization is even more pronounced.</p><p>When estimating Q values using a neural network, it is possible to use either a distinct network for each action, or a network with a distinct output for each action. When the action space is continuous, neither approach is possible. An alternative strategy is to use a single network with both the state and action as input and Q value as the output. Training such a network is not conceptually diicult, but using the network to the optimal action can be a challenge. One method is to do a local gradient-ascent search on the action in order to one with high value <ref type="bibr" target="#b6">Baird &amp; Klopf, 1993</ref><ref type="bibr" target="#b43">. Gullapalli 1990</ref> has developed a reinforcement-learning unit for use in continuous action spaces. The unit generates actions with a normal distribution; it adjusts the mean and variance based on previous experience. When the chosen actions are not performing well, the variance is high, resulting in exploration of the range of choices. When an action performs well, the mean is moved in that direction and the variance decreased, resulting in a tendency to generate more action values near the successful one. This method was successfully employed to learn to control a robot arm with many continuous degrees of freedom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Hierarchical Methods</head><p>Another strategy for dealing with large state spaces is to treat them as a hierarchy o f learning problems. In many cases, hierarchical solutions introduce slight sub-optimality i n performance, but potentially gain a good deal of eeciency in execution time, learning time, and space.</p><p>Hierarchical learners are commonly structured as gated b ehaviors, as shown in <ref type="figure" target="#fig_11">Figure 8</ref>. There is a collection of behaviors that map environment states into low-level actions and a gating function that decides, based on the state of the environment, which behavior's actions should be switched through and actually executed. Maes and Brooks 1990 used a v ersion of this architecture in which the individual behaviors were a priori and the gating function was learned from reinforcement. Mahadevan and Connell 1991b used the dual approach: they the gating function, and supplied reinforcement functions for the individual behaviors, which w ere learned. <ref type="bibr" target="#b59">Lin 1993a</ref><ref type="bibr">and Dorigo and Colombetti 1995</ref> both used this approach, training the behaviors and then training the gating function. Many of the other hierarchical learning methods can be cast in this framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Feudal Q-learning</head><p>Feudal Q-learning <ref type="bibr" target="#b30">Dayan &amp; Hinton, 1993;</ref><ref type="bibr" target="#b128">Watkins, 1989</ref> involves a hierarchy of learning modules. In the simplest case, there is a high-level master and a low-level slave. The master receives reinforcement from the external environment. Its actions consist of commands that it can give to the low-level learner. When the master generates a particular command to the slave, it must reward the slave for taking actions that satisfy the command, even if they do not result in external reinforcement. The master, then, learns a mapping from states to commands. The slave learns a mapping from commands and states to external actions. The set of and their associated reinforcement functions are established in advance of the learning. This is really an instance of the general behaviors" approach, in which the slave can execute any of the behaviors depending on its command. The reinforcement functions for the individual behaviors commands are given, but learning takes place simultaneously at both the high and low levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Compositional Q-learning</head><p>Singh's compositional Q-learning 1992b, 1992a C-QL consists of a hierarchy based on the temporal sequencing of subgoals. The elemental tasks are behaviors that achieve some recognizable condition. The high-level goal of the system is to achieve some set of conditions in sequential order. The achievement of the conditions provides reinforcement for the elemental tasks, which are trained to achieve individual subgoals. Then, the gating function learns to switch the elemental tasks in order to achieve the appropriate high-level sequential goal. This method was used by Tham and Prager 1994 to learn to control a simulated multi-link robot arm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Hierarchical Distance to Goal</head><p>Especially if we consider reinforcement learning modules to be part of larger agent architectures, it is important to consider problems in which goals are dynamically input to the learner. Kaelbling's HDG algorithm 1993a uses a hierarchical approach to solving problems when goals of achievement the agent should get to a particular state as quickly as possible are given to an agent dynamically.</p><p>The HDG algorithm works by analogy with navigation in a harbor. The environment is partitioned a priori, but more recent w ork Ashar, 1994 addresses the case of learning the partition into a set of regions whose centers are known as If the agent i s currently in the same region as the goal, then it uses low-level actions to move to the goal. If not, then high-level information is used to determine the next landmark on the shortest path from the agent's closest landmark to the goal's closest landmark. Then, the agent uses low-level information to aim toward that next landmark. If errors in action cause deviations in the path, there is no problem; the best aiming point is recomputed on every step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Partially Observable Environments</head><p>In many real-world environments, it will not be possible for the agent t o h a ve perfect and complete perception of the state of the environment. Unfortunately, complete observability is necessary for learning methods based on <ref type="bibr">MDPs.</ref> In this section, we consider the case in which the agent makes observations of the state of the environment, but these observations may be noisy and provide incomplete information. In the case of a robot, for instance, it might observe whether it is in a corridor, an open room, a T-junction, etc., and those observations might be error-prone. This problem is also referred to as the problem of perception," aliasing," or state." In this section, we will consider extensions to the basic MDP framework for solving partially observable problems. The resulting formal model is called a partially observable Markov decision process or POMDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">State-Free Deterministic Policies</head><p>The most naive strategy for dealing with partial observability is to ignore it. That is, to treat the observations as if they were the states of the environment and try to learn to behave. <ref type="figure" target="#fig_12">Figure 9</ref> shows a simple environment in which the agent is attempting to get to the printer from an ooce. If it moves from the ooce, there is a good chance that the agent will end up in one of two places that look like but that require diierent actions for getting to the printer. If we consider these states to be the same, then the agent cannot possibly behave optimally. But how w ell can it do?</p><p>The resulting problem is not Markovian, and Q-learning cannot be guaranteed to converge. Small breaches of the Markov requirement are well handled by Q-learning, but it is possible to construct simple environments that cause Q-learning to oscillate <ref type="bibr" target="#b22">Chrisman &amp; Littman, 1993</ref>. It is possible to use a model-based approach, however; act according to some policy and gather statistics about the transitions between observations, then solve for the optimal policy based on those observations. Unfortunately, when the environment i s n o t Markovian, the transition probabilities depend on the policy being executed, so this new policy will induce a new set of transition probabilities. This approach m a y yield plausible results in some cases, but again, there are no guarantees.</p><p>It is reasonable, though, to ask what the optimal policy mapping from observations to actions, in this case is. It is NP-hard Littman, 1994b to this mapping, and even the best mapping can have v ery poor performance. In the case of our agent trying to get to the printer, for instance, any deterministic state-free policy takes an innnite number of steps to reach the goal on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">State-Free Stochastic Policies</head><p>Some improvement can be gained by considering stochastic policies; these are mappings from observations to probability distributions over actions. If there is randomness in the agent's actions, it will not get stuck in the hall forever. Jaakkola, Singh, and Jordan 1995 have developed an algorithm for locally-optimal stochastic policies, but a globally optimal policy is still NP hard.</p><p>In our example, it turns out that the optimal stochastic policy is for the agent, when in a state that looks like a hall, to go east with probability 2 , p 2 0:6 and west with probability p 2 , 1 0:4. This policy can be found by solving a simple in this case quadratic program. The fact that such a simple example can produce irrational numbers gives some indication that it is a diicult problem to solve exactly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Policies with Internal State</head><p>The only way to behave truly eeectively in a wide-range of environments is to use memory of previous actions and observations to disambiguate the current state. There are a variety of approaches to learning policies with internal state.</p><p>Recurrent Q-learning One intuitively simple approach is to use a recurrent neural network to learn Q values. The network can be trained using backpropagation through time or some other suitable technique and learns to retain features" to predict value. This approach has been used by a n umber of researchers Meeden, <ref type="bibr" target="#b76">McGraw, &amp; Blank, 1993;</ref><ref type="bibr" target="#b61">Lin &amp; Mitchell, 1992;</ref><ref type="bibr" target="#b102">Schmidhuber, 1991b</ref>. It seems to work eeectively on simple problems, but can suuer from convergence to local optima on more complex problems.</p><p>Classiier Systems <ref type="bibr">Classiier systems Holland, 1975;</ref><ref type="bibr" target="#b41">Goldberg, 1989</ref> were explicitly developed to solve problems with delayed reward, including those requiring short-term memory. The internal mechanism typically used to pass reward back through chains of decisions, called the bucket brigade algorithm, bears a close resemblance to Q-learning. In spite of some early successes, the original design does not appear to handle partially observed environments robustly.</p><p>Recently, this approach has been reexamined using insights from the reinforcementlearning literature, with some success. Dorigo did a comparative study of Q-learning and classiier systems <ref type="bibr" target="#b36">Dorigo &amp; Bersini, 1994</ref>. Clii and Ross 1994 start with Wilson's zeroth- level classiier system Wilson, 1995 and add one and two-bit memory registers. They that, although their system can learn to use short-term memory registers eeectively, the approach is unlikely to scale to more complex environments. Dorigo and Colombetti applied classiier systems to a moderately complex problem of learning robot behavior from immediate reinforcement <ref type="bibr" target="#b38">Dorigo, 1995;</ref><ref type="bibr" target="#b37">Dorigo &amp; Colombetti, 1994.</ref> Finite-history-window Approach One way to restore the Markov property is to allow decisions to be based on the history of recent observations and perhaps actions. Lin and Mitchell 1992 used a history window to learn a pole balancing task. McCallum 1995 describes the suux memory" which learns a variable-width window that serves simultaneously as a model of the environment and a policy. This system has had excellent results in a very complex driving-simulation domain . Ring 1994 has a neural-network approach that uses a variable history window, adding history when necessary to disambiguate situations.</p><p>POMDP Approach Another strategy consists of using hidden Markov model HMM techniques to learn a model of the environment, including the hidden state, then to use that model to construct a perfect memory controller Cassandra, <ref type="bibr" target="#b19">Kaelbling, &amp; Littman, 1994;</ref><ref type="bibr" target="#b66">Lovejoy, 1991;</ref><ref type="bibr" target="#b78">Monahan, 1982.</ref> Chrisman 1992 showed how the forward-backward algorithm for learning HMMs could be adapted to learning POMDPs. He, and later McCallum 1993, also gave heuristic statesplitting rules to attempt to learn the smallest possible model for a given environment. The resulting model can then be used to integrate information from the agent's observations in order to make decisions. <ref type="figure" target="#fig_0">Figure 10</ref> illustrates the basic structure for a perfect-memory controller. The component on the left is the state estimator, which computes the agent's belief state, b as a function of the old belief state, the last action a, and the current observation i. In this context, a belief state is a probability distribution over states of the environment, indicating the likelihood, given the agent's past experience, that the environment is actually in each of those states. The state estimator can be constructed straightforwardly using the estimated world model and Bayes' rule. Now w e are left with the problem of a policy mapping belief states into action. This problem can be formulated as an MDP, but it is diicult to solve using the techniques described earlier, because the input space is continuous. Chrisman's approach 1992 does not take i n to account future uncertainty, but yields a policy after a small amount of computation. A standard approach from the operations-research literature is to solve for the optimal policy or a close approximation thereof based on its representation as a piecewiselinear and convex function over the belief space. This method is computationally intractable, but may serve as inspiration for methods that make further approximations <ref type="bibr" target="#b19">Cassandra et al., 1994;</ref><ref type="bibr" target="#b64">Littman, Cassandra, &amp; Kaelbling, 1995a.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Reinforcement Learning Applications</head><p>One reason that reinforcement learning is popular is that is serves as a theoretical tool for studying the principles of agents learning to act. But it is unsurprising that it has also been used by a n umber of researchers as a practical computational tool for constructing autonomous systems that improve themselves with experience. These applications have ranged from robotics, to industrial manufacturing, to combinatorial search problems such as computer game playing.</p><p>Practical applications provide a test of the eecacy and usefulness of learning algorithms. They are also an inspiration for deciding which components of the reinforcement learning framework are of practical importance. For example, a researcher with a real robotic task can provide a data point to questions such as:</p><p>How important is optimal exploration? Can we break the learning period into exploration phases and exploitation phases?</p><p>What is the most useful model of long-term reward: Finite horizon? Discounted?</p><p>Innnite horizon?</p><p>How m uch computation is available between agent decisions and how should it be used?</p><p>What prior knowledge can we build into the system, and which algorithms are capable of using that knowledge? Let us examine a set of practical applications of reinforcement learning, while bearing these questions in mind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Game Playing</head><p>Game playing has dominated the Artiicial Intelligence world as a problem domain ever since the was born. Two-player games do not into the established reinforcement-learning framework since the optimality criterion for games is not one of maximizing reward in the face of a environment, but one of maximizing reward against an optimal adversary minimax. Nonetheless, reinforcement-learning algorithms can be adapted to work for a very general class of games Littman, 1994a and many researchers have used reinforcement learning in these environments. One application, spectacularly far ahead of its time, was Samuel's checkers playing system Samuel, 1959. This learned a value function represented by a linear function approximator, and employed a training scheme similar to the updates used in value iteration, temporal diierences and Q-learning.</p><p>More recently, <ref type="bibr">T esauro 1992, 1994, 1995</ref>   <ref type="table">Table 2</ref>: TD-Gammon's performance in games against the top human professional players. A backgammon tournament i n volves playing a series of games for points until one player reaches a set target. TD-Gammon won none of these tournaments but came suuciently close that it is now considered one of the best few players in the world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>neural network as a function approximator for the value function</head><p>Board Position ! Probability of victory for current player:</p><p>Two v ersions of the learning algorithm were used. The which w e will call Basic TDGammon, used very little predeened knowledge of the game, and the representation of a board position was virtually a raw encoding, suuciently powerful only to permit the neural network to distinguish between conceptually diierent positions. The second, TD-Gammon, was provided with the same raw state information supplemented by a n umber of handcrafted features of backgammon board positions. Providing hand-crafted features in this manner is a good example of how inductive biases from human knowledge of the task can be supplied to a learning algorithm. The training of both learning algorithms required several months of computer time, and was achieved by constant self-play. No exploration strategy was used|the system always greedily chose the move with the largest expected probability of victory. This naive exploration strategy proved entirely adequate for this environment, which is perhaps surprising given the considerable work in the reinforcement-learning literature which has produced numerous counter-examples to show that greedy exploration can lead to poor learning performance. Backgammon, however, has two important properties. Firstly, whatever policy is followed, every game is guaranteed to end in time, meaning that useful reward information is obtained fairly frequently. Secondly, the state transitions are suuciently stochastic that independent of the policy, all states will occasionally be visited|a wrong initial value function has little danger of starving us from visiting a critical part of state space from which important information could be obtained.</p><p>The results <ref type="table">Table 2</ref> of TD-Gammon are impressive. It has competed at the very top level of international human play. Basic TD-Gammon played respectably, but not at a professional standard. Although experiments with other games have in some cases produced interesting learning behavior, no success close to that of TD-Gammon has been repeated. Other games that have been studied include Go <ref type="bibr" target="#b103">Schraudolph, Dayan, &amp; Sejnowski, 1994 and</ref><ref type="bibr">Chess Thrun, 1995</ref>. It is still an open question as to if and how the success of TD-Gammon can be repeated in other domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Robotics and Control</head><p>In recent y ears there have been many robotics and control applications that have used reinforcement learning. Here we will concentrate on the following four examples, although many other interesting ongoing robotics investigations are underway.</p><p>1. Schaal and Atkeson 1994 constructed a two-armed robot, shown in <ref type="figure" target="#fig_0">Figure 11</ref>, that learns to juggle a device known as a devil-stick. This is a complex non-linear control task involving a six-dimensional state space and less than 200 msecs per control decision. After about 40 initial attempts the robot learns to keep juggling for hundreds of hits. A typical human learning the task requires an order of magnitude more practice to achieve proociency at mere tens of hits. The juggling robot learned a world model from experience, which w as generalized to unvisited states by a function approximation scheme known as locally weighted regression <ref type="bibr" target="#b24">Cleveland &amp; Delvin, 1988;</ref><ref type="bibr" target="#b81">Moore &amp; Atkeson, 1992</ref>. Between each trial, a form of dynamic programming speciic to linear control policies and locally linear transitions was used to improve the policy. The form of dynamic programming is known as linear-quadratic-regulator design <ref type="bibr" target="#b96">Sage &amp; White, 1977.</ref> 2. Mahadevan and Connell 1991a discuss a task in which a mobile robot pushes large boxes for extended periods of time. Box-pushing is a well-known diicult robotics problem, characterized by immense uncertainty in the results of actions. Q-learning was used in conjunction with some novel clustering techniques designed to enable a higher-dimensional input than a tabular approach w ould have permitted. The robot learned to perform competitively with the performance of a human-programmed solution. Another aspect of this work, mentioned in Section 6.3, was a pre-programmed breakdown of the monolithic task description into a set of lower level tasks to be learned.</p><p>3. Mataric 1994 describes a robotics experiment with, from the viewpoint of theoretical reinforcement learning, an unthinkably high dimensional state space, containing many dozens of degrees of freedom. Four mobile robots traveled within an enclosure collecting small disks and transporting them to a destination region. There were three enhancements to the basic Q-learning algorithm. Firstly, pre-programmed signals called progress estimators were used to break the monolithic task into subtasks. This was achieved in a robust manner in which the robots were not forced to use the estimators, but had the freedom to proot from the inductive bias they provided. Secondly, control was decentralized. Each robot learned its own policy independently without explicit communication with the others. Thirdly, state space was brutally quantized into a small number of discrete states according to values of a small number of pre-programmed boolean features of the underlying sensors. The performance of the Q-learned policies were almost as good as a simple hand-crafted controller for the job.</p><p>4. Q-learning has been used in an elevator dispatching task <ref type="bibr" target="#b28">Crites &amp; Barto, 1996</ref>. The problem, which has been implemented in simulation only at this stage, involved four elevators servicing ten The objective w as to minimize the average squared wait time for passengers, discounted into future time. The problem can be posed as a discrete Markov system, but there are 10 22 states even in the most simpliied version of the problem. Crites and Barto used neural networks for function approximation and provided an excellent comparison study of their Q-learning approach against the most popular and the most sophisticated elevator dispatching algorithms. The squared wait time of their controller was approximately 7 less than the best alternative algorithm Empty the System" heuristic with a receding horizon controller and less than half the squared wait time of the controller most frequently used in real elevator systems.</p><p>5. The example concerns an application of reinforcement learning by one of the authors of this survey to a packaging task from a food processing industry. The problem involves containers with variable numbers of non-identical products. The product characteristics also vary with time, but can be sensed. Depending on the task, various constraints are placed on the container--lling procedure. Here are three examples:</p><p>The mean weight of all containers produced by a shift must not be below the manufacturer's declared weight W.</p><p>The number of containers below the declared weight m ust be less than P. No containers may be produced below w eight W 0 .</p><p>Such tasks are controlled by machinery which operates according to various setpoints. Conventional practice is that setpoints are chosen by h uman operators, but this choice is not easy as it is dependent on the current product characteristics and the current task constraints. The dependency is often diicult to model and highly non-linear. The task was posed as a Markov decision task in which the state of the system is a function of the product characteristics, the amount of time remaining in the production shift and the mean wastage and percent below declared in the shift so far. The system was discretized into 200,000 discrete states and local weighted regression was used to learn and generalize a transition model. Prioritized sweeping was used to maintain an optimal value function as each new piece of transition information was obtained. In simulated experiments the savings were considerable, typically with wastage reduced by a factor of ten. Since then the system has been deployed successfully in several factories within the United States.</p><p>Some interesting aspects of practical reinforcement learning come to light from these examples. The most striking is that in all cases, to make a real system work it proved necessary to supplement the fundamental algorithm with extra pre-programmed knowledge. Supplying extra knowledge comes at a price: more human eeort and insight is required and the system is subsequently less autonomous. But it is also clear that for tasks such a s these, a knowledge-free approach w ould not have a c hieved worthwhile performance within the lifetime of the robots.</p><p>What forms did this pre-programmed knowledge take? It included an assumption of linearity for the juggling robot's policy, a manual breaking up of the task into subtasks for the two mobile-robot examples, while the box-pusher also used a clustering technique for the Q values which assumed locally consistent Q values. The four disk-collecting robots additionally used a manually discretized state space. The packaging example had far fewer dimensions and so required correspondingly weaker assumptions, but there, too, the assumption of local piecewise continuity in the transition model enabled massive reductions in the amount of learning data required.</p><p>The exploration strategies are interesting too. The juggler used careful statistical analysis to judge where to prootably experiment. However, both mobile robot applications were able to learn well with greedy exploration|always exploiting without deliberate exploration. The packaging task used optimism in the face of uncertainty. None of these strategies mirrors theoretically optimal but computationally intractable exploration, and yet all proved adequate.</p><p>Finally, it is also worth considering the computational regimes of these experiments. They were all very diierent, which indicates that the diiering computational demands of various reinforcement learning algorithms do indeed have an array of diiering applications. The juggler needed to make v ery fast decisions with low latency between each hit, but had long periods 30 seconds and more between each trial to consolidate the experiences collected on the previous trial and to perform the more aggressive computation necessary to produce a new reactive controller on the next trial. The box-pushing robot was meant t o operate autonomously for hours and so had to make decisions with a uniform length control cycle. The cycle was suuciently long for quite substantial computations beyond simple Qlearning backups. The four disk-collecting robots were particularly interesting. Each robot had a short life of less than 20 minutes due to battery constraints meaning that substantial number crunching was impractical, and any signiicant combinatorial search w ould have used a signiicant fraction of the robot's learning lifetime. The packaging task had easy constraints. One decision was needed every few minutes. This provided opportunities for fully computing the optimal value function for the 200,000-state system between every control cycle, in addition to performing massive cross-validation-based optimization of the transition model being learned.</p><p>A great deal of further work is currently in progress on practical implementations of reinforcement learning. The insights and task constraints that they produce will have a n important eeect on shaping the kind of algorithms that are developed in future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusions</head><p>There are a variety of reinforcement-learning techniques that work eeectively on a variety of small problems. But very few of these techniques scale well to larger problems. This is not because researchers have done a bad job of inventing learning techniques, but because it is very diicult to solve arbitrary problems in the general case. In order to solve highly complex problems, we m ust give u p tabula rasa learning techniques and begin to incorporate bias that will give leverage to the learning process.</p><p>The necessary bias can come in a variety of forms, including the following:</p><p>shaping: The technique of shaping is used in training animals <ref type="bibr" target="#b45">Hilgard &amp; Bower, 1975;</ref><ref type="bibr">a</ref> teacher presents very simple problems to solve then gradually exposes the learner to more complex problems. Shaping has been used in supervised-learning systems, and can be used to train hierarchical reinforcement-learning systems from the bottom up Lin, 1991, and to alleviate problems of delayed reinforcement b y decreasing the delay u n til the problem is well understood <ref type="bibr" target="#b37">Dorigo &amp; Colombetti, 1994;</ref><ref type="bibr" target="#b38">Dorigo, 1995.</ref> local reinforcement signals: Whenever possible, agents should be given reinforcement signals that are local. In applications in which it is possible to compute a gradient, rewarding the agent for taking steps up the gradient, rather than just for achieving the goal, can speed learning signiicantly <ref type="bibr" target="#b72">Mataric, 1994.</ref> imitation: An agent can learn by y w atching" another agent perform the task Lin, 1991.</p><p>For real robots, this requires perceptual abilities that are not yet available. But another strategy is to have a h uman supply appropriate motor commands to a robot through a joystick or steering wheel <ref type="bibr" target="#b88">Pomerleau, 1993.</ref> problem decomposition: Decomposing a huge learning problem into a collection of smaller ones, and providing useful reinforcement signals for the subproblems is a very powerful technique for biasing learning. Most interesting examples of robotic reinforcement learning employ this technique to some extent <ref type="bibr" target="#b27">Connell &amp; Mahadevan, 1993.</ref> reeexes: One thing that keeps agents that know nothing from learning anything is that they have a hard time even the interesting parts of the space; they wander around at random never getting near the goal, or they are always immediately. These problems can be ameliorated by programming a set of that cause the agent to act initially in some way that is reasonable <ref type="bibr" target="#b72">Mataric, 1994;</ref><ref type="bibr" target="#b106">Singh, Barto, Grupen, &amp; Connolly, 1994</ref>. These reeexes can eventually be overridden by more detailed and accurate learned knowledge, but they at least keep the agent alive and pointed in the right direction while it is trying to learn. Recent w ork by Millan 1996 explores the use of reeexes to make robot learning safer and more eecient. With appropriate biases, supplied by h uman programmers or teachers, complex reinforcementlearning problems will eventually be solvable. There is still much w ork to be done and many interesting questions remaining for learning techniques and especially regarding methods for approximating, decomposing, and incorporating bias into problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The standard reinforcement-learning model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Environment: You are in state 65. You have 4 possible actions. Agent: I'll take action 2. Environment: You received a reinforcement of 7 units. You are now in state 15. You have 2 possible actions. Agent: I'll take action 1. Environment: You received a reinforcement of -4 units. You are now in state 65. You have 4 possible actions. Agent: I'll take action 2. Environment: You received a reinforcement of 5 units. You are now in state 44. You have 5 possible actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparing models of optimality. All unlabeled arrows produce a reward of zero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A Tsetlin automaton with 2N states. The top row shows the state transitions that are made when the previous action resulted in a reward of 1; the bottom row shows transitions after a reward of 0. In states in the left half of the action 0 is taken; in those on the right, action 1 is taken.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>an arbitrary policy 0 loop := 0 compute the value function of policy : solve the linear equations V s = Rs; s + P s 0 2S Ts; s; s 0 V s 0 improve the policy at each state: 0 s := arg max a Rs; a + P s 0 2S Ts; a; s 0 V s 0 until = 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Architecture for the adaptive heuristic critic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: In this environment, due to Whitehead 1991, random exploration would take take O2 n steps to reach the goal even once, whereas a more intelligent exploration strategy e.g. any u n tried action leads directly to goal" would require only On 2 steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: A 3277-state grid world. This was formulated as a shortest-path reinforcementlearning problem, which yields the same result as if a reward of 1 is given at the goal, a reward of zero elsewhere and a discount factor is used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: a A t wo-dimensional maze problem. The point robot must a path from start to goal without crossing any of the barrier lines. b The path taken by PartiGame during the entire trial. It begins with intense exploration to a route out of the almost entirely enclosed start region. Having eventually reached a suuciently high resolution, it discovers the gap and proceeds greedily towards the goal, only to be temporarily blocked by the goal's barrier region. c The second trial.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: A structure of gated behaviors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: An example of a partially observable environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Structure of a POMDP agent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Schaal and Atkeson's devil-sticking robot. The tapered stick is hit alternately by each of the two hand sticks. The task is to keep the devil stick from falling for as many hits as possible. The robot has three motors indicated by torque vectors 1 ; ; 2 ; ; 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>.</head><label></label><figDesc></figDesc><table>The technique only applies under the 

discounted expected reward criterion. For each action, consider the number of times it has 

been chosen, n, v ersus the number of times it has paid oo, w. F or certain discount factors, 
there are published tables of values," In; w for each pair of n and w. L o o k u p 
the index value for each action i, In i ; w i . It represents a comparative measure of the 
combined value of the expected payoo of action i given its history of payoos and the value 

of the information that we w ould get by c hoosing it. Gittins has shown that choosing the 
action with the largest index value guarantees the optimal balance between exploration and 
exploitation. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>applied the temporal diierence algorithm to backgammon. Backgammon has approximately 10 20 states, making table-based rein- forcement learning impossible. Instead, Tesauro used a backpropagation-based three-layer</figDesc><table>Training 
Games 
Hidden 
Units 
Results 

Basic 
Poor 
TD 1.0 
300,000 
80 
Lost by 13 points in 51 
games 
TD 2.0 
800,000 
40 
Lost by 7 points in 38 
games 
TD 2.1 
1,500,000 
80 
Lost by 1 point i n 4 0 
games 

</table></figure>

			<note place="foot" n="1">. This assumption may be disappointing; after all, operation in non-stationary environments is one of the motivations for building learning systems. In fact, many of the algorithms described in later sections are eeective in slowly-varying non-stationary environments, but there is very little theoretical analysis in this area.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Thanks to Marco Dorigo and three anonymous reviewers for comments that have helped to improve this paper. Also thanks to our many colleagues in the reinforcement-learning community who have done this work and explained it to us.</p><p>Leslie Pack Kaelbling was supported in part by NSF grants IRI-9453383 and IRI-9312395. Michael Littman was supported in part by Bellcore. Andrew Moore was supported in part by an NSF Research Initiation Award and by 3M Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalization and scaling in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Ackley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Touretzky, D. S. Ed</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1990" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5500557</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A new approach to manipulator control: Cerebellar model articulation controller cmac</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Albus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Dynamic Systems, Measurement and Control</title>
		<imprint>
			<biblScope unit="page">97</biblScope>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Brains, Behavior, and Robotics. BYTE Books, Subsidiary of McGrawHill</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Albus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
			<pubPlace>Peterborough, New Hampshire</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning and Problem Solving with Multilayer Connectionist Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<pubPlace>Amherst, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Hierarchical learning in stochastic domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Ashar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<pubPlace>Providence, Rhode Island</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Brown University</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Residual algorithms: Reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Machine Learning</title>
		<editor>Prieditis, A., &amp; Russell, S</editor>
		<meeting>the Twelfth International Conference on Machine Learning<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page">30037</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Reinforcement learning with high-dimensional, continuous actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Klopf</surname></persName>
		</author>
		<idno>WL-TR-93-1147</idno>
		<imprint>
			<date type="published" when="1993" />
			<pubPlace>Wright-Patterson Air Force Base Ohio</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Wright Laboratory</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to act using real-time dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Bradtke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artiicial Intelligence</title>
		<imprint>
			<biblScope unit="volume">721</biblScope>
			<biblScope unit="page">811138</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neuronlike adaptive elements that can solve diicult learning control problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Anderson</surname></persName>
		</author>
		<idno>SMC-135</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">8344846</biblScope>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Artiicial neural networks and approximate reasoning for intelligent control in space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Berenji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">American Control Conference</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page">107551080</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bandit Problems: Sequential Allocation of Experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ristedt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Chapman and Hall</publisher>
			<pubPlace>London, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Dynamic Programming: Deterministic and Stochastic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliis, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dynamic Programming and Optimal Control. A thena Scientiic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Belmont, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive aggregation for innnite horizon dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Casta~ Non</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">346</biblScope>
			<biblScope unit="page">5899598</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Parallel and Distributed Computation: Numerical Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliis, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E P</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Draper</surname></persName>
		</author>
		<title level="m">Empirical Model-Building and Response Surfaces</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generalization in reinforcement learning: Safely approximating the value function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Boyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Tesauro, G., Touretzky, D. S., &amp; Leen, T. K</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Introduction to Control Theory including Optimal Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burghes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<publisher>Ellis Horwood</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Acting optimally in partially observable stochastic domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Cassandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth National Conference o n Artiicial Intelligence</title>
		<meeting>the Twelfth National Conference o n Artiicial Intelligence<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Input generalization in delayed reinforcement learning: An algorithm and performance comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference o n A rtiicial Intelligence</title>
		<meeting>the International Joint Conference o n A rtiicial Intelligence<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reinforcement learning with perceptual aliasing: The perceptual distinctions approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chrisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth National Conference o n A rtiicial Intelligence</title>
		<meeting>the Tenth National Conference o n A rtiicial Intelligence<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page">1833188</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hidden state and short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chrisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presentation at Reinforcement Learning Workshop, Machine Learning Conference</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast and eecient reinforcement learning with truncated temporal diierences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cichosz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Mulawka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Machine Learning</title>
		<editor>Prieditis, A., &amp; Russell, S</editor>
		<meeting>the Twelfth International Conference on Machine Learning<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page">999107</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Locally weighted regression: An approach t o regression analysis by local</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Delvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">83403</biblScope>
			<biblScope unit="page">5966610</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adding temporary memory to ZCS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adaptive Behavior</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">1011150</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The complexity of stochastic games. Information and Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Condon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">962</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rapid task learning for real robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Connell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robot Learning</title>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving elevator performance using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Crites</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems 8</title>
		<editor>Touretzky, D., Mozer, M., &amp; Hasselmo, M</editor>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The convergence of TD for general</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">362</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feudal reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 5</title>
		<editor>Hanson, S. J., Cowan, J. D., &amp; Giles, C. L</editor>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ca</forename><forename type="middle">Morgan</forename><surname>San Mateo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaufmann</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<title level="m">TD converges with probability 1 . Machine Learning</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">143</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Planning with deadlines in stochastic domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicholson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh National Conference o n A rtiicial Intelligence</title>
		<meeting>the Eleventh National Conference o n A rtiicial Intelligence<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A probabilistic production and inventory problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>&amp;apos;epenoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">988108</biblScope>
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Finite State Markovian Decision Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Derman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<publisher>Academic Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A comparison of q-learning and classiier systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bersini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From Animals to Animats: Proceedings of the Third International Conference on the Simulation of Adaptive Behavior</title>
		<meeting><address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robot shaping: Developing autonomous agents through learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Colombetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artiicial Intelligence</title>
		<imprint>
			<biblScope unit="volume">712</biblScope>
			<biblScope unit="page">3211370</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Alecsys and the AutonoMouse: Learning to control a real robot by distributed classiier systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">EEcient reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><surname>Fiechter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory</title>
		<meeting>the Seventh Annual ACM Conference on Computational Learning Theory</meeting>
		<imprint>
			<publisher>Association of Computing Machinery</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page">88897</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Multi-armed Bandit Allocation Indices. Wiley-Interscience series in systems and optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gittins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Wiley</publisher>
			<pubPlace>Chichester, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Genetic algorithms in search, optimization, and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Addison-Wesley, MA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stable function approximation in dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference o n Machine Learning</title>
		<editor>Prieditis, A., &amp; Russell, S</editor>
		<meeting>the Twelfth International Conference o n Machine Learning<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page">2611268</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A stochastic reinforcement learning algorithm for learning real-valued functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gullapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6711692</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Reinforcement learning and its application to control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gullapalli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<pubPlace>Amherst, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hilgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Bower</surname></persName>
		</author>
		<title level="m">Theories of Learning fourth edition</title>
		<meeting><address><addrLine>Englewood Cliis, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On nonterminating stochastic games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Hooman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Karp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">3599370</biblScope>
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Adaptation in Natural and Artiicial Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<publisher>University of Michigan Press</publisher>
			<pubPlace>Ann Arbor, MI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Dynamic Programming and Markov Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Howard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On the convergence of stochastic iterative dynamic programming algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="page">66</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Monte-carlo reinforcement learning in non-Markovian decision problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Tesauro, G., Touretzky, D. S., &amp; Leen, T. K</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hierarchical learning in stochastic domains: Preliminary results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Machine Learning</title>
		<meeting>the Tenth International Conference on Machine Learning<address><addrLine>Amherst, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Associative reinforcement learning: A generate and test algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Cambridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page">153</biblScope>
		</imprint>
	</monogr>
	<note>Learning in Embedded Systems</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Associative reinforcement learning: Functions in k-DNF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page">153</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Predicting Real-Time Planner Performance by Domain Characterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Brown University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Complexity analysis of real-time reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Simmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh National Conference o n A rtiicial Intelligence</title>
		<meeting>the Eleventh National Conference o n A rtiicial Intelligence<address><addrLine>Menlo Park, California</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI PresssMIT Press</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page">999105</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Stochastic Systems: Estimation, Identiication, and Adaptive Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Prentice Hall</publisher>
			<pubPlace>Englewood Cliis; New Jersey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A self learning rule-based controller employing approximate reasoning and neural net concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Intelligent Systems</title>
		<imprint>
			<biblScope unit="page">71193</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Programming robots using reinforcement learning and teaching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth National Conference o n A rtiicial Intelligence</title>
		<meeting>the Ninth National Conference o n A rtiicial Intelligence</meeting>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Hierachical learning of robot skills by reinforcement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Networks</title>
		<meeting>the International Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Reinforcement Learning for Robots Using Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Memory approaches to reinforcement learning in nonMarkovian domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University, S c hool of Computer Science</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. rep. CMU-CS-92-138</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Markov games as a framework for multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Machine Learning</title>
		<meeting>the Eleventh International Conference on Machine Learning<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page">1577163</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Memoryless policies: Theoretical limitations and practical results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behavior</title>
		<editor>Clii, D., Husbands, P., Meyer, J.-A., &amp; Wilson, S. W</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning policies for partially observable environments: Scaling up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cassandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Machine Learning</title>
		<editor>Prieditis, A., &amp; Russell, S</editor>
		<meeting>the Twelfth International Conference on Machine Learning<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page">3622370</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">On the complexity of solving Markov decision problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Annual Conference o n Uncertainty in Artiicial Intelligence UAII95 Montreal, Qu ebec</title>
		<meeting>the Eleventh Annual Conference o n Uncertainty in Artiicial Intelligence UAII95 Montreal, Qu ebec</meeting>
		<imprint>
			<publisher>Canada</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A survey of algorithmic methods for partially observable Markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lovejoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">47766</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning to coordinate behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Eighth National Conference o n A rtiicial Intelligence</title>
		<meeting>Eighth National Conference o n A rtiicial Intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page">7966802</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">To discount or not to discount in reinforcement learning: A case study comparing R learning and Q learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Machine Learning</title>
		<meeting>the Eleventh International Conference on Machine Learning<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page">1644172</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Average reward reinforcement learning: Foundations, algorithms, and empirical results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page">221</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Automatic programming of behavior-based robots using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Connell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth National Conference o n Artiicial Intelligence</title>
		<meeting>the Ninth National Conference o n Artiicial Intelligence<address><addrLine>Anaheim, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Scaling reinforcement learning to robotics by exploiting the subsumption architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Connell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Workshop on Machine Learning</title>
		<meeting>the Eighth International Workshop on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page">3288332</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Reward functions for accelerated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Mataric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Machine Learning</title>
		<editor>Cohen, W. W., &amp; Hirsh, H</editor>
		<meeting>the Eleventh International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Reinforcement Learning with Selective Perception and Hidden State</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University o f R o c hester</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Overcoming incomplete perception with utile distinction memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Machine Learning</title>
		<meeting>the Tenth International Conference on Machine Learning<address><addrLine>Amherst, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Instance-based utile distinctions for reinforcement learning with hidden state</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference Machine Learning</title>
		<meeting>the Twelfth International Conference Machine Learning<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page">3877395</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Emergent control and planning in an autonomous vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meeden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mcgraw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Annual Meeting of the Cognitive Science S o ciety</title>
		<editor>Touretsky, D. Ed</editor>
		<meeting>the Fifteenth Annual Meeting of the Cognitive Science S o ciety<address><addrLine>Hillsdale, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page">7355740</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Rapid, safe, and incremental learning of navigation strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D R</forename><surname>Millan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="page">263</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A survey of partially observable Markov decision processes: Theory, models, and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Monahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">1116</biblScope>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Variable resolution dynamic programming: EEciently learning action maps in multivariate real-valued spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eighth International Machine Learning Workshop</title>
		<meeting>Eighth International Machine Learning Workshop</meeting>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">The parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Cowan, J. D., Tesauro, G., &amp; Alspector, J</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">An investigation of memory-based function approximators for learning control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Atkeson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>MIT Artiical Intelligence Laboratory</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Prioritized sweeping: Reinforcement learning with less data and less real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Atkeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Memory-based learning for control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Atkeson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<idno>CMU-RI-TR-95-18</idno>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>CMU Robotics Institute</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Learning Automata: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A L</forename><surname>Thathachar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliis, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Learning automata|a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Narendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A L</forename><surname>Thathachar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page">3233334</biblScope>
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">EEcient learning and planning within the Dyna framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adaptive Behavior</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">4377454</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Incremental multi-step Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Machine Learning</title>
		<meeting>the Eleventh International Conference on Machine Learning<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page">2266232</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Neural network perception for mobile robot guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Pomerleau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Kluwer Academic Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Markov Decision Processes|Discrete Stochastic Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Modiied policy iteration algorithms for discounted Markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="page">112771137</biblScope>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Continual Learning in Reinforcement Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Ring</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<pubPlace>Austin, Texas</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University o f T exas at Austin</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Mathematical and computational techniques for multilevel adaptive methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rr Ude</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematics</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Parallel Distributed P r ocessing: Explorations in the microstructures of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eds</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Foundations. The MIT Press</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">On-line Q-learning using connectionist systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Rummery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. rep. CUEDDF-INFENGGTR166</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>Cambridge University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Numerical dynamic programming in economics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Computational Economics</title>
		<imprint>
			<publisher>North Holland</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Optimum Systems Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Sage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>White</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Active exploration and learning in real-valued spaces using multi-armed bandit allocation indices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salganicoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Machine Learning</title>
		<editor>Prieditis, A., &amp; Russell, S</editor>
		<meeting>the Twelfth International Conference on Machine Learning<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page">4800487</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Some studies in machine learning using the game of checkers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Samuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computers and Thought</title>
		<editor>E. A. Feigenbaum and J. Feldman</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1959" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2111229</biblScope>
		</imprint>
	</monogr>
	<note>Reprinted in</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Robot juggling: An implementation of memory-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Atkeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Control Systems Magazine</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">A general method for multi-agent learning and incremental selfimprovement in unrestricted environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolutionary Computation: Theory and Applications</title>
		<editor>Yao, X. Ed</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Scientiic Publ. Co</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Curious model-building control systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Joint Conference on Neural Networks</title>
		<meeting>International Joint Conference on Neural Networks<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page">145881463</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Reinforcement learning in Markovian and non-Markovian environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lippman, D. S., Moody, J. E., &amp; Touretzky, D. S</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1991" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5000506</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Temporal diierence learning of position evaluation in the game of Go</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Cowan, J. D., Tesauro, G., &amp; Alspector, J. Eds.</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">8177824</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Theory of Linear and Integer Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schrijver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Wiley-Interscience</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">A reinforcement learning method for maximizing undiscounted rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Machine Learning</title>
		<meeting>the Tenth International Conference on Machine Learning<address><addrLine>Amherst, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page">2988305</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Robust reinforcement learning in motion planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grupen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Connolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Cowan, J. D., Tesauro, G., &amp; Alspector, J. Eds.</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">6555662</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Reinforcement learning with replacing eligibility traces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page">221</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Reinforcement learning with a hierarchy of abstract models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth National Conference o n A rtiicial Intelligence</title>
		<meeting>the Tenth National Conference o n A rtiicial Intelligence</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page">2022207</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>San Jose</surname></persName>
		</author>
		<imprint>
			<publisher>AAAI Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Transfer of learning by composing solutions of elemental sequential tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">3233340</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Learning to Solve Markovian Decision Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<idno>93-77</idno>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Massachusetts. Also, CMPSCI</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Stochastic Optimal Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Stengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>John Wiley and Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems 8</title>
		<editor>Touretzky, D., Mozer, M., &amp; Hasselmo, M</editor>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Temporal Credit Assignment in Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<pubPlace>Amherst, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Learning to predict by the method of temporal diierences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page">9944</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Integrated architectures for learning, planning, and reacting based on approximating dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Machine Learning</title>
		<meeting>the Seventh International Conference on Machine Learning<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Planning by incremental dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Workshop on Machine Learning</title>
		<meeting>the Eighth International Workshop on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page">3533357</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Practical issues in temporal diierence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">2577277</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">TD-Gammon, a self-teaching backgammon program, achieves masterlevel play</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page">2155219</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Temporal diierence learning and TD-Gammon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">383</biblScope>
			<biblScope unit="page">58867</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">A modular q-learning architecture for manipulator task decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Prager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference o n Machine Learning</title>
		<meeting>the Eleventh International Conference o n Machine Learning<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Learning to play the game of chess</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Tesauro, G., Touretzky, D. S., &amp; Leen, T. K</editor>
		<meeting><address><addrLine>7 Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Issues in using function approximation for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">; M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Touretzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1993 Connectionist Models</title>
		<meeting>the 1993 Connectionist Models<address><addrLine>Summer School Hillsdale, NJ. Lawrence Erlbaum</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note>Mozer,</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">The role of exploration in learning control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches. V an Nostrand Reinhold</title>
		<editor>White, D. A., &amp; Sofge, D. A</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Asynchronous stochastic approximation and Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page">163</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Feature-based methods for large scale dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page">221</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">A theory of the learnable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="page">113441142</biblScope>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">Learning from Delayed R ewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<pubPlace>King&apos;s College, Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">2799292</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Complexity and cooperation in Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Whitehead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Workshop on Machine Learning</title>
		<meeting>the Eighth International Workshop on Machine Learning<address><addrLine>Evanston, IL</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">A class of gradient-estimating algorithms for reinforcement learning in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE First International Conference o n Neural Networks</title>
		<meeting>the IEEE First International Conference o n Neural Networks<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">2299256</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Analysis of some incremental variants of policy iteration: First steps toward understanding actor-critic learning systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename></persName>
		</author>
		<idno>rep. NU-CCS-93-11</idno>
		<imprint>
			<date type="published" when="1993" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Northeastern University, College of Computer Science</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech.</note>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Tight performance bounds on greedy policies based on imperfect value functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename></persName>
		</author>
		<idno>NU-CCS-93-14</idno>
		<imprint>
			<date type="published" when="1993" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Northeastern University, College of Computer Science</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Classiier based on accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">1477173</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">A reinforcement learning approach to job-shop scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference o n A rtiicial Intellience</title>
		<meeting>the International Joint Conference o n A rtiicial Intellience</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
