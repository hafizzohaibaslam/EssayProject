<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-07-16T23:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AN OVERVIEW ON CLUSTERING METHODS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-04">Apr. 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Soni Madhulatha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Alluri Institute of Management Sciences</orgName>
								<address>
									<region>Warangal</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AN OVERVIEW ON CLUSTERING METHODS</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">2</biblScope>
							<biblScope unit="issue">4</biblScope>
							<biblScope unit="page" from="719" to="725"/>
							<date type="published" when="2012-04">Apr. 2012</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Clustering</term>
					<term>hierarchical algorithm</term>
					<term>partitional algorithm</term>
					<term>distance measure</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Clustering is a common technique for statistical data analysis, which is used in many fields, including machine learning, data mining, pattern recognition, image analysis and bioinformatics. Clustering is the process of grouping similar objects into different groups, or more precisely, the partitioning of a data set into subsets, so that the data in each subset according to some defined distance measure. This paper covers about clustering algorithms, benefits and its applications. Paper concludes by discussing some limitations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HIERARCHICAL CLUSTERING</head><p>A key step in a hierarchical clustering is to select a distance measure. A simple measure is manhattan distance, equal to the sum of absolute distances for each variable. The name comes from the fact that in a two-variable case, the variables can be plotted on a grid that can be compared to city streets, and the distance between two points is the number of blocks a person would walk. A more common measure is Euclidean distance, computed by finding the square of the distance between each variable, summing the squares, and finding the square root of that sum. In the two-variable case, the distance is analogous to</p><p>The Euclidean distance function measures the "asthe-crow-flies distance. The formula for this distance between a point X (X1, X2, etc.) and a point Y (Y1, Y2, etc.) is:</p><formula xml:id="formula_1">? ? ? n y x d 2 ) ( ? j j j 1</formula><p>Deriving the Euclidean distance between two data points involves computing the square root of the sum of the squares of the differences between corresponding values.</p><p>The following figure illustrates the difference between Manhattan distance and Euclidean distance: ? The sum of all intra-cluster variance ? The increase in variance for the cluster being merged ? The probability that candidate clusters spawn from the same distribution function.</p><p>This method builds the hierarchy from the individual elements by progressively merging clusters. Again, we have six elements {a} {b} {c} {d} {e} and {f}. The first step is to determine which elements to merge in a cluster. Usually, we want to take the two closest elements, therefore we must define a distance between elements. One can also construct a distance matrix at this stage.</p><p>Each agglomeration occurs at a greater distance between clusters than the previous agglomeration, and one can decide to stop clustering either when the clusters are too far apart to be merged or when there is a sufficiently small number of clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agglomerative hierarchical clustering</head><p>For example, suppose these data are to be analyzed, where pixel euclidean distance is the distance metric.</p><p>Usually the distance between two clusters and is one of the following:</p><p>? The maximum distance between elements of each cluster is also called complete linkage clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? ?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Divisive clustering</head><p>So far we have only looked at agglomerative clustering, but a cluster hierarchy can also be generated top-down. This variant of hierarchical clustering is called top-down clustering or divisive clustering. We start at the top with all documents in one cluster. The cluster is split using a flat clustering algorithm. This procedure is applied recursively until each document is in its own singleton cluster. Hierarchal method suffers from the fact that once the merge/split is done, it can never be undone. This rigidity is useful in that is useful in that it leads to smaller computation costs by not worrying about a combinatorial number of different choices.</p><p>represented by one of the objects located near the center of the cluster. PAM (Partitioning around Medoids) was one of the first k-medoids algorithm is introduced.</p><p>However there are two approaches to improve the quality of hierarchical clustering</p><p>The pseudo code of the k-medoids algorithm is to explain how it works:</p><p>Perform careful analysis of object linkages at each hierarchical partitioning such as CURE and Chameleon.</p><p>Integrate hierarchical agglomeration and then redefine the result using iterative relocation as in BRICH k-mean algorithm k-medoids algorithm</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K-means algorithm</head><p>The K-means algorithm assigns each point to the cluster whose center also called centroid is nearest. The center is the average of all the points in the cluster that is, its coordinates are the arithmetic mean for each dimension separately over all the points in the cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DENSITY-BASED CLUSTERING</head><p>Density-based clustering algorithms are devised to discover arbitrary-shaped clusters. In this approach, a cluster is regarded as a region in which the density of data objects exceeds a threshold. DBSCAN and SSN are two typical algorithms of this kind.</p><p>The pseudo code of the k-means algorithm is to explain how it works: A. Choose K as the number of clusters. B. Initialize the codebook vectors of the K clusters (randomly, for instance) C. For every new sample vector: a. Compute the distance between the new vector and every cluster's codebook vector. b. Re-compute the closest codebook vector with the new vector, using a learning rate that decreases in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DBSCAN algorithm</head><p>The DBSCAN algorithm was first introduced by Ester, and relies on a density-based notion of clusters. Clusters are identified by looking at the density of points. Regions with a high density of points depict the existence of clusters whereas regions with a low density of points indicate clusters of noise or clusters of outliers. This algorithm is particularly suited to deal with large datasets, with noise, and is able to identify clusters with different sizes and shapes.</p><p>The reason behind choosing the k-means algorithm to study is its popularity for the following reasons:</p><p>? Its time complexity is O (nkl), where n is the number of patterns, k is the number of clusters, and l is the number of iterations taken by the algorithm to converge.</p><p>? Its space complexity is O (k+n). It requires additional space to store the data matrix.</p><p>? It is order-independent; for a given initial seed set of cluster centers, it generates the same partition of the data irrespective of the order in which the patterns are presented to the algorithm.</p><p>The key idea of the DBSCAN algorithm is that, for each point of a cluster, the neighbourhood of a given radius has to contain at least a minimum number of points, that is, the density in the neighbourhood has to exceed some predefined threshold.</p><p>This algorithm needs three input parameters: -k, the neighbour list size;</p><p>-Eps, the radius that delimitate the neighbourhood area of a point (Eps neighbourhood); -MinPts, the minimum number of points that must exist in the Eps-neighbourhood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K-medoids algorithm:</head><p>The basic strategy of k-medoids algorithm is each cluster is ISSN:  www.iosrjen.org</p><p>The clustering process is based on the classification of the points in the dataset as core points, border points and noise points, and on the use of density relations between points to form the clusters. The pseudo code of the DBSCAN algorithm is to explain how it works: To clusters a dataset, our DBSCAN implementation starts by identifying the k nearest neighbours of each point and identify the farthest k nearest neighbour. The average of all this distance is then calculated.</p><p>Then the similarity between pairs of points is calculated in terms of how many nearest neighbours the two points share. Using this similarity measure, the density of each point can be calculated as being the numbers of neighbours with which the number of shared neighbours is equal or greater than Eps.</p><p>For each point of the dataset the algorithm identifies the directly density-reachable points using the Eps threshold provided by the user and classifies the points into core or border points.</p><p>The points are classified as being core points, if the density of the point is equal or greater than MinPts. At this point, the algorithm has all the information needed to start to build the clusters. Those start to be constructed around the core points.</p><p>It then loop trough all points of the dataset and for the core points it starts to construct a new cluster with the support of the GetDRPoints() procedure that follows the definition of density reachable points.</p><p>However, these clusters do not contain all points. They contain only points that come from regions of relatively uniform density. The points that are not classified into any cluster are classified as noise points.</p><p>In this phase the value used as Eps threshold is the average distance calculated previously. At the end, the composition of the clusters is verified in order to check if there exist clusters that can be merged together. This can append if two points of different clusters are at a distance less than Eps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GRID-BASED CLUSTERING</head><p>The grid based clustering approach uses a multiresolution grid data structure. It quantizes the space into a finite number of cells that form a grid structure on which all the operations for clustering are performed. Grid approach includes STING (STatistical INformation Grid) approach and CLIQUE Note: DBSCAN does not deal very well with clusters of different densities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNN ALGORITHM</head><p>The SNN algorithm, as DBSCAN, is a density-based clustering algorithm. The main difference between this algorithm and DBSCAN is that it defines the similarity between points by looking at the number of nearest neighbours that two points share. Using this similarity measure in the SNN algorithm, the density is defined as the sum of the similarities of the nearest neighbours of a point. Points with high density become core points, while points with low density represent noise points. All remainder points that are strongly similar to a specific core points will represent a new clusters. -Eps, the threshold density;</p><p>-MinPts, the threshold that define the core points.</p><p>Parameters of higher level cells can be easily calculated from parameters of lower level cell count, mean, s, min, max type of distribution-normal, uniform, etc. Use a top-down approach to answer spatial data queries</p><p>The pseudo code of the SSN algorithm is to explain how it works:</p><p>Start from a pre-selected layer-typically with a small number of cells from the pre-selected layer until you reach the bottom layer do the following:</p><p>Define the input parameters. www.iosrjen.org2. If it irrelevant, remove cell from further consideration</p><formula xml:id="formula_2">? 1 ? ? ? T k i</formula><p>3. otherwise, look for relevant cells at the next lower layer</p><formula xml:id="formula_3">? ? ? ? ? ? ? ? ) ( ) ( 2 1 exp ) , ; ( k i k x x x ? ? ? ? ) 2 det( k k i</formula><p>1. Combine relevant cells into relevant regions (based on grid-neighborhood) and return the so obtained clusters as your answers. All the cluster boundaries are either horizontal or vertical, and no diagonal boundary is detected</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? ??</head><formula xml:id="formula_4">?? ? k k i k ? ? i k 1 1</formula><p>MCLUST is probably the most well known model-based</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MODEL-BASED CLUSTERING</head><p>Model-Based Clustering methods attempt to optimize the fit between the given data and some mathematical model. Such methods often based on the assumption that the data are generated by mixture of underlying probability distributions. Model-Based Clustering methods follow two major approaches: Statistical Approach or Neural network approach This is all about various clustering algorithms.</p><p>1. Clustering is also performed by having several units competing for the current object 2. The unit whose weight vector is closest to the current object wins 3. The winner and its neighbors learn by having their weights adjusted 4. SOMs are believed to resemble processing that can occur in the brain 5. Useful for visualizing high-dimensional data in 2-or 3-D space</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HOW TO DETERMINE THE NUMBER OF CLUSTERS</head><p>Many clustering algorithms require the specification of the number of clusters to produce in the input data set, prior to execution of the algorithm. Barring knowledge of the proper value beforehand, the appropriate value must be determined, a problem on its own for which a number of techniques have been developed.</p><p>In model-based clustering, the data x are viewed as coming P from a mixture density -If the number of clusters known, termination condition is given! -In general, set a distance threshold value (termination condition) -The K-cluster lifetime as the range of threshold values on the dendrogram tree that leads to the identification of K clusters -Heuristic rule: cut a dendrogram tree with maximum life time</p><p>One simple rule of thumb sets the number to</p><formula xml:id="formula_5">? ? G x f T x f ) ( ) ( ? k k 2 n k ?</formula><p>with n as the number of objects .</p><formula xml:id="formula_6">k 1</formula><p>where fk is the probability density function of the observations in group k, and T k is the probability that an observation comes from the kth mixture component Each component is usually modeled by the normal or Gaussian distribution. Component distributions are characterized by the mean ¦Ìk and the covariance matrix ¡Æk, and have the probability density function</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Elbow criterion</head><p>The elbow criterion is a common rule of thumb to determine what number of clusters should be chosen, for example for k-means and agglomerative hierarchical clustering. The elbow criterion says that you should choose a number of clusters so that adding another cluster doesn't add sufficient information. More precisely, if you graph the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information, but at some point the marginal gain will drop, giving an angle in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ISSN: 2250-3021</head><p>www.iosrjen.org</p><p>Another set of methods for determining the number of clusters are information criteria, such as : behavior given a large database of customer data containing their properties and past buying records; ? Financial task: Forecasting stock market, currency exchange rate, bank bankruptcies, un-derstanding and managing financial risk, trading futures, credit rating, ? Biology: classification of plants and animals given their features; ? Libraries: book ordering; ? Insurance: identifying groups of motor insurance policy holders with a high average claim cost; identifying frauds; ? City-planning: identifying groups of houses according to their house type, value and geographical location; ? Earthquake studies: clustering observed earthquake epicenters to identify dangerous zones; ? WWW: document classification; clustering web log data to discover groups of similar access patterns Clustering is a descriptive technique. The solution is not unique and it strongly depends upon the analyst"s choices. We described how it is possible to combine different results in order to obtain stable clusters, not depending too much on the criteria selected to analyze data. Clustering always provides groups, even if there is no group structure.</p><formula xml:id="formula_7">&amp; Small</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>When applying a cluster analysis we are hypothesizing that the groups exist. But this assumption may be false or weak. Clustering results" should not be generalized. Cases in the same cluster are similar only with respect to the information cluster analysis was based on i.e., dimensions/variables inducing the dissimilarities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Basic Grid-based Algorithm 1 .</head><label>1</label><figDesc>Define a set of grid-cells 2. Assign objects to the appropriate grid cell and compute the density of each cell. 3. Eliminate cells, whose density is below a certain threshold t. 4. Form clusters from contiguous (adjacent) groups of dense cells. The pseudo code of the STING algorithm is to explain how it works: The spatial area is divided into rectangular cells There are several levels of cells corresponding to different levels of resolution The SNN algorithm needs three inputs parameters: Each cell is partitioned into a number of smaller cells in the next level. Statistical info of each cell is calculated and stored beforehand and is used to answer queries -K, the neighbours" list size;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Find</head><label></label><figDesc>the K nearest neighbours of each point of the dataset. For each cell in the current level compute the confidence interval indicating a cell"s relevance to a given query; 1. If it is relevant, include the cell in a cluster ISSN: 2250-3021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>where K is the number of grid cells at the lowest level</head><label></label><figDesc></figDesc><table>k 

Advantages: 
Query-independent, 
easy 
to 
parallelize, 
incremental update O(K), For univariate data, the covariance matrix reduces to a 
scalar variance. The likelihood for data consisting of n 
observations assuming a Gaussian mixture model with G 
multivariate mixture components is 

n 

G 

x 
T 

). 
, 
; 

( ? 
? 

Disadvantages: 
</table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Comparision between clustering algorithms-Osama</head><p>Abu Abbas. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Data Mining: Concepts and Part C</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mechanical Engineering Science</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Algorithms for Clustering Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Prentice Hall</publisher>
			<pubPlace>Englewood Cliffs, New Jersey, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Convergence properties of the k-means algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="585" to="592" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Techniques of cluster algorithms in data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grabmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="303" to="360" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cluster Analysis and Its Applications In</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R C T</forename><surname>Lee</surname></persName>
		</author>
		<editor>J.T</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tou</surname></persName>
		</author>
		<title level="m">Advances in Information Systems Science</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Plenum Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<title level="m">Model-based Methods of Classification: Using the mclust Software in Chemo metrics Chris</title>
		<imprint/>
		<respStmt>
			<orgName>Fraley University of Washington Adrian E. Raftery University of Washington</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
