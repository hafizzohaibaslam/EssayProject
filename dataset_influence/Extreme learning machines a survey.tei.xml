<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-07-16T23:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extreme learning machines: a survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature</publisher>
				<availability status="unknown"><p>Copyright Springer Nature</p>
				</availability>
				<date type="published" when="2011">2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Bin</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><forename type="middle">Hui</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Lan</surname></persName>
						</author>
						<title level="a" type="main">Extreme learning machines: a survey</title>
					</analytic>
					<monogr>
						<title level="j" type="main">International Journal of Machine Learning and Cybernetics</title>
						<title level="j" type="abbrev">Int. J. Mach. Learn. &amp; Cyber.</title>
						<idno type="ISSN">1868-8071</idno>
						<idno type="eISSN">1868-808X</idno>
						<imprint>
							<publisher>Springer Nature</publisher>
							<biblScope unit="volume">2</biblScope>
							<biblScope unit="issue">2</biblScope>
							<biblScope unit="page" from="107" to="122"/>
							<date type="published" when="2011">2011</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/s13042-011-0019-y</idno>
					<note type="submission">Received: 1 April 2011 / Accepted: 16 April 2011 / Published online: 25 May 2011 ? Springer-Verlag 2011</note>
					<note>ORIGINAL ARTICLE</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Computational intelligence techniques have been used in wide applications. Out of numerous computational intelligence techniques, neural networks and support vector machines (SVMs) have been playing the dominant roles. However, it is known that both neural networks and SVMs face some challenging issues such as: (1) slow learning speed, (2) trivial human intervene, and/or (3) poor computational scalability. Extreme learning machine (ELM) as emergent technology which overcomes some challenges faced by other techniques has recently attracted the attention from more and more researchers. ELM works for generalized single-hidden layer feedfor-ward networks (SLFNs). The essence of ELM is that the hidden layer of SLFNs need not be tuned. Compared with those traditional computational intelligence techniques, ELM provides better generalization performance at a much faster learning speed and with least human intervene. This paper gives a survey on ELM and its variants, especially on (1) batch learning mode of ELM, (2) fully complex ELM, (3) online sequential ELM, (4) incremental ELM, and (5) ensemble of ELM.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There exist many types of neural networks, however, feedforward neural networks may be one of the most popular neural networks. A feedforward neural network consists of one input layer receiving the stimuli from external environments, one or multi-hidden layers, and one output layer sending the network output to external environments. Three main approaches are usually used in training feedforward networks:</p><p>1. Gradient-descent based (e.g. backpropagation (BP) method <ref type="bibr" target="#b0">[1]</ref> for multi-layer feedforward neural networks). Additive type of hidden nodes are most often used in such networks. For additive hidden node with the activation function g?x? : R ! R (e.g. sigmoid:</p><p>g?x? ? 1=?1 ? exp??x??), the output function of the ith node in the lth hidden layer is given by </p><p>i and x ?l? : Gradient-descent based learning algorithms usually run much slower than expected. 2. Standard optimization method based (e.g. support vector machines, SVMs <ref type="bibr" target="#b1">[2]</ref>, for a specific type of SLFNs, the so-called support vector network). Rosenblatt <ref type="bibr" target="#b2">[3]</ref> investigated perceptrons (multi-layer feedforward neural networks) half a century ago. Rosenblatt suggested a learning mechanism where only the weights of the connections from the last hidden layer to the output layer were adjusted. After all the rest weights fixed the input data are actually transformed into a feature space Z of the last hidden layer (cf. <ref type="figure" target="#fig_1">Fig. 1</ref>). In this feature space a linear decision function is constructed:</p><formula xml:id="formula_1">) (x f ! 1 m f ?x? ? sign X L b i z i<label>?x?</label></formula><formula xml:id="formula_2">?2? i?1 i ¦Â 1 ¦Â L ¦Â</formula><p>where b i is the output weight between the output node and the ith neuron in the last hidden layer of a perceptron, and z i ?x? is the output of the ith neuron in the last hidden layer of the perceptron. In order to find an alternative solution of z i ?x?; in 1995 Cortes and Vapnik <ref type="bibr" target="#b1">[2]</ref> proposed the SVM which maps the data from the input space to some high dimensional feature space Z through some nonlinear mapping chosen a priori. Optimization methods are used to find the separating hyperplane which maximizes the separating margins of two different classes in the feature space. 3. Least-square based (e.g. radial basis function (RBF) network learning <ref type="bibr" target="#b3">[4]</ref>). For RBF hidden node with activation function g?x? : R ! R (e.g. Gaussian:</p><p>g?x? ? exp??x 2 ?; G?a i ; b i ; x? is given by where a i and b i are the center and impact factor of the ith RBF hidden node. R ? indicates the set of all positive real values. The RBF network is a special case of SLFNs with RBF nodes in its hidden layer (cf. <ref type="figure" target="#fig_5">Fig. 2</ref>). Each RBF node has its own centroid and impact factor, and its output is given by a radially symmetric function of the distance between the input and the center. In Lowe's RBF network implementation <ref type="bibr" target="#b3">[4]</ref>, the centers a i of RBF hidden nodes can be randomly selected from the training data or from the region of training data instead of tuning, and all the impact factors b i of RBF hidden nodes are usually set with the same value (p. 173 of <ref type="bibr" target="#b3">[4]</ref>). After RBF hidden nodes parameters ?a i ; b i ? fixed, the output weight vector b i linking the ith RBF hidden node to the output layer becomes the only unknown parameter which can be resolved by least-square method. <ref type="figure" target="#fig_5">Fig. 2</ref> Single-hidden layer feedforward network</p><formula xml:id="formula_3">) (x f 1 m ¦Â ¦Â ¦Â i ¦Â 1 ¦Â L ¦Â 1 L i ( ) x a , , 1 1 b G ( ) x a , , i i b G ( ) x a , , L L b G 1 d x</formula><p>Extreme learning machines (ELMs) were originally developed for the SLFNs <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> and then extended to the ''generalized'' SLFNs. Such generalized SLFNs need not be neuron alike <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. The essence of ELM is that: different from the common understanding of learning, the hidden layer of SLFNs need not be tuned. One of the typical implementation of ELMs is to apply random computational nodes in the hidden layer, which may be independent of the training data. Different from traditional learning algorithms for neural networks ELM not only tends to reach the smallest training error but also the smallest norm of output weights. According to the neural network theory <ref type="bibr" target="#b9">[10]</ref>, for feedforward neural networks reaching smaller training error the smaller the norm of weights is, the better generalization performance the networks tend to have. Since in ELM the hidden layer need not be tuned and the hidden layer parameters can be fixed, the output weights can then be resolved using the lease-square method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning theories of ELMs</head><p>The interpolation capability and universal approximation capability of ELMs have been investigated in Huang et al. <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>.The output function of SLFNs with L hidden nodes can be represented by</p><formula xml:id="formula_4">f L ?x? ? X L b i g i ?x? ? X L b i G?a i ; b i ; x?; i?1 i?1 x 2 R d ; b i 2 R m<label>?4?</label></formula><p>independent of the training samples <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. One of the typical implementation of ELMs is that the hidden node parameters ?a i ; b i ? of ELM can be randomly generated. The learning capability of extreme learning machines have been studied in two aspects: interpolation capability <ref type="bibr" target="#b5">[6]</ref> and universal approximation capability <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>.</p><p>where g i denotes the output function G?a i ; b i ; x? of the ith hidden node. For additive nodes with activation function g, g i is defined as 2.1 Interpolation theorem</p><formula xml:id="formula_5">g i ? G?a i ; b i ; x? ? g?a i ? x ? b i ?; a i 2 R d ; b i 2 R<label>?5?</label></formula><p>For N arbitrary distinct samples</p><formula xml:id="formula_6">?x i ; t i ? 2 R d ? R m ;</formula><p>SLFNs with L hidden nodes are mathematically modeled as and for RBF nodes with activation function g, g i is defined as</p><formula xml:id="formula_7">X L b i g i ?x j ? ? X L b i G?a i ; b i ; x j ? ? o j ; j ? 1; . . .; N ?7? i?1 i?1 g i ? G?a i ; b i ; x? ? g b i kx ? a i k ? ? ; a i 2 R d ; b i 2 R ?<label>?6?</label></formula><p>In the past two decades, the interpolation and universal approximation capabilities of SLFNs have been investigated thoroughly. It was proved <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> that N arbitrary distinct samples can be learned precisely by SLFNs with N threshold hidden nodes. Further study <ref type="bibr" target="#b12">[13]</ref> gave a more complete answer on the interpolation capability of SLFNs and proved that an SLFN with at most N hidden nodes and with any arbitrary bounded nonlinear activation function which has a limit at one infinity can learn any N arbitrary distinct samples with zero error. Such activation functions include the threshold, ramp and sigmoid functions as well as the radial basis, ''cosine squasher'' <ref type="bibr" target="#b13">[14]</ref> and many nonregular functions. Many researchers <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> have rigorously proved in theory that given activation function g(x) satisfying certain mild conditions there exists a sequence of network functions {f L } approximating to any given continuous target function f with any expected learning error [ 0: In all these conventional neural network theories, all the parameters in any f L of the network sequence (e.g. the hidden layer parameters ?a i ; b i ? and the output weights b i ) are required freely adjustable. According to these conventional neural network theories, hidden layer parameters ?a i ; b i ? need to be tuned properly and appropriate values of network parameters (e.g. ?a i ; b i ? and b i ) need to be found for any given target function f. To minimize the effort spent on adjusting hidden layer parameters ?a i ; b i ? has been tried in the past two decades. Instead of adjusting all the parameters of hidden layers in all f L of the network sequence, some researchers <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref> suggested incremental methods for SLFNs which adjust the parameters of newly added hidden nodes and then fix them after tuning. The parameters of the existing hidden nodes will remain fixed and never be updated in the further learning procedure. Hidden layer parameters in those conventional learning models need to be adjusted at least once based on the training samples. In contrast, all the parameters of the hidden layer in the ELMs need not be tuned and can be That SLFNs can approximate these N samples with zero error means that</p><formula xml:id="formula_8">P L j?1 ko j ? t j k ? 0; i.e., there exist ?a i ; b i ? and b i such that X L b i G?a i ; b i ; x j ? ? t j ; j ? 1; . . .; N:<label>?8? i?1</label></formula><p>The above N equations can be written compactly as:</p><formula xml:id="formula_9">Hb ? T<label>?9?</label></formula><p>where h?x N ? </p><formula xml:id="formula_10">h?x 1 ? 2 3 H ? . . .</formula><formula xml:id="formula_11">G?a 1 ; b 1 ; x 1 ? ? ? ? G?a L ; b L ; x 1 ? 2 3 ? . . . ? ? ? . . .</formula><formula xml:id="formula_12">G?a 1 ; b 1 ; x N ? ? ? ? G?a L ; b L ; x N ? N?L b T 1 2 3 t T 1 2 3 b ? .<label>?10?</label></formula><p>. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">4 7 5</head><p>and T ? . . . H is called the hidden layer output matrix of the SLFN <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref>; the ith column of H is the ith hidden node output with respect to inputs x 1 ; x 2 ; . . .; x N : h?x? ? G?a 1 ; b 1 ; x?; . . .; g?a L ; b L ; x? is called the hidden layer feature mapping. The ith row of H is the hidden layer feature mapping with respect to the ith input x i : h?x i ?: It has been proved <ref type="bibr" target="#b5">[6]</ref> that from the interpolation capability point of view, if the activation function g is infinitely differentiable in any interval the hidden layer parameters can be randomly generated.</p><p>Theorem 2.1 [6] Given any small positive value [ 0; activation function g : R ! R which is infinitely differentiable in any interval, and N arbitrary distinct samples</p><formula xml:id="formula_13">?x i ; t i ? 2 R d ? R m ;</formula><p>there exists L B N such that for any</p><formula xml:id="formula_14">fa i ; b i g L i?1</formula><p>randomly generated from any intervals of R d ? R; according to any continuous probability distribution, with probability one,</p><formula xml:id="formula_15">H N?L b L?m ? T N?m k k \: 2 3 1=2 kf L ? f k ? Z jf L ?x? ? f ?x?j 2 dx 4 5<label>?13?</label></formula><p>X From the interpolation point of view the maximum number of hidden nodes required is not larger than the number of training samples. In fact, if L = N, the training errors can be zero. From interpolation point of view, wide type of activation functions can be used in ELM, which include the sigmoid functions, the radial basis, sine, cosine, exponential, and many other non-regular functions <ref type="bibr" target="#b12">[13]</ref>. It may be too strict to request that activation functions of hidden nodes are infinitely differentiable. For example, it may not include some important activation functions such as threshold function: g?x? ? 1 x ! 0 ? 0 x\0 : Threshold networks are very popular in real applications, especially in digital hardware implementation. However, as threshold function is not differentiable, researchers did not manage to find any efficient direct learning algorithms for threshold networks in the past two decades <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>. Interestingly, from the universal approximation point of view, the above mentioned interpolation theorem can be extended to almost any type of nonlinear piecewise continuous function including the threshold function, and thus an efficient direct learning algorithm (e.g. ELM) can be applied to those cases which cannot be handled by other learning techniques in the past decades.</p><p>Different from the randomness mentioned in other learning methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>, all the hidden node parameters ?a i ; b i ? in ELMs can be independent of the training samples and can be randomly generated before the training samples observed. (Refer to <ref type="bibr" target="#b33">[34]</ref> for the details of the differences between ELM and Igelnik and Pao <ref type="bibr" target="#b32">[33]</ref> and Lowe et al. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32]</ref>).</p><p>Definition 2.3 The function sequence fg i ? G?a i ; b i ; x?g is said randomly generated if the corresponding parameters</p><formula xml:id="formula_16">?a i ; b i ? are randomly generated from R d ? R or R d ? R ?</formula><p>based on a continuous sampling distribution probability.</p><p>Lemma 2.1 (Proposition 1 of <ref type="bibr" target="#b15">[16]</ref>) Given g : R ! R; spanfg?a ? x ? b? : ?a; b? 2 R d ? Rg is dense in L p for every p 2 ?1; 1?; if and only if g is not a polynomial (almost everywhere). and</p><formula xml:id="formula_17">R d R k?x? dx 6 ? 0: Then spanfk? x?a b ? : ?a; b? 2 R d ? R ? g</formula><p>is dense in L p for every p 2 ?1; 1?:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Universal approximation theorem</head><p>Huang et al. <ref type="bibr" target="#b6">[7]</ref> proved in theory that SLFNs with randomly generated additive or RBF nodes can universally approximate any continuous target functions over any compact subset</p><formula xml:id="formula_18">X 2 R d : Let L 2 (X) be a space of functions f on a compact subset X in the d-dimensional Euclidean space R d such that jf j 2 are integrable, that is, R X jf ?x?j 2 dx\1: Let L 2 ?R d ? denoted by L 2 . For u; v 2 L 2<label>?X?</label></formula><p>; the inner product hu; vi is defined by</p><formula xml:id="formula_19">hu; vi ? Z u?x?v?x? dx ?12? X</formula><p>The norm in L 2 (X) space is denoted as k ? k; and the closeness between the network function f L and the target function f is measured by the L 2 (X) distance: Lemmas 2.1 and 2.2 show that feedforward neural networks with additive or RBF hidden nodes can approximate any target continuous function provided that the hidden node parameters ?a i ; b i ? are tuned properly and appropriate values are given. Lemmas 2.1 and 2.2 only show the universal approximation capability of feedforward neural networks with additive or RBF hidden nodes, however, how to find the suitable hidden node parameters ?a i ; b i ? remains open, and many tuning based learning algorithms have been suggested in the past. Huang et al. <ref type="bibr" target="#b6">[7]</ref> proved that given any bounded nonconstant piecewise continuous activation function g : R ! R for additive nodes or integrable piecewise continuous activation function g : R ! R (and R R g?x? dx 6 ? 0) for RBF nodes, the hidden layer of such SLFN need not be tuned, in fact, all the hidden nodes can be randomly generated. SLFNs with randomly generated hidden nodes can universally approximate any target functions. Let e L : f -f L denote the residual error function for the current network f L with L hidden nodes where f 2 L 2 ?X? is the target function. The output layer may have more than one nodes, m [ 1, that is, the function f is a multi-output function: f ? ?f ?1? ; . . .; f ?m? ? T : The corre-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ELM</head><p>The essence of ELM is that: 1. The hidden layer of ELM need not be iteratively tuned <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. 2. According to feedforward neural network theory <ref type="bibr" target="#b9">[10]</ref>, both the training error kHb ? Tk and the norm of weights kbk need to be minimized <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. 3. The hidden layer feature mapping need to satisfy the universal approximation condition (Theorems 2.3 and 2.4) <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>.</p><formula xml:id="formula_20">sponding output function of the network with L hidden nodes is f L ? ?f ?1? ?m? L ; . . .; f L ? T : Let b L (j) denote</formula><p>According to Theorems 2.1 and 2.4 the hidden nodes can be randomly generated, the only unknown parameters in SLFNs are the output weights vectors b i between the hidden layer and the output layer, which can simply be resolved by ordinary least-square directly.</p><formula xml:id="formula_21">?j? 3.1 Basic ELM [5, 6] b ?j? he L ? L?1 ; g L i kg L k 2 ; j ? 1; . . .; m:<label>?14?</label></formula><p>Theorem 2.3 can be further extended from additive or RBF hidden nodes cases to ''generalized'' SLFNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Given a type of piecewise computational hidden nodes (possibly not neural alike nodes), if SLFNs can work as universal approximators with adjustable hidden parameters, from a function approximation point of view the hidden node parameters of such ''generalized'' SLFNs can actually be randomly generated according to any continuous sampling distribution. In theory, the parameters of these SLFNs can be analytically determined by ELM instead of being tuned. Tuning is actually not required in such generalized SLFNs which include sigmoid networks, RBF networks, trigonometric networks, threshold networks, fully complex neural networks, high-order networks, ridge polynomial networks, etc.</p><p>Hidden node parameters ?a i ; b i ? remain fixed after randomly generated. To train an SLFN is simply equivalent to finding a least-squares solution ^ b of the linear system Hb ? T :  <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> where the output parameters are determined by ordinary least square can work as universal approximators if only the activation function g is nonconstant piecewise and spanfG?a; b; x? : ?a;</p><formula xml:id="formula_22">kH ^ b ? Tk ? min b kHb ? Tk ?<label>15?</label></formula><formula xml:id="formula_23">b? 2 R d ? Rg is dense in L 2 .</formula><p>If the number L of hidden nodes is equal to the number N of distinct training samples, L = N, according to Theorem 2.1 matrix H is square and invertible when hidden node parameters ?a i ; b i ? are randomly chosen, and thus SLFNs can approximate these training samples with zero error.However, in most cases the number of hidden nodes is much less than the number of distinct training samples, L ( N; H is a nonsquare matrix and there may not exist a i ;</p><formula xml:id="formula_24">b i ; b i (i ? 1; . . .; L) such that Hb ? T:</formula><p>The smallest norm least-squares solution of the above linear system is:</p><formula xml:id="formula_25">^ b ? H y T<label>?16?</label></formula><p>where H y is the Moore-Penrose generalized inverse of matrix H <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>. Thus, ELM can be summarized as follows:</p><p>Algorithm </p><formula xml:id="formula_26">b ? H y T<label>?17?</label></formula><p>ELM algorithm can work with wide type of activation function. Many popular learning algorithms do not deal with threshold networks directly. Instead some analog networks are used to approximate threshold networks such that gradient-descent method can finally be used <ref type="bibr" target="#b26">[27]</ref>. However, ELM can be used to train threshold networks directly <ref type="bibr" target="#b35">[36]</ref>. Different methods can be used to calculate Moore-Penrose generalized inverse of a matrix: orthogonal projection method, orthogonalization method, iterative method, and singular value decomposition (SVD) <ref type="bibr" target="#b37">[38]</ref>. ?1</p><formula xml:id="formula_27">b ? H T I k ? HH T T<label>?22?</label></formula><formula xml:id="formula_28">or b ? I k ? H T H ?1 H T T<label>?23?</label></formula><p>3.2 Random hidden layer feature mapping based ELM <ref type="bibr" target="#b38">[39]</ref> The orthogonal projection method can be efficiently used in ELM <ref type="bibr" target="#b38">[39]</ref>: </p><formula xml:id="formula_29">H y ? H T H ? ? ?1 H T if H T H is nonsingular or H y ? H T HH T ? ? ?1 if</formula><formula xml:id="formula_30">b ? H T I k ? HH T T<label>?18?</label></formula><p>and the corresponding output function of ELM is: ?1</p><formula xml:id="formula_31">f ?x? ? h?x?b ? h?x?H T I k ? HH T T<label>?19?</label></formula><p>In these implementations, the condition on the number of hidden nodes can be mild, it does not closely depend on the number of training samples N. It works for both the cases L \ N or L C N. This is different from the interpolation theorem which requires L B N (Theorem 2.1), but consistent to the universal approximation theorem (Theorem 2.4). <ref type="figure" target="#fig_7">Figure 3</ref> shows a classification boundary obtained by ELM for a binary-class case. Formula <ref type="formula">(18)</ref>   <ref type="bibr" target="#b38">[39]</ref> further extended this study to generalized SLFNs with different type of hidden nodes (feature mappings) as well as kernels and showed that the simple unified algorithm of ELM can be obtained for regression, binary and multi-label classification cases which, however, have to be handled separately by SVMs and its variants <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref>.</p><p>Or we can have 3.3 Kernel based ELM <ref type="bibr" target="#b38">[39]</ref> </p><formula xml:id="formula_32">b ? I k ? H T H ?1 H T T<label>?20?</label></formula><p>and the corresponding output function of ELM is:</p><p>Huang et al. <ref type="bibr" target="#b38">[39]</ref> also studied the kernel based ELM. If the hidden layer feature mapping h(x) is unknown to users, one can define a kernel matrix for ELM as follows:</p><formula xml:id="formula_33">f ?x? ? h?x?b ? h?x? I k ? H T H ?1 H T T<label>?21?</label></formula><p>Huang et al. <ref type="bibr" target="#b38">[39]</ref> shows that the solutions <ref type="formula">(18)</ref> and <ref type="formula">(20)</ref>   </p><note type="other">X ELM ? HH T : X ELM i;j ? h?x i ? ? h?x j ? ? K?x i ,x j ? ?24?</note><p>Then the output function of ELM <ref type="formula">(19)</ref> can be written compactly as: ?1</p><formula xml:id="formula_34">f ?x? ? h?x?H T I k ? HH T T K?x; x 1 ? 2 3 T ?1<label>?25?</label></formula><p>? . . .</p><formula xml:id="formula_35">6 6 4 7 7 5 I k ? X ELM T K?x; x N ? 1.</formula><p>Real-valued neural network models such as feedforward neural networks, RBF networks and recurrent neural networks. 2. Complex-valued neural networks: this approach has attracted considerable attention in channel equalization applications in the past 15 years <ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref>. Split-complex activation (basis) functions consisting of two realvalued activation functions, one processing the real part and the other processing the imaginary part, have been traditionally employed in these complex-valued neural networks.</p><p>In this specific kernel implementation of ELM, the hidden layer feature mapping h(x) need not be known to users, instead its corresponding kernel K(u, v) (e.g. K?u; v? ? exp??cku ? vk 2 ?) is given to users. The number of hidden nodes L (the dimensionality of the hidden layer feature space) need not be specified either. Thus, the ELM algorithm can be rewritten for the kernel case as follows:</p><formula xml:id="formula_36">Algorithm ELM (single-step kernel version): Given a training set @ ? f?x i ; t i ?jx i 2 R d ; t i 2 R m ; i ? 1; . . .; Ng; kernel K(u, v):</formula><p>Calculate the output function:</p><p>Instead of using split-complex activation function, extreme learning machine can use fully complex activation function directly. Li et al <ref type="bibr" target="#b33">[34]</ref> proved the universal approximation capability of extreme learning machine with fully complex activation function: </p><formula xml:id="formula_37">- ated function sequence fg L ? Q s n l?1 r a Ll ? z ? b L ? ? g ; lim L!1 kf ? f L k ? 0 holds with probability one if f ?x? ? K?x; x 1 ? . . .<label>6 4 7 5</label></formula><formula xml:id="formula_38">I k ? X ELM T<label>?26?</label></formula><formula xml:id="formula_39">?j? K?x; x N ? b ?j? he L ? L?1 ; g L i kg L k 2 ; j ? 1; . . .; m:<label>?28?</label></formula><p>It can be seen that kernel based ELM algorithm can be implemented in a single learning step. Fr¨¦nay and Verleysen [50, 51] studied the kernel implementation of ELM if h?x? is known to users. If the hidden layer feature mapping h(x) is known to users, Fr¨¦nay and Verleysen <ref type="bibr" target="#b50">[51]</ref> defined the ELM kernel as</p><p>When the network architecture is fixed (with fixed L), from Theorem 4.1 we have</p><formula xml:id="formula_40">K?u; v? ? lim L!?1 1 L h?u? ? h?v? ?<label>27?</label></formula><p>A parameter-insensitive kernel with analytic form can then be obtained for SVM for regression, which significantly reduces the computational complexity. We conjecture that Fr¨¦nay and Verleysen's ELM kernel <ref type="bibr" target="#b50">[51]</ref> can work for SVM and its variants as well as in (25). All the above mentioned can be applied in regression, binary and multilabel classification applications directly. ELMs can be applied to complex space as well.</p><p>Theorem 4.2 <ref type="bibr" target="#b33">[34]</ref> Given any complex continuous discriminatory or any complex bounded nonlinear piecewise continuous function r : C ! C; for any continuous target function f :</p><formula xml:id="formula_41">C d ! C and any function sequence fg L ? Q s L l?1 r a Ll ? z ? b L ?</formula><p>? g randomly generated based on any continuous sampling distribution probability, lim L!1 kf ? f L k ? 0 holds with probability one if the output weights b i are determined by ordinary least square to minimize f ?z? ? P L i?1 b i g i ?z? :</p><p>Thus, the ELM algorithms introduced in Sect. 3 can be linearly extended to the complex domain. Compared to others equalizers, ELM can obtain much lower symbol error rate (SER) and provide parsimonious structures for applications in the complex domain <ref type="bibr" target="#b51">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Fully complex ELM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Online sequential ELM (OS-ELM)</head><p>In high speed digital communication systems, equalizers are very often used at receivers to recover the original symbols from the received signals <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b51">52]</ref>. Two conventional approaches are usually used for solving equalization problems.</p><p>ELM algorithms introduced in Sect. 3 learn training samples only after all training samples are ready. In many industrial applications training data may come one by one or chunk by chunk. In these cases, on-line sequential learning algorithms are preferred over batch learning algorithms as sequential learning algorithms do not require retraining whenever a new data is received. Sequential learning is difficult to be implemented for feedforward neural networks with additive (e.g. <ref type="bibr" target="#b55">[56]</ref>) or RBF hidden nodes <ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref><ref type="bibr" target="#b63">[64]</ref>. Most of the conventional online sequential learning algorithms have several parameters for users to specify and it is very time-consuming to tune those parameters. OS-ELM <ref type="bibr" target="#b64">[65]</ref> is a simple and efficient online sequential learning algorithm that can handle both additive and RBF nodes in a unified framework. OS-ELM can learn the training data not only one-by-one but also chunk by chunk (with fixed or varying length) and discard the data for which the training has already been done. The training observations are sequentially presented to the learning algorithm (one-by-one or chunk-by-chunk with varying or fixed chunk length). A single or a chunk of training observations is discarded and may not be used any more as soon as the learning procedure for that particular observation(s) is completed. According to Sect. 3, one of the solutions of the output weight vector b is:</p><p>where N k?1 denotes the number of observations in the (k ? 1)th chunk. (b) Calculate the partial hidden layer output matrix H k?1 for the (k ? 1)th chunk of data @ k?1 : </p><formula xml:id="formula_42">0 1 0 1 2 3 G a1; b1; x P k @ A ? ? ? G aL; bL; x P k @ A j?0 Nj ?1 j?0 Nj ?1 Hk?1 ? . . . ? ? ? . . . G a1; b1; x P k?1 ? ? ? G</formula><formula xml:id="formula_43">P k?1 ? P k ? P k H T T k?1 ?I ? H k?1 P k H k?1 ? ?1 H k?1 P k b ?k?1? ? b ?k? ? P k?1 H T k?1 ?T k?1 ? H k?1 b ?k? ? ?32? (d) Set k = k ? 1. Go to step 2a. b ? ?H T H? ?1 H T T<label>?29?</label></formula><p>Sequential implementation of the least-squares solution of Eq. 29 results in the OS-ELM which uses the recursive least squares algorithm <ref type="bibr" target="#b65">[66]</ref>. OS-ELM Algorithm: <ref type="bibr" target="#b64">[65]</ref> step 1 Initialization Phase: Initialize the learning using a small chunk of initial training data @ 0 ? f?x i ; t i ?g Seen from the above OS-ELM algorithm, OS-ELM and ELM can achieve the same learning performance (training error and generalization accuracy) when rank(H 0 ) = L. In addition, if N 0 = N, OS-ELM also becomes the batch ELM. In OS-ELM, the chunk size of incoming training data need not be constant. When the training data is received one-by-one instead of chunk-by-chunk, N k?1 : 1, formula (32) has the following simple format (ShermanMorrison formula <ref type="bibr" target="#b66">[67]</ref>): </p><formula xml:id="formula_44">N 0 P k?1 ? P k ? P k h?x k?1 ?h T ?x k?1 ?P k i?1 from the given training set @ ? f?x i ; t i ?jx i 2 R n ; t i 2 R m ; i ? 1; . . .g; N 0 C L. 1 ? h T ?x k?1 ?P k h?x k?1 ?<label>?33?</label></formula><formula xml:id="formula_45">h?x k?1 ? ? ?G?a 1 ; b 1 ; x k?1 ? ? ? ? G?a L ; b L ; x k?1 ??:</formula><p>OS-ELM is efficient in time-series prediction which is required in many real-world problems. The chaotic Mackey-Glass differential delay equation <ref type="bibr" target="#b67">[68]</ref> is one of the classical benchmark time series problems in literature:</p><formula xml:id="formula_46">H 0 ? G?a 1 ; b 1 ; x 1 ? ? ? ? G?a L ; b L ; x 1 ? . . . ? ? ? . . .<label>6 4</label></formula><p>7 5</p><formula xml:id="formula_47">G?a 1 ; b 1 ; x N 0 ? ? ? ? G?a L ; b L ; x N 0 ? N 0 ?L dx?t? dt ? ax?t ? s? 1 ? x 10 ?t ? s? ? bx?t? ? 34?<label>?30?</label></formula><p>(c) Estimate the initial output weight b ?0? ?</p><formula xml:id="formula_48">P 0 H T -1 0 T 0 ; where P 0 = (H 0 T H 0 )</formula><p>and</p><formula xml:id="formula_49">T 0 ? ?t 1 ; . . .; t N 0 ? T :</formula><p>for a = 0.2, b = 0.1, and s = 17. Integrating the equation over the time interval ?t; t ? Dt? by the trapezoidal rule yields: </p><formula xml:id="formula_50">(d) Set k = 0. x?t ? Dt? ? 2 ? bDt 2 ? Dt x?t? ! step 2 Sequential Learning Phase: ? aDt 2 ? bDt x?t ? Dt ? s? 1 ? x 10 ?t ? Dt ? s? ? x?t ? s? 1 ? x 10 ?t ? s?</formula><p>vations:</p><formula xml:id="formula_52">@ k?1 ? f?x i ; t i ?g j?0 N j ; i? P k j?0 N j ?1</formula><p>The time series is generated under the condition x(t -s) = 0.3 for 0 B t B s and predicted with t = 50 available. These conventional methods may also face local minima issues. Although many other incremental learning algorithms have been proposed in literature <ref type="bibr">[57-60, 63, 64]</ref>, unlike I-ELM the universal approximation capability of these previous learning algorithms has not been proved. Different from other conventional incremental learning algorithms which may have several parameters for us to specify, I-ELM has no parameters for users to specify except the maximum network architecture and the expected accuracy. Experimental results show that I-ELM outperforms other learning algorithms (including support vector regression (SVR) <ref type="bibr" target="#b68">[69,</ref><ref type="bibr">70]</ref>, stochastic gradient-descent BP <ref type="bibr" target="#b55">[56]</ref>, and incremental RBF networks <ref type="figure">(RAN [57]</ref>, RANEKF <ref type="bibr" target="#b57">[58]</ref>, MRAN <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref>, GAP-RBF <ref type="bibr" target="#b62">[63]</ref>, GGAP-RBF <ref type="bibr" target="#b63">[64]</ref>) in terms of generalization performance and learning speed. I-ELMs can be implemented in different ways:</p><p>1.4</p><p>1.  <ref type="figure">Fig. 4</ref> Time-series prediction: the approximated curve obtained by OS-ELM. OS-ELM is trained by the training observations is from t = 1 to t = 4, 000, and the predicted period is from t = 4,001 to t = 4,500 sample steps ahead using the four past samples: s n-t , s n-t-6 , s n-t-12 , and s n-t-18 . Hence, the nth input-output instance is:</p><p>1. Basic I-ELM Every time only one hidden node is randomly generated and added to the existing network <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. 2. Enhanced I-ELM Every time k hidden nodes are randomly generated. However, among the k randomly generated hidden nodes only the most appropriate hidden node will be added to the existing network <ref type="bibr" target="#b8">[9]</ref>.</p><p>x n ? ?s n?t ; s n?t?6 ; s n?t?12 ; s n?t?18 ? T y n ? s n <ref type="figure">Figure 4</ref> shows the approximated curve of OS-ELM in this time-series prediction. In this simulation, Dt ? 1; and the training observations is from t = 1 to t = 4,000 and the testing observations from t = 4,001 to t = 4,500. The number of hidden nodes of OS-ELM is L = 120.</p><p>Compared to the original I-ELM <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, this enhanced implementation <ref type="bibr" target="#b8">[9]</ref> will produce a more compact network architecture and the learning can be completed in a faster convergence rate and learning speed. I-ELM is a specific case of EI-ELM when k = 1. The universal approximation capability of ELMs was proved using incremental learning method where the hidden nodes are added one by one <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. The proof itself is indeed a practical incremental constructive method, which actually shows an efficient way to construct an incremental feedforward network (referred to as I-ELMs). Different from other incremental learning algorithms which may only work with some type of hidden nodes (e.g. resource allocation network and its variants <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64]</ref> work only for RBF networks), I-ELM can work well with a wide type of activation functions no matter whether they are sigmoidal or nonsigmoidal, continuous or noncontinuous, and differentiable or non-differentiable. The traditional gradient-descent based learning algorithms cannot be applied to networks with non-differential activation functions such as threshold networks since the required derivatives are not</p><formula xml:id="formula_53">? b ?j?? he L?1 ; g L ? L i kg ? L k 2 ; j ? 1; . . .; m<label>?36?</label></formula><p>where</p><formula xml:id="formula_54">f L (j)* = P (j)* i=1 L b i (j)* g i * , e L (j)* = f (j) -f L and g ? ?j?? ?j? L ? fg i j min ?L?1?k?1 i Lk k?f ?j? ? f L?1 ? ? b L g i kg:</formula><p>According to formula (36), the weight b L (j) between the Lth newly added node and the jth output node should be</p><formula xml:id="formula_55">? chosen as he ?j?? L?1 ;g L i kg ? L k 2 :</formula><p>In real applications, only the training samples are available, the target function f(x) is unknown and the exact functional form of e L-1 (j)* is not available, thus, formula (36) cannot be calculated explicitly. Instead, formula (36) can be estimated based on the training samples:</p><formula xml:id="formula_56">T b ?j? E ?j? ? ^ h L ? ^ h ? ^ h T ? P N p?1 e ?j? ?p?G?a L ; b L ; x p ? P N p?1 G 2 ?a L ; b L ; x p ?<label>?37?</label></formula><p>where e (j) (p) is the corresponding residual error of the jth output node before the Lth new hidden neuron is added.</p><formula xml:id="formula_57">^ h ? ?G?a L ; b L ; x 1 ?; . . .; G?a L ; b L ; x N ??</formula><p>T is the activation vector of the newly added node for all the N training samples and E ?j? ? ?e ?j? ?1?; . . .; e ?j? ?N?? T is the residual vector the jth output node with respect to all the N training samples before this new hidden node added. Let E ? ?E ?1? ; . . .; E ?m? ?:</p><p>the ith trial of hidden node for all the N training samples and b (i) (j) is the corresponding output weight between the ith trial of hidden node and the jth output node. E (i) (j) of formula (39) is the residual error vector of the jth output node if the ith trial of hidden node is added. E (j) in the right hand of formula (39) represents the earlier residual error vector corresponding to the jth output node before the new node added.</p><p>EI-ELM Algorithm: Given a training set @ ? f?x i ; t i ?jx i 2 R d ; t i 2 R; i ? 1; . . .; Ng; hidden node output function G <ref type="figure">(a, b, x)</ref>, maximum number L max of hidden nodes, maximum number k of trials of assigning random hidden nodes at each step, and expected learning accuracy ;  </p><formula xml:id="formula_58">Set E = E (i) , a L ? a ?i ? ? ; b L ? b ?i ? ? ; and b L ? b ?i ? ? : endwhile</formula><p>Before learning, there is no node in the network and the initial residual error is set as the expected target vector T of the training data set as shown in step 1. Learning will stop when the number L of hidden nodes has exceeded the predefined maximum number L max or the residual error E is small enough (kEk\). step 2b randomly generates k new hidden nodes and step 2c will choose and add the most appropriate hidden node of the k randomly generated hidden nodes. ^ h ?i? in formula <ref type="formula">(38)</ref> is the activation vector of</p><p>The idea of neural network ensemble was proposed by Hansen and Salamon <ref type="bibr" target="#b69">[71]</ref> . Their work showed that a single network's performance can be expected to improve using an ensemble of neural networks with a plurality consensus scheme. This technique has been spread widely after that. The most prevailing approaches for training neural networks comprised the ensemble are Bagging <ref type="bibr" target="#b70">[72]</ref> and Boosting <ref type="bibr" target="#b71">[73]</ref><ref type="bibr" target="#b72">[74]</ref><ref type="bibr" target="#b73">[75]</ref>. An integration of several ELMs was proposed by Sun et al <ref type="bibr" target="#b74">[76]</ref> to predict the future sales amount. Several ELM networks were connected in parallel and the average of the ELMs' outputs was used as the final predicted sales amount. The resulting ensemble has better generalization performance. Heeswijk et al. <ref type="bibr" target="#b75">[77]</ref> investigated the adaptive ensemble models of ELM on the application of one-step ahead prediction in (non-)stationary time series. It was verified that the method did work on stationary time series and the capability of the method on non-stationary time series was tested. The empirical studies showed that the adaptive ensemble model achieved an acceptable testing error with good adaptivity. Heeswijk et al.</p><p>[78] also studied ELM ensemble for large scale regression applications. Furthermore, network ensembles are potentially important methods to perform sequential learning <ref type="bibr" target="#b77">[79,</ref><ref type="bibr" target="#b78">80]</ref>. Network ensemble consists of a few of single networks that may have different adaptabilities to the new data. Some of the networks in the ensemble may adapt faster and better to the new data than others, which could make the ensemble overcome the problem of networks that could not adapt well to the new data. <ref type="bibr">Lan et al. [81]</ref> proposed an integrated network structure, which is called ensemble of online sequential ELM (EOS-ELM). EOS-ELM comprised several OS-ELM networks. The average value of outputs of each OS-ELM in the ensemble was used as the final measurement of network performance.</p><p>The simulation results proved that EOS-ELM is more stable than original OS-ELM in each trial of simulation for most problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Pruning ELM</head><p>Rong et al. <ref type="bibr" target="#b80">[82]</ref> presented a pruned ELM (referred to as P-ELM) as a systematic and automated method for ELM classifier network design. It starts with a large network and then eliminates the hidden nodes that have low relevance to the class labels by using statistical criteria, namely, the Chisquared (v 2 ) and information gain (IG) measures. P-ELM mainly focuses on pattern classification applications. Another pruning algorithm called optimally-pruned ELM (referred to as OP-ELM) was proposed by Miche et al. <ref type="bibr" target="#b81">[83]</ref>. The OP-ELM methodology has three steps: (1) build the SLFN using the original ELM algorithm; (2) rank the hidden nodes by applying multi-response sparse regression algorithm (MRSR) <ref type="bibr" target="#b82">[84]</ref>; and (3) select the hidden nodes through leave-one-out (LOO) validation. OP-ELM is applicable for both regression and classification applications. squares (OLS). OLS selects a suitable set of variables to form the subset model from a large set of candidates. At each step, the net decrease in the residual error is maximized. The key advantage of the algorithm is that it can explicitly identify the net contribution of the newly added node without solving the whole least-squares problem, which significantly reduces the computational complexity. However, OLS cannot guarantee an optimal solution because it is greedy and on the basis of a local optimization <ref type="bibr" target="#b86">[88]</ref>. By modifying the classic forward selection algorithm, a constructive hidden nodes selection method for ELM (CS-ELM) <ref type="bibr" target="#b87">[89]</ref> was proposed,which is less greedy and without any matrix decompositions. At each step of CS-ELM, the hidden node with an output that has the highest correlation with the current residual is selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Two-stage ELM for regression</head><p>9 Constructive model selection of ELM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Error minimized ELM</head><p>Error minimized ELM (EM-ELM) <ref type="bibr" target="#b83">[85]</ref> is an error minimization based method in which the number of hidden nodes can grow one-by-one or group-by-group until optimal. The approach can significantly reduce the computational complexity and its convergence was proved as well. In EM-ELM, the hidden nodes are randomly generated and added to the network sequentially. Further study of EM-ELM shows <ref type="bibr" target="#b84">[86]</ref> that some newly added hidden nodes may be more efficient in reducing the residual error as compared to other hidden nodes. Hence, an enhancement of EM-ELM (referred to as EEM-ELM) <ref type="bibr" target="#b84">[86]</ref> was proposed by applying random search method. In the enhancement of EM-ELM, the hidden node is added to the network one-byone. At each incremental learning step, k hidden nodes are randomly generated and the hidden node that leads to highest residual error reduction will be added to the network, and then the output weights are updated incrementally in the same way of original EM-ELM.</p><p>It is found <ref type="bibr" target="#b88">[90]</ref> that the parsimonious network structure is probably missed by some greedy selection methods due to the fact that the hidden nodes added earlier may become insignificant when other hidden nodes are added to the network. In FCA <ref type="bibr" target="#b85">[87]</ref>, the researchers solved this problem by adding a fine tuning phase after the forward selection, which reviewed the hidden nodes selected in forward selection phase and replaced the selected hidden nodes with candidate nodes that achieve more contribution. Inspired by the above mentioned CS-ELM and the FCA algorithm, a two-stage algorithm was proposed and it is called TS-ELM <ref type="bibr" target="#b88">[90]</ref>. The first stage attempts to select hidden nodes by forward recursive algorithm and the selection is terminated by the final prediction error (FPE) criterion; while the second stage is a backward refinement phase that removes the insignificant hidden nodes by applying LOO method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">SVM with ELM feature mapping</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Stepwise forward selection based constructive ELM for regression</head><p>Instead of using a simple selection method that randomly generates a group of hidden nodes in each step of the training process (i.e. like in EEM-ELM), one could randomly generate a large number of hidden nodes as the candidate reservoir and then pick the hidden node one-by-one via a stepwise forward selection method. The fast construction algorithm (FCA) proposed in <ref type="bibr" target="#b85">[87]</ref> is a constructive hidden node selection method for ELM based on orthogonal least SVM <ref type="bibr" target="#b1">[2]</ref> has become one of the most popular classifiers. SVM has been extensively applied in wide type of applications. As explained in Cortes and Vapnik <ref type="bibr" target="#b1">[2]</ref>, SVM can be seen as a specific type of SLFNs, the socalled support vector networks. A multi-layer feedforward network (cf. <ref type="figure" target="#fig_1">Fig. 1</ref>) can be considered to transform the input data into a feature space Z of the last hidden layer <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. In order to find a solution of z i (x) where z i (x) is the activation function of the ith node of the last hidden layer, Cortes and Vapnik <ref type="bibr" target="#b1">[2]</ref> proposed the support vector machine which maps the data from the input space to some high dimensional feature space Z through some nonlinear mapping /?x? : x i ! /?x i ?: Standard optimization methods are used to find the separating hyperplane which maximizes the separating margins of two different classes in the feature space:</p><formula xml:id="formula_59">minimize: L P ? 1 2 kbk 2 ? k X N n i i?1 subject to: t i ?b ? /?x i ? ? b? ! 1 ? n i ; i ? 1; . . .; N n i ! 0; i ? 1; . . .; N<label>?40?</label></formula><p>sign?b ? h?x??: In ELM, to minimize the norm of the output weights kbk is actually to maximize the distance of the separating margins of the two different classes in the ELM feature space: 2=kbk; which is similar to SVM's target. From the standard optimization theory point of view, the objective (44) of ELM in minimizing both the training errors and the output weights can be written as:</p><p>where k is a user specified parameter and provides a tradeoff between the distance of the separating margin and the training error. Vectors x i for which t i ?b ? /?x i ? ? b? ? 1 is termed support vectors. The hyperplane w ? /?x? ? b ? 0 separates the training data with a maximal margin in the feature space. It maximizes the distance 2=kbk between two different classes in the feature space Z. To train such a SVM is equivalent to solving the following dual optimization problem:</p><p>Minimize:</p><formula xml:id="formula_60">L P ? 1 2 kbk 2 ? k X N n i i?1</formula><p>Subject to:</p><formula xml:id="formula_61">t i b ? h?x i ? ! 1 ? n i ; i ? 1; . . .; N n i ! 0; i ? 1; . . .; N<label>?45?</label></formula><p>which is very similar to SVM's optimization problem (40) with two main differences:</p><formula xml:id="formula_62">minimize: L D ? 1 2 X N X N t i t j a i a j /?x i ? ? /?x j ? ? X N a i i?1 j?1 i?1</formula><p>subject to:</p><formula xml:id="formula_63">X N t i a i ? 0 0 a i k; i ? 1; . . .; N i?1<label>?41?</label></formula><p>where each Lagrange multiplier a i corresponds to a training example (x i , t i ). Kernel functions K?u; v? ? /?u? ? /?v? are usually used in the implementation of SVM learning algorithm:</p><formula xml:id="formula_64">minimize: L D ? 1 2 X N X N t i t j K?x i ; x j ?a i a j ? X N a i i?1 j?1 i?1</formula><p>1. Different from the conventional SVM, the randomness can be adopted in the ELM mapping h(x), that is, all the parameters of h(x) are chosen randomly. 2. The bias b is not required in the ELM's optimization constrains since in theory ELM with h(x) has universal approximation capability and the separating hyperplane in the ELM feature space tends to pass through the origin. In SVM, the feature mapping /(x) is unknown and it is not required to satisfy universal approximation condition. However, in ELM, the feature mapping h(x) is required to satisfy universal approximation conditions (Theorems 2.3 and 2.4). Based on the Karush-Kuhn-Tucker (KKT) conditions <ref type="bibr" target="#b91">[93]</ref>, the equivalent dual optimization problem can be obtained:</p><p>The SVM kernel function K(u, v) needs to satisfy Mercer's condition <ref type="bibr" target="#b1">[2]</ref>. The decision function of SVM is: Further study <ref type="bibr" target="#b90">[92]</ref> showed that SVM's optimization constrains can be milder if ELM kernel is used, and the optimal solution can be obtained more efficiently. ELM is to minimize the training error as well as the norm of the output weights <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>:</p><p>As the separating hyperplane tends to pass through the origin in the ELM feature space, the above dual ELM optimization problem does not have the condition P i=1 N t i a i = 0, Vi, which is, however, required in the conventional dual SVM optimization problem (41). With the ELM kernel <ref type="formula">(24)</ref>  Experimental results <ref type="bibr" target="#b90">[92]</ref> have shown that the generalization performance of ELM is less sensitive to the user specified parameters especially the number of hidden nodes. Thus, compared to SVM, users can use ELM easily and effectively by avoiding tedious and time-consuming parameter tuning.</p><p>As a learning technique, ELM has demonstrated good potentials to resolving regression and classification problems. Recently, ELM techniques have received considerable attention in computational intelligence and machine learning communities, in both theoretic study and applications <ref type="bibr">[41-44, 50, 51, 78, 80, 91, 94-119]</ref>. Fundamentals of ELM techniques are composed of twofold: universal approximation capability with random hidden layer, and various learning techniques with easy and fast implementations. The following issues on ELM remain open and may be worth investigating in the future.</p><p>1. As observed in experimental studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">39]</ref>, the performance of ELM is stable in a wide range of number of hidden nodes. Compared to the BP learning algorithm, the performance of ELM is not very sensitive to the number of hidden nodes. However, how to prove it in theory remains open. 2. One of the typical implementations of ELM is to use random nodes in the hidden layer and the hidden layer of SLFNs need not be tuned. It is interesting to see that the generalization performance of ELM turns out to be very stable. How to estimate the oscillation bound of the generalization performance of ELM remains open too. 3. It seems that ELM performs better than other conventional learning algorithms in applications with higher noise. How to prove it in theory is not clear. 4. Experimental results <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b90">92]</ref> show that compared to backpropagation algorithm, SVM and least-square SVM (LS-SVM) ELM usually achieve similar or better generalization in regression and classification applications. How to prove it in theory is still an open problem. 5. ELM provides a batch learning kernel solution <ref type="bibr" target="#b24">(25)</ref> which is much simpler than other kernel learning algorithms such as LS-SVM <ref type="bibr" target="#b48">[49]</ref>. It is known that it is not straightforward to have an efficient online sequential implementation of SVM and LS-SVM. However, due to the simplicity of ELM, it may be easier to implement the online sequential variant of the kernel based ELM (25).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>G?a ?l? ?l? ?l? ?l? ?l? i ; b i ; x ?l? ? ? g?a i ? x ?l? ? b i ?; b i 2 R ?1? where a ?l? i is the weight vector connecting the (l -1)th layer to the ith node of the lth layer and b i (l) is the bias ?l? Keywords Extreme learning machine ? Support vector machine ? ELM kernel ? ELM feature space ? Ensemble ? Incremental learning ? Online sequential learning of the ith node of the lth layer. a i ? x ?l? denotes the inner product of vectors a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Multi hidden layers feedforward network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Lemma 2 .</head><label>2</label><figDesc>2 [17] Let k : R d ! R be an integrable boun- ded function such that k is continuous (almost everywhere)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>the output weight between the Lth hidden node and the jth output node, and e L (j) : f (j) -f L (j) the residual error function of the jth output node of the network with L hidden nodes, j ? 1; . . .; m: In theory, we have Theorem 2.3 [7] Given any bounded nonconstant piecewise continuous function g : R ! R for additive nodes or any integrable piecewise continuous function g : R ! R and R R g?x?dx 6 ? 0 for RBF nodes, for any continuous target function f and any randomly generated function sequence {g L }, lim L!1 kf ? f L k ? 0 holds with probabil- ity one if</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Classification boundary obtained by ELM for a binary class classification: L = 10 3 and k = 10 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Given any complex continuous dis- criminatory or any complex bounded nonlinear piecewise continuous function r : C ! C; for any target complex continuous function f : C d ! C and any randomly gener</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>(</head><label></label><figDesc>a) Present the (k ? 1)th chunk of new obser- P k?1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>6</head><label></label><figDesc>Incremental ELM (I-ELM) Theorem 6.1 [9] Given a SLFN with any nonconstant piecewise continuous hidden nodes G(a, b, x), if spanfG?a; b; x? : ?a; b? 2 R d ? Rg is dense in L 2 , for any continuous target function f and any randomly generated function sequence {g L } and any positive integer k; lim L!1 kf ? f ? L k ? 0 holds with probability one if ?j??</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Initialization: Let L = 0 and residual error E = T. step 2 Learning step: while L \ L max and kEk [</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>for i = 1 : k (i) Assign random parameters (a (i) , b (i) ) for the new hidden node L according to any contin- uous sampling distribution probability. (ii) Calculate the output weight b (i) (j) for the new hidden node: b ?j? E ?j? ? ^ h T ?i? ?i? ? ^ h ?i? ? ^ h T ?i? ; j ? 1; . . .; m ?38? (iii) Calculate the residual error after adding the new hidden node L: E ?j? ?i? ? E ?j? ? b ?j? ?i? ^ h ?i? ; j ? 1; . . .; m ?39? endfor (c) Let i ? ? fij min 1 i k kE ?i? kg where E ?i? ? ?E ?1? ?m? ?i? ; . . .; E ?i? ?:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>!</head><label></label><figDesc>minimize: L D ? 1 2 X N X N t i t j a i a j h?x i ? ? h?x j ? ? X N a i f ?x? ? sign X N s a s t s K?x; x s ? ? b ?43? i?1 j?1 i?1 s?1 subject to: 0 a i k; i ? 1; . . .; N Liu et al. [91] and Fr¨¦nay and Verleysen [50] made a significant contribution showing that (random) ELM kernels can be used in SVM and better generalization can be achieved. Their methods keep the same optimization methods as the conventional SVM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>is used in this testing case and the number of hidden nodes L is much larger than the number of training samples. Toh [41] and Deng et al. [42] studied such regularization enhancement under sigmoid additive type of SLFNs. Deng et al. [42] and Man et al. [43] focused on obtaining the analytical solution (21) based on optimization methods. Toh [41] proposed a corresponding total error rate based multi-class solution of ELM (TER-ELM). Miche et al. [44] studied ELM with a cascade of two regularization penal- ties. Huang et al.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagation errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Support vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Principles of neurodynamics: perceptrons and the theory of brain mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962" />
			<publisher>Spartan Books</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive radial basis function nonlinearities and the problem of generalisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of first IEE international conference on artificial neural networks</title>
		<meeting>first IEE international conference on artificial neural networks</meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="171" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extreme learning machine: a new learning scheme of feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of international joint conference on neural networks (IJCNN2004)</title>
		<meeting>international joint conference on neural networks (IJCNN2004)<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="985" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extreme learning machine: theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="489" to="501" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Universal approximation using incremental constructive feedforward networks with random hidden nodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="879" to="892" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convex incremental extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3056" to="3062" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enhanced random search based incremental extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="3460" to="3468" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Inf Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="525" to="536" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bounds on the number of hidden neurons in multilayer perceptrons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="55" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple method to derive bounds on the size and to train multilayer neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sartori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Antsaklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="471" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Upper bounds on the number of hidden neurons in feedforward networks with arbitrary bounded nonlinear activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Babri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="224" to="229" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">There exists a neural network that does not make avoidable mistakes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gallant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">White H (ed) Artificial neural networks: approximation and learning theory</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Blackwell</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="5" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks with a nonpolynomial activation function can approximate any function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leshno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schocken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="861" to="867" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Universal approximation using radial-basis-function networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Sandberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="246" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math Control Signals Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the approximate realization of continuous mappings by neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Funahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="183" to="192" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Universal approximation using feedforward networks with non-sigmoid hidden layer activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">White H (ed) Artificial neural networks: approximation and learning theory</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Blackwell</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="29" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Universal approximation bounds for superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Inf Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="930" to="945" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Objective functions for training new hidden units in constructive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T-Y</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1131" to="1148" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the optimality of neural-network approximation using incremental algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Meir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Maiorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="323" to="337" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Function approximation with SAOCIF: a general sequential method and a particular algorithm with feedforward neural networks. Departament de Llenguatges i Sistemes Inform¨¤tics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romero</surname></persName>
		</author>
		<ptr target="http://www.lsi.upc.es/dept/techreps/html/R01-41.html" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>Universitat Polit¨¨cnica de Catalunya</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning capability and storage capacity of two-hidden-layer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="274" to="281" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An iterative method for training multilayer networks with threshold function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Corwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Logar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wjb</forename><surname>Oldham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="507" to="508" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Training binary node feedforward neural networks by backpropagation of error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Toms</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron Lett</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="1745" to="1746" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A learning algorithm for multilayer perceptrons with hard-limiting threshold units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1994 IEEE workshop of neural networks for signal processing</title>
		<meeting>the 1994 IEEE workshop of neural networks for signal processing</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training multilayer networks with discrete activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Plagianakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Magoulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Nousis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Vrahatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international joint conference on neural networks (IJCNN&apos;2001)</title>
		<meeting>the IEEE international joint conference on neural networks (IJCNN&apos;2001)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Advanced calculus: an introduction to modern analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Voxman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Goetschel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
			<publisher>Marcel Dekker</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multivariable functional interpolation and adaptive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Broomhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="321" to="355" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stochastic choice of basis functions in adaptive function approximation and the functional-link net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Igelnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Pao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1320" to="1329" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Incremental extreme learning machine with fully complex hidden nodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="576" to="583" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Extreme learning machine: RBF network case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth international conference on control, automation, robotics and vision (ICARCV 2004)</title>
		<meeting>the eighth international conference on control, automation, robotics and vision (ICARCV 2004)<address><addrLine>Kunming, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-06-09" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1029" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Can threshold networks be trained directly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K-Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-K</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saratchandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Circuits Syst II</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="187" to="191" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Matrices: theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Generalized inverse of matrices and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Extreme learning machine for regression and multi-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>submitted</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ridge regression: biased estimation for nonorthogonal problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Hoerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Kennard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deterministic neural classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K-A</forename><surname>Toh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1565" to="1595" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Regularized extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE symposium on computational intelligence and data mining (CIDM2009)</title>
		<imprint>
			<date type="published" when="2009-03-30" />
			<biblScope unit="page" from="389" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A new robust training algorithm for a class of single-hidden layer feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">TROP-ELM: a double-regularized elm using lars and tikhonov regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Heeswijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Simula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lendasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Support vector regression machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural information processing systems</title>
		<editor>Mozer M, Jordan J, Petscbe T</editor>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="155" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A comparison of methods for multiclass support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="415" to="425" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A study on reduced support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K-M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1449" to="1459" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">RSVM: reduced support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">L</forename><surname>Mangasarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM international conference on data mining</title>
		<meeting>the SIAM international conference on data mining<address><addrLine>Chicago, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-04" />
			<biblScope unit="page" from="5" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Least squares support vector machine classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jak</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Process Lett</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="300" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Using SVMs with randomised feature spaces: an extreme learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fr¨¦nay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th European symposium on artificial neural networks (ESANN)</title>
		<meeting>the 18th European symposium on artificial neural networks (ESANN)<address><addrLine>Bruges, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-04-30" />
			<biblScope unit="page" from="315" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Parameter-insensitive kernel in extreme learning for non-linear support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fr¨¦nay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fully complex extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saratchandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="306" to="314" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Channel equalization using adaptive complex radial basis function networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Kassam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J Sel Areas Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="122" to="131" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Communication channel equalization using complex-valued minimal radial basis function neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jianping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saratchandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="687" to="696" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Approximation by fully complex multilayer perseptrons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Adali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1641" to="1666" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K-R</forename><surname>M¨¹ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lect Notes Comput Sci</title>
		<imprint>
			<biblScope unit="volume">1524</biblScope>
			<biblScope unit="page" from="9" to="50" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
<note type="report_type">Efficient BackProp</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A resource-allocating network for function interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="213" to="225" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A function estimation approach to sequential learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kadirkamanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="954" to="975" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A sequential learning scheme for function approximation using minimal radial basis function (RBF) neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yingwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saratchandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="461" to="478" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Performance evaluation of a sequential minimal radial basis function (RBF) neural network learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yingwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saratchandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="308" to="318" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Improved RAN sequential prediction using orthogonal techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salmer¨®n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Puntonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="153" to="172" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Time series analysis using normalized PG-RBF network with regression weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pomares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Bernier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Pelayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="267" to="285" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">An efficient sequential learning algorithm for growing and pruning RBF (GAP-RBF) networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saratchandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Syst Man Cybern Part B</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2284" to="2292" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A generalized growing and pruning RBF (GGAP-RBF) neural network for function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saratchandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="67" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A fast and accurate on-line sequential learning algorithm for feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N-Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saratchandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1411" to="1423" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">An introduction to optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekp</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Zak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Matrix computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cfv</forename><surname>Loan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>The Johns Hopkins University Press</publisher>
			<pubPlace>Baltimore</pubPlace>
		</imprint>
	</monogr>
	<note>3rd edn</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Oscillation and chaos in physiological control systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Mackey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="page" from="287" to="289" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">New York 70. Smola A, Sch?lkopf B (1998) A tutorial on support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<idno>NC2-TR-1998-030</idno>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
<note type="report_type">NeuroCOLT2 technical report</note>
	<note>Statistical learning theory</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Neural network ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="993" to="1001" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Bagging predictor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The strength of weak learnability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="197" to="227" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Boosting a weak algorithm by majority</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf Comput</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="256" to="285" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of online learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Comput Syst Sci</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Sales forecasting using extreme learning machine with applications in fashion retailing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z-L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T-M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K-F</forename><surname>Au</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decis Support Syst</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="411" to="419" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Adaptive ensemble models of extreme learning machines for time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Heeswijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindh-Knuutila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Hilbers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Honkela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lendasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lect Notes Comput Sci</title>
		<imprint>
			<biblScope unit="volume">5769</biblScope>
			<biblScope unit="page" from="305" to="314" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Gpuaccelerated and parallelized ELM ensembles for large-scale regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Heeswijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lendasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Negative correlation in incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>Minku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Comp</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">An OS-ELM based distributed ensemble classification framework in p2p networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Ensemble of online sequential extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="3391" to="3395" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A fast prunedextreme learning machine for classification problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H-J</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">OP-ELM: theory, experiments and a toolbox</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorjamaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lendasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lect Notes Comput Sci</title>
		<imprint>
			<biblScope unit="volume">5163</biblScope>
			<biblScope unit="page" from="145" to="154" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Multiresponse sparse regression with application to multidimensional scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tikka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings in artificial neural networks: formal models and their applications, ICANN 2005</title>
		<meeting>in artificial neural networks: formal models and their applications, ICANN 2005</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">3697</biblScope>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Error minimized extreme learning machine with growth of hidden nodes and incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1352" to="1357" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Random search enhancement of error minimized extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European symposium on artificial neural networks</title>
		<meeting><address><addrLine>Bruges, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-04" />
			<biblScope unit="page" from="327" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Fast construction of single hidden layer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of natural computing</title>
		<editor>Rozenberg G, B?ck T, Kok JN</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Algorithms for minimal model structure detection in nonlinear dynamic system identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K-Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Bilings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Control</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="311" to="330" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Constructive hidden nodes selection of extreme learning machine for regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="3191" to="3199" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Two-stage extreme learning machine for regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="3028" to="3038" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Extreme support vector machine classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lect Notes Comput Sci</title>
		<imprint>
			<biblScope unit="volume">5012</biblScope>
			<biblScope unit="page" from="222" to="233" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Optimization method based extreme learning machine for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="155" to="163" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Practical methods of optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Constrained optimization</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1981" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Extreme learning machine for predicting hla-peptide binding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Handoko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Keong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Y</forename><surname>Soon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Brusic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lect Notes Comput Sci</title>
		<imprint>
			<biblScope unit="volume">3973</biblScope>
			<biblScope unit="page" from="716" to="721" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">A neuro-fuzzy inference system through integration of fuzzy logic and extreme learning machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z-L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K-F</forename><surname>Au</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T-M</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Syst Man Cybern Part B Cybern</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1321" to="1331" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Partial lanczos extreme learning machine for single-output regression problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="3066" to="3076" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">OP-ELM: optimally pruned extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorjamaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Simula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jutten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lendasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="158" to="162" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A new machine learning paradigm for terrain reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-Wt</forename><surname>Yeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-S</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci Remote Sens Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="382" to="386" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">BELM: Bayesian extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Soria-Olivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomez-Sanchis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vila-Frances</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Magdalena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Serrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="505" to="509" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Real-time transient stability assessment model using extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Gener Transm Distrib</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="314" to="322" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Sensory system for implementing a human-computer interface based on electrooculography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boquete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rodriguez-Ascariz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="310" to="328" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Change detection of land use and land cover in an urban region with SPOT-5 images and partial lanczos extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N-B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Appl Remote Sens</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">ICGA-PSO-ELM approach for accurate multiclass cancer classification resulting in reduced gene sets in which genes encoding secreted proteins are highly represented</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saraswathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nilsen-Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ACM Trans Comput Biol Bioinforma</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="452" to="463" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Comparison of the primitive classifiers with extreme learning machine in credit scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F-C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P-K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-E</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE international conference on industrial engineering and engineering management</title>
		<imprint>
			<biblScope unit="page" from="685" to="688" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Realtime training on mobile devices for face recognition applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K-A</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="386" to="400" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Sales forecasting system based on gray extreme learning machine with Taguchi method in retail industry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Ou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1336" to="1345" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Incremental-based extreme learning machine algorithms for time-variant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Squartim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Piazza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lect Notes Comput Sci</title>
		<imprint>
			<biblScope unit="volume">6215</biblScope>
			<biblScope unit="page" from="9" to="16" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Performance enhancement of extreme learning machine for multi-category sparse data classification problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saraswathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng Appl Artif Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1149" to="1157" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">A new online learning algorithm for structure-adjustable extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Math Appl</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="389" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Simple ensemble of extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 2nd international congress on image and signal processing</title>
		<meeting>the 2009 2nd international congress on image and signal processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2177" to="2181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Color image watermarking using regularized extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Network World</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="317" to="330" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Application of wave atoms decomposition and extreme learning machine for fingerprint classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qmj</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sid-Ahmed</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lect Notes Comput Sci</title>
		<imprint>
			<biblScope unit="volume">6112</biblScope>
			<biblScope unit="page" from="246" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Human action recognition using extreme learning machine based on visual vocabularies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Minhas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baradarani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seifzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qmj</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1906" to="1917" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Intelligent approaches using support vector machine and extreme learning machine for transmission line protection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Malathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Marimuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="2160" to="2167" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Ternary reversible extreme learning machines: the incremental tri-training method for semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X-L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl Inf Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="345" to="372" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Power utility nontechnical loss analysis with extreme learning machine method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Nizar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Power Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="946" to="955" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Testing correct model specification using extreme learning machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">A study on effectiveness of extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Fast automatic two-stage nonlinear model identification based on the extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Irwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
