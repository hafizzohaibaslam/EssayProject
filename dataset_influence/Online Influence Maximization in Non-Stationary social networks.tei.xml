<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-07-17T00:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Influence Maximization in Non-Stationary Social Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-04-26">26 Apr 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Bao</surname></persName>
							<email>yxbao@cs.hku.hk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoke</forename><surname>Wang</surname></persName>
							<email>xkwang@cs.hku.hk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Wang</surname></persName>
							<email>wangzhi@sz.tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Graduate School at Shenzhen</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">C M</forename><surname>Lau</surname></persName>
							<email>fcmlau@cs.hku.hk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Online Influence Maximization in Non-Stationary Social Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-04-26">26 Apr 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Social networks have been popular platforms for information propagation. An important use case is viral marketing: given a promotion budget, an advertiser can choose some influential users as the seed set and provide them free or discounted sample products; in this way, the advertiser hopes to increase the popularity of the product in the users&apos; friend circles by the world-of-mouth effect, and thus maximizes the number of users that information of the production can reach. There has been a body of literature studying the influence maximization problem. Nevertheless, the existing studies mostly investigate the problem on a one-off basis, assuming fixed known influence probabilities among users, or the knowledge of the exact social network topology. In practice, the social network topology and the influence probabilities are typically unknown to the advertiser, which can be varying over time, i.e., in cases of newly established, strengthened or weakened social ties. In this paper, we focus on a dynamic non-stationary social network and design a randomized algorithm, RSB, based on multi-armed bandit optimization, to maximize influence propagation over time. The algorithm produces a sequence of online decisions and calibrates its explore-exploit strategy utilizing outcomes of previous decisions. It is rigorously proven to achieve an upper-bounded regret in reward and applicable to large-scale social networks. Practical effectiveness of the algorithm is evaluated using both synthetic and real-world datasets, which demonstrates that our algorithm outperforms previous stationary methods under non-stationary conditions. its neighbors at each time stamp independently of the history thus far, and a node only attempts to activate a neighbor once. In the linear threshold model [1], a node will be activated only when the sum of influence probabilities from its neighbors exceeds a threshold. The influence probability in the above models, namely the probability for node u to activate its neighbor v after u has been activated, is often decided empirically in studies designing influence maximization algorithms, e.g., according to inverse of the indegree of v. Based on these information propagation models, existing studies mostly tackle the influence maximization problem on a one-off basis, assuming that both the social network topology and influence probabilities are fixed and available as input. Kempe et al. [1] prove that the influence maximization problem is NP hard but can be approximated to within a factor of (1 ? 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-Armed Bandit with Non-Stationary Rewards</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Influence Maximization with Bandit Optimization</head><p>Recently, multi-armed bandit optimization has been applied to solve influence maximization problem with incomplete information of the social network. In particular, combinatorial bandits are highly correlated to the influence maximization problem, where the decision-making agent needs to select multiple arms in each time stage. Chen et al. <ref type="bibr" target="#b8">[9]</ref> define</p><p>The simplest idea to tackle non-stationary rewards is to decrease the weights of earlier feedback in next-step decision making <ref type="bibr" target="#b19">[20]</ref>. The problem it may lead to is that without sufficient feedback information, it is hard to achieve a good accuracy of reward estimation. Some algorithm designs assume abrupt changes of the distributions occurring at arbitrary intervals <ref type="bibr" target="#b20">[21]</ref>, and allow the agent to query a set of arms not picked before and obtain outcomes as if these arms were played. This assumption is reasonable in a stock market, i.e., people can acquire information of stocks they have never purchased by following bearish or bullish trends, but not for influence propagation, where there is no channel to obtain outcomes of untried arms. Besbes et al. <ref type="bibr" target="#b21">[22]</ref> assume that the total variation of the rewards is given and design a randomized algorithm based on Exp3, which assigns exponential weights to arms for exploration and exploitation in adversary bandit <ref type="bibr" target="#b22">[23]</ref>. Only one arm is selected in each time stage, while we focus on the case of combinatorial bandits. It is non-trivial to extend the algorithm to combinatorial scenarios.</p><p>Gai et al.</p><p>[24] study non-stationary bandit optimization under the assumption that the state of a selected arm evolves as an irreducible finite-state Markov process with unknown transition matrix, while the distributions of other arms stay unchanged (rested arms). Their work is applicable to many graph theory problems, e.g., channel allocation in cognitive radio networks. However, assuming rested arms is not realistic in influence propagation in social networks. The same authors further investigate restless bandits with Markov rewards <ref type="bibr" target="#b10">[11]</ref>, where the states of an arm evolve dynamically over time no matter whether it has been played. The algorithm utilizes regenerative property of a Markov chain and achieves a regret near logarithmic on the total number of time stages. Both studies rely on an initialization stage, in which each arm is tried for at least once. This is impractical for influence propagation (e.g., market campaign) in a large-scale network, as the cost of trying all nodes is unaffordable. <ref type="bibr">Granmo et al. [25]</ref> use Kalman filter to update estimation of the reward distribution, and evaluate their results by simulation without theoretical analysis. Kalman filter is only applicable to linear dynamic system and the states inherently form a Markov chain. It is not realistic to make the Markov chain assumption in influence propagation, since human behavior does not simply depend on one's latest status.</p><p>number of activated users after we add a into S. Let f t (S) be an influence spread function in time stage t, indicating the total number of activated nodes in t based on seed set S. The value of f t (S) is a random variable. The expectation E[f t (S)] is non-negative, monotone and submodular, as proven in <ref type="bibr" target="#b25">[26]</ref>. The submodularity of the spread function is useful such that we can utilize the benchmark based on greedy optimal value. The expected reward of selecting an arm a|S in t is hence E[f t (S ¡È {a})] ? E[f t (S)]. Note that the expectation E[¡¤] is taken over both randomized rewards and randomized policies, where a policy refers to the agent's strategy for seed selection, which is random given the random nature of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>In each time stage t, starting from an empty set S t = ?, we obtain a seed set of size K by adding nodes to S t one by one in some order. Let S t = (a We model the social network as an influence graph G = (N , E). N = {1, 2, . . . , N } is the set of users (nodes), where N is the total number of nodes. E is the set of social connections among the nodes. An unknown influence probability p t is selected as the k th seed in t given previous choices in S</p><formula xml:id="formula_0">(1:k?1) t . Let ? r k t (a k t |S (1:k?1) t ) = E[f t (S (1:k?1) t ¡È{a})]?E[f t (S (1:k?1) t )</formula><p>] denote the expected marginal gain of choosing a k t as the k th t n,m is associated with each edge (n, m) ¡Ê E, which is time varying following an unknown, non-stationary distribution: after user n is activated (e.g., obtained information of a product), he may activate his neighbor m (e.g., share information of the product) with different probabilities at different time stages t. In this way, each edge (n, m) is associated with a non-stationary Bernoulli distribution: in t, user n may activate his neighbor m with probability p seed in t. The expected total reward in time stage t is ? r t (</p><formula xml:id="formula_1">S t ) = K k=1 ? r k t (a k t |S</formula><p>(1:k?1) t t n,m , or not with probability 1 ? p t n,m . We do not assume any cascade model of the information propagation system (e.g., independent cascade model or linear threshold model), and our algorithm works with various cascade models as long as the information spread brought by an activate node can be modeled as a random variable.</p><p>Let T be the total number of time stages that the system spans. In each time stage, a set of K seeds are selected as information sources (e.g., the seed users that an advertiser directly promotes the product to, whose number is decided by the promotion budget), from which the information spreads to other nodes in the network. The seed set is repeatedly selected over different time stages. For example, a company may carry out a promotion campaign for a series of time stages, e.g., a number of consecutive days. After the promotion in each time stage via a potentially different set of seeds, the company collects statistics on the number of purchases of their promoted product(s) and utilizes this feedback to update its seed selection strategies in later time stages. The goal is to maximize the expected overall influence spread in the whole time span 1, 2, . . . , T , i.e., the expected total number of activated nodes. Let M be the collection of all subsets of N . In our bandit optimization framework, we define a|S, meaning node a under a given set S ¡Ê M, as an arm. The expected reward of selecting an arm a|S is the expected marginal gain by adding a into the existing seed set S, i.e., the expected additional</p><formula xml:id="formula_2">) = E[f t (S t )].</formula><p>In this model, maximizing the expected total number of activated nodes in 1, . . . , T is equivalent to maximizing the expected overall reward in the entire span,</p><formula xml:id="formula_3">T t=1 ? r t (S t ) = T t=1 E[f t (S t )].</formula><p>It is further equivalent to minimizing the regret, the gap between the expected overall reward that the agent can obtain by running our online algorithm and the offline optimal expected overall reward computed using full knowledge of the system. In our algorithm design, we aim to minimize the weak regret, i.e., the gap between the expected overall reward achieved by our algorithm and the offline expected overall reward achieved by using the same best seed set S * in all time stages, namely S * ¡Ê arg max</p><formula xml:id="formula_4">S¡ÊM T t=1 E[f t (S)]</formula><p>, computed based on full knowledge of the entire system. Such a weak regret is the difference between the expected overall reward obtained by our algorithm and that achieved by the best single action, i.e., sticking with one seed set in all time stages. Weak regret is commonly used in the literature on analysing non-stationary bandit algorithms <ref type="bibr" target="#b23">[24]</ref> <ref type="bibr" target="#b10">[11]</ref>[12] <ref type="bibr" target="#b26">[27]</ref>, and the key ingredient is to form accurate estimates on the average condition for each arm <ref type="bibr" target="#b27">[28]</ref>, so as to find the arm performing best in a long term. In particular, we analyze a greedy weak regret, with detailed definition given in Definition 2 in Sec. V, that compares the expected overall reward produced by our algorithm with the lower bound of an approximate offline overall reward achieved by a single best seed set derived by a greedy approach. Greedy weak regret is a concept narrowed down from weak regret, when the best single action is decided by a greedy algorithm. We apply this notion so as to compare with the lower bound of the greedy optimal value. </p><note type="other">algorithm, which satisfies C ¡Ý ¦Ãr t (n|S) N q k t (n|S)</note><p>, ?n ¡Ê N , S ¡Ê M. N # of nodes N the set of nodes M the collection of subsets of N T the total number of time stages C input parameter to Alg. 1 K the size of seed set ¦Ã input parameter to Alg. 1</p><p>In Alg. 1, the K seeds are selected sequentially (line 3). The weights w associated with the nodes should be equal at the beginning of each time stage, and adjusted based on updated v, each time after the seed set has been updated (lines 4-6).</p><p>The computation of w</p><note type="other">n|S (1:k?1) t t S (1:k?1) t the set containing the first k ? 1 selected seeds in t a|S (1:k?1) t an arm in t, selecting node a given S (1:k?1) t ft(S)</note><p>the influence spread of seed set S in t r k t (a|S</p><formula xml:id="formula_5">(1:k?1) t )</formula><p>reward of choosing a as k th seed based on S</p><p>(1:k?1) t in t ? r k t (a|S</p><formula xml:id="formula_6">(1:k?1) t )</formula><p>expected reward of choosing a as k th seed based on S</p><formula xml:id="formula_7">(1:k?1) t in t Reg G (T )</formula><p>greedy weak regret in the whole system span Reg k (t) position weak regret for the k th seed in t a k aims to balance exploitation and exploration: the first term is calculated based on past reward information (exploitation) and the second constant term is assigned for each arm no matter how many times it has been tried (exploration). Next, the probability for adding an additional node into the already selected set of seeds is decided by normalizing its weight over the weights of all the remaining nodes not in the existing seed set (lines 7-9). An arm is randomly selected according to the probability distribution and a reward a|S </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RSB: RANDOMIZED MULTI-ARMED BANDIT ALGORITHM FOR NON-STATIONARY SOCIAL NETWORKS</head><p>Main Idea. We next design an online multi-armed bandit algorithm to minimize the greedy weak regret. In each time stage, we select the best seed set by sequentially selecting the next best node given previous seed decisions. Given the set of already selected seeds, we associate weights with candidate arms, and deal with the varying environment (time-varying underlying distributions of influence probabilities) by adjusting the weights of arms based on rewards received due to previous seed selection (the exploitation component of our algorithm). Besides, we also include a constant ) by dividing the actual reward by the probability of selecting the arm (line 13) compensates the reward of actions with less probability to be chosen and guarantees that the expectation of the estimated reward and the actual reward are equal, when the expectation is taken over both randomized policies and randomized rewards. This equality helps us to derive the expected reward of RSB in the proof. The updated weights will be used in selecting future seeds in this time stage.</p><p>We will evaluate the impact of the input parameter ¦Ã under practical settings in simulations. The input parameter C is related to the largest spread brought by a seed, which is unknown before running the algorithm. In fact, requiring</p><formula xml:id="formula_8">C ¡Ý ¦Ãr k t (n|S) N q k t (n|S)</formula><p>¦Ã N in the weight of each arm, where ¦Ã ¡Ê (0, 1] is a gaugeable value, in order to enable exploration of arms never tried before. Different from deterministic stationary bandit algorithms, our algorithm is randomized in arm selection according to the weights, and hence even if the environment changes abruptly, the algorithm still has a chance to switch to the new best arm. Algorithm Steps. Our multi-armed bandit algorithm for selecting the best seed set in each time stage t is given in Alg. 1.</p><p>is only needed for regret analysis. We can set the value of C empirically when running the algorithm in practice, and will evaluate the performance of the algorithm under an empirical value of C in simulations, which does not necessarily satisfies the above condition.</p><p>Here w n|S (1:k?1) t t is the weight for selecting node n as the k th We note that our algorithm does not rely on any knowledge of the underlying social network topology and the influence probabilities, but only utilizes the outcomes that are decided by them. In addition, although the entire space of arms, a|S, ?a ¡Ê N , S ¡Ê M, is exponential, the number of arms that need to be dealt with in each time stage in Alg. 1 (weights and probabilities computed and used in seed selection) is still polynomial, as given in the following theorem. seed in time stage t, while the set of already selected seeds in t is S    ) denotes the realization of the reward (actual marginal influence spread) by choosing node a as the k th seed in t. C is an input parameter to the Proof: In each time stage t, we select K seeds. When selecting the k th seed based on already selected seeds in S</p><p>(1:k?1) t , we compute/update weights, and compute selection probabilities for at most N arms corresponding to N nodes in the network. Therefore, the time complexity is O(KN ). Input: N , K, C, ¦Ã Output: the seed set S k k (1:K) t in different time stages. We have OP T k = max T t=1 ? r t (a|S</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 RSB: Randomized Sequential</head><note type="other">Multi-armed Bandit Algorithm for Non-Stationary Networks may form different arms, ? a k |S (1:k?1) t , under different seed sets S (1:k?1)</note><formula xml:id="formula_9">(1:k?1) t ) = T t=1 ? r t (? a k |S (1:k?1) t ).</formula><p>for each time stage t a¡ÊN 1: set v n|S (1:k?1) 1 Definition 1. The position weak regret for the k th seed is 1 = 1, ?n ¡Ê N , k = 1, . . . , K 2: for t = 1, 2, . . . , T do </p><formula xml:id="formula_10">for k = 1, 2, . . . , K do Reg k (T ) = ? r k k k t (? a k |S (1:k?1) t ) ? ? r t (a t |S (1:k?1) t ) 4:</formula><p>for each node n ¡Ê N do for each node n ¡Ê N \S</p><p>(1:k?1) t do the arm selected by Alg. 1 in time stage t. The conditional set S</p><p>(1:k?1) t is also decided by Alg. 1.</p><p>8:</p><formula xml:id="formula_11">q n|S (1:k?1) t w t = n|S (1:k?1) t t w n |S (1:k?1) t t n ¡ÊN \S (1:k?1) t 9:</formula><p>end for</p><p>The following theorem states the relationship between position weak regret and OP T , which will be used to bound the greedy weak regret in Theorem 3. Its proof can be found in Appendix A. <ref type="bibr">10:</ref> draw an arm a|S receive a reward r k t (a|S</p><formula xml:id="formula_12">(1:k?1) t ) ? r t (S (1:k) t ) ? ? r t (S (1:k?1) t ) 12: set S (1:k) t = S (1:k?1) t ¡È {a} t=1 k 13:</formula><p>set?rset? set?r k r ) t (a|S</p><formula xml:id="formula_13">(1:k?1) t ) = t (a|S (1:k?1) t T q a|S (1:k?1) t t OP T ? ? r t (S (1:k?1) t ) ? Reg k (T ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>for all n ¡Ê N \{a}, set?rset? set?r</p><formula xml:id="formula_14">k ¡Ý 1 K t (n|S (1:k?1) t ) = 0 t=1 15:</formula><p>for each arm n|S</p><formula xml:id="formula_15">(1:k?1) t , ?n ¡Ê N do 16: update v n|S (1:k?1) t+1 t+1 = k v n|S (1:k?1) t ¦Ã ? r t exp { t (n|S (1:k?1) t ) N C } 17:</formula><p>end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>18:</head><p>end for 19: end for</p><formula xml:id="formula_16">V. REGRET ANALYSIS Let F (S) = T t=1 E[f t (S)]</formula><p>, ?S ¡Ê M, which denotes the expected overall influence spread over the whole system span. F (S) is a submodular function since it is the summation of submodular functions E[f t (S)], ?t = 1, . . . , T . Then we can design a greedy approach to compute a S that approximately maximizes the expected overall reward T t=1 ? r t (S) = T t=1 E[f t (S)] based on full knowledge of the system: after deciding S</p><p>(1:k?1) , we select a local optimal node as the k th seed, that maximizes the expected marginal influence spread, i.e., node u such that u ¡Ê arg max</p><formula xml:id="formula_17">{F (S (1:k?1) ¡È</formula><p>We next analyze an upper bound of the greedy weak regret achieved by Alg. 1. Let OP T denote the offline maximal value of the expected overall reward T t=1 ? r t (S) = T t=1 E[f t (S)] over all S ¡Ê M, computed based on complete knowledge of the influence probability distributions and the social graph topologies in 1, . . . , T . Let S * be the offline optimal seed set, i.e., the single best seed set that maximizes T t=1 ? r t (S).</p><formula xml:id="formula_18">v¡ÊN \S (1:k?1)</formula><p>{v})?F (S (1:k?1) )}. We can easily prove that the approximate offline solution computed this way achieves an approximation ratio of 1 ? 1 e , i.e., the expected overall reward it achieves is at least (1 ? 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Reduction from Greedy Weak Regret to Position Weak Regret</head><p>We define a position optimal reward OP T k as the sum of the expected marginal gains achieved by using the best k th seed in all time stages. The best k th seed maximizes T t=1 ? r e )OP T , following Theorem 3.5 in <ref type="bibr" target="#b25">[26]</ref>, based on submodularity of the spread function and local optimality when selecting each seed. The reason that we compute this approximate offline solution using the greedy approach (which runs in polynomial time) is that computing S * has been shown to be an NP hard problem <ref type="bibr" target="#b0">[1]</ref>.</p><p>Using the approximate offline overall reward computed as above, we define a greedy weak regret as follows, which we use to evaluate the performance of our algorithm RSB.  ) based on full knowledge of the system, given the first k ? 1 seeds in S</p><p>(1:k?1) t in each t derived using RSB. The idea is to reduce the original problem of finding the best solution of the full set to a parallel bandit setting, finding the best k th element under the condition determined by our algorithm. Let?Let?Let? k denote this optimal k th seed, i.e., ? a k ¡Ê arg max Definition 2. The greedy weak regret is defined as the gap between the lower bound of the approximate offline overall reward derived by the greedy approach and the expected overall reward produced by RSB in Alg. 1, i.e.,</p><formula xml:id="formula_19">T )</formula><p>.</p><formula xml:id="formula_20">T t=1 ? r k )</formula><p>. Such a best k th seed</p><formula xml:id="formula_21">Reg G (T ) = (1 ? 1 e )OP T ? ? r t (S (1:K) t a¡ÊN t (a|S (1:k?1) t t=1</formula><p>The following theorem shows that the overall position weak regret provides an upper bound of the greedy weak regret. The proof can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. PERFORMANCE EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Sets</head><p>Theorem 3. The greedy weak regret is upper bounded by the sum of position weak regrets over all positions k = 1, 2, . . . , K, i.e.,</p><formula xml:id="formula_22">K Reg G (T ) ¡Ü Reg k (T ). k=1</formula><p>Based on Theorem 3, we seek to bound the position weak regret for each k, in order to derive an upper bound of Reg G (T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bounding Greedy Weak Regret</head><p>According to Definition 1, the position weak regret for the k th seed is</p><formula xml:id="formula_23">T T Reg k (T ) = r k k k t (? a k |S (1:k?1) t ) ? r t (a t |S (1:k?1) t ) t=1 t=1 T T = max k k k n¡ÊN r t (n|S (1:k?1) t ) ? r t (a t |S (1:k?1) t ). t=1 t=1</formula><p>Let D be the upper bound of the realization of reward, i.e., r k t (n|S) ¡Ü D, ?n ¡Ê N , S ¡Ê M. The following theorem states an upper bound of the position weak regret for each k. In particular, if ¦Ã is set to a special value, it can minimize the regret bound. The proof can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Tencent Weibo Traces: We produce a dynamic social network based on Tencent Weibo</head><p>2 traces containing the following links among 4257 users for 7 consecutive days during November 2011. Each directed following link (n, m) indicates that user n follows user m <ref type="bibr" target="#b28">[29]</ref>. The links among the users vary from one day to the next, giving a dynamic social graph. To prolong the trace duration, we further repeat the variation of the social graph on 7-day cycles to form a 100-day duration (T ), which we believe reasonable since human behavior may well follow a weakly periodicity.</p><p>2) Synthetic Data: As Weibo traces only provide the dynamic behavior of a specific social network, we also generate a synthetic dynamic social network by combining the model in <ref type="bibr" target="#b29">[30]</ref> with the Erd? os-R¨¦nyi model and preferential attachment: we generate an initial graph with 5000 nodes and connect each pair of node with probability 0.005 (a directed link); then in each time stage, we select 1000 edges uniformly and change their heads to other nodes randomly picked with probabilities proportional to their indegrees. In this way, we generate a sequence of social graphs for T = 100 time stages. Preferential attachment is a representative mechanism to model the topology of a social network, that the more connected a node is, the more likely it is to receive new links.</p><formula xml:id="formula_24">Theorem 4. Let R k max = max k n¡ÊN T t=1 ? r t (n|S (1:k?1) t</formula><p>) be the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Time-varying Influence Probabilities</head><p>We employ the following three models to generate nonuniform and time-varying influence probabilities in a social graph. </p><formula xml:id="formula_25">Reg k (T ) = R k max ? R k RSB ¡Ü (1 + (e ? 2) D C )¦ÃR k max + N C ln N ¦Ã .</formula><p>If we set ¦Ã = min{1,</p><formula xml:id="formula_26">N C ln N (1+(e?2) D } where constant g ¡Ý C )g R k max , ?k = 1, . . . , K, we have the following minimum upper bound K Reg k (T ) ¡Ü 2K gCN ln N . k=1 1 + (e ? 2) D C Corollary 1.</formula><p>The greedy weak regret achieved by Alg. 1 is upper bounded as follows:</p><p>m is the indegree of node m at t. The probabilities are varying due to the changes of links in a dynamic social graph.</p><p>? The Trivalency (TR) model <ref type="bibr" target="#b30">[31]</ref>: in each time stage, the influence probability of an edge in the social graph is assigned a value among {0.1, 0.01, 0.001} uniformly randomly, corresponding to three types of social tiesstrong, medium and weak. The assigned probability on an edge may change from one time stage to the next.</p><p>? A Fluctuating Reward (FR) model. We design this model such that influence probabilities evolve over time in a similar fashion as a sinusoidal wave (also similar to that used in <ref type="bibr" target="#b31">[32]</ref>): the influence probability of each edge starts from a random value drawn uniformly within [0, 0.1]; then in each time stage, it increases or decreases at a constant rate 0.3</p><p>T until reaching the largest value 0.1 or the smallest value 0.</p><formula xml:id="formula_27">Reg G (T ) ¡Ü 2K 1 + (e ? 2) D C ¡Ì DCT N ln N ,</formula><p>i.e., the upper bound of the greedy weak regret of Alg. 1 is</p><formula xml:id="formula_28">O( ¡Ì T N ln N ).</formula><p>It shows that our greedy weak regret is sublinear with both N and T . The proof can be found in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Schemes for Comparison</head><p>We compare RSB with a random algorithm and OG-UCB proposed in <ref type="bibr" target="#b18">[19]</ref>. With the random algorithm, the agent always selects a seed uniformly randomly among all candidate nodes. OG-UCB is designed for stationary scenarios, which associates a confidence bound with each arm and chooses the arm with the highest upper confidence bound greedily.</p><p>We note that although a number of bandit algorithms have been proposed for influence maximization (as discussed in Sec. II-A), most are not directly comparable since they run with the complete knowledge of a social network. We compare with OG-UCB since it is the only existing bandit algorithm without requiring knowledge of the social graph topology. In addition, the bandit algorithms designed for non-stationary systems in Sec. II-B either deal with 1 arm or assume Markov rewards, and hence cannot be readily extended for comparison.</p><p>In computing greedy weak regret, we also compute the approximate offline optimal overall reward by the greedy offline algorithm discussed before Definition 2 in Sec. V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation Results</head><p>To show greedy weak regret values in a unified range in our figures, we plot the ratio between greedy weak regret and the approximate offline optimal overall reward, i.e., approx. offline opt. overall reward?overall reward by RSB approx. offline opt. overall reward . Especially, a data point at a specific T represents the above ratio computed using overall rewards in <ref type="bibr">[1, T ]</ref>. We set K = 5, ¦Ã = 0.2 (default), D = 120 and C = 1 in our experiments. <ref type="figure" target="#fig_0">Fig. 1-Fig. 4</ref> show the results obtained using synthetic data or Tencent Weibo traces under different time-varying models of influence probabilities. We observe that the regret ratios (and hence information spread) achieved by RSB and the random algorithm are usually similar at the early stages of the system, when RSB has not cumulated much feedback. RSB gets better than the other algorithms (lower regret and hence better spread) after more time stages, validating that RSB can improve with more feedback received from the real system. Besides, OG-UCB performs the worst especially with the ongoing of time, showing that it is only suitable for fixed influence probability distributions and does not work well in cases of time-varying influence probabilities. The increase of cumulative regret by RSB with the increase of time stages, if any, is always slower than that of the other algorithms.</p><p>In <ref type="figure" target="#fig_15">Fig. 5</ref>, we compare the regret ratios of RSB achieved under different values of input parameter ¦Ã, using Tencent Weibo traces under the FR model. From line 5 of Alg. 1, we can see ¦Ã = 0 represents pure exploitation and ¦Ã = 1 indicates pure exploration. Although Theorem 4 requires ¦Ã &gt; 0 for the bound to be meaningful, we can still test the extreme case that ¦Ã = 0 when running the algorithm in practice. RSB performs worst in these extreme cases. ¦Ã ? = 0.18 is computed following the formula in Theorem 4 which minimizes the theoretical upper bound. We observe that ¦Ã ? achieves nearlowest regrets in actual execution of our algorithm under practical settings as well.</p><p>In <ref type="figure" target="#fig_16">Fig. 6</ref>, we evaluate the impact of different graph sizes N , by extracting subgraphs of different sizes using Tencent Weibo traces. We observe that the regret is larger in larger networks, but it always improve when the system runs for longer period of time.     Proof: We prove the following inequality for each position k by induction.</p><formula xml:id="formula_29">V t+1 V t = n=1 V t N n|S (1:k?1) t k T k = v t t (n|S (1:k?1) t ) N C } OP T ? ? r t (S (1:k) t ) ¡Ü (1 ? 1 K ) k OP T + Reg m (T ) V t exp { ¦Ã ? r n=1 t=1 m=1 (2) N n|S (1:k?1) t k = w t ? ¦Ã N t (n|S (1:k?1) t 1 ? ¦Ã exp { ¦Ã ? r ) N C }</formula><p>The base case k = 0 is trivial. In the induction, let  .</p><formula xml:id="formula_30">n=1 T k T N w n|S (1:k?1) t ? k Z k = OP T ? ? r t (S (1:k) t ) = OP T ? ?(? r m t ¦Ã N t (n|S (1:k?1) t t ). ¡Ü 1 ? ¦Ã [1 + ¦Ã ? r ) N C t=1 m=1 t=1 n=1 k Thus Z k = Z k?1 ? T t=1 ?(? r k + (e ? 2)( ¦Ã ? r t (n|S</formula><p>Then we have The inequality (5) uses the fact that e x ¡Ü 1 + x + (e ? 2)x Then we can get for x ¡Ü 1 and the inequality (6) is derived by the facts (3) and (4). Using the inequality 1 + x ¡Ü e x and taking logarithms, we have ).</p><formula xml:id="formula_32">Z k ¡Ü (1 ? 1 K )Z k?1 + Reg k (T ).</formula><formula xml:id="formula_33">T R k k RSB ¡Ý (1 ? ¦Ã) ? r t (n j |S (1:k?1) t t=1 ) ? N C ln N ¦Ã T N ln V t+1 V t ¡Ü ¦Ã N C k k 1 ? ¦Ã r t (a|S</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>n=1</head><p>Since over the whole period, the expected marginal gain of any node n ¡Ê N is no larger than that of the best seed, which is R . Then summing over t, we can get all reward obtained by the agent over period 1, . . . , T for the position k as follows.</p><p>max . Since node n j is chosen arbitrarily, we can choose it as the best seed?seed?seed? k under the conditional set S</p><p>(1:k?1) t , thus we have T t=1 ? r k t (? a k |S</p><p>(1:k?1) t ) = R k max . Combined with these two results, we have</p><formula xml:id="formula_34">ln V T +1 V 1 ¡Ü ¦Ã N C 1 ? ¦Ã r k RSB R k k k max ? R RSB ¡Ü (1 + (e ? 2) D C )¦ÃR max + N C ln N ¦Ã . ¦Ã T N + (e ? 2)( N C ) 2 D k 1 ? ¦Ã ? r t (n|S (1:k?1) t ) t=1 n=1</formula><p>Note that the expectation above is under any conditional set S</p><p>(1:k?1) t For any node n j ¡Ê N whatever the agent selects it, we have , ?t = 1, 2, . . . , T , which is related to previous choices for position 1, . . . , k ? 1. This is consistent with Definition 1, that S is decided by Alg. 1. The expectation in R</p><formula xml:id="formula_35">k ln V T +1 V 1 ¡Ý ln v T +1 k V 1 .</formula><p>max is also reduced to randomizing on reward only. Since  Then we take the expectation on policy's actions as well as random rewards. Noting that given the choice a 1 , a 2 , . . . , a t?1 before, for any node n j ¡Ê N \S Note that if ¦Ã = 1, we have ¡Ì N C ln N ¡Ý (1 + (e ? 2)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 t</head><label>1</label><figDesc>, . . . , a K III. PROBLEM MODEL t ) be the completed seed set, in which the k th element is the k th seed selected in this time stage. Let S (1:k?1) t represent the selected seed set with elements 1, . . . , k ? 1, and a k t |S (1:k?1) t mean that node a k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>is observed (lines 10-12), e.g., the addi- tional number of product purchases received by promoting to node a is collected. We then update v n|S (1:k?1) t t t selected node as k th seed in t ? a k optimal node as k th seed in all time stages by multiplying an exponential factor (line 16), decided by?rby? by?r k t (a|S (1:k?1) t E[¡¤] expectation taken over both random policies and random rewards OP T the offline maximal value of the expected overall reward ), which can be understood as an unbiased estimation of the reward of the arm. Computing?rComputing? Computing?r k t (a|S (1:k?1) t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>is an auxiliary quantity to compute the weights, updated based on the past reward information of Theorem 1. The time complexity of Alg. 1, executed in each time stage t, is O(KN ). arm n|S (1:k?1) t , as an exploration measure. q n|S (1:k?1) t t is the probability of playing arm n|S</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 1 :</head><label>1</label><figDesc>k?1) t in t, derived from the weights of the arms. r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>expected overall reward achieved by selecting the best k th arm given S (1:k?1) t , ?t = 1, . . . , T , derived by Alg. 1. Let R ? The Weighted Cascade (WC) model [1]: the influence probability p t n,m of edge (n, m) at time t isdenote the expected over- all marginal gain obtained by adding the k th seeds into the given S , where d (1:k?1) t , ?t = 1, . . . , T . For any parameter ¦Ã ¡Ê (0, 1], we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>60%</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Tencent Weibo trace and FR model: ¦Ã = 0.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Tencent Weibo trace and FR model: different values of ¦Ã.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Tencent Weibo trace and TR model: different graph sizes N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>= 1 ,</head><label>1</label><figDesc>we can derive the following inequality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>)</head><label></label><figDesc>? ln N Taking the first derivative of the right part in inequality (8), we can set ¦Ã = min{1, t=1 N C ln N (1+(e?2) D } to minimize the C )g Based on the inequalities above, we can derive bound, then in the period 1, . . . , T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>TABLE I : Notation k</head><label>I</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>)</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Although [26] does not consider randomized policies, the submodularity of E[ft(S)] still holds following results in [26], as expectation over policies is a linear combination of submodular functions.</note>

			<note place="foot" n="2"> http://t.qq.com/</note>

			<note place="foot">This paper investigates online influence maximization in dynamic social networks with non-stationary influence probability distributions among participants. We design a randomized algorithm based on multi-armed bandit optimization to guide source selection for information dissemination over multiple time stages, aiming to maximize the overall spread over the system span. The algorithm is simple and neat, relying on carefully designed, continuously updating preferences on seed selection, which exploit real-world feedback from previous decisions, as well as explore new choices. As the first in the literature, the algorithm does not require knowledge of the dynamic social graph topology, nor time-varying influence probabilities, but is able to achieve an upper-bounded weak regret, as compared to an approximate offline optimal reward. Simulations based on both synthetic and real-world datasets further validate that our algorithm is more adaptive to a changing environment than heuristic and stationary bandit algorithms. In addition, our algorithm is also applicable to many other real-world problems such as advertisement placement [9], as long as the reward functions are submodular or there exists an approximate offline algorithm that can achieve an approximation ratio of (1 ? 1 e ). In future work, we seek to apply similar algorithms to solve the other real-world problems.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A PROOF OF THEOREM 2</head><p>Proof: At any time t, given fixed S</p><p>(1:k?1) t , there exists a node a ¡Ê S * so that a ¡Ê arg max </p><p>Combining with the induction hypothesis, we can obtain the inequality 2. By taking k = K and using</p><p>The inequality (1) holds because of pigeonhole principle. As we can select node a with the largest total marginal gain over the whole time period, its marginal reward is equal or larger than the mean value of all nodes belonging to S * . Note that although ? r e , we have</p><p>Combining with Definition 2, the proof is completed.</p><p>) is the expectation taken of random policy's actions, the optimal solution is deterministic thus ? r k t (a|S</p><p>) reduces to the expectation of random reward only here.</p><p>Let?Let?Let? k be the selected seed with full information fixing S</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C PROOF OF THEOREM 4</head><p>Proof: We show the following trivial facts derived from the definitions where a|S is the selected arm in Alg. 1.</p><p>(1:k?1) t</p><p>(1:k?1) t , i.e., it maximizes the total marginal gain which is equal or larger than</p><p>which is the expected value of marginal gain by adding a</p><p>Then we have</p><p>We will prove the inequality under any position k. In the end we will illustrate that it still holds for the whole K? size seed set. Under the conditional set S</p><p>(1:k?1) t D C )g. The bound (9) is larger than the maximal reward Kg, then it must holds.</p><p>(1:k?1) t , we have</p><p>For any node n j ¡Ê S</p><p>(1:k?1) t , if we play this node again, the realization of marginal gain might not be zero. But if we consider the expected marginal reward, it must be zero. Since we do not allow the agent to select the same node as a seed twice in each time stage in Algorithm 1, we hav¨º r</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D PROOF OF COROLLARY 1</head><p>Proof: It is apparent that the expected number of activated nodes from a seed can not exceed C, the size of the largest connected component in the social graph. Then the overall reward achieved over the entire system span can not exceed CT . Thus we can set g = CT . According to Theorems 3 and 4, we have (1:k?1) t ) = 0, the equation <ref type="formula">(7)</ref> still holds.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Maximizing the Spread of Influence through a Social Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th ACM SIGKDD</title>
		<meeting>of the 9th ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mining Knowledge-Sharing Sites for Viral Marketing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 8th ACM SIGKDD</title>
		<meeting>of the 8th ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cost-effective Outbreak Detection in Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanbriesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Glance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 13th ACM SIGKDD</title>
		<meeting>of the 13th ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CELF++: Optimizing the Greedy Algorithm for Influence Maximization in Social Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Lakshmanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 20th WWW</title>
		<meeting>of the 20th WWW</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="47" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Influence Maximization: Near-Optimal Time Complexity Meets Practical Efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIG-MOD</title>
		<meeting>of ACM SIG-MOD</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="75" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Influence Maximization in Near-Linear Time: A Martingale Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGMOD</title>
		<meeting>of ACM SIGMOD</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1539" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finite-time Analysis of the Multiarmed Bandit Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Regret Analysis of Stochastic and Nonstochastic Multiarmed Bandit Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combinatorial Multi-Armed Bandit: General Framework, Results and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 30th ICML</title>
		<meeting>of the 30th ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="151" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Contextual Combinatorial Bandit and its Application on Diversfied Online Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIAM International Conference on Data Mining</title>
		<meeting>of SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online Learning for Combinatorial Network Optimization with Restless Markovian Rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnamachari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th IEEE SECON</title>
		<meeting>of the 9th IEEE SECON</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Online Learning in Opportunistic Spectrum Access: A Restless Bandit Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2462" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Online Influence Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maniu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Senellart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 21th ACM SIGKDD</title>
		<meeting>of the 21th ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="645" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Influence Maximization with Bandits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lakshmanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Networks in the Social and Information Sciences</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Slivkins</surname></persName>
		</author>
		<title level="m">The Best of Both Worlds: Stochastic and Adversarial Bandits</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>The 25th Annual Conference on Learning Theory (COLT)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On Upper-Confidence Bound Policies for Switching Bandit Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Moulines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="174" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Greedy-Bayes for Targeted News Dissemination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Massouli¨¦</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Ohannessian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prout¨¬</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGMETRICS</title>
		<meeting>of ACM SIGMETRICS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="285" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linear Submodular Bandits and their Application to Diversified Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2483" to="2491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stochastic Online Greedy Learning with Semi-bandit Feedbacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="352" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On Upper-Confidence Bound Policies for Non-Stationary Bandit Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Moulines</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0805.3415</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Piecewise-stationary Bandit Problems with Side Observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 26th ICML</title>
		<meeting>of the 26th ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stochastic Multi-Armed-Bandit Problem with Non-stationary Rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Besbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Nonstochastic Multi-armed Bandit Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="77" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the Combinatorial MultiArmed Bandit Problem with Markovian Rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnamachari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE GLOBE-COM</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Solving Non-Stationary Bandit Problems by Random Sampling from Sibling Kalman Filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trends in Applied Intelligent Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Information and Influence Propagation in Social Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Lakshmanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Data Management</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="177" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning in a Changing World: Restless Multiarmed Bandit With Unknown Dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1902" to="1916" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Information Theory</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An Online Approach to Dynamic Channel Access and Transmission Scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 16th ACM MobiHoc</title>
		<meeting>of the 16th ACM MobiHoc</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="187" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Building a Network Highway for Big Data: Architecture and Challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network, IEEE</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="5" to="13" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Influence Maximization in Dynamic Social Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 13th IEEE ICDM</title>
		<meeting>of the 13th IEEE ICDM</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1313" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scalable Influence Maximization for Prevalent Viral Marketing in Large-Scale Social Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 16th ACM SIGKDD</title>
		<meeting>of the 16th ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1029" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Optimal Exploration-Exploitation in a Multi-Armed-Bandit Problem with Non-stationary Rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Besbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeevi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3316</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
