<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-07-16T23:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Part A 1 Introduction 1.1 WHAT IS A DISTRIBUTED SYSTEM?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-01-18">18 January 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leslie</surname></persName>
						</author>
						<title level="a" type="main">Part A 1 Introduction 1.1 WHAT IS A DISTRIBUTED SYSTEM?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-01-18">18 January 2016</date>
						</imprint>
					</monogr>
					<note>Background Materials ? 2007 by Taylor &amp; Francis Group, LLC Downloaded by [Auckland University of Technology] at 17:42</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Lamport once said: A distributed system is one in which the failure of a computer you didn&apos;t even know existed can render your own computer unusable. While this is certainly not a definition, it characterizes the challenges in coming up with an appropriate definition of a distributed system. What is distributed in a distributed system? If the processor of computer system is located 100 yards away from its main memory, then is it a distributed system? What if the I/O devices are located three miles away from the processor? If physical distribution is taken into account, then the definition of a distributed system becomes uncomfortably dependent on the degree of physical distribution of the hardware components, which is certainly not acceptable. To alleviate this problem, it is now customary to characterize a distributed system using the logical or functional distribution of the processing capabilities. The logical distribution of the functional capabilities is usually based on the following set of criteria: Multiple processes. The system consists of more than one sequential process. These processes can be either system or user processes, but each process should have an independent thread of control-either explicit or implicit. Interprocess communication. Processes communicate with one another using messages that take a finite time to travel from one process to another. The actual nature or order of the delay will depend on the physical characteristics of the message links. These message links are also called channels. Disjoint address spaces. Processes have disjoint address spaces. We will thus not take into account shared-memory multiprocessors as a true representation of a distributed computing system, although shared memory can be implemented using messages. The relationship between shared memory and message passing will be discussed in a subsequent chapter. Collective goal. Processes must interact with one another to meet a common goal. Consider two processes P and Q in a network of processes. If P computes f(x) = x 2 for a given set of values of x, and Q multiplies a set of numbers by дл , then we hesitate to call it a distributed system, since there is no interaction between P and Q. However, if P and Q cooperate with one another to compute the areas of a set of circles of radius x, then the system of processes (P and Q) is an example of a meaningful distributed system. The above definition is a minimal one. It does not take into consideration system wide executive control for interprocess cooperation, or security issues, which are certainly important concerns in connection with the runtime management and support of user computations. Our definition highlights the simplest possible characteristics for a computation to be logically distributed. Physical distribution is only a prerequisite for logical distribution.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The above definition is a minimal one. It does not take into consideration system wide executive control for interprocess cooperation, or security issues, which are certainly important concerns in connection with the runtime management and support of user computations. Our definition highlights the simplest possible characteristics for a computation to be logically distributed. Physical distribution is only a prerequisite for logical distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">WHY DISTRIBUTED SYSTEMS?</head><p>Over the past years, distributed systems have gained substantial importance. The reasons of their growing importance are manifold:</p><p>Geographically distributed environment. First, in many situations, the computing environment itself is geographically distributed. As an example, consider a banking network. Each bank is supposed to maintain the accounts of its customers. In addition, banks communicate with one another to monitor inter-bank transactions, or record fund transfers from geographically dispersed ATMs. Another common example of a geographically distributed computing environment is the Internet, which has deeply influenced our way of life. The mobility of the users has added a new dimension to the geographic distribution.</p><p>Speed up. Second, there is the need for speeding up computation. The speed of computation in traditional uniprocessors is fast approaching the physical limit. While superscalar and VLIW processors stretch the limit by introducing parallelism at the architectural (instruction issue) level, the techniques do not scale well beyond a certain level. An alternative technique of deriving more computational power is to use multiple processors. Dividing a total problem into smaller subproblems, and assigning these subproblems to separate physical processors that can operate concurrently is potentially an attractive method of enhancing the speed of computation. Moreover, this approach promotes better scalability, where the users can incrementally increase the computational power by purchasing additional processing elements or resources. Quite often, this is simpler and more economical than investing in a single superfast uniprocessor.</p><p>Resource sharing. Third, there is the need for resource sharing. Here, the term resource represents both hardware and software resources. The user of computer A may want to use a fancy laser printer connected with computer B, or the user of computer B may need some extra disk space available with computer C for storing a large file. In a network of workstations, workstation A may want to use the idle computing powers of workstations B and C to enhance the speed of a particular computation. Distributed databases are good examples of the sharing of software resources, where a large database may be stored in several host machines, and consistently updated or retrieved by a number of agent processes.</p><p>Fault-tolerance. Fourth, powerful uniprocessors, or computing systems built around a single central node are prone to a complete collapse when the processor fails. Many users consider this to be risky. They are however willing to compromise with a partial degradation in system performance, when a failure cripples a fraction of the many processing elements or links of a distributed system. This is the essence of graceful degradation. The flip side of this approach is that, by incorporating redundant processing elements in a distributed system, one can potentially increase system reliability or system availability. For example, in a system having triple modular redundancy (TMR), three identical functional units are used to perform the same computation, and the correct result is determined by a majority vote. In other fault-tolerant distributed systems, processors cross-check one another at predefined checkpoints, allowing for automatic failure detection, diagnosis, and eventual recovery. A distributed system thus provides an excellent opportunity for incorporating fault-tolerance and graceful degradation.</p><p>Downloaded by [Auckland University of Technology] at 17: <ref type="bibr">42</ref> 18 January 2016</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">EXAMPLES OF DISTRIBUTED SYSTEMS</head><p>There are numerous examples of distributed systems that are used in everyday life in a variety of applications. Some systems primarily provide a variety of useful services to the users. A fraction of these services are data intensive and the computational component is very small. Examples are database-oriented applications (think about Google searching and collecting information from computers all over the world). Others are computation intensive. Most systems are structured as client-server systems, where the server machine is the custodian of data or resources, and provide service to a number of geographically distributed clients. A few applications however do not rely on a central server -these are peer-to-peer systems whose popularity is on the rise. We present here a Introduction few examples of distributed systems:</p><p>World Wide Web. World Wide Web (www) is a popular service running on the Internet. It allows documents in one computer to refer to textual or non-textual information stored in other computers. For example, a document in the United States may contain references to the photograph of a rainforest in Africa, or a music recording in Australia. Such references are highlighted on the user's monitor, and when selected by the user, the system fetches the item from a remote server using appropriate protocols, and displays the picture or plays the music on the client machine.</p><p>The Internet and the World Wide Web have changed the way we do our research or carry out business. For example, a large fraction of airline and hotel reservations are now done through the Internet. Shopping through the Internet has dramatically increased in the past few years. Millions of people now routinely trade stocks through the Internet. With the evolution of MP3, many users download and exchange CD-quality music, giving the recording industry a run for their money. Finally, digital libraries provide users instant access to archival information from the comfort of their homes.</p><p>Network file server. A local-area network consists of a number of independent computers connected through high-speed links. When you log into your computer from your office, chances are that your machine is a part of a local-area network. In many local-area networks, a separate machine in the network serves as the file server. Thus, when a user accesses a file, the operating system directs the request from the local machine to the file server, which in turn checks the authenticity of the request, and decides whether access can be granted. This shows that with a separate file server, access to a file requires the cooperation of more than one process, in this case the operating system of the user process and the server process.</p><p>Banking network. Amy needs $300 on a Sunday morning, so she walks to a nearby ATM to withdraw some cash. Amy has a checking account in Iowa City, but she has two savings accountsone in Chicago, and the other in Denver. Each bank has set an upper limit of $100 on the daily cash withdrawal, so Amy uses three different bankcards to withdraw the desired cash. These debits are immediately registered in her bank accounts in three different cities and her new balances are recomputed.</p><p>Peer-to-peer networks. The Napster system used an unconventional way to distribute MP3 files. Instead of storing the songs on a central computer, the songs live on users' machines. There are millions of them scattered all over the world. When you want to download a song using Napster, you are downloading it from another person's machine, and that person could be your next-door neighbor or someone halfway around the world. This led to the development of peer-to-peer (P2P) data sharing. Napster was not a true P2P system, since it used a centralized directory. But many subsequent systems providing similar service (e.g., Gnutella) avoided the use of a central server or a central directory. P2P systems are now finding applications in areas beyond exchanging music files. For example, the Oceanstore project at the University of California, Berkeley built an online data archiving mechanism on top of an underlying P2P network Tapestry.</p><p>Process control systems. Industrial plants extensively use networks of controllers to oversee production and maintenance. Consider a chemical plant, in which a controller maintains the pressure of a certain chamber to 200 psi. As the vapor pressure increases, the temperature has a tendency to increase -so there is another controller 300 ft away that controls the flow of a coolant. This coolant ensures that the temperature of the chamber never exceeds 200 ? F. Furthermore, the safety of the plant requires that the product of the pressure and the temperature does not exceed 35,000. This is a simple example of a distributed computing system that maintains an invariance relationship on system parameters monitored and controlled by independent controllers.</p><p>As another example of a distributed process control system, consider the problem of rendezvous in space. When two space stations want a rendezvous in space for the purpose of transferring fuel or food or scientific information or crew, each of them constantly monitor the velocity of the other spacecraft, as well as the physical distance separating them. The rendezvous needs a mutually agreed protocol, where controllers fire designated rockets at appropriate moments for avoiding collisions and bridging the distance separating the spacecrafts in a controlled manner, until the distance reduces to a few feet and their velocities become almost identical. This is an example of a sophisticated distributed control system in which a malfunction will have catastrophic side effects.</p><p>Sensor networks. The declining cost of hardware, and the growth of wireless technology have led to new opportunities in the design of application specific, or special purpose distributed systems. One such application is a sensor network <ref type="bibr">[ASSC02]</ref>. Each node is a miniature processor (called a mote) equipped with a few sensors, and is capable of wireless communication with other motes. Such networks can be potentially used in a wide class of problems: these range from battlefield surveillance, biological and chemical attack detection, to home automation, ecological and habitat monitoring. This is a part of a larger vision of ubiquitous computing.</p><p>Grid computing. Grid computing is a form of distributed computing that supports parallel programming on a network of variable size computers. At the low end a computational grid can use a fraction of the computational resources of one or two organizations, whereas at the high end, it can combine millions of computers worldwide to work on extremely large computational projects. The goal is to solve difficult computational problems more quickly and less expensively than by conventional methods. We provide two examples here.</p><p>Our first example is particle accelerator and collider being built at the European Organization of Nuclear Research CERN. The accelerator will start operation from 2007, and will be used to answer fundamental questions of science. The computations will generate 12-14 petabytes of data each year (1 petabyte = 10 15 bytes, and this is the equivalent of more than 20 million CDs). The analysis will require the equivalent of 70,000 of today's fastest PCs. It will be handled by a worldwide computational grid, which will integrate computers from Europe, United States, and Asia into one virtual organization.</p><p>Our second example is the SETI@home project. Do extra-terrestrials exist? SETI (acronym for Search for Extra Terrestrial Intelligence) is a massive project aimed at discovering the existence of extraterrestrial life in this universe. The large volume of data that is constantly being collected from hundreds of radio telescopes need to be analyzed to draw any conclusion about the possible existence of extraterrestrial life. This requires massive computing power. Rather than using supercomputers, the University of California Berkeley SETI team decided to harness the idle computing power of the millions of PCs and workstations belonging to you and me, computing power that is otherwise wasted by running useless screensavers programs. Currently, about 40 gigabytes of data are pulled down daily by the telescope and sent to over three million computers all over the world to be analyzed. The results are sent back through the Internet, and the program then collects a new segment of radio signals for the PC to work on. The system executes 14 trillion floating-point operations per second and has garnered over 500,000 years of PC time in the past year and a half. It would normally cost millions of dollars to achieve that type of power on one or even two supercomputers.</p><p>Downloaded by [Auckland University of Technology] at 17: <ref type="bibr">42</ref> 18 January 2016</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">IMPORTANT ISSUES IN DISTRIBUTED SYSTEMS</head><p>This book will mostly deal with process models, and distributed computations supported by these models. A model is an abstract view of a system. It ignores many physical details of the system. That does not mean that the implementation issues are unimportant, but these are outside the scope of this book. Thus, when discussing about a network of processes, we will never describe the type of processors running the processes, or the characteristics of the physical memory, or the rate at which the bits of a message are being pumped across a particular channel. Our emphasis is on computational activities represented by the concurrent execution of actions a network of sequential processes. Some of the important issues in the study of such computationalmodels are as follows:</p><p>Knowledge of a process. Since each process has a private address space, no process is expected to have global knowledge about either the network topology or the system state. Each process thus has a myopic view of the system. It is fair to expect that a process knows (i) its own identity, (ii) the identity of its immediate neighbors, and (iii) the channels connecting itself with its immediate neighbors. In some special cases, a process may also have knowledge about the size (i.e., the number of nodes) of the network. Any other knowledge that a process might need has to be acquired from time to time through appropriate algorithms.</p><p>Network topology. A network of processes may either be completely connected, or sparsely connected. In a completely connected network, a channel (also called a link) exists between every pair of processes in the system. This condition does not hold for a sparsely connected topology. As a result, message routing is an important activity. A link between a pair of processes may be unidirectional or bidirectional. Examples of sparse topologies are trees, rings, arrays, or hypercubes <ref type="figure">(Figure 1.1)</ref>.</p><p>Degree of synchronization. Some of the deeper issues in distributed systems center around the notion of synchrony and asynchrony. According to the laws of astronomy, real time is defined in terms of the rotation of earth in the solar system. This is called Newtonian time, which is the primary standard of time. However, the international time standard now is the Coordinated Universal Time (UTC). UTC is the current term for what was commonly referred to as Greenwich Meridian Time (GMT). Zero hours UTC is midnight in Greenwich England, which lies on the zero longitudinal meridian. UTC is based on a 24 h clock, therefore, afternoon hours such as 6 p.m. UTC are expressed as 18:00 UTC. Each second in UTC is precisely the time for 9,192,631,770 orbital transitions of the Cesium 133 atom. The time keeping in UTC is based on atomic clocks. UTC signals are regularly broadcast from satellites as well as many radio stations. In United States, this is done from the WWV radio station in Fort Collins Colorado, whereas satellite signals are received through GPS. A useful aspect of atomic clocks is the fact that these can, unlike solar clocks, be made available anywhere in the universe.</p><p>Assume that each process in a distributed system has a local clock. If these clocks represent the UTC (static differences due to time zones can be easily taken care of, and ignored from this equation), then every process has a common notion of time, and the system can exhibit synchronous behavior by the simultaneous scheduling of their actions. Unfortunately, in practical distributed systems this is difficult to achieve, since the drift of physical clocks is a fact of life. One approach to handle this is to use a time-server that keeps all the local clocks synchronized with one another.</p><p>The concept of a synchronous system has evolved over many years. There are many facets of synchrony. A loosely synchronous system is sometimes characterized by the existence of an upper bound on the propagation delay of messages. If the message sent by process A is not received by process B within the expected interval of real time, then process B suspects some kind of failure. Another feature of a synchronous system is the first-in-first-out (FIFO) behavior of the channels connecting the processes. With these various possibilities, it seems prudent to use the attribute "synchronous" to separately characterize the behaviors of clocks, or communication, or channels.</p><p>In a fully asynchronous system, not only there is clock drift, but also there is no upper bound on the message propagation delays. Processes can be arbitrarily slow, and out-of-order message delivery between any pair of processes is considered feasible. In other words, such systems completely disregard the rule of time, and processes schedule events at an arbitrary pace. The properties of a distributed system depend on the type of synchrony. Results about one system often completely fall apart when assumptions about synchrony changes from synchronous to asynchronous. In a subsequent chapter, we will find out how the lack of a common basis of time complicates the notion of global state and consequently the ordering of events in a distributed system.</p><p>Failures. The handling of failures is an important area of study in distributed systems. A failure occurs, when a system as a whole, or one or more of its components do not behave according to their specifications. Numerous failure models have been studied. A process may crash, when it ceases to produce any output. In another case of failure, a process does not stop, but simply fails to send one or more messages, or execute one or more steps. This is called omission failure. This includes the case when a message is sent, but lost in transit. Sometimes, the failure of a process or a link may alter the topology by partitioning the network into disjoint sub-networks. In the byzantine failure model, a process may behave in a completely arbitrary manner -for example, it can send inconsistent or conflicting message to its neighboring processes. There are two aspects of failures: one is the type of failure, and the other is the duration of failure. It is thus possible that a process exhibits byzantine failure for 5 sec, then resumes normal behavior, and after 30 min, fails by stopping. We will discuss more about various fault models in Chapter 13.</p><p>Scalability. An implementation of a distributed system is considered scalable, when its performance can be improved by incrementally adding resources regardless of the final scale of the system. Some systems deliver the expected performance when the number of nodes is small, but fail to deliver when the number of nodes increases. From an algorithmic perspective, when the space or time complexity of a distributed algorithm is log N or lower where N is the size of the system, its scalability is excellent -however, when it is O(N) or higher, the scalability is considered poor. Well-designed distributed systems exhibit good scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">COMMON SUBPROBLEMS</head><p>Most applications in distributed computing center around a set of common subproblems. If we can solve these common subproblems in a satisfactory way, then we have a good handle on system design. Here are a few examples of common subproblems:</p><p>Downloaded by [Auckland University of Technology] at 17: <ref type="bibr">42</ref> 18 January 2016</p><p>Leader election. When a number of processes cooperate from solving a problem, many implementations prefer to elect one of them as the leader, and the remaining processes as followers. If the leader crashes, then one of the followers is elected the leader, after which the system runs as usual.</p><p>Mutual exclusion. There are certain hardware resources that cannot be accessed by more than one process at a time: an example is a printer. There are also software resources where concurrent accesses run the risk of producing inconsistent results: for example, multiple processes are not ordinarily allowed to update a shared data structure. The goal of mutual exclusion is to guarantee that at most one process succeeds in acquiring the resource at a time, and regardless of request patterns, the accesses to each resource are serialized.</p><p>Time synchronization. Local clocks invariably drift and need periodic resynchronization to support a common notion of time across the entire system.</p><p>Global state. The global state of a distributed system consists of the local states of its component processes. Any computation that needs to compute the global state at a time t has to read the local states of every component process at time t. However, given the fact that local clocks are never perfectly synchronized, computation of the global state is a nontrivial problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Replica management. To support fault-tolerance and improve system availability, the use of process replicas is quite common. When the main server is down, one of the replica servers replace the main server. Data replication (also known as caching) is widely used for saving system bandwidth. However, replication requires that the replicas be appropriately updated. Since such updates can never be instantaneously done, it leaves open the possibility of inconsistent replicas. How to update the replicas and what kind of response can a client expect from these replicas? Are there different notions of consistency?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6">IMPLEMENTING A DISTRIBUTED SYSTEM</head><p>A model is an abstract view of a system. Any implementation of a distributed computing model must involve the implementation of processes, message links, routing schemes, and timing. The most natural implementation of a distributed system is a network of computers, each of which runs one or more processes. Using the terminology from computer architecture, such implementations belong to the class of loosely coupled MIMD machines, where each processor has a private address space. The best example of a large-scale implementation of a distributed system is the World Wide Web. A cluster of workstations connected to one another via a local-area network serves as a mediumscale implementation. In a smaller scale, mobile ad-hoc networks or a wireless sensor network are appropriate examples.</p><p>Distributed systems can also be implemented on a tightly coupled MIMD machine, where processes running on separate processors are connected to a globally shared memory. In this implementation, the shared memory is used to simulate the interprocess communication channels. Finally, a multiprogrammed uniprocessor can be used to simulate a shared-memory multiprocessor, and hence a distributed system. For example, the very old RC4000 was the first message-based operating system designed and implemented by Brinch Hansen [BH73] on a uniprocessor. Amoeba, Mach, and Windows NT are examples of micro-kernel based operating systems where processes communicate via messages.</p><p>Distributed systems have received significant attention from computer architects because of scalability. In a scalable architecture, resources can be continuously added to improve performance and there is no appreciable bottleneck in this process. Bus-based multiprocessors do not scale beyond 8 to 16 processors because the bus bandwidth acts as a bottleneck. Shared-memory symmetric multiprocessors (also called SMPs) built around multistage interconnection networks suffer from some degree of contention when the number of processors reaches 1000 or more. Recent trends in scalable architecture show reliance on multicomputers, where a large number of autonomous machines (i.e., processors with private memories) are used as building blocks. For the ease of programming, various forms of distributed shared memory are then implemented on it, since programmers do not commonly use message passing in developing application programs.</p><p>Another implementation of a distributed system is a neural network, which is a system mimicking the operation of a human brain. A neural network contains a number of processors operating in parallel, each with its own small sphere of knowledge and access to data in its local memory. Such networks are initially trained by rules about data relationships (e.g., "A mother is older than her daughter"). A program can then tell the network how to behave in response to input from a computer user. The results of the interaction can be used to enhance the training. Some important applications of neural networks include: weather prediction, oil exploration, the interpretation of nucleotide sequences, etc. For example, distributed chess is a distributed computing project in the field of artificial intelligence. The goal is the creation of chess-playing artificial neural networks using distributed evolutionary algorithms for the training of the networks. The training is performed using a program to be installed on the computers of project participants.</p><p>Remember that these different architectures merely serve as platforms for implementation or simulation. A large number of system functions is necessary to complete the implementation of a particular system. For example, many models assume communication channels to be FIFO. Therefore, if the architecture does not naturally support FIFO communication between a pair of processes, then FIFO communication has to be implemented first. Similarly, many models assume there will be no loss or corruption of messages. If the architecture does not guarantee these features, then appropriate protocols have to be used to remedy this shortcoming. No system can be blamed for not performing properly, if the model specifications are not appropriately satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.7">PARALLEL VS. DISTRIBUTED SYSTEMS</head><p>What is the relationship between a parallel system and a distributed system? Like distributed systems, parallel systems are yet to be clearly defined. The folklore is that, any system in which the events can at best be partially ordered is a parallel system. This naturally includes every distributed system, all shared-memory systems with multiple threads of control. According to this view, distributed systems form a subclass of parallel systems, where the state spaces of processes do not overlap. Processes have greater autonomy. This view is not universally accepted. Some distinguish parallel systems from distributed systems on the basis of their objectives: parallel systems focus on increasing performance, whereas distributed systems focus on tolerating partial failures. As an alternative view, parallel systems consist of processes in an SIMD type of synchronous environment, or a synchronous MIMD environment, asynchronous processes in a shared-memory environment are the building blocks of concurrent systems and cooperating processes with private address spaces constitute a distributed system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.8">BIBLIOGRAPHIC NOTES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXERCISES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Downloaded by [Auckland University of Technology] at 17:42 18 January 2016</head><p>To solve these problems, identify all sequential processes involved in your solution, and give an informal description of the messages exchanged among them. No code is necessary -pseudocodes are ok. can assume that (i) the clocks are synchronized, and (ii) the robots advance in discrete steps -with each tick of the clock, they move one foot at a time. 4. On a Friday afternoon, a passenger asks a travel agent to reserve the earliest flight next week from Cedar Rapids to Katmandu via Chicago and London. United Airlines operates hourly flights in the sector Cedar Rapids to Chicago. In the sector ChicagoLondon, British Airways operates daily flights. The final sector is operated by the Royal Nepal Airlines on Tuesdays and Thursdays only. Assuming that each of these airlines has an independent agent to schedule its flights, outline the interactions between the travel agent and these three airline agents, so that the passenger eventually books a flight to Katmandu. 5. Mr. A plans to call Ms. B from a pay phone using his calling card. The call is successful only if (i) A's calling card is still valid (ii) A does not have any past due in his account, and (iii) B's telephone number is correctly dialed by A. Assuming that a process CARD checks the validity of the calling card, a second process BILL takes care of billing, and a third process SWITCH routes the call to B, outline the sequence of actions during the call establishment period. 6. In many distributed systems, resource sharing is a major goal. Provide examples of systems, where the shared resource is (i) a disk, (ii) network bandwidth, and (iii) a processor. 7. Napster is a system that allows users to automatically download music files in a transparent way from another computer that may belong to a next-door neighbor, or to someone halfway around the world. Investigate how this file sharing is implemented. 8. A customer wants to fly from airport A to airport B within a given period of time by paying the cheapest fare. She submits the query to a proper service and expects to receive the reply in a few seconds. Travelocity.com, expedia.com, and orbitz.com already have such services in place. Investigate how these services are implemented. 9. Sixteen motes (miniature low-cost processors with built-in sensors) are being used to monitor the average temperature of a furnace. Each mote has limited communication ability and can communicate with two other motes only. The wireless network of motes is not partitioned. Find out how each mote can determine the average temperature of the furnace. 10. How can a single processor system be used to implement a unidirectional ring of N processes?</p><p>Downloaded by [Auckland University of Technology] at 17: <ref type="bibr">42</ref> 18 January 2016</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 . 1</head><label>11</label><figDesc>FIGURE 1.1 Examples of network topologies (a) ring, (b) directed tree, (c) 3-dimensional cube. Each black node represents a process, and each edge connecting a pair of nodes represents a channel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The book by Coulouris et al. [CDK04] contains a good overview of distributed systems and their applications. Tel [T00] covers numerous algorithmic aspects of distributed systems. Tannenbaum and van Steen's book [TS02] addresses practical aspects and implementation issues of distributed systems. Distributed operating systems have been presented by Singhal and Shivaratri [SS94]. Greg Andrews' book [A00] provides a decent coverage of concurrent and distributed programming meth- odologies. The SETI@home project and its current status are described in [SET02]. [ASSC02] presents a survey of wireless sensor networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 1 . 2</head><label>12</label><figDesc>FIGURE 1.2 Robot B crossing a road and trying to avoid collision with robot A.</figDesc></figure>

			<note place="foot" n="3"> ? 2007 by Taylor &amp; Francis Group, LLC</note>

			<note place="foot">? 2007 by Taylor &amp; Francis Group, LLC</note>

			<note place="foot">Downloaded by [Auckland University of Technology] at 17:42 18 January 2016 ? 2007 by Taylor &amp; Francis Group, LLC</note>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
