<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-07-17T00:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Learning for Efficient Emergence of Social Norms in Networked Multiagent Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">No</forename><forename type="middle">Author</forename><surname>Given</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">No Institute Given</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Learning for Efficient Emergence of Social Norms in Networked Multiagent Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Norm Emergence</term>
					<term>Learning</term>
					<term>Multiagent Systems</term>
				</keywords>
			</textClass>
			<abstract>
				<p>This paper investigates how norm emergence can be facilitated by agents&apos; adaptive learning behaviors in networked multiagent systems. A general learning framework is proposed, in which agents can dynamically adapt their learning behaviors through social learning of their individual learning experience. Extensive verification of the proposed framework is conducted in a variety of situations, using comprehensive evaluation criteria of efficiency, effectiveness and efficacy. Experimental results show that the adaptive learning framework is robust and efficient for evolving stable norms among agents.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Coordination of agent behaviors is central in Multiagent Systems (MASs). Social norm is an effective technique to achieve coordination in MASs by placing social constraints on agent action choices <ref type="bibr" target="#b0">[1]</ref>. Understanding how social norms can emerge through local interactions has gained increasingly high attention in the research of MASs. Numerous investigations of norm emergence have been done in recent years under different assumptions about agent interaction protocols, societal topologies and observation capabilities <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>.</p><p>Learning from individual experience has been shown to be a robust mechanism to enable norm emergence in MASs <ref type="bibr" target="#b4">[5]</ref>. A great deal of work has studied norm emergence achieved through agent learning behaviors <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. The focus of these existing studies is to examine general mechanisms behind efficient emergence of social norms while agents interact with each other using basic learning (particularly reinforcement learning) methods. These mechanisms include the social learning strategy <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, the collective interaction protocol <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, the utilization of topological knowledge <ref type="bibr" target="#b7">[8]</ref>, and the observation capability of agents <ref type="bibr" target="#b9">[10]</ref>, etc. Although these studies provide us with a deep understanding of efficient mechanisms of norm emergence, they share the same limitation inevitably. That is, learning parameters in these studies are often fine-tuned by hand and thus cannot be adapted according to the varying norm emerging situations. A key question then arises that how agents can adapt their learning behaviors dynamically during the process of norm emergence, and how this kind of adaptiveness can influence the final emergence performance?</p><p>To this end, this paper provides another perspective in the research of norm emergence by simply focusing on the role of learning itself in affecting the process of norm emergence. A double-layered adaptive learning framework is proposed, in which agents interact with each other using basic Reinforcement Learning (RL) methods <ref type="bibr" target="#b12">[13]</ref> in the local layer learning, and generate supervision policies by exploiting historical learning experience in the upper layer learning. The supervision policies are then passed down to the local layer learning in order to adapt agents' learning behaviors based on the consistency between agents' behaviors and the supervision policies. In the framework, two challenging technical issues are needed to be carefully resolved: (1) how to generate supervision policies simply based on agents' historical learning experience? and (2) how to adapt agents' local learning behaviors based on the supervision policies from the upper layer learning? To solve the former problem, the historical learning experience of each agent is synthesized into a strategy that competes with other strategies in the population. The strategies that have better performance are more likely to survive and thus be accepted by other agents. The competing process of the agents' strategies then can be carried out through a social learning process based on the principle of Evolutionary Game Theory (EGT) <ref type="bibr" target="#b13">[14]</ref>. For the latter, the concept of "winning" or "losing" in the well-known Multi-Agent Learning (MAL) algorithm WoLF (Win-or-Learn-Fast) <ref type="bibr" target="#b14">[15]</ref> is elegantly borrowed to indicate whether an agent's behavior is consistent with the supervision policy. According to the "winning" or "losing" situation, agents then can dynamically adapt their learning behaviors in local layer learning. Experiments show that the proposed framework enables norms to emerge more efficiently and with higher convergence levels than the static learning framework, and some critical factors such as size of norm space and network topologies can have significant influences on norm emergence.</p><p>The remainder of the paper is organized as follows. Section 2 briefly introduces social norms and RL. Section 3 describes the proposed framework. Section 4 presents experimental studies. Finally, Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Social Norms and RL</head><p>As most previous studies do, this paper also uses learning "rules of the road" <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref> as a metaphor to study emergence of norms, which can be viewed as a Coordination Game (CG) in <ref type="table">Table 1</ref>. The abstraction of coordination given by the CG covers a number of practical scenarios, such as distributed robots coordinating on which object to work on together, and wireless nodes coordinating on which channel to transmit message <ref type="bibr" target="#b16">[17]</ref>. The problem to deal with the CG is that there is nothing in the structure of the game itself that allows players (even purely rational players) to infer what they ought to do <ref type="bibr" target="#b15">[16]</ref>. Therefore, social norms can be used to guide agent behaviors towards specific ones when moral or rational reasoning does not provide a clear guidance of how to behave. To study the impact of norm space on norm emergence, the CG can be extended to a general form by considering actions in the norm space <ref type="table" target="#tab_0">(Table 2)</ref>. </p><formula xml:id="formula_0">L R L 1, 1 -1, -1 R -1, -1 1, 1 1 2 . . . 1 1,1 -1,-1 . . . -1,-1 2 -1,-1 1,1 . . . -1,-1 . . . . . . . . . . . . . . . -1,-1 -1,-1 . . . 1,1</formula><p>In this paper, agent interactions are purely local and are constrained by the network structures. Three different kinds of topologies are considered: grid networks, small-world networks and scale-free networks <ref type="bibr" target="#b17">[18]</ref>.</p><p>RL algorithms <ref type="bibr" target="#b12">[13]</ref> has been widely used for agent interactions in previous work on norm emergence, and Q-learning <ref type="bibr" target="#b18">[19]</ref> is the most adopted one. In Qlearning, an agent makes a decision through the estimation of a set of Q-values, which can be updated by:</p><formula xml:id="formula_1">= + + max ¡ä , ¡ä ) ?<label>(1)</label></formula><p>¡ä where ¡Ê (0, 1] is a learning rate of agent and ¡Ê [0, 1) is a discount factor, and are the immediate and expected reward of choosing action in state at time step respectively, and Q(s ¡ä , a ¡ä ) is the expected discounted reward of choosing action ¡ä in state ¡ä at time step + 1. Q-values of each state-action pair are stored in a table for a discrete stateaction space. At each time step, agent chooses the best-response action with the highest Q-value with a probability of 1 ? (i.e., exploitation), or chooses other actions randomly with a probability of (i.e., exploration).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Learning Framework</head><p>In the traditional RL framework, the critical parameters such as learning rate and exploration rate are usually set to fixed values or changed according to a fixed schedule (e.g., making the agent explore less as time goes by). In the context of learning for norm emergence, as all the agents are adapting their opinions at the same time, the norm emerging process can be quite dynamic and unpredictable. Therefore, agents should learn in an adaptive way in order to properly react to the varying norm emerging situations, rather than simply follow a fixed learning schedule.</p><p>To better reflect the nature of adaptive learning behaviors during norm emergence, a general adaptive RL framework is proposed, which extends the traditional RL framework by equipping an agent with a double-layered learning ability. In the framework, the local layer learning represents an individual learning process, in which the agent interacts with the environment using traditional RL algorithms. The upper layer learning represents a social learning process, in which the agent synthesizes its learning experience from the local layer learning and generates a supervision policy through exchanging its learning experience with other agents. The supervision policy is then utilized to guide the local layer learning process by tuning the critical learning parameters heuristically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generating Supervision Policies</head><p>In the proposed learning framework, each agent is equipped with a capability to memorize a certain period of learning experience in terms of the chosen action and the corresponding reward. Assuming a memory capability of agent is well justified and quite common in the MAS research <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>. Formally, let denote an agent's memory length. At step the agent can memorize the historical information in the period of steps prior to A memory table of agent at time step , then can be denoted as  </p><formula xml:id="formula_2">= {( +1 , +1 ), ..., ( , ), (</formula><formula xml:id="formula_3">¡Æ ( = ) (2)</formula><p>where <ref type="formula">(</ref> ) is 1 if = and 0 otherwise.</p><p>( stores the historical information of how often action has been chosen in the past. To exclude those actions that have never been chosen, set ) is defined to contain all the actions that have been taken at least once in the last steps by agent i.e., ) = { ( &gt; 0}. The average reward of choosing action ( then can be given by:</p><formula xml:id="formula_4">¡Æ ( = 1 ), ? ¡Ê ) (3) (<label>where (</label></formula><p>) is if = and 0 otherwise. The past learning experience in terms of table ( and ( indicates how successful the strategy of choosing action is in the past. The upper layer learning makes use of this information in order to generate a supervision policy for local layer learning. To realize the supervision policy generation, each agent learns from other agents by comparing their learning experience. The motivation of this comparison comes from the EGT, which provides a powerful methodology to model how strategies evolve overtime based on their performance. In the context of EGT, an individual's payoff represents its fitness or social success. The dynamics of strategy change in a population is governed by social learning, that is, the most successful agents will tend to be imitated by the others. Two different approaches are proposed in this paper to realize the EGT concept in the upper layer learning process, depending on how to define the competing strategy and the corresponding performance evaluation criteria (i.e., fitness) in EGT.</p><p>Reward-based approach: This approach is a performance-driven approach in that agents are aiming at maximizing their own rewards. If an action has brought about the highest reward among all the actions in the past, this action is the most profitable one and thus should be more likely to be imitated by the others in the population. Therefore, the strategy in EGT is represented by the most profitable action, and the fitness is represented by the corresponding reward of that action. More formally, let ¡ä denote the most profitable action. It can be given by Equation 4:</p><formula xml:id="formula_5">¡ä = ) (<label>(4)</label></formula><p>Action-based approach: Unlike the reward-based approach, the action-based approach is a behavior-driven approach. If an agent has chosen the same action all the time, it considers this action to be the most successful one (being the norm accepted by the population). Therefore, action-based approach considers the action which has been most adopted in the past to be the strategy in EGT, and the corresponding reward of that action to be the fitness in EGT. Let ¡ä denote the most adopted action. It can be given by:</p><formula xml:id="formula_6">¡ä = ) (<label>(5)</label></formula><p>After synthesizing the historical learning experience, agent then gets a strategy ¡ä ¡ä and its corresponding fitness of ). It then interacts with other agents through social learning based on the pairwise comparison rule, i.e., learning from a randomly selected neighbor in the neighborhood. Imitation rules in EGT then model how the strategies of these two agents evolve overtime based on their performance, which can be realized by the famous Fermi function:</p><formula xml:id="formula_7">= 1 1 + ( ¡ä ) ? ( ¡ä ))]<label>(6)</label></formula><p>where denotes the probability that agent switches to the strategy of agent and is the selection bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adapting Local Learning Behaviors</head><p>Based on the principle of EGT, the upper layer learning process generates a supervision policy represented as the new strategy</p><formula xml:id="formula_8">¡ä ¡ä .</formula><p>The new strategy indicates the most successful strategy in the neighborhood and therefore should be integrated into the local layer learning in order to entrench its influence. By comparing its action at time step ¡ä , with the supervision strategy , agent can evaluate whether it is performing well or not so that its learning behavior can be dynamically adapted to fit the supervision strategy. Depending on the consistency between the agent's behavior and the supervision policy, the local layer learning can be adapted based on the following mechanisms:</p><p>Supervision-In RL, the learning performance heavily depends on the learning rate parameter, which is difficult to tune. This mechanism adapts the learning rate in the local layer learning. When agent has chosen the same action with the supervision policy, it decreases its learning rate to maintain its current state, otherwise, it increases its learning rate to learn faster from its interaction experience. Formally, learning rate can be adjusted as follows:</p><formula xml:id="formula_9">¡ä = { ? + = ,<label>(7)</label></formula><p>where is the changing rate of and can be given by:</p><formula xml:id="formula_10">¡ä = { ? ) = ,<label>(8)</label></formula><p>where ¡Ê [0, 1] is a variable to control the adaption rate. Supervision-Exploration-exploitation trade-off has a crucial impact on the learning process. Therefore, this mechanism adapts the exploration rate in the local layer learning. The motivation of this mechanism is that an agent needs to explore more of the environment when it is performing poorly and explore less otherwise. Similarly, the exploration rate can be adjusted according to Equation 9:</p><formula xml:id="formula_11">¡ä = { ? + = ,<label>(9)</label></formula><p>in which is if</p><formula xml:id="formula_12">¡ä =</formula><p>, and ? ) otherwise. In RL, exploration rate is always set to a small value in order to indicate a small probability of exploration. Thus, a variable is introduced to confine the exploration rate to a maximal value of . Supervision-both: This mechanism adapts the learning rate and the exploration rate at the same time based on Supervision-and Supervision-</p><p>The above mechanisms are based on the concept of "winning" and "losing" in the well-known MAL algorithm WoLF. Although the original meaning of "winning" or "losing" in WoLF and its variants is to indicate whether an agent is doing better or worse than its Nash-Equilibrium policy, this heuristic is gracefully introduced into the proposed framework to evaluate an agent's performance against the supervision policy. Specifically, an agent is considered to be winning (i.e., performing well) if its action is the same with the supervision strategy and losing (i.e., performing poorly) otherwise. The different situations of "winning" or "losing" thus indicate whether the agent's behavior is complying with the norm in the society. If an agent is in a losing state (i.e., its action is against the norm in the society), it needs to learn faster or explores more of the environment in order to escape from this adverse situation. On the contrary, it should decrease its learning and/or exploration rate to stay in the winning state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>Experiments are carried out to demonstrate the performance of the proposed framework, based on metrics of effectiveness (i.e., possibility of convergence), efficiency (i.e., speed of convergence) and efficacy (i.e., level of convergence). Effectiveness indicates the percentage of runs in which a norm can successfully emerge, efficiency represents how many steps are needed for a norm to emerge, and efficacy denotes the ratio of agents in the system that can achieve the norm emergence. The Watts-Strogatz model <ref type="bibr" target="#b22">[23]</ref> is used to generate a small-world network and the Barabasi-Albert model <ref type="bibr" target="#b17">[18]</ref> is used to generate a scale-free network.  Stateless version of Q-learning algorithm is used, thus = 0 in Equation 1. Unless specified otherwise, the rewiring probability in Watts-Strogatz model is set to 0.1 and average number of neighbors in the small-world network is set to 12; learning rate is set to 0.1 initially, exploration rate is set to 0.01 initially and is 0.3 by default; each agent can choose from 4 actions and have a moderate memory length of 4 steps; moreover, the parameter and are both set to 0.1. <ref type="figure" target="#fig_3">Fig. 1(a)</ref>-1(c) plots the learning curves of the three adaptive learning approaches under the proposed framework, and the static learning approach under the social learning framework <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref>. In the social learning framework, each agent interacts with one of its neighbors and learns directly from this interaction. Comparing with this approach thus enables to demonstrate the merits of agents' adaptive learning behaviors in influencing the norm emerging process. The results show that the three adaptive learning approaches outperform the static social learning approach in all three networks in terms of a higher level of convergence and a faster convergence speed. Through dynamically adapting their learning behaviors during the norm emerging process, agents are able to reach an agreement more easily, and thus norms can emerge more quickly to a higher level of convergence in the adaptive learning framework. The distinction of performance between the adaptive learning and the social learning approaches is less apparent in the scale-free network because this kind of network is easier to emerge a norm compared with other networks. This is due to the small graph diameter of scale-free networks, as reported in various previous studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. <ref type="figure" target="#fig_3">Fig. 1(d)</ref> shows the performance of the two different kinds of approaches adopted in the upper layer learning. As can be seen, norms emerge faster with the action-based approach at the beginning. As the process proceeds, the reward-based approach catches up with the action-based approach, and then brings about a higher level of norm emergence afterwards. This result implies that it is more reasonable to use the most profitable action rather than the most adopted action in the past as the competing strategy in EGT.  <ref type="table" target="#tab_1">Table 3</ref> summarizes the final performance of the different approaches in 1000 independent runs. In order to better demonstrate the different performance of these approaches, we also include the results when 100% agents have chosen the same action as the norm. Achieving 100% norm emergence is an extremely challenging issue due to the widely recognized existence of subnorms <ref type="bibr" target="#b16">[17]</ref>. Clearly, the proposed learning approaches outperform the social learning approach in all aspect of comparison. For example, in the grid network, social learning can only enable averagely 86.1% agents in the population to achieve a consensus over the social norm. This performance is upgraded to as high as 92.2%, 91.9% and 95.7% using the three proposed approaches, respectively. Regarding effectiveness, the possibility that a norm can successfully emerge using social learning is quite low (i.e., 55.0% for 90% convergence, and 46.6% for 100% convergence). The adaptive learning approaches, however, can greatly increase the possibility of norm emergence (e.g., 86.7% for 90% and 100% convergence using supervision-both). As for efficiency, it takes averagely 4288 steps for 100% convergence using social learning approach, against 4113, 1180 and 1029 steps using the three adaptive learning approaches, respectively. To sum up, the adaptive learning approaches can promote the emergence of social norms to higher levels of convergence with fewer steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emergence of social norms</head><p>Learning dynamics of and To have a better understanding of the learning dynamics under the proposed framework, it is necessary to see how the critical learning parameters of learning rate and exploration rate evolve during the norm emerging process. The dynamics of and with different sizes of norm space are shown in <ref type="figure" target="#fig_4">Fig. 2</ref>. In both action sizes, the values of and increase sharply at the beginning, and then drop to nearly zero gradually. This is because the whole agent system is still in chaos at the beginning of the norm emerging process and it is more likely that the stochastic learning process cause the agents to be in a "losing" state. Therefore, agents increase their learning rate and/or exploration rate to learn faster and/or explore more, so as to get over the "losing" state. As the norm is emerging, the action choice is more and more consistent with supervision policy. Thus, and decrease accordingly to indicate a "winning" state of the agents. Comparing <ref type="figure" target="#fig_4">Fig. 2 (a)</ref> with (b), we can also see that, in 10-action scenario, the values change more dramatically at first and then it takes a longer time for these values to decrease to zero. This is because agents are more likely to choose the same action as the norm with a smaller action size. When the norm space gets larger, the probability to find the right action as the norm is greatly reduced. The large number of conflicts among the agents thus causes the agents to be in a "losing" state more often in a large norm space, and thus the norm emerging process is greatly prolonged.</p><p>Influence of norm space and population size The influence of norm space on norm emergence is given by <ref type="figure" target="#fig_6">Fig. 3</ref>. We can see that a larger number of available actions results in a delayed convergence of norms. This is because a larger number of actions are more likely to produce local sub-norms, leading to diversity across the society. It thus takes a longer time for the agents to eliminate this diversity and achieve a final consensus. In all cases, the adaptive learning approach performs better than the social learning approach in terms of a faster convergence speed and a higher convergence level. This result shows that the proposed adaptive learning framework is indeed effective for norm emergence in a large norm space.  The influence of population size on norm emergence is shown in <ref type="figure" target="#fig_7">Fig. 4</ref>. In both learning approaches, the norm emergence process is hindered as the population is growing larger. This result occurs because the larger the society, the more difficult to diffuse the effect of local learning to the whole society. This phenomenon can be observed in human societies where small groups can more easily establish social norms than larger groups <ref type="bibr" target="#b5">[6]</ref>. The proposed adaptive learning approach, however, can greatly facilitate norm emergence in different population sizes. In cases of 100, 500 and 1000 population size, the adaptive learning approach can achieve almost 100% convergence, which is a great promotion from the low convergence levels using the static social learning approach. In a population of 5000 agents, the norm emerging process is steadily facilitated to a level of 90% during 10000 steps using the adaptive learning approach, against a convergence level close to 70% using the social learning approach. <ref type="table" target="#tab_2">Table 4</ref> summarizes the performance of 100% norm emergence with various network topologies in small-world networks. The   rewiring possibility indicates different levels of network randomness. The results show that it is more efficient for a norm to emerge in a network with higher randomness. This is because the increase in randomness can reduce the network diameter (i.e., the largest number of hops in order to traverse from one vertex to another <ref type="bibr" target="#b7">[8]</ref>), and the smaller a network diameter, the more efficient for the network to evolve a social norm <ref type="bibr" target="#b23">[24]</ref>. The results also show that a minor increase of the rewiring possibility (from 0 to 0.1, especially from 0.01 to 0.1) can bring about significant improvement of norm emergence, while further increasing the rewiring possibility (from 0.2 to 1.0) cannot cause a further significant improvement. This is due to the fact that the network randomness is already quite high when the rewiring possibility is in-between [0.01, 0.1]. In all scenarios, the proposed learning approaches outperform the social learning approach in all three comparison criteria. Specially, when the randomness is high, approach supervision-and supervision-both can achieve norm convergence with 100% possibility. This robust norm emergence, however, only takes very short time (e.g., 117 and 112 steps for supervision-and supervision-both, respectively, againt 2984 steps for social learning, when = 1.0.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of network topology</head><p>As for neighborhood size, norm emergence is steadily promoted when the average number of neighbors is increased. This effect is due to the clustering coefficient of the network <ref type="bibr" target="#b9">[10]</ref>. When the average number of neighbors increases, the clustering coefficient also increases. Therefore, agents located in different parts of the network only need a smaller number of interactions to reach a consensus. On the other hand, when agents have a small neighborhood size, they only interact with their neighbors, which account for a small proportion of the whole population. This results in diverse sub-norms formed at different regions of the network. Such sub-norms conflict with each other in the network, and thus more interactions are needed to solve these conflicts and achieve a uniform norm for the whole society. In all cases of neighborhood sizes, the adaptive learning approaches can bring about more robust norm emergence with a faster convergence speed and a higher convergence level than the social learning approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, a novel learning framework was proposed to investigate how agents' adaptive learning behaviors can facilitate the process of norm emergence in network MASs. The highlight of the proposed model is the integration of social learning into the local individual learning in order to dynamically adapt agents' learning behaviors for a better performance of norm emergence. Our work thus bridges the gap between the two distinct research paradigms for learning of norm emergence by coupling a social learning process (through imitation in EGT) with a local individual learning process (i.e., RL). Although it can be expected that requiring communication among agents or additional information through social learning can facilitate norm emergence, this is not straightforward in the proposed model as the synthesized information used in social learning is generated from trail-and-error individual learning interactions, and this information is then utilized as a guide to heuristically adapt the local learning further. Tight coupling between these two learning processes can make the whole learning system rather dynamic. However, by synthesizing the individual learning experience into competing strategies in EGT and adapting local learning behaviors based on the principle of "Win-or-Learn-Fast", our work has illustrated that this kind of interplay between individual learning and social learning is indeed helpful in facilitating the emergence of social norms among agents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Based on the memory table, agent then synthesizes its past learning experience into two tables ( and ( ( denotes the frequency of choosing action in the last steps and ( denotes the overall reward of choosing action in the last steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>can be calculated by Equation 2,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Norm emergence using the different approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Learning dynamics of and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Influence of number of actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Influence of population size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 . The Extended CG Table 1. The CG</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 3 . Overall performance in three different networks</head><label>3</label><figDesc></figDesc><table>Grid 
Efficacy 

90% convergence 
100% convergence 

Effectiveness Efficiency Effectiveness Efficiency 

Supervision-
0.922 
74.7% 
1087 
74.7% 
1180 

Supervision-
0.919 
74.8% 
1509 
66.1% 
4113 

Supervision-both 0.957 
86.7% 
970 
86.7% 
1029 

Social learning 
0.861 
55.0% 
1617 
46.6% 
4288 

Small-world 
Efficacy 

90% convergence 
100% convergence 

Effectiveness Efficiency Effectiveness Efficiency 

Supervision-
0.972 
91.7% 
1692 
91.6% 
1735 

Supervision-
0.937 
84.2% 
1969 
71.6% 
4077 

Supervision-both 0.992 
98.4% 
818 
98.4% 
862 

Social learning 
0.871 
54.9% 
2212 
46.5% 
4450 

Scale-free 
Efficacy 

90% convergence 
100% convergence 

Effectiveness Efficiency Effectiveness Efficiency 

Supervision-
0.997 
100% 
181 
100% 
246 

Supervision-
0.969 
99.9% 
183 
93.1% 
3075 

Supervision-both 0.996 
100% 
114 
100% 
162 

Social learning 
0.967 
99.1% 
331 
90.4% 
3204 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Performance of the learning approaches in various topological settings (av- eraged over 1000 runs)</figDesc><table>Approach 
Criterion 

Randomness (rewiring possibility 
Neighborhood Size ( 

0 
0.001 0.01 
0.1 
0.2 
0.4 
0.6 
0.8 
1.0 
4 
8 
12 
16 
20 

Social learning 2.8% 4.5% 7.3% 46.5% 81.5% 92.7% 92.8% 92.9% 93.2% 1.8% 22.0% 46.5% 59.8% 77.0% 

Effectiveness 

Supervision-16.5% 19.4% 30.4% 91.6% 98.7% 100% 100% 100% 100% 15.4% 71.1% 91.6% 97.6% 99.0% 

Supervision-12.2% 14.1% 17.8% 71.6% 87.6% 93.0% 93.8% 94.1% 94.9% 6.9% 42.9% 71.6% 79.4% 86.0% 

Supervision-both 45.3% 50.2% 67.5% 98.4% 100% 100% 100% 100% 100% 38.9% 90.6% 98.4% 100% 100% 

Social learning 5432 4975 4628 4450 3537 3120 3083 3001 2984 6336 5371 4450 3923 3832 

Efficiency 

Supervision-
4400 4041 4023 1374 473 
134 
121 
118 
117 5321 2761 1374 979 
642 

Supervision-
4769 4494 3913 4077 3494 3081 3005 2998 2932 5658 4420 4077 3835 3649 

Supervision-both 4389 4153 4082 862 
283 
122 
114 
113 
112 4071 1796 862 
509 
325 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the emergence of social conventions: modeling, analysis, and simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tennenholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intel</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="139" to="166" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Heuristic collective learning for efficient and robust emergence of social norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 14th AAMAS</title>
		<meeting>of 14th AAMAS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1647" to="1648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast convention formation in dynamic networks using topological knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bazzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 29th AAAI</title>
		<meeting>of 29th AAAI</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2067" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling the emergence and convergence of norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Iba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 22nd IJCAI</title>
		<meeting>of 22nd IJCAI</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Norm learning in multi-agent societies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savarimuthu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Emergence of norms through social learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Airiau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 20th IJCAI</title>
		<meeting>of 20th IJCAI</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1507" to="1512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Norm emergence under constrained interactions in diverse societies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Airiau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 7th AAMAS</title>
		<meeting>of 7th AAMAS</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="779" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Topology and memory effect on convention emergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Villatoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sabater-Mir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WI-IAT&apos;09</title>
		<meeting>of WI-IAT&apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Aspects of active norm learning and the effect of lying on norm emergence in agent societies. Agents in Principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savarimuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arulanandam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Purvis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Agents in Practice</title>
		<imprint>
			<biblScope unit="volume">7047</biblScope>
			<biblScope unit="page" from="36" to="50" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>LNAI</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Social instruments for robust convention emergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Villatoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sabater-Mir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 22nd IJCAI</title>
		<meeting>of 22nd IJCAI</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="420" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Emergence of social norms through collective learning in networked agent societies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAMAS2013</title>
		<meeting>of AAMAS2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="475" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Norm emergence via influential weight propagation in complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shibusawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sugawara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of ENIC2014</title>
		<meeting>of ENIC2014</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="30" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>The MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Evolutionary games on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Szabo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>F¨¢th</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rep</title>
		<imprint>
			<biblScope unit="volume">446</biblScope>
			<biblScope unit="issue">4-6</biblScope>
			<biblScope unit="page" from="97" to="216" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiagent learning using a variable learning rate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="215" to="250" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The economics of convention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Econ. Pers</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="105" to="122" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A decentralized approach for convention emergence in multi-agent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Now</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Agents and Multi-Agent Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Statistical mechanics of complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Barab¨¢si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rev. Modern Phys</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="47" to="97" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adaptive policy gradient in multiagent learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAMAS2003</title>
		<meeting>of AAMAS2003</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="686" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convergence and no-regret in multiagent learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Emergence of conventions through social learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Airiau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Villatoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Agents and Multi-Agent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="779" to="804" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Collective dynamics of small-world networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="page" from="440" to="442" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Emergence of social conventions in complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Delgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="185" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Emergence of conventions for efficiently resolving conflicts in complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sugawara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IAT2014</title>
		<meeting>of IAT2014</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Effects of social network topology and options on norm emergence. Coordination, Organizations, Institutions and Norms in Agent Systems V, LNAI 6069</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="211" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust convention emergence in social networks through self-reinforcing structures dissolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Villatoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sabater-Mir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Autonomous and Adaptive Systems (TAAS)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Collective learning for the emergence of social norms in networked multiagent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2342" to="2355" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Social learning strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Laland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Animal Learning &amp; Behavior</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="14" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Opinion dynamics and bounded confidence models, analysis, and simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hegselmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Societies and Social Simulation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Consensus and ordering in language dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Castell¨®</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baronchelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Loreto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The European Physical Journal B</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="557" to="564" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Continuous limit of a crowd motion and herding model: analysis and numerical simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Markowich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pietschmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kinet. Relat. Models</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1025" to="1047" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A comprehensive survey of multiagent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">De</forename><surname>Schutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="156" to="172" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convergence and no-regret in multiagent learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to cooperate in multiagent social dilemmas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Munoz De Cote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazaric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Restelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 5th AAMAS</title>
		<meeting>of 5th AAMAS</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="783" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A multiagent reinforcement learning algorithm with non-linear dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abdallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lesser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="521" to="549" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
