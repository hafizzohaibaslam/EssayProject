<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-07-17T00:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">New Perspectives and Methods in Link Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">N</forename><surname>Lichtenwalter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep3">Dame Notre Dame</orgName>
								<orgName type="department" key="dep4">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep5">Dame Notre Dame</orgName>
								<orgName type="institution" key="instit1">The University of Notre Dame Notre Dame</orgName>
								<orgName type="institution" key="instit2">The University of Notre</orgName>
								<orgName type="institution" key="instit3">The University of Notre</orgName>
								<address>
									<postCode>46556, 46556, 46556</postCode>
									<region>Indiana, Indiana, Indiana</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><forename type="middle">T</forename><surname>Lussier</surname></persName>
							<email>jlussier@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep3">Dame Notre Dame</orgName>
								<orgName type="department" key="dep4">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep5">Dame Notre Dame</orgName>
								<orgName type="institution" key="instit1">The University of Notre Dame Notre Dame</orgName>
								<orgName type="institution" key="instit2">The University of Notre</orgName>
								<orgName type="institution" key="instit3">The University of Notre</orgName>
								<address>
									<postCode>46556, 46556, 46556</postCode>
									<region>Indiana, Indiana, Indiana</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
							<email>nchawla@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep3">Dame Notre Dame</orgName>
								<orgName type="department" key="dep4">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep5">Dame Notre Dame</orgName>
								<orgName type="institution" key="instit1">The University of Notre Dame Notre Dame</orgName>
								<orgName type="institution" key="instit2">The University of Notre</orgName>
								<orgName type="institution" key="instit3">The University of Notre</orgName>
								<address>
									<postCode>46556, 46556, 46556</postCode>
									<region>Indiana, Indiana, Indiana</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">New Perspectives and Methods in Link Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H28 [Database Management]: Database Applications- Data mining General Terms Algorithms</term>
					<term>Performance</term>
					<term>Theory Keywords Link Prediction</term>
					<term>Networks</term>
					<term>Machine Learning</term>
					<term>Class Imbal- ance</term>
				</keywords>
			</textClass>
			<abstract>
				<p>This paper examines important factors for link prediction in networks and provides a general, high-performance framework for the prediction task. Link prediction in sparse networks presents a significant challenge due to the inherent disproportion of links that can form to links that do form. Previous research has typically approached this as an unsu-pervised problem. While this is not the first work to explore supervised learning, many factors significant in influencing and guiding classification remain unexplored. In this paper , we consider these factors by first motivating the use of a supervised framework through a careful investigation of issues such as network observational period, generality of existing methods, variance reduction, topological causes and degrees of imbalance, and sampling approaches. We also present an effective flow-based predicting algorithm, offer formal bounds on imbalance in sparse network link prediction , and employ an evaluation method appropriate for the observed imbalance. Our careful consideration of the above issues ultimately leads to a completely general framework that outperforms unsupervised link prediction methods by more than 30% AUC.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Link prediction is an important task in network science that offers unique ways whereby the study of networks can Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. benefit researchers and organizations in a variety of fields. Security agencies can more precisely focus their efforts based on probable relationships in malicious networks that have heretofore gone unobserved <ref type="bibr" target="#b9">[10]</ref>. In social networks, individuals can efficiently and effectively find companions, assistants, or colleagues <ref type="bibr" target="#b8">[9]</ref>. In medicine and biology, link prediction can be used to find relationships and associations that exist, but which might otherwise surface only after arduous and expensive research and study on a huge selection of agents. Finally, researchers can easily adapt link prediction methods to identify links that are surprising given their surrounding network, or which may not belong at all <ref type="bibr" target="#b14">[15]</ref>. Put simply, any environment that naturally maps to a network probably has an equally coherent mapping from link prediction in that network back to an important question in the environment.</p><p>This broad applicability demands a powerful yet general framework, and we promote supervised learning. Unsupervised methods, which receive the most attention in link prediction literature, are fundamentally unable to cope with dynamics, interdependencies, and other properties in networks. We recognize that this is not the first paper to apply supervised learning to the link prediction problem, but there are important differences versus past work. First, in spite of the excellent intentions of past researchers, they have fallen prey to unique pitfalls endemic to problems with highly imbalanced class distributions. In <ref type="bibr" target="#b1">[2]</ref>, the holdout test set is undersampled to balance, and the authors of <ref type="bibr" target="#b16">[17]</ref> also contribute only a sample of the negative instances to their test set. As researchers familiar with high skew are aware, modifying the data distribution on which testing is performed generates uninterpretable results. The distribution of the resulting testing data no longer presents the same challenges as the real-world distribution, and performance measures in testing no longer reflect the real capabilities and limitations of the model. Additionally, both of these works employ semantic and contextual information that pertains almost exclusively to the bibliographic domain. Finally, these works do not consider the important impact of geodesic distance and the intricacies of class imbalance specific to the task of link prediction.</p><p>We demonstrate that decomposition by geodesic distance has important impacts on predictor performance irrespective of the choice of predictor. We also expand the library of unsupervised measures with an intuitive flow-based metric that is over 15% AUC more predictive than baseline methods in certain networks. After illustrating the benefits of supervised learning, we cast link prediction as a problem in class imbalance. The result of these considerations is a framework that improves upon the best baseline unsupervised methods by over 30% AUC in our test networks. Furthermore, the framework is entirely general, operating over any class of network whether it be weighted, unweighted, directed, or undirected. It does not require any node attributes but is capable of accepting them.</p><p>In Section 2 we describe the data sources and evaluation measures. Section 3 explains standard unsupervised approaches and defines a new metric, PropFlow. Section 4 lays out the rationale for supervised learning. This leads to a discussion of class imbalance in Section 5. In Section 6 we describe our realization of the framework and Section 7 presents results. Finally, Section 8 provides recommendations and concludes. components provides some insight into the broad topological structure of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DATA AND EVALUATION</head><p>In order to make a compelling, novel case for a supervised framework, we offer a comprehensive explanation of the nature of link prediction, primarily through an examination of two real-world data sets. We also report the prediction results relative to an appropriate metric for predicting in imbalanced environments. It is therefore necessary to first present the two data sources and the principal measure of performance that we employ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Network Data Sources</head><p>The first data source is a stream of 712 million cellular phone calls from a major non-American cellular phone service provider. We construct weighted, directed networks from the calls by creating a node vi for each caller and a weighted, directed link eij from vi to vj if and only if vi calls vj. Weights correspond to the number of calls over the link. We shall henceforth refer to this network as phone.</p><p>For all experiments except those in section 4.1 we use the first 5 weeks of data (5.5M nodes, 19.7M links) for extracting features and the sixth week (4.4M nodes, 8.5M links) for obtaining ground truth.</p><p>The second data source is a stream of 19,464 multi-agent events representing condensed matter physics collaborations from 1995 to 2000. We construct weighted, undirected networks from the collaborations by creating a node for each author in the event and a weighted, undirected link connecting each pair of authors. Weights correspond to the number of collaborations two authors share. We shall henceforth refer to this network as condmat. For all experiments involving condmat, we use the years 1995 to 1999 (13.9K nodes, 80.6K links) for extracting features and the year 2000 (8.5K nodes, 41.0K links) for obtaining ground truth.</p><p>The networks exhibit different quantitative characteristics. <ref type="table" target="#tab_0">Table 1</ref> contains some summary network statistics in order to provide context for the two networks. These statistics result from the complete 6 weeks of data for phone and the complete 1995-2000 network for condmat. The assortativity coefficient measures the tendency to find highly connected nodes that are connected to each other. The average clustering coefficient measures the tendency of nodes in the network to be connected in dense groups. Stronglyconnected components (SCCs), or connected components in the undirected network, are clusters of vertices in the network in which every vertex in the cluster has a path to all other vertices in the cluster. The size and diameter of such Scalar measures often used in link prediction, such as precision on the top-N predictions and factors of improvement in precision over random models, rely upon the application of an arbitrary and often unjustified threshold. Most of our evaluation relies instead upon receiver operating characteristic (ROC) curves. These curves present achievable true positive rates (T P ) with respect to all false positive rates (F P ) by varying the decision threshold on probability estimations or scores. ROC curves provide information about the operating range of classifiers. For example, classifier A may outperform classifier B when we dictate F P &lt; 20% but B may outperform A when we allow F P ≡ 20%. The expected performance of a random classifier is the line y = x, and curves below this line indicate an inverted predictor. Finally, we can say that classifier A dominates classifier B in ROC space if all points on the convex hull of A dominate all points on the convex hull of B in the xy-plane, and this is a condition known to correlate highly with superiority in many other measures <ref type="bibr" target="#b12">[13]</ref>. The area under the ROC curve (AUC) is a related scalar measure of the performance over all thresholds. AUC has classically been used as a measure of performance in imbalanced learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">UNSUPERVISED METHODS</head><p>Most existing studies in link prediction consider baseline unsupervised methods to assign scores to potential links. The state-of-the-art in these methods is aggregated and compared in <ref type="bibr" target="#b10">[11]</ref>, and in Section 3.1 we offer a brief explanation of the particular methods that we study. Moreover, since our goal is to derive a robust feature set, we introduce a novel, effective method in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline Predictors</head><p>Most unsupervised methods either generate scores based on node neighborhoods or path information. The common neighbors predictor is the number of neighbors, or out-degree neighbors in our directed network, that are shared by nodes vi and vj. Jaccard's coefficient simply divides the number of common neighbors by the number of total neighbors. The Adamic/Adar measure <ref type="bibr" target="#b0">[1]</ref> weights the importance of a common neighbor v k by the rarity of relationships between other nodes and v k . Finally, the preferential attachment link prediction score <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref> is the product of the degrees of vi and vj. When we observed especially poor performance for this predictor in phone, we tried using in-degree, out-degree, and their sum but observed only minor differences. We report our results based on out-degree performance. From the path-based methods we employ the unweighted Katz Algorithm 1 PropFlow Predictor Require: network G = (V, E), node vs, max length l Ensure: score S sd for all n ≒ l-degree neighbors v d of vs 1: insert vs into F ound 2: push vs onto N ewSearch 3: insert (vs, 1) into S 4: for CurrentDegree ↘ 0 to l do 5:</p><p>OldSearch ↘ N ewSearch 6: empty N ewSearch 7:</p><p>while OldSearch is not empty do 8:</p><p>pop vi from OldSearch 9:</p><p>find N odeInput using vi in S 10:</p><p>SumOutput ↘ 0 11:</p><p>for each vj in neighbors of vi do 12:</p><p>add weight of eij to SumOutput 13:</p><p>end for 14:</p><p>F low ↘ 0 15:</p><p>for each vj in neighbors of vi do 16:</p><p>wij ↘ weight of eij 17:</p><p>F low ↘ N odeInput ℅ source such as information flows, propagates, or cascades. In transportation networks, when a resource frequently travels from one node through neighbors to another, there is often some cost for the intermediaries. When the expected cost inherent in traveling through intermediaries overcomes the cost of establishing a new link, one can expect formation of that particular link. In transmission networks, the measure represents the link-weighted probability that a randomly outward-propagated transmission sent by one node will reach another. In condmat, there is no strong analogy and PropFlow is not as effective. Later in the paper, we will explore the utility of PropFlow both as an individual predictor and as a feature in our supervised classification framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">A CASE FOR SUPERVISED LEARNING</head><formula xml:id="formula_0">w ij SumOutput 18: insert or sum (vj, F low) into S 19:</formula><p>if vj is not in F ound then 20: insert vj into F ound 21:</p><p>push vj onto N ewSearch 22: end if 23:</p><p>end for 24:</p><p>end while 25: end for measure <ref type="bibr" target="#b7">[8]</ref>, which had better, more stable performance in the networks than the weighted variant. This method contributes each path to a sum with an influence damped in exponential proportion to its length, l, using the parameter 汕. We select 汕 = 0.005 and, for performance reasons, we restrict our examination of the measure such that l ≒ 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The PropFlow Method</head><p>We introduce a new unsupervised prediction method on networks, PropFlow, which corresponds to the probability that a restricted random walk starting at vi ends at vj in l steps or fewer using link weights as transition probabilities. The restrictions are that the walk terminates upon reaching vj or upon revisiting any node including vi. The walk selects links based on their weights. This produces a score sij that can serve as an estimation of the likelihood of new links. PropFlow is somewhat similar to Rooted PageRank, but it is a more localized measure of propagation, and is insensitive to topological noise far from the source node. Unlike Rooted PageRank, the computation of PropFlow does not require walk restarts or convergence but simply employs a modified breadth-first search restricted to height l. It is thus much faster to compute. It may be used on weighted, unweighted, directed, or undirected networks. We supply the detailed procedure for weighted, directed networks in Algorithm 1.</p><p>In the phone network, PropFlow outperforms baseline unsupervised methods by &gt; 15% AUC on average. It outperforms Rooted PageRank by more than 8.75% AUC. We attribute this success to the nature of the mechanisms in phone underlying the appearance of new links. Although it may be used in any network, PropFlow has special intuitive significance as a link predictor in networks where some reWhile past studies on link prediction have focused on unsupervised single metrics, some recent works have used a supervised classification scheme, and rightly so. If one accepts the basic premise that ground truth, whether a link forms or not, is always available from prior incarnations of the network, there is no practical disadvantage to using a supervised framework. Even training classifiers based on a single unsupervised method has the potential to outperform rankings generated by sorting the scores of the method if there are multiple differentiating boundaries in the domain of scores. Supervised algorithms are also able to capture important interdependency relationships between topological properties. While past studies simply acknowledged this fact and trained classifiers, we probe more deeply into the relevant issues so that we can fully understand how to frame the prediction problem and why a supervised framework is best for the task. We first address the how question in Section 4.1 by examining how to best transform network data into standard data sets. We then address the why question in sections 4.2, 4.3, and throughout section 5. More specifically, Section 4.2 explains that supervised approaches are adaptive and may be more general whereas unsupervised methods are invariant. Section 4.3 demonstrates that unsupervised methods cannot be, or at least have not been, combined into ensembles to reduce variance. Section 5 explains that unsupervised methods are inherently incapable of combating extreme class imbalance, a natural characteristic of link prediction in nearly any network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Constructing Data Sets</head><p>Some networks may always be observable, such as WWW, the Internet, and electricity grids. Others are observable only through events that indicate the presence of links. In the former, one need only select a moment at which to observe the structure directly. In the latter, one must collect events to construct an approximation of the underlying structure. Regardless, the network evolves through time to present a longitudinal source of data. We see then that link prediction, a domain in which unsupervised topological measures receive much attention, is very often suitable for supervised learning. The acquisition of ground truth for constructing models does not mitigate the necessity of the task; future forms of the static network will raise the same questions that exist in the present.</p><p>In a typical supervised learning task, we are given a unified set of data with each instance of the form ( x, y). To convert networks such as phone or condmat into this format, we have  <ref type="bibr">Gx = (Vx, Ex)</ref>, constructed from t0 to t0+而 x , we extract topological measures, and potentially node attributes, that serve as features for each pair of nodes <ref type="bibr">(vi, vj)</ref>. From the second network, Gy = (Vy, Ey), constructed from t而 x+1 to t而 x+而y , we examine (vi, vj) to discover whether eij exists and determine the class label. This yields a data set in the standard ( x, y) format with |Vx| 2 ? |Ex| instances. The two parameters 而x and 而y have important but predictable influences on the success of models. We can expect that increasing 而x will increase the quality of topological measures as the network reaches saturation. This is the point at which 而x is large enough that the observed events form a topology that closely reflects the underlying static network. As 而x approaches this point, the topological measures converge to their actual unobservable static network values, thus allowing improved individual predictive capacity. We can expect that increasing 而y will increase the number of positives. We investigate 而x on the phone network in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>Increasing 而x has the expected result. The strength of the predictors increases greatly from 而x = 2 weeks to 而x = 5 weeks and again from 而x = 5 weeks to 而x = 8 weeks. Although measures of network saturation and convergence are outside the scope of this paper, we can remark that they are highly correlated with performance. Since we observe an increase in the predictive power of unsupervised methods, we can expect increases in supervised classification performance too. In effect, the features more closely reflect actual relationships underlying observable events, so models are more closely related to reality.</p><p>Although this suggests that the results in Section 7 could be even higher with 而x = 8 weeks, we choose to present the rest of the paper based on 而x = 5 weeks and 而y = 1 week. This observational period corresponds to a network that is only partially approaching saturation and might more realistically represent the data available for training in realworld environments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generality</head><p>While classifiers can generalize well to many environments in the sense that they can adjust models depending on posterior information, unsupervised methods are domain-specific. The figures in <ref type="bibr" target="#b10">[11]</ref> show that predictors that serve well in one network do not necessarily serve well in all networks; our observations concur. It is clear throughout our results that the performance of the unsupervised methods is unstable not only from one network to the other, but from one graph-distance to another. The preferential attachment predictor is a particularly clear example in <ref type="figure" target="#fig_3">Figure 2</ref>. The figure shows the percentage of a given score that is positive. Intuitively, the model serves as a good predictor when low scores produce low percentages and high scores produce high percentages and an inverted predictor when the opposite is true.</p><p>In the phone network, we see that the predictions are inverted, with a higher percentage of positives falling into low scores than high scores. In the condmat network, the predictions are much better, with the highest scores corresponding to much higher incidences of links. Finally, for both networks, we observe a similar trend with increasing geodesic distance n. The greater the distance, the better preferential attachment models the appearance of links. That is, we see lower percentages for lower scores and higher percentages classifier, but expect the same results for other unsupervised methods.</p><p>Supervised classification, on the other hand, offers many strong options for reducing variance such as bagging <ref type="bibr" target="#b3">[4]</ref> and random forests for decision trees <ref type="bibr" target="#b4">[5]</ref>, the latter of which also increases classification efficiency. While a single classifier that incorporates several of the unsupervised methods can greatly improve classification versus those methods, variance reduction techniques can further improve it. A significant novelty of link prediction as a supervised learning problem is its extreme imbalance, which reaches past the most skewed distributions studied by the imbalance community. While unsupervised methods cannot combat this imbalance because they are agnostic to class distributions by definition, supervised learning schemes are able to balance data and focus on class boundaries. In this section, we will study some of the properties of that imbalance, especially as it relates to graph distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.56</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">GRAPH DISTANCE AND IMBALANCE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sparse Networks</head><p>for higher scores as we move from n = 2 to n = 4. This supports the intuition that preferential attachment is better as a global indicator where underlying local mechanisms such as neighbor recommendations are weaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Variance Reduction and Sampling Issues</head><p>We proceed by constructing a formal proof of the lower bound on the class imbalance ratio for link prediction in sparse networks. The proof operates on two reasonable, almost ubiquitously satisfied assumptions. First, the network maintains the property of sparseness throughout the period of interest. Second, the network growth is limited such that the number of nodes may only double during the period of interest, although the theorem holds for any factor of growth g such that g ? |V |.</p><p>Yet another benefit of supervised learning is that classification algorithms, especially unstable algorithms like decision trees, can benefit from reduced variance by placing them in an ensemble framework. Ensembles consist of many models that have been trained on slightly perturbed variations of the data. It is difficult or impossible to accomplish the same goal with unsupervised methods common in link prediction because the score is invariant for a given potential link. Furthermore, it is likely that network analogs to common ensemble sampling techniques are fundamentally flawed as a rough corollary of work in <ref type="bibr" target="#b15">[16]</ref>, where samples of networks with ill-behaving distributions produce new networks with different properties. Nevertheless, we wanted to explore the potential for one method of ensemble construction using unsupervised methods. To achieve the values in <ref type="figure" target="#fig_5">Figure 3</ref>, we construct 10 new networks, randomly selecting p percent of the edges in the original network for each. Then, we compute a common neighbors score for the pair <ref type="bibr">(vi, vj)</ref> in each network and combine the scores using a summary statistic.</p><p>The figure shows that the attempt at constructing an ensemble out of an unsupervised method fails. The best AUC appears at 100%, where the network is unsampled and there is no ensemble, which suggests that sampling the network to construct the ensemble does nothing but remove important information, a result we find unsurprising. What the figure does not show is that the p = 100 ROC curve dominates all ROC curves for p &lt; 100, including mean and max, and that transformations of the ROC curves into precision-recall space show p = 100 greatly outperforms even p = 95. We performed these experiments only for the common neighbors Definition 1. Let a network G = (V, E) be described as sparse if it maintains the property |E| = k|V | for some constant k ? |V |. , equivalent to ? |V | 1 , as the class ratio.</p><p>Thus the imbalance issue in the general link imbalance problem becomes clear. No matter how many links we hope to anticipate, T P , we must accept a baseline random model that produces F P such that F P ≦ T P ℅ |V |. Even a model thousands of times better than random performs poorly. The severity of the problem is exacerbated by the fact that positives often represent occurrences of greater interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Graph Distance and Neighborhoods</head><p>In link prediction, graph distance plays a primary role in determining the imbalance ratio. We define the n-degree neighborhood of a node vi as the set of nodes exactly n hops away from vi. As n increases, the number of potential links will increase in proportion to the superlinear increase  in the number of neighbors. Simultaneously, it is reasonable to expect that the new links will tend to form between nodes that are close, such as in phone where local influences such as recommendations and common neighbors pertain. <ref type="figure" target="#fig_7">Figure 4</ref> illustrates the imbalance behavior of the phone and condmat networks. It also demonstrates the distribution of distances between pairs of nodes for all distances where the underlying computation is feasible. It is important to note here that phone has a diameter in its largest strongly connected component of 25 while condmat has a diameter of only 19 in its largest connected component. Further, the n ≒ 6-degree neighborhood of any given node in phone still includes only a moderate fraction of nodes in the network. In the much smaller condmat network, the n ≒ 6-degree neighborhood of any given node often approaches the periphery and includes almost every node in the network. The simultaneous severe increase in unformed potential links and severe decrease in links that actually form causes even more dramatic increases in imbalance ratios. phone imbalance goes from 131:1 at n = 2 to 32,880:1 at n = 4 and 606,926:1 at n = 6. condmat imbalance goes from 179:1 at n = 2 to 6,247:1 at n = 4. Fortunately, there is often little reason to believe that the benefit of successfully predicting links to nodes at high n is greater than the benefit of predicting them at low n.</p><p>Given that imbalance increases so sharply between neighborhoods, and local mechanisms quickly give way to global mechanisms at higher values of n, we suggest that each neighborhood should be treated as a separate problem in supervised learning. This also allows us to avoid the V :1 imbalance of the general problem. Additionally, in the case of the entire class of neighbor-based models, there is null output for n ≡ 3 in undirected networks because there is no sense in which such nodes can have common neighbors. Many unsupervised methods have implicit or explicit adjustments for graph distance, but the fundamental distinction of neighborhood comes for free. Supervised models may benefit from a decrease in noise and, for networks in which the distance spanned by the predicted link is inconsequential, the consideration of low n saves computational time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CLASSIFICATION</head><p>With the nature of the problem and the advantages of supervised learning carefully considered, we now present the details of the high-performance link prediction (HPLP) framework. In most cases, we reserved two-thirds of the labeled data for training the model and the remaining third for testing, but for the presentation of significance results, we employed 10-fold cross-validation with care to use unmodified folds for testing. At no time do we change the class distribution in any testing data. Due to computational complexity, we restricted our consideration of classifiers to the WEKA [18] C4.5 <ref type="bibr" target="#b13">[14]</ref> equivalent, J48 (parameters -A -U), na“?vena“?ve Bayes (default parameters), and WEKA bagging (10 bags, default parameters) with random forests (10 trees, default parameters). The last is easy to parallelize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">General Feature Extraction</head><p>Any network, no matter its type or source, necessarily supports basic topological measures such as vi and vj indegree and out-degree, vi and vj in-volume and out-volume, or their undirected equivalents in the case of condmat. We also employ the baseline unsupervised models from Section 3, including PropFlow, and path-oriented measures such as the number of shortest paths from vi to vj and the maximum flow that can travel from vi to vj within 5 steps. Though we use only these features for generality, we could use any other available features, including measures of reciprocity, or node attributes such as age and gender.</p><p>To illustrate that we are able not only to achieve performance that vastly exceeds baseline methods, but that we do so without using them as features, we include both a restricted feature set that does not use the existing unsupervised methods (HPLP) and the full feature set (HPLP+). <ref type="table" target="#tab_2">Table 2</ref> contains details of the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ensemble of Classifiers</head><p>We capitalize on the ability of supervised frameworks to reduce variance, as described in Section 4.3, by using ensembles of classifiers. We use two different ensemble methods: bagging and random forests. Random forests is an excel-  Percentage Positive lent method for these data sets for two reasons. First, the data sets are composed of a combination of strong features and weak features. While the weak features are occasionally helpful, random forests is an excellent method to prevent overfitting them. Second, the decreased training time for each single tree counters the increased training time to build the forest, making it an especially efficient method of variance reduction for these large data sets. In both data sets, after undersampling to balance, we found on average a 4.04% AUC improvement moving from a single tree to 10 bagged trees and an additional 3.91% improvement moving from 10 bagged trees to 10 bagged random forests. The total average improvement of 8.11% justifies the use of these variance reduction methods, and the sheer size of the testing sets lends significance to even fractional percentage improvements. We found that neither 100 bagged trees without random forests nor 100 random forest trees without bagging offered the same improvements. Finally, we note that the improvements we observed after the application of these techniques are impressive, but still suboptimal. We constructed our ensembles from the same selection of undersampled negative class instances. With a minimal penalty in computational time, each member of the ensemble could make use of a random selection of the entire set of negative class instances. </p><formula xml:id="formula_1">(i) - In-Volume(i) - In-Degree(j) - In-Volume(j) - Out-Degree(i) - Out-Volume(i) - Out-Degree(j) - Out-Volume(j) - Common Nbrs(i,j) - Max. Flow(i,j) l = 5 Shortest Paths(i,j) l = 5 PropFlow(i,j) l = 5 Adamic/Adar(i,j) - Jaccard's Coef(i,j) - Katz(i,j) l = 5, 汕 = 0.005 Pref Attach(i,j) -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Overcoming Imbalance</head><p>Aside from reducing variance, we considered different approaches to overcome the imbalance described in Section 5. In doing so, we had to carefully consider the enormity of the data sets, especially for large values of n. As <ref type="figure" target="#fig_7">Figure 4</ref> shows, in phone n = 2 produces 81.4 million instances and n = 4 produces 2.2 billion instances. Even in the smaller condmat network, n = 2 produces 431.6 thousand instances and n = 4 produces 8.1 million instances.</p><p>One of the best oversampling strategies, SMOTE <ref type="bibr" target="#b5">[6]</ref>, is O(p 2 ﹞ | x|), the product of the number of positive class instances p and the length of the feature vector. While this may work for condmat where p is on the order of thousands, it certainly will not work for phone. We also theorize that phone, with p in the order of tens or hundreds of thousands, does not suffer as much from lack of definition in the positive class as from a strong classifier bias toward f (x) = 0 from prior information. Furthermore, oversampling approaches only increase data set size and training time. We considered training skew-insensitive decision trees based on Hellinger distance <ref type="bibr" target="#b6">[7]</ref>. Such trees are best when trained on the original training set distribution, however, and performed poorly with undersampled data. Without undersampling, the training set sizes for the data often render training with these trees infeasible. Undersampling, on the other hand, can help to mitigate the problem of class imbalance while also reducing the size of the training set.</p><p>In section 5.2 we argue for treating each neighborhood as a separate problem. This also allows for skew-combating methods that are appropriate to the particular neighborhood. If n ≡ 2 is combined into a single data set and subsequently uniformly undersampled, negative representatives of n = 2 will be underrepresented causing a distortion of the real n = 2 class boundary.</p><p>The class ratio to which the data set is undersampled serves as a significant parameter to our framework. In <ref type="figure" target="#fig_9">Fig- ure 5</ref> we explore a wide range of possible sampling parameters using a single C4.5 decision tree evaluated according to AUC. The performance of the phone data set is relatively stable, but it exhibits an interesting trend wherein AUC drops slightly when the class distribution is balanced. condmat achieves AUC values that are higher with increasing negative class representation through the ratios we tested. Despite these results, in Section 7 we undersample the training sets to balance to present a consistent view of performance.</p><p>We would like to mention that the beneficial relationship between link prediction researchers and class imbalance researchers is mutual. Class imbalance research contributes many options to the link prediction community. Simultaneously, link prediction offers the potential for a wide variety of data sets that match or surpass the imbalance ratios of the most demanding publicly available data. Any large network can become an authentic source for data with selectable features, selectable imbalance ratios depending on the chosen value of n, and a large pool of positive instances from which to draw the desired positive class cardinality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">DETAILED ANALYSIS</head><p>To achieve the following results, we trained bagged random forests, which exhibited universally superior performance. For uniformity of reporting, all training sets are undersampled to balance rather than to a level optimized for each network and n. Because AUC alone can sometimes be misleading, we also include ROC curves. <ref type="figure" target="#fig_10">Figure 6</ref> contains curves describing the performance of unsupervised methods and the supervised framework.</p><p>The phone and condmat curves illustrate that the mechanism by which links arise is indeed different both across networks and geodesic distances. In fact, this leads to an interesting broad observation about mechanisms of link formation. In the condmat network, individuals have a global view of the topology through a variety of means. In essence, researchers know of other eminent researchers in the field however remote they may be in terms of geodesic distance. In the phone network, there is little reason to suspect that individuals have much knowledge of other individuals at remote locations in the network. The performance of the preferential attachment method supports this theory in the two networks; it is much stronger for condmat than for phone. Additionally, it shows performance that increases with n in both networks. The more distance potential links span in a network, the weaker local influences such as neighbor recommendations or path-based considerations become. The discriminative power of methods based on these principles generally drops accordingly. On the other hand, global influences such as degree have the same interpretation at any distance in the network. As the local influences lose their meaning with increasing n, the preferential attachment method becomes an increasingly pure estimation of link formation biases.</p><p>We can clearly see the deterioration in predictive capacity of the local methods. Neighbor-based methods perform worse for n = 3 than for n = 2 phone, and they perform much worse for n = 4 than for n = 3. Neighbor-based methods have no meaning for n ≡ 3 in directed networks such as condmat; there is no sense in which two unconnected individuals greater than two hops away from each other can share a neighbor. The PropFlow predictor degrades more gracefully than neighbor-based methods in phone but suffers mediocre performance on condmat. Despite much greater curve areas in phone, PropFlow does not dominate other measures. Instead, methods based on common neighbors achieve slightly higher T P rates at very low F P rates, but PropFlow rapidly surpasses them. Importantly, the HPLP dominates all other methods in ROC space in every case except phone n = 2, where PropFlow actually crosses at F P = 0.99. On a more general note, especially in phone where imbalance ratios grow higher, the increasing difficulty of more distant neighborhoods is exhibited in the form of ROC curves that converge toward T P = F P .</p><p>We now move to the discussion of AUC values and <ref type="figure" target="#fig_12">Fig- ure 7</ref>. HPLP achieves performance levels as much as 30% higher than the best unsupervised methods. The difference in performance between HPLP and HPLP+ averages &lt; 1% AUC. Though it does not appear in the figures, in phone we also created a data set with all unsupervised methods except PropFlow. We found that PropFlow alone achieves higher performance than using all other unsupervised methods put together when using the same basic supporting features, such as node degree. To substantiate the hypothe-  sis that there are useful dependencies between features, we compared na“?vena“?ve Bayes to a single C4.5 tree and confirmed that the latter wins by &gt; 2.3%. Nonetheless, even using a single, fast na“?vena“?ve Bayes classifier, HPLP always greatly outperforms the strongest of the unsupervised methods. To provide statistical significance to this statement, we used two-tailed paired t-tests. The paired samples come from 10-fold cross validation AUC scores. For all values of n, HPLP outperforms unsupervised methods at over 99% confidence and HPLP+ outperforms them at over 99.99% confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSION</head><p>The general framework we propose in this paper achieves major improvements over existing methods. Although it outperforms such methods by &gt; 30% in terms of AUC, it does not require any domain-specific node attributes to do so. It can be applied in any domain exactly as described or it can accept any number of domain-specific features. It is also highly scalable; feature computation for a single pathbased method requires more time than the entire classification framework. The feature computation itself is embarrassingly parallel.</p><p>In addition to the results, the supporting study allows for some recommendations. Unsurprisingly, for networks where topological convergence and saturation may be a concern, the training observation period, 而x, should be as long as possible. The parameter 而y for the static network from which labels are gathered should match the size of the real-world prediction window so that testing and real-world prior distributions are as similar as possible. In link prediction on networks such as the Internet or electricity grids, these concerns are moot since snapshots contain the entire network structure.</p><p>Optimal class ratios for undersampling are specific to the problem at hand, but the results we obtained for both networks indicate that undersampling to balance may not be ideal in the link prediction domain. Those employing this classification framework should be aware of this fact and should investigate other ratios as resources permit. For small networks where computational resources are not problematic, we advise the use of skew-insensitive classifiers such as Hellinger trees in an ensemble framework. We encourage the community to consider the link prediction task as a separate problem for each desired neighborhood in domains where local mechanisms are likely to pertain. This not only decreases computational time by considering those links most likely to rank highly regardless but has the potential to sensitize supervised classification to the specific mechanisms and boundaries present for predictions within the target graph distances.</p><p>In general, the application of unsupervised methods, at least without due study and consideration, is highly suboptimal. No such method, no matter how high its performance in some subset of our data, provides acceptable performance for the entire range of problems. Where there is some limitation on supervised learning due to the unavailability of labels for training, we hope that the included study of unsupervised performance measures proves helpful in the selection of an appropriate option. For networks where one expects local mechanisms to dominate, especially local mechanisms related to flow or propagation, we highly encourage the use of the unsupervised method, PropFlow, proposed in this paper.</p><p>We have published all scripts and source code for prediction and evaluation at http://www.nd.edu/~rlichten/ linkpred along with the condmat data set. We regret that we are unable to make the phone data set available due to non-disclosure agreements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">ACKNOWLEDGMENTS</head><p>Research was sponsored in part by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-09-2-0053 and in part by the National Science Foundation (NSF) Grant BCS-0826958. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon.</p><p>We sincerely thank Mark E.J. Newman for the condmat data set and Albert-L芍szl車 Barab芍si for the phone data set. Finally, we thank our colleagues in the University of Notre Dame Interdiscplinary Center for Network Science and Applications (iCeNSA).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>KDD' 10 ,</head><label>10</label><figDesc>July 25-28, 2010, Washington, DC, USA. Copyright 2010 ACM 978-1-4503-0055-1/10/07 ...$10.00.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance in the second-degree neighborhood as a function of 而x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Preferential attachment performance by scoring region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: AUC performance variation in common neighbors 'ensemble' created by randomly selecting p percent of edges for removal 10 times, obtaining the measure, and combining it using the series statistic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Neighborhood imbalance properties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance reaction of a single C4.5 decision tree to different undersampling levels. The x-axis is in terms of the percentage of all training examples that are positive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The ROC curves for phone (top) and condmat (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The AUC values for phone and condmat.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Network Characteristics</head><label>1</label><figDesc></figDesc><table>phone condmat 
Assortativity Coef. 
0.293 
0.177 
Average Clustering Coef. 
0.187 
0.642 
Mean Degree 
3.88 
6.42 
Median Degree 
3 
4 
Number of SCCs 
1,023,044 
652 
Largest SCC 
4,293,751 
15,081 
Largest SCC Diameter 
25 
19 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Theorem 1 . The class imbalance ratio for link prediction in a sparse network G is ?</head><label>1</label><figDesc></figDesc><table>|V | 
1 


when at most |V | nodes 
may join the network. 

Proof. The number of possible links in G is |V | 
2 . Then 
the number of missing links, |E 
C |, is |V | 
2 ? k|V | ﹋ 成(|V | 
2 ). 
Let |V 
∩ | nodes and |E 
∩ | links join the network. Since |V | + 
|V 
∩ | ≒ 2|V | ﹋ 成(|V |), |E| + |E 
∩ | ﹋ 成(|V |), which requires 
that |E 
∩ | ﹋ O(|V |). The number of positives is |E 
∩ |, and 

there are 


﹍ E 
∩ ) 

C 



﹋ 成(|V | 
2 ) negatives. This gives us 

成(|V | 

2 ) 

O(|V |) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Feature Listing</head><label>2</label><figDesc></figDesc><table>1 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Friends and neighbors on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="211" to="230" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Link prediction using supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Al</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chaoji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Link Discovery: Issues, Approaches and Apps</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evolution of the social network of scientific collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-L</forename><surname>Barab芍si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>N谷da</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ravasz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vicsek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A</title>
		<imprint>
			<biblScope unit="volume">311</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="590" to="614" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Smote: Synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of A.I. Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="341" to="378" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning decision trees for unbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Cieslak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ECML</title>
		<meeting>of the ECML</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A new status index derived from sociometric analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="43" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Referral web: combining social networks and collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">63</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mapping networks of terrorist cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Krebs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connections</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="43" to="52" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liben-Nowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Clustering and preferential attachment in growing networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E J</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters E</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust classification for imprecise environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="203" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4.5: Programs for Machine Learning</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The case for anomalous link discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rattigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="41" to="47" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Subnets of scale-free networks are not scale-free: sampling properties of networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wiuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>May</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. of the Nat Acad. of Sci</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4221" to="4224" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Local probabilistic models for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Satuluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2007 7th IEEE ICDM</title>
		<meeting>of the 2007 7th IEEE ICDM<address><addrLine>Washington, D.C., USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="322" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques</title>
		<meeting><address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
