<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-07-17T00:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse Cooperative Q-learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelle</forename><forename type="middle">R</forename><surname>Kok</surname></persName>
							<email>jellekok@science.uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Informatics Institute</orgName>
								<orgName type="department" key="dep2">Faculty of Science</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Vlassis</surname></persName>
							<email>vlassis@science.uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Informatics Institute</orgName>
								<orgName type="department" key="dep2">Faculty of Science</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse Cooperative Q-learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Learning in multiagent systems suffers from the fact that both the state and the action space scale exponentially with the number of agents. In this paper we are interested in using Q-learning to learn the coordinated actions of a group of cooperative agents, using a sparse representation of the joint state-action space of the agents. We first examine a compact representation in which the agents need to explicitly coordinate their actions only in a predefined set of states. Next, we use a coordination-graph approach in which we represent the Q-values by value rules that specify the coordination dependencies of the agents at particular states. We show how Q-learning can be efficiently applied to learn a coordinated policy for the agents in the above framework. We demonstrate the proposed method on the predator-prey domain, and we compare it with other related multiagent Q-learning methods. in uncertain environments. In principle, it is possible to treat a multiagent system as a &apos;big&apos; single agent and learn the optimal joint policy using standard single-agent reinforcement learning techniques. However, both the state and action space scale exponentially with the number of agents, rendering this approach infeasible for most problems. Alternatively, we can let each agent learn its policy independently of the other agents, but then the transition model depends on the policy of the other learning agents, which may result in oscillatory behavior. On the other hand, in many problems the agents only need to coordinate their actions in few states (e.g., two cleaning robots that want to clean the same room), while in the rest of the states the agents can act independently. Even if these &apos;coordinated&apos; states are known in advance, it is not a priori clear how the agents can learn to act cooperatively in these states. In this paper we describe a multiagent Q-learning technique , called Sparse Cooperative Q-learning, that allows a group of agents to learn how to jointly solve a task when the global coordination requirements of the system (but not the particular action choices of the agents) are known beforehand.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A multiagent system (MAS) consists of a group of agents that can potentially interact with each other <ref type="bibr" target="#b9">(Weiss, 1999;</ref><ref type="bibr" target="#b2">Vlassis, 2003)</ref>. In this paper, we are interested in fully cooperative multiagent systems in which the agents have to learn to optimize a global performance measure. One of the key problems in such systems is the problem of coordination: how to ensure that the individual decisions of the agents result in jointly optimal decisions for the group.</p><p>We first examine a compact representation in which the agents learn to take joint actions in a predefined set of states. In all other (uncoordinated) states, we let the agents learn independently. Then we generalize this approach by using a context-specific coordination graph ( <ref type="bibr" target="#b0">Guestrin et al., 2002b</ref>) to specify the coordination dependencies of subsets of agents according to the current context (dynamically). The proposed framework allows for a sparse representation of the joint state-action space of the agents, resulting in large computational savings.</p><p>Reinforcement learning (RL) techniques <ref type="bibr" target="#b8">(Sutton &amp; Barto, 1998</ref>) have been applied successfully in many single-agent systems for learning the policy of an agent</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MDPs and Q-learning</head><p>In this section, we review the Markov Decision Process (MDP) framework. An observable MDP is a tuple A, T, R where S is a finite set of world states, A is a set of actions, T : S ℅ A ℅ S ↙ [0, 1] is the Markovian transition function that describes the probability p(s ∩ |s, a) of ending up in state s ∩ when performing action a in state s, and R : S ℅ A ↙ IR is a reward function that returns the reward R(s, a) obtained after taking action a in state s. An agent's policy is defined as a mapping 羽 : S ↙ A. The objective is to find an optimal policy 羽 * that maximizes the expected discounted future reward</p><formula xml:id="formula_0">U * (s) = max 羽 E [ ﹢ t=0 污 t R(s t )|羽, s 0 = s] for each state s. The expectation operator E[﹞]</formula><p>averages over reward and stochastic transitions and 污 ﹋ [0, 1) is the discount factor. We can also represent this using Q-values which store the expected discounted future reward for each state s and possible action a:</p><p>and R i : S ℅ A ↙ IR is the reward function that returns the reward R i (s, a) for agent i after the joint action a is taken in state s. As global reward function R(s, a) = n i=1 R i (s, a) we take the sum of all individual rewards received by the n agents. This framework differs from a stochastic game <ref type="bibr" target="#b7">(Shapley, 1953)</ref> in that each agent wants to maximize social welfare (sum of all payoffs) instead of its own payoff.</p><p>Within this framework different choices can be made which affect the problem description and possible solution concepts, e.g., whether the agents are allowed to communicate, whether they observe the selected joint action, whether they perceive the individual rewards of the other agents, etc. In our case we assume that the agents are allowed to communicate and thus are able to share individual actions and rewards. Before we discuss our approach, we first describe two other learning methods for environments with multiple agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">MDP Learners</head><formula xml:id="formula_1">Q * (s, a) = R(s, a)+污 p(s ∩ |s, a) max Q * (s ∩ , a ∩ ). (1) s ∩ a ∩</formula><p>The optimal policy for a state s is the action arg max a Q * (s, a) that maximizes the expected future discounted reward.</p><p>Reinforcement learning (RL) <ref type="bibr" target="#b8">(Sutton &amp; Barto, 1998)</ref> can be applied to estimate Q * (s, a). Q-learning is a widely used learning method when the transition and reward model are unavailable. This method starts with an initial estimate Q(s, a) for each state-action pair. When an exploration action a is taken in state s, reward R(s, a) is received and next state s ∩ is observed, the corresponding Q-value is updated by</p><formula xml:id="formula_2">Q(s, a) := Q(s, a)+汐[R(s, a)+污 max a ∩ Q(s ∩ , a ∩ )?Q(s, a)]</formula><p>(2) where 汐 ﹋ (0, 1) is an appropriate learning rate. Under conditions, Q-learning is known to converge to the optimal Q * (s, a) <ref type="bibr">(Watkins &amp; Dayan, 1992)</ref>.</p><p>In principle, a collaborative multiagent MDP can be regarded as one large single agent in which each joint action is represented as a single action. The optimal Q-values for the joint actions can then be learned using standard single-agent Q-learning. In order to apply this MDP learners approach a central controller models the complete MDP and communicates to each agent its individual action, or all agents model the complete MDP separately and select the individual action that corresponds to their own identity. In the latter case, no communication is needed between the agents but they all have to observe the joint action and all individual rewards. Moreover, the problem of exploration can be solved by using the same random number generator (and the same seed) for all agents <ref type="bibr" target="#b2">(Vlassis, 2003)</ref>. Although this approach leads to the optimal solution, it is infeasible for problems with many agents since the joint action space, which is exponential in the number of agents, becomes intractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multiagent Q-learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Independent Learners</head><p>The framework discussed in the previous section only involves single agents. In this work, we are interested in systems in which multiple agents, each with their own set of actions, have to collaboratively solve a task. A collaborative multiagent MDP (Guestrin, 2003) extends the single agent MDP framework to include multiple agents whose joint action impacts the state transition and the received reward. Now, the transition model T : S ℅ A ℅ S ↙ [0, 1] represents the probability p(s ∩ |s, a) the system will move from state s to s ∩ after performing the joint action a ﹋ A = ℅ At the other extreme, we have the independent learners (IL) approach <ref type="bibr" target="#b10">(Claus &amp; Boutilier, 1998</ref>) in which the agents ignore the actions and rewards of the other agents in the system, and learn their strategies independently. The standard convergence proof for Qlearning does not hold in this case, since the transition model depends on the unknown policy of the other learning agents. Despite the lack of guaranteed convergence, this method has been applied successfully in multiple cases <ref type="bibr">(Tan, 1993;</ref><ref type="bibr" target="#b5">Sen et al., 1994)</ref>.</p><formula xml:id="formula_3">n i=1 A i ∩ ∩∩ 4. Context-Specific Q-learning s s s R 1 (s, a) R 1 (s ∩ , a)</formula><p>In many problems, agents only have to coordinate their actions in a specific context <ref type="bibr" target="#b0">(Guestrin et al., 2002b</ref>). For example, two cleaning robots only have to take care that they do not obstruct each other when they are cleaning the same room. When they work in two different rooms, they can work independently.</p><formula xml:id="formula_4">Q 1 (s ∩ , a 1 ) A 1 Q 2 (s ∩ , a 2 ) Q(s, a) Q(s ∩∩ , a) R 2 (s, a) R 2 (s ∩ , a) A 2 Q 3 (s ∩ , a 3 ) R 3 (s, a) R 3 (s ∩ , a) A 3</formula><p>In this section, we describe a reinforcement learning method which explicitly models these types of contextspecific coordination requirements. The main idea is to learn joint action values only in those states where the agents actually need to coordinate their actions.</p><p>We create a sparse representation of the joint stateaction space by specifying in which states the agents do (and in which they do not) have to coordinate their actions. During learning the agents apply the IL method in the uncoordinated states and the MDP learners approach in the coordinated states. Since in practical problems the agents typically need to coordinate their actions only in few states, this framework allows for a sparse representation of the complete action space, resulting in large computational savings.  updates its individual Q-value Q i using Eq. (5).</p><formula xml:id="formula_5">Q i (s ∩ , a i ) := (1 ? 汐)Q i (s ∩ , a i ) + 汐 R i (s ∩ , a i ) + 污 1 n max a ∩ Q(s ∩∩ , a ∩ ) .<label>(5</label></formula><p>There are four different situations that must be taken into account. When moving between two coordinated or between two uncoordinated states, we respectively apply the MDP Learners and IL approach. In the case that the agents move from a coordinated state s to an uncoordinated state s ∩ we back up the individual Qvalues to the joint Q-value by</p><formula xml:id="formula_6">Q(s, a) := (1 ? 汐)Q(s, a) + n 汐 R i (s, a) + 污 max Q i (s ∩ , a ∩ i ) . (4) i=1 a ∩ i</formula><p>In terms of implementation, the shared Q-table can be either stored centrally (and the agents should have access to this shared resource) or updated identically by all individual agents. Note that in the latter case the agents rely on (strong) common knowledge assumptions about the observed actions and rewards of the other agents. Furthermore, all agents have to coordinate their actions in a coordinated state. In the remainder of this paper, we will discuss a coordinationgraph approach which is a generalization of the described algorithm in this section. In that framework the coordination requirements are specified over subsets of agents and the global Q-value is distributed among the different agents. Before we discuss this generalized approach, we first review the notion of a context-specific coordination graph.</p><p>Conversely, when moving from an uncoordinated state</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Context-Specific Coordination Graphs</head><formula xml:id="formula_7">A 1 A 1 A 1</formula><p>A context-specific coordination graph (CG) represents a dynamic (context-dependent) set of coordination requirements of a multiagent system ( <ref type="bibr" target="#b0">Guestrin et al., 2002b</ref>). If A 1 , . . . , A n is a group of agents, then a node of the CG in a given context represents an agent A i , while an edge defines a dependency between two agents. Only interconnected agents have to coordinate their actions at any time step. For example, the left graph in <ref type="figure">Fig. 2</ref> </p><note type="other">shows a CG for a 4-agent problem in which agent A 3 has to coordinate with A 2 , A 4 has to coordinate with A 3 , and A 1 has to coordinate with both A 2 and A 3 . If the global payoff function is decomposed as a sum of local payoff functions, a CG replaces a global coordination problem by a number of local coordination problems that involve fewer agents, which can be solved in a distributed manner using a message passing scheme. A 2 A 2 A 2 A 3 A 3 A 4 a1 ＿ a3 ＿ s : 4 a1 ＿ a2 ＿ s : 5 a2 ＿ s : 2 a3 ＿ a2 ＿ s : 5 a3 ＿ a4 ＿ s : 10 a1 ＿ a3 : 4 a1 ＿ a2 : 5 a2 : 2 a3 ＿ a2 : 5 a1 ＿ a2 : 5 a2 : 2 a2</note><p>: 5 a2 ＿ a1 : 4 <ref type="figure">Figure 2</ref>. Initial CG (left), after conditioning on the context s = true (center), and after elimination of A3 (right).</p><p>In ( <ref type="bibr" target="#b0">Guestrin et al., 2002b</ref>) the global payoff function is distributed among the agents using a set of value rules. These are propositional rules in the form c : v where c (the context) is an element from the set of all possible combinations of the state and action variables c ﹋ C ? S ﹍ A, and 老(c) = v ﹋ IR is a payoff that is added to the global payoff when c holds. By definition, two agents are neighbors in the CG if and only if there is a value rule c : v that contains the actions of these two agents in c. Clearly, the set of value rules form a sparse representation of the global payoff function since not all state and action combinations have to be defined.</p><p>on the context (center <ref type="figure">figure)</ref>, the agents are eliminated from the graph one by one. Let us assume that we first eliminate A 3 . This agent first collects all rules in which it is involved, these are 1 ＿a 3 : 4 3 ＿a 2 : 5 Next, for all possible actions of A 1 and A 2 , agent A 3 determines its conditional strategy, in this case equal to 2 : 5 1 ＿ a 2 : 4 and is then eliminated from the graph. The algorithm continues with agent A 2 which computes its conditional strategy 1 : 11 1 : 5 and is then also eliminated. Finally, A 1 is the last agent left and fixes its action to a 1 . Now a second pass in the reverse order is performed, where each agent distributes its strategy to its neighbors, who then determine their final strategy. This results in the optimal joint action {a 1 , a 2 , a 3 } with a global payoff of 11. <ref type="figure">Fig. 2</ref> shows an example of a context-specific CG, where for simplicity all actions and state variables are assumed binary</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Sparse Cooperative Q-learning</head><p>1 . In the left graph we show the initial CG together with the corresponding set of value rules. Note that agents involved in the same rules are neighbors in the graph. In the center we show how the value rules, and therefore the CG, are updated after the agents condition on the current context (the state s = true). Based on this information about state s, rule 老 5 is irrelevant and is removed. As a consequence, the optimal joint action is independent of A 4 and its edge is deleted from the graph as shown in the center of <ref type="figure">Fig. 2</ref>.</p><p>The method discussed in section 4 defined a state either as a coordinated state in which all agents coordinate their actions, or as an uncoordinated state in which all agents act independently. However, in many situations only some of the agents have to coordinate their actions. In this section we describe Sparse Cooperative Q-learning which allows a group of agents to learn how to coordinate based on a predefined coordination structure that can differ between states.</p><p>In order to compute the optimal joint action (with maximum total payoff) in a CG, a variable elimination algorithm can be used which we briefly illustrate in the example of <ref type="figure">Fig. 2</ref>. After the agents have conditioned As in (Guestrin et al., 2002b) we begin by distributing the global Q-value among the different agents. Every agent i is associated with a local value function Q i (s, a) which only depends on a subset of all possible state and action variables. The global Q-value equals the sum of the local Q-values of all n agents:</p><formula xml:id="formula_8">n Q(s, a) = Q i (s, a).<label>(6)</label></formula><p>1 Action a1 corresponds to a1 = true and action a1 to a1 = false.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>i=1</head><p>Suppose that an exploration joint action a is taken from state s, each agent receives reward R i (s, a), and next state s ∩ is observed. Based on the decomposition (6) the global Q-learning update rule now reads value rules:</p><formula xml:id="formula_9">s s ∩ Q 1 (s, a 1 ) R 1 (s, a) Q 1 (s ∩ , a 1 , a 2 ) A 1 n n n Q i (s, a) := R i (s, a)+ i=1 i=1 Q i (s, a) + 汐 i=1 Q 2 (s ∩ , a 1 , a 2 ) R 2 (s, a) n Q 2 (s, a 2 , a 3 ) A 2 污 max a ∩ Q(s ∩ , a ∩ ) ? Q i (s, a) .<label>(7</label></formula><formula xml:id="formula_10">Q i (s, a) := Q i (s, a)+汐[R i (s, a)+污Q i (s ∩ , a * )?Q i (s, a)].<label>(8)</label></formula><formula xml:id="formula_11">1 ; a 1 ＿ s : v 1 2 ; a 1 ＿ a 2 ＿ s ∩</formula><p>We still have to discuss how the local Q-functions are represented. In our notation, we use the value rule representation of section 5 to specify the coordination requirements between the agents for a specific state. This is a much richer representation than the IL-MDP variants since it allows us to represent all possible dependencies between the agents in a context-specific manner. Every Q i (s, a) depends on those value rules that are consistent with the given state-action pair (s, a) and in which agent i is involved:</p><formula xml:id="formula_12">: v 2 3 ; a 1 ＿ a 2 ＿ s ∩ : v 3 4 ; a 1 ＿ a 2 ＿ s : v 4 5 ; a 2 ＿ a 3 ＿ s : v 5 6 ; a 3 ＿ s ∩ : v 6 i Q i (s, a) = 老 j (s, a) n j ,<label>(9)</label></formula><p>j Furthermore, assume that a = {a 1 , a 2 , a 3 } is the performed joint action in state s and a * = {a 1 , a 2 , a 3 } is the optimal joint action found with the variable elimination algorithm in state s ∩ . After conditioning on the context, the rules 老 1 and 老 5 apply in state s, whereas the rules 老 3 and 老 6 apply in state s ∩ . This is graphically depicted in <ref type="figure">Fig. 3</ref>. Next, we use Eq. (10) to update the value rules 老 1 and 老 5 in state s as follows:</p><p>where n j is the number of agents (including agent i) involved in rule 老 <ref type="figure">a)</ref> can be regarded as a linear expansion into a set of basis functions 老</p><formula xml:id="formula_13">老 1 (s, a) = v 1 + 汐[R 1 (s, a) + 污 v 3 2 ? v 1 1 ] i j . Such a representation for Q i (s,</formula><formula xml:id="formula_14">老 5 (s, a) = v 5 + 汐[R 2 (s, a) + 污 v 3 2 ? v 5 2 + i j</formula><p>, each of them peaked on a specific state-action context which may potentially involve many agents. The 'weights' of these basis functions (the rules' values) can then be updated as follows:</p><formula xml:id="formula_15">R 3 (s, a) + 污 v 6 1 ? v 5 2 ]. nj 老 j (s, a) := 老 j (s, a) + 汐 [R i (s, a)+</formula><p>Note that in order to update 老 5 we have used</p><formula xml:id="formula_16">the (discounted) Q-values of Q 2 (s ∩ , a * ) = v 3 /2 and Q 3 (s ∩ , a * ) = v 6 /1. Furthermore, the component Q 2 in state s</formula><p>∩ is based on a coordinated action of agent A 2 with agent A 1 (rule 老 3 ), whereas in state s agent A 2 has to coordinate with agent A 3 (rule 老 5 ).</p><formula xml:id="formula_17">i=1 污Q i (s ∩ , a * ) ? Q i (s, a)] (10)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>where we add the contribution of each agent involved in the rule.</p><p>In this section, we apply our method to a predatorprey problem in which the goal of the predators is to capture a prey as fast as possible in a discrete gridlike world <ref type="bibr" target="#b2">(Kok &amp; Vlassis, 2003)</ref>. We concentrate on</p><p>As an example, assume we have the following set of situation of <ref type="figure" target="#fig_2">Fig. 4</ref> looks as ; prey(0, ?1) ＿ pred(1, ?1) ＿ a 1 = move none ＿ a 2 = move west : 75 a coordination problem in which two predators in a 10℅10 toroidal grid have to capture a single prey. Each agent can move to one of its adjacent cells or remain on its current position. The prey is captured when both predators are located in an adjacent cell to the prey and only one of the two agents moves to the location of the prey. A possible capture situation is depicted in <ref type="figure" target="#fig_2">Fig. 4</ref>. When the two predators move to the same cell or a predator moves to the prey position without a nearby predator, they are penalized and placed on random positions on the field. The policy of the prey is fixed: it stays on its current position with a probability of 0.2, in all other cases it moves to one of its free adjacent cells with uniform probability.</p><p>This results in the generation of 31,695 value rules for the first predator (31,200 for the 1,248 coordinated states and 495 for the 99 uncoordinated states 2 ). The second predator holds only a set of 495 rules for the uncoordinated states since its action is based on the rules from the other predator in the coordinated states.</p><p>During learning we use Eq. (10) to update the payoffs of the rules. Each predator i receives a reward R i = 37.5 when it helps to capture the prey and a negative reward of ?50.0 when it collides with another predator. When an agent moves to the prey without support the reward is ?5.0. In all other cases the reward is ?0.5. We use an ?-greedy exploration step of 0.2, a learning rate 汐 of 0.3, and a discount factor 污 of 0.9.</p><p>The complete state-action space for this problem consists of all combinations of the two predator positions relative to the prey and the joint action of the two predators (almost 250,000 states). However, in many of these states the predators do not have to coordinate their actions. Therefore, we first initialize each predator with a set of individual value rules which do not include the state and action of the other predator. An example rule is defined as We compare our method to the two Q-learning methods mentioned in section 2. In case of the independent learners, each Q-value is derived from a state that consists of both the position of the prey and the other predator and one of the five possible actions. This corresponds to 48, 510 (= 99 ﹞ 98 ﹞ 5) different state-action pairs for each agent. For the MDP Learners we model the system as a complete MDP with the joint action represented as a single action. In this case, the number of state action-pairs equals 242, 550 (= 99 ﹞ 98 ﹞ 5 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">1</head><p>; prey(?3, ?3) ＿ a 1 = move none : 75</p><p>The payoff of all value rules are initialized with a value of 75 which corresponds to the maximal reward at the end of an episode. This ensures that the predators explore all possible action combinations sufficiently. Next, the specific coordination requirements between the two predators are added. Since the predators only have to coordinate their actions when they are close to each other, we add extra value rules, depending on the joint action, for the following situations: <ref type="figure" target="#fig_4">Fig. 5</ref> shows the capture times for the learned policy during the first 500,000 episodes for the different methods. The results are generated by running the current learned policy after each interval of 500 episodes five times on a fixed set of 100 starting configurations. During these 500 test episodes no exploration actions were performed. This was repeated for 10 different runs. The 100 starting configurations were selected randomly beforehand and were used during all 10 runs.</p><p>? the (Manhattan) distance to the other predator is smaller or equal than two cells</p><p>? both predators are within a distance of two cells to the prey Both the independent learners and our proposed method learn quickly in the beginning with respect to the MDP learners since learning is based on fewer state-action pairs. However, the independent learners do not converge to a single policy but keep oscillating. This is caused by the fact that they do not take the action of the other agent into account. When both predators are located next to the prey and one predator moves to the prey position, this predator is not able to distinguish between the situation where the other 2 Note that creating value rules based on the full state information, and only decomposing the action space, would result in 8,454 (= 99 ﹞ 98 ? 1, 248) uncoordinated states</p><p>The value rule for which the prey is captured in the  other, but already coordinating in these states might have a positive influence on the final result. These constraints could be added as extra value rules, but then the learning time would increase with the increased state-action space. Clearly, a trade-off exists between the expressiveness of the model and the learning time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Discussion and Conclusions</head><p>predator remains on its current position or performs one of its other actions (e.g., an exploration action).</p><p>In the first case a positive reward is returned, while in the second case a large negative reward is received. However, in both situations the same Q-value is updated.</p><p>These coordination dependencies are explicitly taken into account for the two other approaches. For the MDP learners, they are modeled in every state which results in a slowly decreasing learning curve; it takes longer before all state-action pairs are explored. The context-specific approach has a quicker decreasing learning curve since only joint actions are considered for these coordinated states. As we we see from <ref type="figure" target="#fig_4">Fig. 5</ref>, both methods result in an almost identical policy.</p><p>In this paper we discussed a Q-learning approach for cooperative multiagent systems that is based on context-specific coordination graphs, and in which value rules specify the coordination requirements of the system for a specific context. These rules can be regarded as a sparse representation of the complete state-action space, since they are defined over a subset of all state and action variables. The value of each rule contributes additively to the global Q-value and is updated based on a Q-learning rule that adds the contribution of all involved agents in the rule. Effectively, each agent learns to coordinate only with its neighbors in a dynamically changing coordination graph. Results in the predator-prey domain show that our method improves the learning time of other multiagent Q-learning methods, and performs comparable to the optimal policy. <ref type="table">Table 1</ref> shows the average capture times for the different approaches for the last 10 test runs from <ref type="figure" target="#fig_4">Fig. 5</ref> and a manual implementation in which both predators first minimize the distance to the prey and then wait till both predators are located next to the prey. When both predators are located next to the prey, social conventions based on the relative positioning are used to decide which of the two predators moves to the prey position.</p><p>The context-specific learning approach converges to a slightly higher capture time than that of the MDP Learners. An explanation for this small difference is the fact that not all necessary coordination requirements are added as value rules. In our construction of value rules we assume that the agents do not have to coordinate when they are located far away from each Our approach is closely related to the coordinated reinforcement learning approach of <ref type="bibr" target="#b12">(Guestrin et al., 2002a</ref>). In their approach the global Q-value is also represented as the sum of local Q-functions, and each local Q-function assumes a parametric function representation. The main difference with our work is that they update the weights of each local Q-value (of each agent) based on the difference between the global Qvalues (over all agents) of the current and (discounted) next state (plus the immediate rewards). In our approach, the update of the Q-function of an agent is based only on the rewards and Q-values of its neighboring agents in the graph. This can be advantageous when subgroups of agents need to separately coordinate their actions. From this perspective, our local Q-learning updates seem closer in spirit to the local Sarsa updates of <ref type="bibr" target="#b3">(Russell &amp; Zimdars, 2003)</ref>.</p><p>Another related approach is the work of ( <ref type="bibr" target="#b4">Schneider et al., 1999</ref>) in which each agent updates its local Qvalue based on the Q-value of its neighboring nodes. A weight function f (i, j) determines how much the Qvalue of an agent j contributes to the update of the Q-value of agent i. Just as in our approach, this function defines a graph structure of agent dependencies. However, these dependencies are fixed throughout the learning process (although they mention the possibility of a dynamically changing f ). Moreover, in their approach Q-learning involves back-propagating averages of individual Q-values, whereas in our case Qlearning involves back-propagating individual components of joint Q-values. We applied their distributed value function approach on our predator-prey problem with a weighting function that averaged the value evenly over the two agents. However, the policy did not converge and oscillated around an average capture time of 33.25 cycles since the agents also affect each other in the uncoordinated states. For instance, an agent ending up in a low-valued state after taking an exploratory action influences the individual action taken by the other agent negatively. ordinated reinforcement learning. Proc. 19th Int.</p><p>Conf. on Machine Learning. Sydney, Australia.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Graphical representation of the Q-tables in the case of three agents A1, A2, and A3. State s and s ∩∩ are coordinated states, while state s ∩ is an uncoordinated state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Possible capture position for two predators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>25</head><label>25</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Capture times for the learned policy for the four different methods during the first 500,000 episodes. Results are averaged over 10 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>)</head><label></label><figDesc></figDesc><table>Because of the distinction in action types for different 
states, we also have to distinguish between different 
representations for the Q-values. Each agent i main-
tains a single-action value table Q i (s, a i ) for the un-
coordinated states, and one joint action value table 
Q(s, a) for the coordinated states. In the coordinated 
states the global Q-value Q(s, a) directly relates to the 
shared joint Q-table. In the uncoordinated states, we 
assume that the global Q-value is the sum of all indi-
vidual Q-values: 

That is, in this case each agent is rewarded with the 
same fraction of the expected future discounted reward 
from the resulting coordinated state. This essentially 
implies that each agent contributes equally to the co-
ordination. 

n 

Q(s, a) = 

Q i (s, a i ). 
(3) 

i=1 

Fig. 1 shows a graphical representation of the tran-
sition between three states for a problem involving 
three agents. In state s the agents have to coordi-
nate their actions and use the shared Q-table to de-
termine the joint action. After taking the joint action 
a and observing the transition to the uncoordinated 
state s 
∩ , the joint action Q-value Q(s, a) is updated 
using Eq. (4). Similarly, in s 
∩ each agent i chooses 
its action independently and after moving to state s 

∩∩ 

When the agents observe a state transition, values 
from the different Q-tables are combined in order to 
update the Q-values. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Tan, M. <ref type="bibr">(1993)</ref>. Multi-agent reinforcement learning: Independent vs. cooperative agents. Proc. 10th Int.</p><p>Conf. on Machine Learning. Amherst, MA.</p><p>We would like to thank the three reviewers for their detailed and constructive comments. This research is supported by PROGRESS, the embedded systems research program of the Dutch organization for Scientific Research NWO, the Dutch Ministry of Economic Affairs and the Technology Foundation STW, project AES 5414. <ref type="bibr" target="#b2">Vlassis, N. (2003)</ref>. A concise introduction to multiagent systems and distributed AI.</p><p>Informatics Institute, University of Amsterdam. http://www.science.uva.nl/?vlassis/cimasdai. <ref type="bibr">Watkins, C., &amp; Dayan, P. (1992)</ref>. Technical note: Qlearning. Machine Learning, 8, 279-292.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Context-specific multiagent coordination and planning with factored MDPs</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Nation. Conf. on Artificial Intelligence</title>
		<meeting>8th Nation. Conf. on Artificial Intelligence<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The pursuit domain package</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
		<idno>IAS-UVA-03-03</idno>
	</analytic>
	<monogr>
		<title level="m">formatics Institute</title>
		<meeting><address><addrLine>University of Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Q-decomposition for reinforcement learning agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Zimdars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning</title>
		<meeting>the 20th International Conference on Machine Learning<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distributed value functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning</title>
		<meeting>Int. Conf. on Machine Learning<address><addrLine>Bled, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to coordinate without sharing information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th</title>
		<meeting>12th</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">There are several directions for future work. In our current implementation we have assumed that all agents contribute equally to the rules in which they are involved (see Eq</title>
	</analytic>
	<monogr>
		<title level="m">Nation. Conf. on Artificial Intelligence</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>We would like to investigate the consequence of this choice. Furthermore, we would like to apply our approach to continuous domains with more agent dependencies. and investigate methods to learn the coordination requirements automatically</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stochastic games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1095" to="1100" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multiagent systems: a modern approach to distributed artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The dynamics of reinforcement learning in cooperative multiagent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Claus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Nation. Conf. on Artificial Intelligence</title>
		<meeting>15th Nation. Conf. on Artificial Intelligence<address><addrLine>Madison, WI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Planning under uncertainty in complex structured environments. Doctoral dissertation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, Stanford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lagoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
