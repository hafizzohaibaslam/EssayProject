<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-07-17T00:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Social learning with heterogeneous agents and sequential decision making</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Elsevier BV</publisher>
				<availability status="unknown"><p>Copyright Elsevier BV</p>
				</availability>
				<date type="published" when="2015">2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><forename type="middle">M</forename><surname>Djuri?</surname></persName>
						</author>
						<title level="a" type="main">Social learning with heterogeneous agents and sequential decision making</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Digital Signal Processing</title>
						<title level="j" type="abbrev">Digital Signal Processing</title>
						<idno type="ISSN">1051-2004</idno>
						<imprint>
							<publisher>Elsevier BV</publisher>
							<biblScope unit="volume">47</biblScope>
							<biblScope unit="page" from="17" to="24"/>
							<date type="published" when="2015">2015</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.dsp.2015.02.016</idno>
					<note>Contents lists available at ScienceDirect Digital Signal Processing a r t i c l e i n f o a b s t r a c t Article history:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>Social learning Bayesian learning Sequential decision making Irrational agents Random/deterministic decision making</keywords>
			</textClass>
			<abstract>
				<p>In this paper, we consider the problem of social learning in a network of agents where the agents make decisions sequentially by choosing one of two hypotheses on the state of nature. Each agent observes a signal generated according to one of the hypotheses and knows the decisions of all the previous agents in the network. The network contains two types of agents: rational and irrational. A rational agent makes a decision by not only using its private observation but also the decisions of each of the agents which already made decisions. To that end, the agent employs Bayesian theory. An irrational agent makes a decision by ignoring the available information and by randomly choosing the hypothesis. We analyze the asymptotic performance of a system with rational and irrational agents where we study rational agents that use either a deterministic or random decision making policies. We propose a specific random decision making policy that is based on the social belief and the private signals of the agents. We prove that under mild conditions the expected social belief in the true state of nature tends to one if the rational agents use the proposed random policy. In a network with rational agents that use deterministic policy, the conditions for convergence are stricter. We provide simulation results on the studied systems and compare their performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>An important issue in social learning is how agents make decisions and learn from the decisions of other agents. When agents make decisions based on private and imperfect information, it is natural that they also use information in the decisions of other agents made in a similar situation. In this work, we consider agents that make their decisions on one of two hypotheses H 0 and H 1 one at a time in a sequential manner. When an agent makes its own decision, it broadcasts it to all the agents that will subsequently be making decisions. The graph that describes our system is a directed graph shown in <ref type="figure">Fig. 1</ref>. We note that the motivation for studying this network is understanding some basics of social learning with random decision making rather than a particular application of signal processing.</p><p>Applications of sequential decision making can be found in many fields including distributed detection in wireless sensor networks. There, the need for low-cost and low-power devices forces every sensor to aggregate all its information into a one bit decision <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. In <ref type="bibr" target="#b2">[3]</ref>, a comprehensive introduction to this problem was provided. Another major application is the understanding of social learning in multi-agent systems. In these systems, the agents make decisions not only from their private observations, but also from the decisions made by others <ref type="bibr" target="#b3">[4]</ref>. As indicated in <ref type="bibr" target="#b4">[5]</ref>, the study of social learning addresses the problems of modeling the interaction of the agents in the network while learning takes place.</p><p>In social learning systems, an agent can learn from the decisions of agents that already made them by using either nonBayesian approaches or the Bayesian methodology. For example, in <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, non-Bayesian social learning methods were studied. In <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>, the agents had Bayesian social learning strategies in obtaining information from previous decisions. Although the Bayesian learning methods provide a fast convergence rate to one of the hypotheses, the agents that employ the Bayesian machinery may ignore their private observations and herd on the wrong decision. Some work on this issue can be found in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>, where the interest was in studying herding behavior and information cascades. In <ref type="bibr" target="#b10">[11]</ref>, the authors provided the sufficient conditions for asymptotic learning in Bayesian social learning systems. In <ref type="bibr" target="#b15">[16]</ref>, the author studied this problem from the perspective of quickest time change detection. Recently, random decision making was addressed in <ref type="bibr" target="#b16">[17]</ref>. In <ref type="bibr" target="#b17">[18]</ref>, the authors considered the effect of the decisions of subsequent agents on the utility of a current agent by using a Chinese restaurant game model. They proposed an optimal strat-egy for decision making. This approach was applied to cognitive radio networks <ref type="bibr" target="#b18">[19]</ref>. Another extension of this problem was a scenario with noisy links <ref type="bibr" target="#b19">[20]</ref>, where the decisions were broadcasted through a noisy channel and thereby the decisions could be randomly flipped or erased. An overview of models and techniques for studying social learning can be found in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>. In this paper, our study is within the framework of Bayesian social learning.</p><p>In the analysis of social learning systems, one typical question is the following: Does the probability of error in decision making converge to zero as the number of agents goes to infinity? The answer depends on (a) whether the log-likelihood ratio of private observations is bounded and (b) whether the memories of the agents are finite (the size of the memory of an agent is the number of decisions of other agents available to the agent). In <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>, the problem with finite memory was addressed. In <ref type="bibr" target="#b20">[21]</ref>, a system where each agent could only observe the decision of its immediate previous agent was investigated. It was shown that if the likelihood ratio test is employed for decision making, the error probability does not converge to zero if the ratio is bounded. In <ref type="bibr" target="#b21">[22]</ref>, social learning with sampling of past decisions was addressed. There, each agent made inference by the Bayesian method and using very few samples from the decision history of the agents. In the case of bounded log-likelihood ratios, a non-Bayesian decision policy was proposed whose probability of decision error converges to zero <ref type="bibr" target="#b22">[23]</ref>. In <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>, the agents had infinite memory, and the conditions for asymptotic learning were listed.</p><p>In the literature, it is common to assume that the agents know the decision making policy of the other agents, or that the policy is identical for every agent (e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20]</ref>). In this work, we address the problem of social learning in a system of heterogeneous agents. The agents can be either rational or irrational and they make the decisions sequentially, one at a time. A rational agent makes a decision according to the posterior of the hypotheses (conditioned on the decisions of the previous agents and its private observation). By contrast, an irrational agent simply ignores all its available information and makes a decision by flipping a fair coin. The agents do not know which of them are rational and irrational but they know the percentage of irrational agents in the network.</p><p>More related work on social learning with heterogeneous agents can be found in <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>. In <ref type="bibr" target="#b24">[25]</ref>, the problem of misinformation spread was addressed, where the social system was modeled by a random network and the agents used a gossip style method to update their beliefs from the exchanged information. In <ref type="bibr" target="#b25">[26]</ref>, with a binary voter model, the authors studied the effect of stubborn agents in social systems. There, the stubborn agents are agents that never change their beliefs. In <ref type="bibr" target="#b26">[27]</ref>, the authors discussed the relationship between the convergence rate of an average-based learning method and the homophily of the multitype random networks. In all of these papers, however, there is no randomness in the decisions making.</p><p>In this paper, we study two cases. In the first, the rational agents for decision making use a deterministic policy whereas in the second, they employ a random policy. In <ref type="bibr" target="#b16">[17]</ref>, we proposed a random decision making policy and compared it to the deterministic policy. In that comparison, all the agents were rational. We showed that with the deterministic policy, the probability of decision error may fail to converge to zero due to an information cascade. On the contrary, with the random policy, the probability of decision error is guaranteed to converge to zero. In this work, because of the presence of irrational agents, this probability does not converge to zero. However, if an agent is rational, its probability of a decision error converges to zero. Furthermore, if the social belief is defined by the posterior on the true state of nature conditioned on all the known decisions, then we show that it always converges to one in probability if the rational agents adopt the random decision making policy. This is not the case when they <ref type="figure">Fig. 1</ref>. Social learning in a sequential decision making system. The symbol y n is the private signal of agent A n , and ¦Á n is the decision of this agent.</p><p>use the deterministic policy. In summary, the main contributions of this paper are (1) a proof that the presence of irrational agents does not affect the asymptotic probability of the rational agents if they adopt random policy and (2) a proof that in a system with random agents herding does not take place.</p><p>The paper is organized as follows. In the next section we describe the models of the sequential system and explain the social learning process. In Section 3, we introduce both the deterministic and the random decision making policies. The analyses of the convergence of the social belief and the probability of decision error are provided in Section 4. Simulation results are presented in Section 5, and concluding remarks in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem formulation</head><p>The process of social learning is illustrated in <ref type="figure">Fig. 1</ref>. We consider the decision making problem in networks of agents A n , n ¡Ê N + , where the agents make decisions and broadcast them sequentially one at a time to all the subsequent agents. Mathematically, each agent A n receives an independent private observation y n that is generated according to one of the following two hypotheses:</p><formula xml:id="formula_0">H 1 : y n ? ¦Õ 1 (y n ) H 0 : y n ? ¦Õ 0 (y n ),<label>(1)</label></formula><p>where ¦Õ k (y n ) denotes the distribution of observation y n under H k , ?k ¡Ê {0, 1}. We assume that these two distributions are known to all the agents and that each agent has noninformative prior probabilities on the hypotheses. Thus, we have p(</p><formula xml:id="formula_1">H 0 ) = p(H 1 ) = 1/2. Let log ¦Õ 1 (y n ) ¦Õ 0 (y n )</formula><p>be the log-likelihood ratio (LLR) of the hypotheses. We discriminate two types of LLRs, bounded and unbounded. An LLR is bounded if there exist two finite real numbers m and</p><formula xml:id="formula_2">M, such that ? y n , log ¦Õ 1 (y n ) ¦Õ 0 (y n ) ¡Ê [m, M]. Otherwise, the LLR is unbounded.</formula><p>In the network, there is a positive probability that an agent A n behaves irrationally. An irrational agent A n , makes its decision by drawing from a Bernoulli distribution with parameter ¦Ç ¡Ê (0, 1),</p><formula xml:id="formula_3">i.e., ¦Á n ? Ber(¦Ç),<label>(2)</label></formula><p>where ¦Á n denotes the decision made by A n and ¦Á n ¡Ê A = {0, 1}.</p><p>Let I n be an indicator function taking a value of one if the agent A n is irrational; otherwise, it is zero. Then we set that p(I n = 1) = ¦Î &lt; 1, ?n ¡Ê N + . In the rest part of this paper, we assume that both ¦Ç and ¦Î are known by all the agents in the network.</p><p>Here we remark that the model in (2) is just one possible formulation of irrational behavior. In our work, an irrational agent is an agent that ignores all the available information to it and makes decision randomly according to a certain law. The simplicity of this irrational behavior model notwithstanding, allows us to demonstrate very interesting asymptotical properties of the studied system, which are of primary interest here.</p><p>The rational agents, on the other hand, make their decisions by not only using their private observations, but also learning from where q k denotes the probability that a rational agent A n makes a decision one given that H k is true, i.e.,</p><formula xml:id="formula_4">q k = p(¦Á n = 1|¦Á 1:n?1 , H k , I n = 0)</formula><p>others' decisions. Let ¦Á 1:n be the decision sequence from agent A 1 ¡Þ to agent A n . Then the agent A n can formulate its private belief in H 1 , ¦Â n = p(H 1 |¦Á 1:n?1 , y n ), by using Bayes' rule given by the following equation, ?n ¡Ê N + :</p><formula xml:id="formula_5">= p(¦Á n = 1|¦Á 1:n?1 , y n , I n = 0)p( y n |H k )dy n . (9) ?¡Þ ¦Â n = 1 , (3) 1 + (1 ? ¦Ð n?1 )¦Õ 0 (y n ) ¦Ð n?1 ¦Õ 1 (y n )</formula><p>where we define ¦Ð n to be the social belief in H 1 conditioned on In the proposed system, an irrational agent A n is active only when it is its turn to make a decision, which is made according to <ref type="bibr" target="#b1">(2)</ref>. On the other hand, a rational agent A n behaves as follows. While it is following the decisions of the previous agents, it calculates the action likelihoods l <ref type="formula" target="#formula_9">(0)</ref> (1) t and l t (t &lt; n) and updates the the action sequence until and including agent A n , i.e., social belief ¦Ð t?1 to ¦Ð t by (5). When t = n, A n observes y n , and</p><formula xml:id="formula_6">¦Ð n = p(H 1 |¦Á 1:n ), ?n ¡Ê N + ,<label>(4)</label></formula><p>with ¦Ð 0 being defined by 1/2. We note that the private belief ¦Â n it calculates its private belief by (3). Finally, it makes its decision according to <ref type="bibr" target="#b6">(7)</ref>. This sequential learning algorithm of the rational agents belongs to the class of "social learning filters" <ref type="bibr" target="#b9">[10]</ref>.</p><p>represents the posterior on H 1 conditioned on all the information available to A n . Furthermore, the social belief ¦Ð n?1 serves as the Algorithm 1: The behavior of a rational agent A n .</p><p>prior knowledge of A n before it receives its private observation y n .</p><p>Initialize ¦Ð 0 = 0.5.</p><p>In this system, once A n makes its decision ¦Á n , all the following agents should update the social belief by Bayes' rule from ¦Ð n?1 to ¦Ð n . Recall that the studied network is composed of heterogeneous agents, and therefore when the decision ¦Á n is broadcasted, the folwhile t ¡Ü n do if t &lt; n then A n observes the decision made by agent A t ;</p><p>A n calculates the action likelihoods l A n updates the social belief ¦Ð t?1 to ¦Ð t by (5);</p><p>lowing agents do not know if the decision is made by a rational or an irrational agent. Thus, in updating the social belief, this has to be taken into consideration, which is done by marginalizing the hidden random variable I n . By the definition in (4), we have that the social belief is updated according to else A n calculates its private belief by (3) after observing y n ; A n makes its decision according to <ref type="formula">(7)</ref>;</p><formula xml:id="formula_7">end end (1) 1?¦Á n ¦Ð n = ¦Ð n?1 (1 ? l n ) 3. Decision making policies ¦Ð n?1 (1 ? l (1) n ) + (1 ? ¦Ð n?1 )(1 ? l (0) n ) (1) ¦Á n ¡Á ¦Ð n?1 l n ¦Ð n?1 l<label>(1)</label></formula><p>,</p><formula xml:id="formula_8">n + (1 ? ¦Ð n?1 )l<label>(5)</label></formula><p>n where depending on the action ¦Á n (zero or one), the updated ¦Ð n</p><p>In this paper, we address two decision making policies of rational agents and analyze the asymptotic performance of systems with such agents. Namely, we study heterogeneous systems composed of rational and irrational agents, where the rational agents follow either deterministic or random decision making policies.</p><p>is either equal to the first or second factor, respectively. In both cases, we have that ?k ¡Ê {0, 1},</p><formula xml:id="formula_10">3.1. Deterministic policy l (k) n = p(¦Á n = 1|¦Á 1:n?1 , H k ) 1</formula><p>A common deterministic decision making policy of an agent A n is defined by <ref type="bibr" target="#b3">[4]</ref></p><formula xml:id="formula_11">, = p(¦Á n = 1|¦Á 1:n?1 , H k , I n )p(I n ) (6) I n =0 ¦Á n = 1, if ¦Â n &gt; 0.5, 0, otherwise.<label>(10)</label></formula><p>denotes the probability of agent A n making decision ¦Á n = 1 given the decision sequence up to ¦Á n?1 and the true state of nature being H k . As in <ref type="bibr" target="#b4">[5]</ref>, we refer to l (k) n as action likelihood. Due to the possibility that one decision can be made by an irrational agent, the action likelihood is obtained by using the total probability theIf we set the reward to one when the agent makes a decision identical to the true hypothesis, and zero otherwise, by the above rule, the expected utility of A n is maximized <ref type="bibr" target="#b3">[4]</ref>. We call this policy deterministic because once the private belief ¦Â n has been formed, the orem. For n = 1, we set l (k) decision ¦Á n is made deterministically. In the literature, such agents</p><formula xml:id="formula_12">1 = p(¦Á 1 = 1|H k ).</formula><p>are called myopic.</p><p>A rational agent A n , after obtaining its private belief by the Bayes' rule in (3), makes a decision according to some policy. We</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Random policy</head><p>describe the policy as a random map of the private belief ¦Â n to the action space A = {0, 1}. Mathematically, the agent makes its decision from a certain distribution conditioned on ¦Â n , i.e., ¦Á n ? p(¦Á n |¦Â n ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(7)</head><p>An alternative to the deterministic policy is a random policy. In <ref type="bibr" target="#b16">[17]</ref>, we defined a random policy where the decision of the agent is drawn from a Bernoulli distribution defined by We note that, both the deterministic and random decision making can formally be described by <ref type="bibr" target="#b6">(7)</ref>.</p><formula xml:id="formula_13">p(¦Á n |¦Â n ) = ¦Â n , if ¦Á n = 1, 1 ? ¦Â n , otherwise.<label>(11)</label></formula><p>Given the decision making policy p(¦Á n |¦Â n ) and the known value of ¦Î , the agents can obtain the action likelihood by marginalThus, the decision ¦Á n of A n is a Bernoulli random variable parameterized by the A n 's private belief ¦Â n . In social systems, the actions izing out y n . It is given by of the agents are sometimes naturally or by design random. Although these agents do not maximize an explicit utility function, we still call them rational.</p><formula xml:id="formula_14">l (k) n = ¦Ç¦Î + q k (1 ? ¦Î), ?k ¡Ê {0, 1},<label>(8)</label></formula><formula xml:id="formula_15">4. Analysis Z = ¦Ð n?1 (1 ? l (1) n ) (0) (1) (0)</formula><p>In the proposed social learning system, ¦Ð n (¦Á 1:n ) = p(H 1 |¦Á 1:n ), that is ¦Ð n is a function defined by the action sequence ¦Á 1:n (there is a deterministic mapping from ¦Á 1:n to ¦Ð n ). In the analysis that</p><formula xml:id="formula_16">+ (1 ? ¦Ð n?1 )(1 ? l n ) ¦Ð n?1 l n + (1 ? ¦Ð n?1 )l n .</formula><p>Noting that ¦Ð n ¡Ê [0, 1], and l</p><formula xml:id="formula_17">(k)</formula><p>follows, without loss of generality, we assume that the true hypothesis is H 1 . We will also simplify the notation and will express n ¡Ê [0, 1], then it can be shown that (¦Á 1:n ) ¡Ý 0.</p><p>Because p(¦Á 1:n?1 |H 1 ) ¡Ý 0, ?¦Á 1:n?1 ¡Ê A n?1 , we have proved ¦Ð n (¦Á 1:n ) as ¦Ð n . Considering that ¦Á 1:n is random due to the random observations and the random policy if it is applied, ¦Ð n is a discrete random process. Since ¦Ð n reflects the performance of the system that the expected social belief is non-decreasing. 2</p><p>Considering the symmetric structure of the proposed system, at n, we want to describe it in statistical terms. To that end we use E ¦Ð n , which is given by we can make a similar claim about the behavior of E ¦Ð n when H 0 is true. Then, the sequence E ¦Ð n is monotonically non-increasing.</p><formula xml:id="formula_18">E ¦Ð n = ¦Ð n p(¦Ð n )</formula><p>One can prove this by repeating the same process as in the above proof with a few notational changes.</p><p>¦Ð n By Theorem 1 and the fact that E ¦Ð n is bounded, we can read-= ¦Ð n p(¦Á 1:n ).</p><p>¦Á 1:n ¡ÊA n ily show that the expected social belief must converge. We next will analyze the limit of this sequence when the deterministic and random policies are used.</p><p>Since the agents use the Bayesian method to learn from the decision sequence, it can be shown that their average performance cannot decrease if more information becomes available to the system. Mathematically, in the following we present a theorem that states the relationship between the action likelihood and the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Deterministic policy</head><p>We start our analysis with the definition of information cascade from <ref type="bibr" target="#b3">[4]</ref>. It is defined as follows: an information cascade occurs monotonic property of the E ¦Ð n : at tim¨§ n if the public belief stops evolving after agent A ¡¦ n , i.e.,</p><formula xml:id="formula_20">¦Ð n = ¦Ð ¡¦ n , ?n &gt; ¡¦ Theorem 1.</formula><p>In the proposed sequential decision making system, when the true state of nature is H 1 , the expected value of social belief is monotonic, i.e., ?t ¡Ê N + , n. As is shown in <ref type="bibr" target="#b3">[4]</ref>, if the LLR of the social learning system with deterministic policy is bounded, the probability that information cascade occurs converges to one. Instead of repeating the proof here, we briefly sketch it as follows.</p><formula xml:id="formula_21">E ¦Ð n ? E ¦Ð n?1 ¡Ý 0. (13) Let log ¦Õ 1 (y n ) ¦Õ 0 (y n ) ¡Ê [m, M], ?y n .</formula><p>Then by (10), we get that if the social belief ¦Ð n &lt; u or ¦Ð n &gt; U , the agent A n will make its</p><p>Proof. By (12), we can write decision without using its private information even if it is rational, ? p(¦Á n |¦Á 1:n?1 , H 1 )¦Ð n ? , n , meaning that the social belief does not change after the decision ¦Á n is made, i.e., ¦Ð n = ¦Ð n?1 . Moreover, once the social belief leaves </p><formula xml:id="formula_22">¦Á 1:n?1 ¡ÊA n?1 ¦Á n =0 the range [u, U ], it</formula><formula xml:id="formula_23">E¦Ð n ? E ¦Ð n?1 = p(¦Á 1:n?1 |H 1 ))(¦Á 1:n ),<label>(15)</label></formula><p>is strictly positive. This is because if all the agents after A n make decisions as one or as zero, then the social belief will evolve to the range <ref type="bibr">[U , 1]</ref> or <ref type="bibr">[0, u]</ref> in finite time, respectively. Noting that the ¦Á 1:n?1 ¡ÊA n?1 probability for this event is strictly positive, the above statement where (¦Á 1:n ) is a function of the action sequence ¦Á 1:n , which is is true. Then we have that there exists ¦Ó 0 ¡Ê N + and a p 0 &gt; 0 such that ?¦Ð n ¡Ê [u, U ], p given by,</p><formula xml:id="formula_24">¦Ð n+¦Ó 0 ¡Ê ([0, u] ¡È [U , 1])</formula><p>&gt; p 0 , where ¦Ó 0 is the upper bound of all the ¦Ó s and p 0 is the lower bound of all the</p><formula xml:id="formula_25">1 p ¦Ð n+¦Ó ¡Ê [0, u] ¡È [U , 1]) (¦Á 1:n ) = p(¦Á n |¦Á 1:n?1 , H 1 )¦Ð n ? ¦Ð n?1 .<label>(16)</label></formula><p>s. With the above arguments, we get that the probability of ab-</p><formula xml:id="formula_26">¦Á n =0</formula><p>sence of information cascade after ¦Ó 0 is smaller than 1 ? p 0 . From Noting that ¦Ð n is updated from ¦Ð n?1 according to <ref type="bibr" target="#b4">(5)</ref> and that p(¦Á n |¦Á 1:n?1 , H 1 ) can be expressed by the action likelihood in (9), we can rewrite (¦Á 1:n ) in the following form: the chain rule of probability theory, it follows that the probability of absence of information cascade will converge to zero exponentially as time goes to infinity.</p><p>On the other hand, (6) also implies that if l</p><formula xml:id="formula_27">(0) n = l (1)</formula><p>n , ¦Ð n = ¦Ð n?1 .</p><p>2 Therefore if l</p><formula xml:id="formula_29">(0) n = l<label>(1)</label></formula><p>n for arbitrary ¦Ð n?1 , information cascade will</p><formula xml:id="formula_30">(¦Á 1:n ) = ¦Ð n?1 (1 ? l n ) ¦Ð n?1 (1 ? l (1) n ) + (1 ? ¦Ð n?1 )(1 ? l (0) n ) (1) 2 not happen.</formula><p>With the following lemma, we show that when the LLR is unbounded, information cascade cannot occur.</p><formula xml:id="formula_31">+ ¦Ð n?1 (l n ) ¦Ð n?1 l (1) ? ¦Ð n?1 . (17) n + (1 ? ¦Ð n?1 )l (0)</formula><p>n Lemma 1. With the deterministic policy, if (a) the two distributions ¦Õ 0</p><p>From the above equation, one can derive that and ¦Õ 1 are not identical everywhere, (b) 0 &lt; ¦Ð n?1 &lt; 1, and (c) the LLR is unbounded, then l</p><formula xml:id="formula_32">(0) n = l (1) n . (¦Á 1:n ) = ¦Ð n?1 (1) (0) Z (1 ? ¦Ð n?1 ) 2 (l n ? l n ) 2 ,<label>(18)</label></formula><p>Proof. By <ref type="formula" target="#formula_14">(8)</ref>, we have where Z is the multiplication of the two denominators in (16) and is given by</p><formula xml:id="formula_33">l (1) (19) n ? l (0) n = (1 ? ¦Î)(q 1 ? q 0 ).</formula><p>Then it is sufficient to show q 1 = q 0 , ? 0 &lt; ¦Ð n?1 &lt; 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¡Þ</head><p>By the deterministic policy defined in (10), the decision region</p><formula xml:id="formula_34">I 1 = ¦Õ 1 (y n )(¦Õ 1 (y n ) ? ¦Õ 0 (y n )) ¦Õ 1 (y n ) + ¦Ë¦Õ 0 (y n ) dy n ,<label>(26)</label></formula><p>for ¦Á n = 1 denoted by S 1 can be written as</p><formula xml:id="formula_35">?¡Þ S 1 = y n ¦Õ 1 (y n ) ¦Õ 0 (y n ) &gt; 1 ? ¦Ð n?1 ¦Ð n?1 .<label>(20)</label></formula><p>and ¡Þ Then it follows that p(¦Á n = 1|¦Á 1:n?1 , y n , I n = 0) equals to one if y n ¡Ê S 1 and zero otherwise. Therefore from (9), if the deterministic</p><formula xml:id="formula_36">I 2 = ¦Ë¦Õ 0 (y n )(¦Õ 1 (y n ) ? ¦Õ 0 (y n )) ¦Õ 1 (y n ) + ¦Ë¦Õ 0 (y n ) dy n .<label>(27)</label></formula><p>?¡Þ policy is used, we have that</p><formula xml:id="formula_37">¡Þ From ?¡Þ q k = ¦Õ k (y n )dy n . (21) ¦Õ 1 (y n ) ? ¦Õ 0 (y n )</formula><p>dy n = 0, we have that I 1 + I 2 = 0. Assuming that I 1 = 0, then we must have I 2 = 0. This, however, is contradicted by the following equation:</p><formula xml:id="formula_38">y n ¡ÊS 1 ¡Þ 2</formula><p>Given that 0 &lt; ¦Ð n?1 &lt; 1 and that the LLR is unbounded, both</p><formula xml:id="formula_39">I 1 ? I 2 ¦Ë = (¦Õ 1 (y n ) ? ¦Õ 0 (y n ))<label>(28)</label></formula><p>S 1 and its complimentary set S 0 must be nonempty. Next, we</p><formula xml:id="formula_40">¦Õ 1 (y n ) + ¦Ë¦Õ 0 (y n ) dy n &gt; 0</formula><p>show that q 1 ? q 0 &gt; 0 by considering the following two cases. If</p><formula xml:id="formula_41">?¡Þ ¦Ð n?1 &lt; 0.5, we have that because (¦Õ 1 (y n ) ? ¦Õ 0 (y n ))</formula><p>2 does not equal to zero for all y n .</p><formula xml:id="formula_42">q 1 ? q 0 = ¦Õ 1 (y n ) ? ¦Õ 0 (y n ) dy n &gt; 0,<label>(22)</label></formula><p>Hence we conclude that for arbitrary distributions ¦Õ 0 and ¦Õ 1 , the above lemma holds. 2 y n ¡ÊS 1 because ?y n ¡Ê S 1 , ¦Õ 1 (y n ) &gt; ¦Õ 0 (y n ). Otherwise, ¦Ð n?1 ¡Ý 0.5 shows that ?y n ¡Ê S 0 , ¦Õ 1 (y n ) &lt; ¦Õ 0 (y n ). Then we have Lemma 2 indicates the difference between the deterministic and random policies. When the random policy is adopted, the social belief will not stop evolving regardless of the LLR.</p><formula xml:id="formula_43">q 1 ? q 0 = (1 ? q 0 ) ? (1 ? q 1 ) 4.3. Asymptotic learning = ¦Õ 1 (y n ) ? ¦Õ 0 (y n ) dy n &gt; 0.<label>(23)</label></formula><p>y n ¡ÊS 0</p><p>With <ref type="formula" target="#formula_3">(22)</ref> and <ref type="formula" target="#formula_3">(23)</ref>, the proof of Lemma 1 is completed. 2</p><p>With the above lemmas, we can now get the limits of the expected social belief. Also we show when the probability for a rational agent making an error decision converges to zero. With the next theorem, we first show the limit of the expected social belief.</p><p>We point out that the result in Lemma 1 shows that information cascade cannot occur when the LLR is unbounded in the studied system. In <ref type="bibr" target="#b10">[11]</ref>, a similar result can be found except that there all the agents are rational.</p><p>Theorem 2. Suppose that the agents implement the deterministic policy in (10) with an unbounded LLR or the random decision policy in <ref type="bibr" target="#b10">(11)</ref>. Let In contrast to the deterministic policy, the random policy in <ref type="bibr" target="#b10">(11)</ref> shows that even if the LLR is bounded, the social belief still can evolve after it becomes greater than U or smaller than u. With the following lemma, we show that the information cascade can be prevented by using the random policy.</p><formula xml:id="formula_44">If H 0 is true, lim n¡ú¡Þ E ¦Ð n = 0. (30)</formula><p>Proof. Here we only prove <ref type="bibr">(29)</ref>. The proof of <ref type="formula" target="#formula_9">(30)</ref> is similar. First by theorem one, noting that ¦Ð n is bounded, we have that the limit Lemma 2. With the random policy, if the two distributions ¦Õ 0 and ¦Õ 1 are of (15) must be zero, i.e., not identical everywhere and if 0 &lt; ¦Ð n?1 &lt; 1, it holds that l</p><formula xml:id="formula_45">(0) n = l (1) n . lim (E ¦Ð n ? E ¦Ð n?1 ) = 0,<label>(31)</label></formula><p>Proof. Again, we prove the lemma by showing q 1 ? q 0 = 0 for any n¡ú¡Þ ¦Ð n?1 . From (9), we have that which implies that ?¦Á 1:n?1 ¡Ê A n?1 , lim n¡ú¡Þ p(¦Á 1:n?1 |H 1 ) ¡Á</p><p>¡Þ (¦Á 1:n?1 ) = 0. By Lemmas 1 and 2, we have that (l</p><formula xml:id="formula_47">n ? l n ) 2 = 0 q k = ¦Õ k (y n )dy n .<label>(24)</label></formula><p>?¡Þ</p><formula xml:id="formula_48">¦Ð n?1 ¦Õ 1 (y n ) ¦Ð n?1 ¦Õ 1 (y n ) + (1 ? ¦Ð n?1 )¦Õ 0 (y n )</formula><p>only when ¦Ð n = 0 or ¦Ð n = 1. Then Eq. <ref type="formula" target="#formula_0">(18)</ref> shows that (¦Á 1:n?1 ) = 0 if and only if ¦Ð n = 0 or ¦Ð n = 1. Therefore, for arbitrary ¦Á 1:n?1 , it must hold that either lim n¡ú¡Þ p(¦Á 1:n?1 |H 1 )(1 ? ¦Ð n?1 ) = 0 or Noting that ¦Ð n?1 = 0, we have lim n¡ú¡Þ p(¦Á 1:n?1 |H 1 )¦Ð n?1 = 0. By (5), ¦Ð n?1 converges to zero only when every agent herds on ¡Þ H 0 after a certain agent. Since an information cascade cannot hap-</p><formula xml:id="formula_49">q 1 ? q 0 = ¦Õ 1 (y n )(¦Õ 1 (y n ) ? ¦Õ 0 (y n )) ¦Õ 1 (y n ) + ¦Ë¦Õ 0 (y n ) dy n ,<label>(25)</label></formula><p>?¡Þ pen and given H 1 is true, we have that the probability of ¦Ð n?1 = 0 converges to zero. Namely, for any decision sequence ¦Á 1:n?1 for which ¦Ð n?1 converges to zero, we must have</p><formula xml:id="formula_50">where ¦Ë = 1?¦Ð n?1 ¦Ð n?1</formula><p>represents the ratio of social beliefs in H 0 and H 1 .</p><p>We now prove that q 1 = q 0 by contradiction. Define</p><formula xml:id="formula_51">I 1 = q 1 ? lim n¡ú¡Þ p(¦Á 1:n?1 |H 1 ) = 0.<label>(32)</label></formula><p>By summarizing the two cases, for an arbitrary decision sequence n?1 , we write q 0 by</p><formula xml:id="formula_52">¦Á 1:n?1 ¡Ê A lim n¡ú¡Þ p(¦Á 1:n?1 |H 1 )(1 ? ¦Ð n?1 ) = 0.<label>(33)</label></formula><p>Hence, we have lim n¡ú¡Þ p(¦Á 1:n?1 |H 1 )(1 ? ¦Ð n?1 ) = 0.</p><p>(34)</p><formula xml:id="formula_53">¦Á 1:n?1 ¡ÊA n?1</formula><p>Then by (12), we show that the limit of E ¦Ð n is one, given by</p><formula xml:id="formula_54">lim n¡ú¡Þ E ¦Ð n = 1. 2<label>(35)</label></formula><p>Considering that the rational agents are using Bayes' rule to formulate their private beliefs, it can be shown that given H 1 is true, the expected private belief is no smaller than the expected social belief. Then the expected private belief for rational agents also converges to one. Symmetrically, under H 0 , the expected private belief will converge to zero. In summary, we have that </p><formula xml:id="formula_55">lim n¡ú¡Þ E(¦Â n |I n = 0) = s, ?s ¡Ê {0, 1},<label>(36)</label></formula><p>where H s is the true state of nature.</p><p>Considering the decision policies in <ref type="formula" target="#formula_0">(10)</ref> and <ref type="formula" target="#formula_0">(11)</ref>, the result in (36) suggests that the probability of decision error for a rational agent can converge to zero when the deterministic policy with unbounded LLR or the random policy (independently of the condition on boundedness of the LLR) are applied. Given H s is the true state of nature, we get that lim n¡ú¡Þ p(¦Á n = s|I n = 0) = 1, ?s ¡Ê {0, 1}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(37)</head><p>Therefore, as n goes to infinity, the probability for one agent to make a right decision is given by lim 1 ? ¦Î ¦Ç,</p><formula xml:id="formula_56">if H s = H 0 , n¡ú¡Þ p(¦Á n = s) = 1 ? ¦Î(1 ? ¦Ç), if H s = H 1 . (38)</formula><p>where, we recall, ¦Î is the probability that an agent in the network is irrational. Note that in the above derivation, ¦Î can take any value between zero and one. We remark that asymptotic learning can be achieved by random policy with any 0 &lt; ¦Î &lt; 1. However, as we show in the next section, the convergence speed decreases as ¦Î grows.</p><p>given by the average social belief from all the 2000 trials. From the figure, we see that the expected social beliefs kept increasing when the agents used the random policy, whereas, as expected, it leveled off when they employed the deterministic policy. We also</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Simulations</head><p>observe that a larger ¦Î resulted in a lower convergence speed of In this section, we present simulation results on the evolution of social beliefs in sequential decision making systems along with some numerical comparisons between different decision policies.</p><p>In the first experiment, we applied the methods for a binomial data model given by the expected social belief.</p><p>Another performance metric of interest is the proportion of agents that made the right decision. The results of its investigation are plotted in <ref type="figure" target="#fig_4">Fig. 3</ref>. As shown in the <ref type="figure">figure,</ref> with the random policy, the probability for agents to make correct decision converged to 0.95 and 0.85 when ¦Î = 0.1 and ¦Î = 0.3, respectively.</p><formula xml:id="formula_57">H 1 : y n ? B(m, p 1 ), H 0 : y n ? B(m, p 0 ),<label>(39)</label></formula><p>where B(m, p) represents a binomial distribution parameterized by m and p. It can be shown that the LLR of this model is bounded.</p><p>We verified the analytical result of the expected social belief by Monte Carlo methods, which were conducted with 2000 trials.</p><p>Recall that this probability converged to 0.85 and 0.95, respectively, instead of 1 because both the rational and irrational agents are counted in the calculation. Also, the figure indicates that the probability of decision error for the rational agents with random policy converged to zero. By contrast, if the deterministic policy was used, it can be seen that the right decision probability stopped increasing because of information cascade. Again, we observe that ¦Î can In each trial, we set the number of agents to be N = 1500, and they observed data generated from the data model listed in <ref type="bibr">(39)</ref> and parameterized by m = 6, p 0 = 0.6, and p 1 = 0.7. We tested the system with ¦Ç = 0.5 and with two values of ¦Î , ¦Î = 0.1 and ¦Î = 0.3. The private signals of the agents were generated according to H 1 .</p><p>We show the results of the deterministic and random policies in <ref type="figure" target="#fig_3">Fig. 2</ref>. On the abscissa, we plotted the agent index and on the ordinate the estimate of the expected social belief, which was decrease the speed of learning.</p><p>As shown in Section 4, when the deterministic policy is adopted, herding occurs in finite time with probability one. In <ref type="figure" target="#fig_5">Fig. 4</ref>, for each agent A n , we plotted the proportion of trials with herding already started before A n . In other words, we plotted the empirical cumulative distribution function (c.d.f.) of the starting time of herding when the deterministic policy is used. There, the two curves represent the performance of the studied system with ¦Î = 0.1 and ¦Î = 0.3, respectively. It can be seen that the herding starts earlier when there is a larger proportion of rational agents.    With each of the settings, the random policy was implemented 2000 times. On the left side of <ref type="figure" target="#fig_8">Fig. 5</ref>, we plotted the evolution of the expected social belief. It can be seen that the evolution of the expected social belief is insensitive to the value of ¦Ç. This is because the decisions of the irrational agents provide no information to the other agents, irrespective of the decision rule they use.</p><p>Thus, we can see that ¦Ç has little effect on the decision making of the rational agents. On the right side of <ref type="figure" target="#fig_8">Fig. 5</ref>, we plotted how the proportion of agents making the correct decision evolves for different values of ¦Ç. There we observe that the converged values are In <ref type="figure" target="#fig_6">Fig. 6</ref>, we plotted the evolution of social belief with the Gaussian model. We observe that the expected social beliefs increase for both policies. Unlike with the binomial data model, information cascade did not occur.</p><p>In <ref type="figure" target="#fig_7">Fig. 7</ref>, we plotted the proportion of agents making correct decision. As shown in the figure, the agents applying both policies had a trend of making the right decision asymptotically. This shows that asymptotic learning can be achieved with both policies. As opposed to the result from experiment one, we observed that the probability for correct decision kept increasing at all times. As we have proved in Theorem 2, because of the unbounded LLR, even with the deterministic policy, the proportion of agents making the as predicted by (38), which are 0.97, 0.85, and 0.73, respectively.</p><p>In the third experiment, we let the data be generated by a Gaussian data model given by correct decision will converge to 0.95 when ¦Î = 0.1 and to 0.85 when ¦Î = 0.3. In <ref type="figure" target="#fig_7">Fig. 7</ref>, it is also shown that the irrational agents can slow down the learning in the system. Namely, there was a sig-H 1 : y n ? N (¦È, ¦Ò 2 nificant decrease of convergence speed when ¦Î was changed from ¦Î = 0.1 to ¦Î = 0.3. We also observe that the deterministic policy w ), H 0 :</p><formula xml:id="formula_58">y n ? N (0, ¦Ò 2 w ),<label>(40)</label></formula><p>has a faster speed in the short run, while the random policy converges faster in the long run.</p><p>where y n was a Gaussian random variable, and the parameters ¦È</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>and ¦Ò w were known by the agents. Compared with the binomial data model, we have proved that the expected social belief will converge with both policies since the LLR is now unbounded.</p><p>We conducted the second experiment by repeating the same process of experiment one. The sequential decision making was In this paper we presented a sequential learning system constituted by rational and irrational agents. The rational agents made their decisions by using the Bayesian methodology whereas the irrational agents made decisions arbitrarily. The rational agents can exploit the information from previous decisions by Bayesian learning. We applied two policies to the system and showed their asymptotical properties by investigating the convergence of the expected social belief in the system. In analyzing the deterministic policy, we proved that information cascade takes place with probability one if the data model entails a bounded log-likelihood ratio. By contrast, with the random policy, information cascade does not occur and the probability of decision error of the rational agents converges to zero. We demonstrated the performance of the proposed method by Monte Carlo simulations. We showed that the expected social belief converged to one with the random policy even with a bounded LLR, whereas the expected social belief stopped evolving with the deterministic policy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where u = 1+e M and U = 1+e m . Then by (6), we get that l n = l E ¦Ð n = p(¦Á 1:n?1 |H 1 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>H</head><label></label><figDesc>1 be the true state of nature, then we have 4.2. Random policy lim n¡ú¡Þ E ¦Ð n = 1. (29)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The convergence of social beliefs with two policies, where the agents get observations from a binomial data model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The proportion of agents making the right decision with a binomial data model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The empirical cumulative distribution function for the herding starting time of the deterministic policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The convergence of expected social belief with the Gaussian model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The proportion of agents making the right decision with the Gaussian model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The asymptotical performance of the random policy with different values of ¦Ç.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>remains unchanged forever. Hence, information (14) cascade starts from this agent A n . where with the conditioning we have emphasized that the true hypothesis is H 1 . Then we have, It can be shown that for arbitrary ¦Ð n ¡Ê [u, U ], there exists a finite ¦Ó ¡Ê N + such that the probability p ¦Ð n+¦Ó ¡Ê [0, u] ¡È [U , 1])</head><label></label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the NSF under Award CCF-1320626.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the performance of serial networks in distributed detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Swaszek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Aerosp. Electron. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="254" to="260" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributed detection by a large team of sensors in tandem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Papastavrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Athans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Aerosp. Electron. Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="639" to="653" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Distributed Detection and Data Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Varshney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>New York, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chamley</surname></persName>
		</author>
		<title level="m">Rational Herds: Economic Models of Social Learning</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A tutorial on interactive sensing in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Poor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. Soc. Syst</title>
		<imprint>
			<biblScope unit="page" from="3" to="21" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A theory of fads, fashion, custom, and cultural change as informational cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bikhchandani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hirshleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Polit. Econ</title>
		<imprint>
			<biblScope unit="page" from="992" to="1026" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Non-Bayesian social learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jadbabaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sandroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tahbaz-Salehi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Games Econ. Behav</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="210" to="225" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Naive learning in social networks and the wisdom of crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. Econ. J. Microecon</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="112" to="149" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple model of herd behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Q. J. Econ</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="797" to="817" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Social learning and Bayesian games in multiagent signal processing: how do local and global decision makers interact?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Poor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="43" to="57" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bayesian learning in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acemoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Dahleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ozdaglar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rev. Econ. Stud</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1201" to="1236" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bayesian learning in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kariv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Games Econ. Behav</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="329" to="346" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed Bayesian learning in multiagent systems: improving our understanding of its capabilities and limitations, IEEE Signal Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Djur¨ª C</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="65" to="76" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Djur¨ª c, A gossip method for optimal consensus on a binary state from binary actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Signal Process</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="274" to="283" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Models for the diffusion of beliefs in social networks: an overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chamley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Scaglione</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="16" to="29" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bayesian sequential detection with phase-distributed change time and nonlinear penalty-a POMDP lattice programming approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="7096" to="7124" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Djur¨ª c, Sequential Bayesian learning in linear networks with random decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="6404" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sequential Chinese restaurant game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J R</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="571" to="584" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic Chinese restaurant game: theory and application to cognitive radio networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J R</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Wirel. Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1960" to="1973" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hypothesis testing in feedforward networks with broadcast failures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Moran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Signal Process</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="797" to="810" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hypothesis testing with finite statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="page" from="828" to="835" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Rational social learning by random sampling, Available at SSRN 1138095</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>S?rensen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On learning with finite memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Drakopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ozdaglar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6859" to="6872" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Asymptotic learning on Bayesian social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mossel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tamuz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="31" />
		</imprint>
	</monogr>
<note type="report_type">Probab. Theory Relat. Fields</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spread of (mis)information in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acemoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ozdaglar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parandehgheibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Games Econ. Behav</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="194" to="227" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Discrete opinion dynamics with stubborn agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acemoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ozdaglar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saberi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Scaglione</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>SSRN eLibrary</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How homophily affects the speed of learning and bestresponse dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Q. J. Econ</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1287" to="1338" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Yunlong Wang received his B.E. Degree in electrical engineering from</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">His main research interests are Bayesian learning over networks</title>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Xian, China; Stony Brook</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Xi&apos;an Jiaotong University ; Department of Electrical and Computer Engineering, Stony Brook University</orgName>
		</respStmt>
	</monogr>
	<note>He is currently a Ph.D. candidate in the. distributed estimation and their applications</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">D. degree in electrical engineering from the University of Rhode Island. Currently, he is a professor in the Department of Electrical and Computer Engineering at Stony Brook University. He was an associate editor of several signal processing journals including IEEE Transactions on Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Petar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Djuri?cDjuri?c received his B.S. and M.S. degrees in electrical engineering from the University of Belgrade and his Ph</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>2012, he received the European Association for Signal Processing Technical Achievement Award. He is a Fellow of the IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
