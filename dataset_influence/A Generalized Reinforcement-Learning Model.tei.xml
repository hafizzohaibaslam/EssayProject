<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-07-16T23:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Generalized Reinforcement-Learning Model: Convergence and Applications A Generalized Reinforcement-Learning Model: Convergence and Applicationa A Generalized Reinforcement-Learning Model: Convergence and Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="199823-11-261">Article ¡¤ November 1998 23 261 Retrieved on: 12 September 2016 February, 1996 January 18, 1996</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
							<email>mlittman@cs.brown.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Brown University</orgName>
								<address>
									<addrLine>115 Waterman</addrLine>
									<postCode>02912-1910</postCode>
									<settlement>Providence</settlement>
									<region>RI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Szepesv¨¤ri</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Bolyal Institute of Mathematics, &quot;Jozsef Attila&quot;</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">University of Szeged</orgName>
								<address>
									<postCode>6720</postCode>
									<settlement>Szeged, Aradi</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Bolyai Institute of Mathematics \JJ ozsef Attila&quot;</orgName>
								<orgName type="institution">Brown University Providence</orgName>
								<address>
									<postCode>02912-1910</postCode>
									<region>RI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of Szeged</orgName>
								<address>
									<postCode>6720</postCode>
									<settlement>Szeged</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Szepesvv</surname></persName>
							<email>szepes@math.u-szeged.hu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Generalized Reinforcement-Learning Model: Convergence and Applications A Generalized Reinforcement-Learning Model: Convergence and Applicationa A Generalized Reinforcement-Learning Model: Convergence and Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="199823-11-261">Article ¡¤ November 1998 23 261 Retrieved on: 12 September 2016 February, 1996 January 18, 1996</date>
						</imprint>
					</monogr>
					<note>See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/2373521 Source: CiteSeer CITATIONS READS 1 author: 267 PUBLICATIONS 19,334 CITATIONS SEE PROFILE All in-text references underlined in blue are linked to publications on ResearchGate, letting you access and read them immediately. Available from:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Reinforcement learning</term>
					<term>Q-learning convergence</term>
					<term>Markov games 1</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Reinforcement learning is the process by which an autonomous agent uses its experience interacting with an environment to improve its behavior. The Markov decision process (mdp) model is a popular way of formalizing the reinforcement-learning problem, but it is by no means the only way. In this paper, we show how many of the important theoretical results concerning reinforcement learning in mdps extend to a generalized mdp model that includes mdps, two-player games and mdps under a worst-case optimality criterion as special cases. The basis of this extension is a stochastic-approximation theorem that reduces asynchronous convergence to synchronous convergence.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Reinforcement learning is the process by which an agent improves its behavior in an environment via experience. A reinforcement-learning scenario is deened by the experience presented to the agent at each step, and the criterion for evaluating the agent's behavior.</p><p>One particularly well-studied reinforcement-learning scenario is that of a single agent maximizing expected discounted total reward in a nite-state environment; experiences are of the form hx; a; y; ri, where x is a state, a is an action, y is a resulting state and r is the scalar immediate reward to the agent. A discount parameter 0 &lt; 1 controls the degree to which future rewards are signiicant compared to immediate rewards.</p><p>The theory of Markov decision processes can be used as a theoretical foundation for important results concerning this reinforcement-learning scenario 1]. A ((nite) Markov decision process (mdp) 18] is deened by the tuple hS; A; P; Ri, where S represents a nite set of states, A a nite set of actions, P a transition function, and R a reward function.</p><p>The optimal behavior for an agent in an mdp depends on the optimality criterion; for the innnite-horizon discounted criterion, the optimal behavior can be found by identifying the optimal value function, deened recursively by V (x) = max a R(x; a) + X y P(x; a; y)V</p><p>! ;</p><p>for all states x 2 S, where R(x; a) is the immediate reward for taking action a from state x, 0 &lt; 1 is a discount factor, and P(x; a; y) is the probability that state y is reached from state x when action a 2 A is chosen. These simultaneous equations, known as the Bellman equations, can be solved using a variety of techniques ranging from successive approximation to linear programming In the absence of complete information regarding the transition and reward functions, reinforcement-learning methods can be used to nd optimal value functions. Both modelfree (direct) methods, such as Q-learning and model-based (indirect) methods, such as prioritized sweeping and DYNA have been explored and many have been shown to converge to optimal value functions under the proper conditions <ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>As we mentioned before, not all reinforcement-learning scenarios of interest can be modeled as mdps. A great deal of reinforcement-learning research has been directed to the problem of solving two-player games <ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b3">4]</ref>, for example, and the reinforcementlearning algorithms for solving mdps and their convergence proofs do not apply directly to games. In one form of two-player game, experiences are of the form hx; a; y; ri, where states x and y contain additional information concerning which player (maximizer or minimizer) gets to choose the action in that state, and the optimality criterion is minimax optimality.</p><p>There are deep similarities between mdps and games; for example, it is possible to deene a set of Bellman equations for the optimal minimax value of a two-player zero-sum game, V (x) = ( max a2A R(x; a) + P y P(x; a; y)V (y) ; if maximizer moves in x min a2A (R(x; a) + P x P(x; a; y)V (y)); if minimizer moves in x, where R(x; a) is the reward to the maximizing player. When 0 &lt; 1, these equations have a unique solution and can be solved by successive-approximation methods In addition, we show in this paper that the natural extension of several reinforcement-learning algorithms for mdps converge to optimal value functions in two-player games.</p><p>In this paper, we introduce a generalized Markov decision process model with applications to reinforcement learning, and list some of the important results concerning the model. Generalized mdps provide a foundation for the use of reinforcement learning in mdps and games, as well in risk-sensitive reinforcement learning exploration-sensitive reinforcement learning 11], reinforcement learning in simultaneous-action games 13], and other models. Our main theorem addresses the convergence of asynchronous stochastic processes and shows how this problem can be reduced to determining the convergence of a corresponding synchronous one; it can be used to prove the convergence of model-free and model-based reinforcementlearning algorithms under a variety of diierent reinforcement-learning scenarios.</p><p>In Section 2, we present generalized mdps, and motivate them using two detailed examples. In Section 3, we describe a stochastic-approximation theorem, and in Section 4 we show several applications of the theorem that prove the convergence of learning processes in generalized mdps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE GENERALIZED MODEL</head><p>In this section, we introduce our generalized mdp model. We begin by summarizing some of the more signiicant results regarding the standard mdp model and some important results for two-player games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MARKOV DECISION PROCESSES</head><p>To provide a point of departure for our generalization of Markov decision processes, we begin by describing some results concerning the use of reinforcement learning in the mdp scenario described earlier. These results are well established; proofs of the unattributed claims can be found in Puterman's mdp book 18].</p><p>The ultimate target of learning is an optimal policy. A policy is some function that tells the agent which actions should be chosen under which circumstances. A policy is optimal under the expected discounted total reward criterion if, with respect to the space of all possible policies, maximizes the expected discounted total reward from all states.</p><p>Maximizing over the space of all possible policies is practically infeasible. However, mdps have an important property that makes it unnecessary to consider such a broad space of possibilities. We say a policy is stationary and deterministic if it maps directly from states to actions, ignoring everything else, and we write (x) as the action chosen by when the current state is x. In expected discounted total reward mdp environments, there is always a stationary deterministic policy that is optimal; we will therefore use the word \policy" to mean stationary deterministic policy, unless otherwise stated.</p><p>The value function for a policy , V , maps states to their expected discounted total reward under policy . It can be deened by the simultaneous equations V (x) = R(x; a) + X y P(x; a; y)V (y):</p><p>It is also possible to condition the immedate rewards on the state y as well; this is somewhat more general, but complicates the presentation. The optimal value function V is the value function of an optimal policy; it is unique for 0 &lt; <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ALTERNATING MARKOV GAMES</head><p>In alternating Markov games, two players take turns issuing actions to try to maximize their own expected discounted total reward. The model is deened by the tuple hS 1 ; S 2 ; A; B; P; Ri, where S 1 is the set of states in which player 1 issues actions from the set A, S 2 is the set of states in which player 2 issues actions from the set B, P is the transition function, and R is the reward function for player 1. In the zero-sum games we consider, the rewards to player 2 (the minimizer) are simply the additive inverse of the rewards for player 1 (the maximizer).</p><p>Markov decision processes are a special case of alternating Markov games in which S 2 = ;;</p><p>Condon 5] proves this and the other unattributed results in this section. A common optimality criterion for alternating Markov games is discounted minimax optimality. Under this criterion, the maximizer should choose actions so as to maximize its reward in the event that the minimizer chooses the best possible counter-policy. An equivalent deenition is for the minimizer to choose actions to minimize its reward against the maximizer with the best possible counter-policy. A pair of policies is said to be in equilibrium if neither player has any incentive to change policies if the other player's policy remains The value function for a pair of equilibrium policies is the optimal value function for the game; it is unique when 0 &lt; 1, and can be found by successive approximation. For both players, there is always a deterministic stationary optimal policy. Any myopic policy with respect to the optimal value function is optimal, and any pair of optimal policies is in equilibrium.</p><p>Dynamic-programming operators, Bellman equations, and reinforcement-learning algorithms can be deened for alternating Markov games by starting with the deenitions used in mdps and changing the maximum operators to either maximums or minimums conditioned on the state. We show below that the resulting algorithms share their convergence properties with the analogous algorithms for mdps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">GENERALIZED MDPS</head><p>In alternating Markov games and mdps, optimal behavior can be speciied by the Bellman equations; any myopic policy with respect to the optimal value function is optimal. In this section, we generalize the Bellman equations to deene optimal behavior for a broad class of reinforcement-learning models. The objective criterion used in these models is additive in that the the value of a policy is some measure of the total reward received.</p><p>The generalized Bellman equations can be written</p><formula xml:id="formula_1">V (x) = O R(x; a) + M a y V (y) ! :</formula><p>(1)</p><p>Here, \ N " and \ L " represent operators that summarize values over actions as a function of the state x and next states as a function of the state-action pair (x; a), respectively. For Markov decision processes, N a f(x; a) = max a f(x; a) and L y g(x; a; y) = P y P(x; a; y)g(x; a; y). For alternating Markov games, L y g(x; a; y) = P y P(x; a; y)g(x; a; y) and N a f(x; a) = max a f(x; a) or min a f(x; a) depending whether x is in S 1 or S 2 . A large variety of other models can be represented in this framework; several examples are discussed in Section 4.</p><p>From a reinforcement-learning perspective, the value functions deened by the generalized mdp model can be interpreted as the total value of the rewards received by an agent selecting actions in a stochastic environment. The agent begins in state x, takes action a, and ends up in state y. <ref type="bibr">The</ref>  The next section describes a general theorem that can be used to prove the convergence of several reinforcement-learning algorithms for these and other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CONVERGENCE THEOREM</head><p>The process of nding an optimal value function can be viewed in the following general way. At any moment in time, there is a set of values representing the current approximation of the optimal value function. On each iteration, we apply some dynamic-programming operator, perhaps modiied by experience, to the current approximation to generate a new approximation. Over time, we would like the approximation to tend toward the optimal value function.</p><p>In this process, there are two types of approximation going on simultaneously. The rst is an approximation of the dynamic-programming operator for the underlying model, and the second is the use of the approximate dynamic-programming operator to the optimal value function. This section presents a theorem that gives a set of conditions under which this type of simultaneous stochastic approximation converges to an optimal value function.</p><p>First, we need to deene the general stochastic process. Let the set X be the states of the model, and the set B (X) of bounded, real-valued functions over X be the set of value functions. Let T : B (X) ! B (X) be an arbitrary contraction mapping and V be the point of T.</p><p>If we had direct access to the contraction mapping T, we could use it to successively approximate V . In most reinforcement-learning scenarios, T is not available and we must use our experience to construct approximations of T. Consider a sequence of random operators T t : B (X) ! (B(X) ! B (X)) and deene U t+1 = T t U t ]V where V and U 0 2 B (X) are arbitrary value functions. We say T t approximates T at V with probability 1 uniformly over X, if U t converges to TV uniformly over X 1 . The basic idea is that T t is a randomized version of T in some sense; it uses U t as \memory" to help it approximate TV .</p><p>The following theorem shows that, under the proper conditions, we can use the sequence T t to estimate the point V of T.</p><p>Theorem 1 Let T be an arbitrary mapping with xed point V , and let T t approximate T at V with probability 1 uniformly over X. Let V 0 be an arbitrary value function, and deene V t+1 = T t V t ]V t . If there exist functions 0 F t (x) 1 and 0 G t (x) 1 satisfying the conditions below with probability one, then V t converges to V with probability 1 uniformly over X:</p><p>1. for all U 1 , and U 2 2 B (X) and all x 2 X, j((</p><formula xml:id="formula_2">T t U 1 ]V )(x) ? ((T t U 2 ]V )(x)j G t (x) sup 0 0 x 0 jU 1 (x ) ? U 2 (x )j;</formula><p>2. for all U and V 2 B (X), and all x 2 X, j((</p><formula xml:id="formula_3">T t U]V )(x) ? ((T t U]V )(x)j F t (x) sup 0 0 x 0 jV (x ) ? V (x )j;</formula><p>3. for all k &gt; 0, n t=k G t (x) converges to zero uniformly in x as n increases; and, 4. there exists 0 &lt; 1 such that for all x 2 X and large enough t, F t (x) (1 ? G t (x)):</p><p>Note that from the conditions of the theorem, it follows that T is a contraction operator at V with index of contraction . The theorem is proven in an extended version of this paper We next describe some of the intuition behind the statement of the theorem and its conditions.</p><p>The iterative approximation of V is performed by computing V t+1 = t V t ]V t , where T t approximates T with the help of the \memory" present in V t . Because of Conditions 1 and 2, G t (x) is the extent to which the estimated value function depends on its present value and F t (x) 1 ? G t (x) is the extent to which the estimated value function is based on \new" information (this reasoning becomes clearer in the context of the applications in Section 4).</p><p>In some applications, such as Q-learning, the contribution of new information needs to decay over time to insure that the process converges. In this case, G t (x) needs to converge to one. Condition 3 allows G t (x) to converge to 1 as long as the convergence is slow enough to incorporate suucient information for the process to converge.</p><p>Condition 4 links the values of G t (x) and F t (x) through some quantity &lt; 1. If it were somehow possible to update the values synchronously over the entire state space, the process would converge to V even when = 1. In the more interesting asynchronous case, when = 1, the long-term behavior of V t is not immediately clear; it may even be that V t converges to something other than V . The requirement that &lt; 1 insures that the use of outdated information in the asynchronous updates does not cause a problem in convergence. One of the most noteworthy aspects of this theorem is that it shows how to reduce the problem of approximating V to the problem of approximating T at a particular point V (in particular, it is enough if T can be approximated at V ); in many cases, the latter is much easier to achieve and also to prove. For example, the theorem makes the convergence of Q-learning a consequence of the classical Robbins-Monro theorem</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPLICATIONS</head><p>This section makes use of Theorem 1 to prove the convergence of various reinforcementlearning algorithms. If y t is randomly selected according to the probability distribution deened by P(x t ; a t ; ), N is a non-expansion, and both the expected value and the variance of N a Q(y t ; a) exist given the way y t is sampled, r t has variance and expected value given x t and a t equal to R(x t ; a t ), the learning rates are decayed so that P t (x t = x; a t = a) t (x; a) = 1 and P t (x t = x; a t = a) t (x; a) 2 &lt; 1 with probability 1 uniformly over X A 2 , then a standard result from the theory of stochastic approximation 20] states that T t approximates T with probability 1 uniformly over X A. That is, this method of using a decayed, exponentially weighted average correctly computes the average one-step reward. Let G t (x; a) = ( 1 ? t (x; a); if x = x t and a = a t ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">GENERALIZED Q-LEARNING FOR EXPECTED VALUE MODELS</head><p>0; otherwise, and F t (x; a) = ( t (x; a); if x = x t and a = a t ;</p><p>0; otherwise. These functions satisfy the conditions of Theorem 1 (Condition 3 is implied by the restrictions placed on the sequence of learning rates t ).</p><p>Theorem 1 therefore implies that this generalized Q-learning algorithm converges to the optimal Q function with probability 1 uniformly over X A. The convergence of Q-learning for discounted mdps and alternating Markov games follows trivially from this. Extensions of this result for undiscounted \all-policies-proper" mdps a soft state aggregation learning rule and a \spreading" learning rule 19] are given in an extended version of this paper 28].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Q-LEARNING FOR MARKOV GAMES</head><p>Markov games are a generalization of mdps and alternating Markov games in which both players simultaneously choose actions at each step. The basic model was developed by Shapley and is deened by the tuple hS; A; B; P; Ri and discount factor . As in alternating Markov games, the optimality criterion is one of discounted minimax optimality, but because the players move simultaneously, the Bellman equations take on a more complex form:</p><formula xml:id="formula_4">V (x) = max min b2B X 0 @ R(x; a; b) + X 2(A) a2A (a) y2S P(x; a; b; y)V (y) 1 A :</formula><p>In these equations, R(x; a; b) is the immediate reward for the maximizer for taking action a in state x at the same time the minimizer takes action b, P(x; a; b; y) is the probability that state y is reached from state x when the maximizer takes action a and the minimizer takes action b, and represents the set of discrete probability distributions over the set A. The sets S, A, and B are Once again, optimal policies are policies that are in equilibrium, and there is always a pair of optimal policies that are stationary. Unlike mdps and alternating Markov games, the optimal policies are sometimes stochastic; there are Markov games in which no deterministic policy is optimal. The stochastic nature of optimal policies explains the need for the optimization over probability distributions in the Bellman equations, and stems from the fact that players must avoid being \second guessed" during action selection. An equivalent set of equations can be written with a stochastic choice for the minimizer, and also with the roles of the maximizer and minimizer reversed.</p><p>The Q-learning update rule for Markov games given step t experience hx t ; a t ; b t ; y t ; r t i has the form Q t+1 (x t ; a t ; b t ) := (1 ? t (x t ; a t ; b t ))Q t (x t ; a t ; b t ) + t (x t ; a t ; b t ) The results of the previous section prove that this rule converges to the optimal Q function under the proper conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RISK-SENSITIVE MODELS</head><p>Heger described an optimality criterion for mdps in which only the worst possible value of the next state makes a contribution to the value of a state. An optimal policy under this criterion is one that avoids states for which a bad outcome is possible, even if it is not probable; for this reason, the criterion has a risk-averse quality to it. The generalized Bellman equations for this criterion are ! :</p><p>The argument in Section 4.5 shows that model-based reinforcement learning can be used to nd optimal policies in risk-sensitive models, as long as N does not depend on R or P, and P is estimated in a way that preserves its zero vs. non-zero nature in the limit. For the model in which N a f(x; a) = max a f(x; a), Heger deened a Q-learning-like algorithm that converges to optimal policies without estimating R and P online. In essence, the learning algorithm uses an update rule analogous to the rule in Q-learning with the additional requirement that the initial Q function be set optimistically; that is, Q 0 (x; a) must be larger than Q (x; a) for all x and a. Like Q-learning, this learning algorithm is a generalization of Korf's LRTA* algorithm for stochastic environments.</p><p>Using Theorem 1 it is possible to prove the convergence of a generalization of Heger's algorithm to models where N a f(x; a) = f(x; a (f; x)) for some function a (); that is, as long as the summary value of f(x; a) is equal to f(x; a ) for some a . The proof is based on estimating the Q-learning algorithm from above by an appropriate process where the Q function is updated only if the received experience tuple is an extremity according to the optimality equation; details are given in the extended paper 28].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">EXPLORATION-SENSITIVE MODELS</head><p>John 11] considered the implications of insisting that reinforcement-learning agents keep exploring forever; he found that better learning performance can be achieved if the Qlearning rule is changed to incorporate the condition of persistent exploration. In John's formulation, the agent is forced to adopt a policy from a restricted set; in one example, the agent must choose a stochastic stationary policy that selects actions at random 5% of the time.</p><p>This approach requires that the deenition of optimality be changed to reeect the restric- which corresponds to a generalized mdp model with L y g(x; a; y) = P y P(x; a; y)g(x; a; y) and N a f(x; a) = sup 2P 0 P a (x; a)f(x; a). <ref type="bibr">Because (x;</ref> ) is a probability distribution for any given state x, N is a non-expansion and, thus, the convergence of the associated Qlearning algorithm follows from the arguments in Section 4.1. As a result, John's learning rule gives the optimal policy under the revised optimality criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">MODEL-BASED METHODS</head><p>The deening assumption in reinforcement learning is that the reward and transition functions, R and P, are not known in advance. Although Q-learning shows that optimal value functions can be estimated without ever explicitly learning R and P, learning R and P makes more eecient use of experience at the expense of additional storage and computation 15].</p><p>The parameters of R and P can be learned from experience by keeping statistics for each state-action pair on the expected reward and the proportion of transitions to each next state. In model-based reinforcement learning, R and P are estimated on-line, and the value function is updated according to the approximate dynamic-programming operator derived from these estimates. Theorem 1 implies the convergence of a wide variety of model-based reinforcement-learning methods. The dynamic-programming operator deening the optimal value for generalized mdps is given in Equation 2. Here we assume that L may depend on P and/or R, but N may not. It is possible to extend the following argument to allow N to depend on P and R as well. In model-based reinforcement learning, R and P are estimated by the quantities R t and P t , and L t is an estimate of the L operator deened using R t and P t . As long as every state-action pair is visited innnitely often, there are a number of simple methods for computing R t and P t that converge to R and P. A bit more care is needed to insure that L t converges to L , however. For example, in expected-reward models, L y g(x; a; y) = P y P(x; a; y)g(x; a; y) and the convergence of P t to P guarantees the convergence of L t to L . On the other hand, in a risk-sensitive model, L y g(x; a; y) = min y:P(x;a;y)&gt;0 g(x; a; y) and it is necessary to approximate P in a way that insures that the set of y such that P t (x; a; y) &gt; 0 converges to the set of y such that P(x; a; y) &gt; 0. This can be accomplished easily, for example, by setting P t (x; a; y) = 0 if no transition from x to y under a has been observed.</p><p>Assuming P and R can be estimated in a way that results in the convergence of L t to L , the approximate dynamic-programming operator T t deened by</p><formula xml:id="formula_5">((T t U]V )(x) = ( N a R t (x; a) + L t y V (y)</formula><p>; if x 2 t U(x); otherwise, converges to T with probability 1 uniformly. Here, the set t S represents the set of states whose values are updated on step t; one popular choice is to set t = fx t g. 1; otherwise, and F t (x) = ( if x 2 t ; 0; otherwise, satisfy the conditions of Theorem 1 as long as each x is in innnitely many t sets (Condition 3) and the discount factor is less than 1 (Condition 4).</p><p>As a consequence of this argument and Theorem 1, model-based methods can be used to nd optimal policies in mdps, alternating Markov games, Markov games, risk-sensitive mdps, and exploration-sensitive mdps. Also, if R t = R and P t = P for all t, this result implies that real-time dynamic programming converges to the optimal value function</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we presented a generalized model of Markov decision processes, and proved the convergence of several reinforcement-learning algorithms in the generalized model.</p><p>Other Results We have derived a collection of results for the generalized mdp model that demonstrate its general applicability: the Bellman equations can be solved by value iteration; a myopic policy with respect to an approximately optimal value function gives an approximately optimal policy 9]; when N has a particular \maximization" property, policy iteration converges to the optimal value function; and, for models with nite state and action spaces, both value iteration and policy iteration identify optimal policies in pseudopolynomial time.</p><p>Related Work The work presented here is closely related to several previous research eeorts. Szepesvv ari 27] described a related generalized reinforcement-learning model, and presented conditions under which there is an optimal (stationary) policy that is myopic with respect to the optimal value function. Jaakkola, Jordan, and Singh 10] and Tsitsiklis 31] developed the connection between stochastic-approximation theory and reinforcement learning in mdps. Our work is similar in spirit to that of Jaakkola, et al. We believe the form of Theorem 1 makes it particularly convenient for proving the convergence of reinforcement-learning algorithms; our theorem reduces the proof of the convergence of an asynchronous process to a simpler proof of convergence of a corresponding synchronized one. This idea enables us to prove the convergence of asynchronous stochastic processes whose underlying synchronous process is not of the Robbins-Monro type (e.g., risk-sensitive mdps, model-based algorithms, etc.).</p><p>Future Work There are many areas of interest in the theory of reinforcement learning that we would like to address in future work. The results in this paper primarily concern reinforcement-learning in contractive models ( &lt; 1 or all-policies-proper), and there are important non-contractive reinforcement-learning scenarios, for example, reinforcement learning under an average-reward criterion 14]. It would be interesting to develop a TD() algorithm for generalized mdps; this has already been done for mdps Theorem 1 is not restricted to nite state spaces, and it might be valuable to prove the convergence of a reinforcement-learning algorithm for a innnite state-space model. Conclusion By identifying common elements among several reinforcement-learning scenarios, we created a new class of models that generalizes existing models in an interesting way. In the generalized framework, we replicated the established convergence proofs for reinforcement learning in Markov decision processes, and proved new results concerning the convergence of reinforcement-learning algorithms in game environments, under a risksensitive assumption, and under an exploration-sensitive assumption. At the heart of our results is a new stochastic-approximation theorem that is easy to apply to new situations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>tion on policies.</head><label></label><figDesc>The optimal value function is given by V (x) = sup 2P 0 V (x), where P 0 is the set of permitted (stationary) policies, and the associated Bellman equations are V</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The functions G t (x) = ( 0; if x 2 t ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>. The myopic policy with respect to a value function V is the policy V such that</head><label></label><figDesc></figDesc><table>V (x) = arg max a 

R(x; a) + 
X 

y 

P(x; a; y)V (y) 
! 
: 

Any myopic policy with respect to the optimal value function is optimal. 
The Bellman equations can be operationalized in the form of the dynamic-programming 

operator T, which maps value functions to value functions: 

TV ](x) = max a 


R(x; a) + 
X 

y 

P(x; a; y)V (y) 
! 
: 

For 0 &lt; 1, successive applications of T to a value function bring it closer and closer to 

the optimal value function V 

, which is the unique point of T: V 

= TV 

. 

In reinforcement learning, R and P are not known in advance. They can be learned 

from experience by keeping statistics on the expected reward for each state-action pair, and 
the proportion of transitions to each next state for each state-action pair. In model-based 

reinforcement learning, R and P are estimated on-line, and the value function is updated 

according to the approximate dynamic-programming operator derived from these estimates; 
this algorithm converges to the optimal value function under a wide variety of choices of the 
order states are updated 7]. 
The method of Q-learning uses experience to estimate the optimal value function 

without ever explicitly approximating R and P. The algorithm estimates the optimal Q 

function 

Q 

(x; a) = R(x; a) + 
X 

y 

P(x; a; y)V 

(y); 

from which the optimal value function can be computed via V (x) = max a Q (x; a). Given 

the experience at step t hx t ; a t ; y t ; r t i and the current estimate Q t (x; a) of the optimal Q 

function, Q-learning updates 

Q t+1 (x t ; a t ) := (1 ? t (x t ; a t ))Q t (x t ; a t ) + t (x t ; a t )(r t + max a Q t (y t ; a)); 

where 0 t (x; a) 1 is a learning rate that controls how quickly new estimates are blended 

into old estimates as a function of the state-action pair and the trial number. Q-learning 
converges to the optimal Q function under the proper conditions 31, 10]. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>L operator deenes how the value of the next state should be used in assigning value to the current state. The N operator deenes how an optimal agent should choose actions.</figDesc><table>When 0 &lt; 1 and N and L are non-expansions, the generalized Bellman equations 

have a unique optimal solution, and therefore, the optimal value function is well deened. model/example reference N 
a f(x; a) 
L 
y g(x; a; y) 

disc. exp. mdps 33] 

max a f(x; a) 
P 
y P(x; a; y)g(x; a; y) 
exp. return of 25] 
P 
a (x; a)f(x; a) 
P 
y P(x; a; y)g(x; a; y) 

alt. Markov games 

max a or min a f(x; a) 
P 
y P(x; a; y)g(x; a; y) 

risk-sensitive mdps 8] 

max a f(x; a) 
min y:P(x;a;y)&gt;0 g(x; a; y) 

exploration-sens. mdps 11] max 2P 0 
P 
a (x; a)f(x; a) 
P 
y P(x; a; y)g(x; a; y) 

Markov games 
max A min b 
P 
a A(a)f(x; (a; b)) P 
y P(x; (a; b); y)g(x; (a; b);y) 
information-state mdp 16] max a f(x; a) 
P 
y2N(x;a) P(x; a; y)g(x; a; y) 

Table 1: Some reinforcement-learning scenarios and their speciication as generalized Markov 
decision processes. 

The N operator is a non-expansion if 

O 

O 

a 

f 1 (x; a) ? 

a 

f 2 (x; a) 

max a jf 1 (x; a) ? f 2 (x; a)j 

for all f 1 , f 2 , and x. An analogous condition deenes when L is a non-expansion. 

Many natural operators are non-expansions, such as max, min, midpoint, median, mean, 
and xed weighted averages of these operations. Mode and Boltzmann-weighted averages 
are not non-expansions. Several previously described reinforcement-learning scenarios are 
special cases of this generalized mdp model|Table 1 gives a brief sampling. For more 
information about the speciic models listed, see the associated references. 
As with mdps, we can deene a dynamic-programming operator 

TV ](x) = 
O 


R(x; a) + 
M 

a 

y 

V (y) 
! 
(2) 

such that for 0 &lt; 1 the optimal value function V is the unique xed point of T. The 

operator T is a contraction mapping as long as &lt; 1. Recall that an operator T is a 

contraction mapping if 

sup x jTV 1 ](x) ? TV 2 ](x)j sup x jV 1 (x) ? V 2 (x)j 

where V 1 and V 2 are arbitrary functions and 0 &lt; 1 is the index of contraction. 

We can deene a notion of stationary myopic policies with respect to a value function V ; 
it is any (stochastic) policy V for which T V = TV where 

T V ](x) = 
X 

(x; a) 

R(x; a) + 
M 

a 

y 

V (y) 
! 
: 

Here (x; a) represents the probability that an agent following would choose action a in 
state x. To be certain that every value function possesses a myopic policy, we require that 

the operator N satisfy the following property: for all functions f and states x, min a f(x; a) 

N 

a f(x; a) max a f(x; a). The value function with respect to a policy , V can be deened by the simultaneous 

equations 

V (x) = 
X 

(x; a) 

R(x; a) + 
M 

a 

y 

V (y) 
! 
; 

it is unique. A policy is optimal if it is myopic with respect to its own value function. If 
is an optimal policy, then V is the xed point of T because V = T V = TV . 
Thus, V = V 

, when &lt; 1 because T has a unique point. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Consider the family of nite state and action generalized mdps deened by the Bellman equations</head><label>Consider</label><figDesc>We can derive the assumptions necessary for this learning algorithm to satisfy the con- ditions of Theorem 1 and therefore converge to the optimal Q values. The dynamic- programming operator deening the optimal Q function is</figDesc><table>V 

! 

(x) = 
O 


R(x; a) + 
X 

a 

y 

P(x; a; y)V 

(y) 

where the deenition of N does not depend on R or P. A Q-learning algorithm for this class 

of models can be deened as follows. Given experience hx t ; a t ; y t ; r t i at time t and an estimate 

Q t (x; a) of the optimal Q function, let 

Q t+1 (x t ; a t ) := (1 ? t (x t ; a t ))Q t (x t ; a t ) + t (x t ; a t ) 


r t + 
O 

a 

Q t (y t ; a) 
! 
: 

TQ](x; a) = R(x; a) + 
X 

P(x; a; y) 
O 

0 

y 

a 0 

Q(y; a 

): 

The randomized approximate dynamic-programming operator that gives rise to the Q-
learning rule is 

((T t Q 

0 

]Q)(x; a) = (1 ? t (x; a))Q 0 (x; a) + t (x; a)(r t + N 

a 0 Q(y t ; a 0 )); if x = x t and a = a t 
Q 0 (x; a); 

otherwise. 

</table></figure>

			<note place="foot" n="1"> A sequence of functions f n converges to f with probability 1 uniformly over X if, for the events w for which f n (w; x) ! f , the convergence is uniform in x.</note>

			<note place="foot" n="2"> This condition implies, among other things, that every state-action pair is updated innnitely often. Here, denotes the characteristic function.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning and sequential decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<idno>89-95</idno>
	</analytic>
	<monogr>
		<title level="m">Also published in Learning and Computational Neuroscience: Foundations of Adaptive Networks</title>
		<editor>Michael Gabriel and John Moore</editor>
		<meeting><address><addrLine>Amherst, Massachusetts; Cambridge, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1989" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer and Information Science, University of Massachusetts</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Parallel and Distributed Computation: Numerical Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliis, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Modular neural networks for learning context-dependent game strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">A</forename><surname>Boyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992-08" />
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Engineering and Computer Laboratory, University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The complexity of stochastic games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Condon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Computation</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="224" />
			<date type="published" when="1992-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Derman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Finite State Markovian Decision Processes</title>
		<editor>Richard Bellman</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1970" />
			<biblScope unit="volume">67</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convergence of indirect adaptive asynchronous value iteration algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaykumar</forename><surname>Gullapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 6</title>
		<editor>S. J. Hanson, J. D Cowan, and C. L. Giles</editor>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994-04" />
			<biblScope unit="page" from="695" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Consideration of risk in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Heger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Machine Learning</title>
		<meeting>the Eleventh International Conference on Machine Learning<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="105" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The loss from imperfect value functions in expectation-based and minimax-based tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Heger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note>In preparation</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the convergence of stochastic iterative dynamic programming algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1994-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">When the best move isn&apos;t optimal: Q-learning with exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
	<note>Available on the web</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-time heuristic search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Korf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artiicial Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="189" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Markov games as a framework for multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Machine Learning</title>
		<meeting>the Eleventh International Conference on Machine Learning<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Average reward reinforcement learning: Foundations, algorithms, and empirical results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sridhar Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Prioritized sweeping: Reinforcement learning with less data and less real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">G</forename><surname>Atkeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Approximating optimal policies for partially observable stochastic domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artiicial Intelligence</title>
		<meeting>the International Joint Conference on Artiicial Intelligence</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Incremental multi-step Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Machine Learning</title>
		<meeting>the Eleventh International Conference on Machine Learning<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="226" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Markov Decision Processes|Discrete Stochastic Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Q-learning with a spreading activiation rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Ribeiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note>Submitted to ML&apos;96</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="400" to="407" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using the TD() algorithm to learn an evaluation function for the game of Go</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A reinforcement learning method for maximizing undiscounted rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Machine Learning</title>
		<meeting>the Tenth International Conference on Machine Learning<address><addrLine>Amherst, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="298" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stochastic games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Shapley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="1953" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1095" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reinforcement learning with soft state aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to predict by the method of temporal diierences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="9" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Integrated architectures for learning, planning, and reacting based on approximating dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Machine Learning</title>
		<meeting>the Seventh International Conference on Machine Learning<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">General framework for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Szepesvv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICANN&apos;95</title>
		<meeting>ICANN&apos;95<address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Generalized markov decision processes: Dynamic-programming and reinforcement-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note>In preparation</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal diierence learning and TD-Gammon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="page" from="58" to="67" />
			<date type="published" when="1995-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to play the game of chess</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Asynchronous stochastic approximation and Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1994-09" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning from Delayed Rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C H</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<pubPlace>King&apos;s College, Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C H</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="279" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Tight performance bounds on greedy policies based on imperfect value functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leemon</forename><forename type="middle">C</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<idno>NU-CCS-93-14</idno>
		<imprint>
			<date type="published" when="1993-11" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Northeastern University, College of Computer Science</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
