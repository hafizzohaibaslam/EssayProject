<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-07-17T00:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Relational Data Mining: An Introduction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sa?o</forename><surname>D?eroski</surname></persName>
							<email>saso.dzeroski@ijs.si</email>
							<affiliation key="aff0">
								<orgName type="institution">Jo?ef Stefan Institute</orgName>
								<address>
									<addrLine>Jamova 39</addrLine>
									<postCode>SI-1000</postCode>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Relational Data Mining: An Introduction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Data mining algorithms look for patterns in data. While most existing data mining approaches look for patterns in a single data table, multi-relational data mining (MRDM) approaches look for patterns that involve multiple tables (relations) from a relational database. In recent years, the most common types of patterns and approaches considered in data mining have been extended to the multi-relational case and MRDM now encompasses multi-relational (MR) association rule discovery, MR decision trees and MR distance-based methods, among others. MRDM approaches have been successfully applied to a number of problems in a variety of areas, most notably in the area of bioinformatics. This article provides a brief introduction to MRDM, while the remainder of this special issue treats in detail advanced research topics at the frontiers of MRDM. For example, having extensional representations of the relations mother and father, we can intensionally define the relations grandparent, grandmother, sibling, and ancestor, among others. Intensional definitions of relations typically represent general knowledge about the domain of discourse. For example, if we have extensional relations listing the atoms that make a compound molecule and the bonds between them, functional groups of atoms can be defined intensionally. Such general knowledge is called domain knowledge or background knowledge. Keywords relational data mining, multi-relational data mining, inductive logic programming, relational association rules, relational decision trees, relational distance-based methods</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">IN A NUTSHELL</head><p>Data mining algorithms look for patterns in data. Most existing data mining approaches are propositional and look for patterns in a single data table. Relational data mining (RDM) approaches <ref type="bibr">[16]</ref>, many of which are based on inductive logic programming (ILP, <ref type="bibr" target="#b51">[35]</ref>), look for patterns that involve multiple tables (relations) from a relational database. To emphasize this fact, RDM is often referred to as multirelational data mining (MRDM, <ref type="bibr" target="#b24">[21]</ref>). In this article, we will use the terms RDM and MRDM interchangeably. In this introductory section, we take a look at data, patterns, and algorithms in RDM, and mention some application areas.  <ref type="table" target="#tab_0">Spouse1 Spouse2  c1  c2  c2  c1  c3  c4  c4  c3  c5  c12  c6</ref> c14 ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MarriedTo table</head><p>...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Relational data</head><p>A relational database typically consists of several tables (relations) and not just one table. The example database in <ref type="table" target="#tab_0">Table 1</ref> has two relations: Customer and MarriedTo. Note that relations can be defined extensionally (by tables, as in our example) or intensionally through database views (as explicit logical rules). The latter typically represent relationships that can be inferred from other relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Relational patterns</head><p>Relational patterns involve multiple relations from a relational database. They are typically stated in a more expressive language than patterns defined on a single data table. The major types of relational patterns extend the types of propositional patterns considered in single table data mining. We can thus have relational classification rules, relational regression trees, and relational association rules, among others.</p><p>An example relational classification rule is given in Table 1, which involves the relations Customer and MarriedTo. It predicts a person to be a big spender if the person is married to somebody with high income (compare this to the rule that states a person is a big spender if he has high income, listed above the relational rule). Note that the two persons C1 and C2 are connected through the relation MarriedTo.</p><p>Relational patterns are typically expressed in subsets of first-order logic (also called predicate or relational logic). Essentials of predicate logic include predicates (MarriedTo) and variables (C1, C2), which are not present in propositional logic. Relational patterns are thus more expressive than propositional ones.</p><p>Most commonly, the logic programming subset of firstorder logic, which is strongly related to deductive databases, is used as the formalism for expressing relational patterns. E.g., the relational rule in <ref type="table" target="#tab_0">Table 1</ref> is a logic program clause. Note that a relation in a relational database corresponds to a predicate in first-order logic (and logic programming).</p><p>Besides the ability to deal with data stored in multiple tables directly, RDM systems are usually able to take into account generally valid background (domain) knowledge given as a logic program. The ability to take into account background knowledge and the expressive power of the language of discovered patterns are distinctive for RDM.</p><p>Note that data mining approaches that find patterns in a given single table are referred to as attribute-value or propositional learning approaches, as the patterns they find can be expressed in propositional logic. RDM approaches are also referred to as first-order learning approaches, or relational learning approaches, as the patterns they find are expressed in the relational formalism of first-order logic. A more detailed discussion of the single table assumption, the problems resulting from it and how a relational representation alleviates these problems is given by Wrobel <ref type="bibr" target="#b49">[50]</ref> (Chapter 4 of <ref type="bibr">[16]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Algorithms for relational data mining</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Relational to propositional</head><p>RDM tools can be applied directly to multi-relational data to find relational patterns that involve multiple relations. Most other data mining approaches assume that the data resides in a single table and require preprocessing to integrate data from multiple tables (e.g., through joins or aggregation) into a single table before they can be applied. Integrating data from multiple tables through joins or aggregation, however, can cause loss of meaning or information.</p><p>Suppose we are given the relations customer(CustID, N ame, Age, SpendsALot) and purchase(CustID, P roductID, Date, V alue, P aymentM ode), where each customer can make multiple purchases, and we are interested in characterizing customers that spend a lot. Integrating the two relations via a natural join will give rise to a relation purchase1 where each row corresponds to a purchase and not to a customer. One possible aggregation would give rise to the relation customer1(CustID, Age, N of P urchases, T otalV alue, SpendsALot). In this case, however, some information has been clearly lost during aggregation.</p><p>The following pattern can be discovered if the relations customer and purchase are considered together.</p><p>customer(CID, N ame, Age, yes) ¡û Age &gt; 30 ¡Ä purchase(CID, P ID, D, V alue, P M ) ¡Ä P M = credit card ¡Ä V alue &gt; 100.</p><p>This pattern says: "a customer spends a lot if she is older than 30, has purchased a product of value more than 100 and paid for it by credit card." It would not be possible to induce such a pattern from either of the relations purchase1 and customer1 considered on their own.</p><p>A RDM algorithm searches a language of relational patterns to find patterns valid in a given database. The search algorithms used here are very similar to those used in single table data mining: one can search exhaustively or heuristically (greedy search, best-first search, etc.). Just as for the single table case, the space of patterns considered is typically lattice-structured and exploiting this structure is essential for achieving efficiency. The lattice structure is traversed by using refinement operators <ref type="bibr" target="#b42">[46]</ref>, which are more complicated in the relational case. In the propositional case, a refinement operator may add a condition to a rule antecedent or an item to an item set. In the relational case, a new relation can be introduced as well.</p><p>Just as many data mining algorithms come from the field of machine learning, many RDM algorithms come form the field of inductive logic programming (ILP, <ref type="bibr">[35; 30]</ref>). Situated at the intersection of machine learning and logic programming, ILP has been concerned with finding patterns expressed as logic programs. Initially, ILP focussed on automated program synthesis from examples, formulated as a binary classification task. In recent years, however, the scope of ILP has broadened to cover the whole spectrum of data mining tasks (classification, regression, clustering, association analysis). The most common types of patterns have been extended to their relational versions (relational classification rules, relational regression trees, relational association rules) and so have the major data mining algorithms (decision tree induction, distance-based clustering and prediction, etc.).</p><p>Van Laer and De Raedt <ref type="bibr" target="#b48">[49]</ref> (Chapter 10 of <ref type="bibr">[16]</ref>) present a generic approach of upgrading single table data mining algorithms (propositional learners) to relational ones (firstorder learners). Note that it is not trivial to extend a single table data mining algorithm to a relational one. Extending the key notions to, e.g., defining distance measures for multirelational data requires considerable insight and creativity. Efficiency concerns are also very important, as it is often the case that even testing a given relational pattern for validity is computationally expensive, let alone searching a space of such patterns for valid ones. An alternative approach to RDM (called propositionalization) is to create a single table from a multi-relational database in a systematic fashion <ref type="bibr" target="#b38">[28]</ref> (Chapter 11 of <ref type="bibr">[16]</ref>): this approach shares some efficiency concerns and in addition can have limited expressiveness. A pattern language typically contains a very large number of possible patterns even in the single table case: this number is in practice limited by setting some parameters (e.g., the largest size of frequent itemsets for association rule discovery). For relational pattern languages, the number of possible patterns is even larger and it becomes necessary to limit the space of possible patterns by providing more explicit constraints. These typically specify what relations should be involved in the patterns, how the relations can be interconnected, and what other syntactic constraints the patterns have to obey. The explicit specification of the pattern language (or constraints imposed upon it) is known under the name of declarative bias <ref type="bibr" target="#b30">[38]</ref>. <ref type="table">Table 2</ref>: Database and logic programming terms. DB terminology LP terminology relation name p predicate symbol p attribute of relation p argument of predicate p tuple a1, . . . , an ground fact p(a1, . . . , an) relation ppredicate pa set of tuples defined extensionally by a set of ground facts relation q predicate q defined as a view defined intensionally by a set of rules (clauses)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Applications of relational data mining</head><p>The use of RDM has enabled applications in areas rich with structured data and domain knowledge, which would be difficult to address with single table approaches. RDM has been used in different areas, ranging from analysis of business data, through environmental and traffic engineering to web mining, but has been especially successful in bioinformatics (including drug design and functional genomics). Bioinformatics applications of RDM are discussed in the article by Page and Craven in this issue. For a comprehensive survey of RDM applications we refer the reader to <ref type="bibr">D?eroski [20]</ref> (Chapter 14 of <ref type="bibr">[16]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6">What's in this article</head><p>The remainder of this article first gives a brief introduction to inductive logic programming, which (from the viewpoint of MRDM) is mainly concerned with the induction of relational classification rules for two-class problems. It then proceeds to introduce the basic MRDM techniques of discovery of relational association rules, induction of relational decision trees and relational distance-based methods (that include both classification and clustering). The article concludes with an overview of the MRDM literature and Internet resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">INDUCTIVE LOGIC PROGRAMMING</head><p>From a KDD perspective, we can say that inductive logic programming (ILP) is concerned with the development of techniques and tools for relational data mining. Patterns discovered by ILP systems are typically expressed as logic programs, an important subset of first-order (predicate) logic, also called relational logic. In this section, we first briefly discuss the language of logic programs, then proceed with a discussion of the major task of ILP and some approaches to solving it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Logic programs and databases</head><p>Logic programs consist of clauses. We can think of clauses as first-order rules, where the conclusion part is termed the head and the condition part the body of the clause. The head and body of a clause consist of atoms, an atom being a predicate applied to some arguments, which are called terms. In Datalog, terms are variables and constants, while in general they may consist of function symbols applied to other terms. Ground clauses have no variables.</p><p>Consider the clause f ather(X, Y ) ¡Å mother(X, Y ) ¡û parent(X, Y ). It reads: "if X is a parent of Y then X is the father of Y or X is the mother of Y" (¡Å stands for logical or). parent(X, Y ) is the body of the clause and f ather(X, Y ) ¡Å mother(X, Y ) is the head. parent, f ather and mother are predicates, X and Y are variables, and parent(X, Y ), f ather(X, Y ), mother(X, Y ) are atoms. We adopt the Prolog <ref type="bibr" target="#b20">[4]</ref> syntax and start variable names with capital letters. Variables in clauses are implicitly universally quantified. The above clause thus stands for the logical formula</p><formula xml:id="formula_0">?X?Y : f ather(X, Y ) ¡Å mother(X, Y ) ¡Å ?parent(X, Y ).</formula><p>Clauses are also viewed as sets of literals, where a literal is an atom or its negation. The above clause is then the set {f ather(X, Y ), mother(X, Y ), ?parent(X, Y )}.</p><p>As opposed to full clauses, definite clauses contain exactly one atom in the head. As compared to definite clauses, program clauses can also contain negated atoms in the body. While the clause in the paragraph above is a full clause, the clause ancestor(X, Y ) ¡û parent(Z, Y ) ¡Ä ancestor(X, Z) is a definite clause (¡Ä stands for logical and). It is also a recursive clause, since it defines the relation ancestor in terms of itself and the relation parent. The clause mother(X, Y ) ¡û parent(X, Y ) ¡Ä not male(X) is a program clause.</p><p>A set of clauses is called a clausal theory. Logic programs are sets of program clauses. A set of program clauses with the same predicate in the head is called a predicate definition. Most ILP approaches learn predicate definitions.</p><p>A predicate in logic programming corresponds to a relation in a relational database. A n-ary relation p is formally defined as a set of tuples <ref type="bibr" target="#b46">[48]</ref>, i.e., a subset of the Cartesian product of n domains D1 ¡Á D2 ¡Á . . . ¡Á Dn, where a domain (or a type) is a set of values. It is assumed that a relation is finite unless stated otherwise. A relational database (RDB) is a set of relations.</p><p>Thus, a predicate corresponds to a relation, and the arguments of a predicate correspond to the attributes of a relation. The major difference is that the attributes of a relation are typed (i.e., a domain is associated with each attribute). For example, in the relation lives in(X, Y ), we may want to specify that X is of type person and Y is of type city. Database clauses are typed program clauses.</p><p>A deductive database (DDB) is a set of database clauses. In deductive databases, relations can be defined extensionally as sets of tuples (as in RDBs) or intensionally as sets of database clauses. Database clauses use variables and function symbols in predicate arguments and the language of DDBs is substantially more expressive than the language of RDBs <ref type="bibr">[31; 48]</ref>. A deductive Datalog database consists of definite database clauses with no function symbols. <ref type="table">Table 2</ref> relates basic database and logic programming terms. For a full treatment of logic programming, RDBs, and deductive databases, we refer the reader to <ref type="bibr" target="#b43">[31]</ref> and <ref type="bibr" target="#b46">[48]</ref>. <ref type="table">Table 3</ref>: A simple ILP problem: learning the daughter relation. Positive examples are denoted by ? and negative by .</p><p>Training examples Background knowledge daughter(mary, ann). ? parent(ann, mary). f emale(ann). daughter(eve, tom).</p><p>? parent(ann, tom). f emale(mary). daughter(tom, ann). parent(tom, eve). f emale(eve). daughter(eve, ann). parent(tom, ian).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The ILP task of relational rule induction</head><p>Logic programming as a subset of first-order logic is mostly concerned with deductive inference. Inductive logic programming, on the other hand, is concerned with inductive inference. It generalizes from individual instances/observations in the presence of background knowledge, finding regularities/hypotheses about yet unseen instances. The most commonly addressed task in ILP is the task of learning logical definitions of relations <ref type="bibr" target="#b36">[42]</ref>, where tuples that belong or do not belong to the target relation are given as examples. From training examples ILP then induces a logic program (predicate definition) corresponding to a view that defines the target relation in terms of other relations that are given as background knowledge. This classical ILP task is addressed, for instance, by the seminal MIS system <ref type="bibr" target="#b42">[46]</ref> (rightfully considered as one of the most influential ancestors of ILP) and one of the best known ILP systems FOIL <ref type="bibr" target="#b36">[42]</ref>.</p><p>Given is a set of examples, i.e., tuples that belong to the target relation p (positive examples) and tuples that do not belong to p (negative examples). Given are also background relations (or background predicates) qi that constitute the background knowledge and can be used in the learned definition of p. Finally, a hypothesis language, specifying syntactic restrictions on the definition of p is also given (either explicitly or implicitly). The task is to find a definition of the target relation p that is consistent and complete, i.e., explains all the positive and none of the negative tuples.</p><p>Formally, given is a set of examples E = P ¡È N , where P contains positive and N negative examples, and background knowledge B. The task is to find a hypothesis H such that ?e ¡Ê P : B ¡ÄH |= e (H is complete) and ?e ¡Ê N : B ¡ÄH |= e (H is consistent), where |= stands for logical implication or entailment. This setting, introduced by Muggleton <ref type="bibr" target="#b50">[34]</ref>, is thus also called learning from entailment. In an alternative setting proposed by <ref type="bibr">De Raedt and D?eroski [15]</ref>, the requirement that B ¡Ä H |= e is replaced by the requirement that H be true in the minimal Herbrand model of B ¡Ä e: this setting is called learning from interpretations.</p><p>In the most general formulation, each e, as well as B and H can be a clausal theory. In practice, each e is most often a ground example (tuple), B is a relational database (which may or may not contain views) and H is a definite logic program. The semantic entailment (|=) is in practice replaced with syntactic entailment () or provability, where the resolution inference rule (as implemented in Prolog) is most often used to prove examples from a hypothesis and the background knowledge. In learning from entailment, a positive fact is explained if it can be found among the answer substitutions for h produced by a query ? ? b on database B, where h ¡û b is a clause in H. In learning from interpretations, a clause h ¡û b from H is true in the minimal Herbrand model of B if the query b ¡Ä ?h fails on B.</p><p>As an illustration, consider the task of defining relation daughter(X, Y ), which states that person X is a daughter of person Y , in terms of the background knowledge relations f emale and parent. These relations are given in <ref type="table">Table 3</ref>. There are two positive and two negative examples of the target relation daughter. In the hypothesis language of definite program clauses it is possible to formulate the following definition of the target relation,</p><formula xml:id="formula_1">daughter(X, Y ) ¡û f emale(X), parent(Y, X).</formula><p>which is consistent and complete with respect to the background knowledge and the training examples.</p><p>In general, depending on the background knowledge, the hypothesis language and the complexity of the target concept, the target predicate definition may consist of a set of clauses, such as</p><formula xml:id="formula_2">daughter(X, Y ) ¡û f emale(X), mother(Y, X). daughter(X, Y ) ¡û f emale(X), f ather(Y, X).</formula><p>if the relations mother and f ather were given in the background knowledge instead of the parent relation.</p><p>The hypothesis language is typically a subset of the language of program clauses. As the complexity of learning grows with the expressiveness of the hypothesis language, restrictions have to be imposed on hypothesized clauses. Typical restrictions are the exclusion of recursion and restrictions on variables that appear in the body of the clause but not in its head (so-called new variables).</p><p>From a data mining perspective, the task described above is a binary classification task, where one of two classes is assigned to the examples (tuples): ? (positive) or (negative). Classification is one of the most commonly addressed tasks within the data mining community and includes approaches for rule induction. Rules can be generated from decision trees <ref type="bibr" target="#b37">[43]</ref> or induced directly <ref type="bibr">[33; 7]</ref>. ILP systems dealing with the classification task typically adopt the covering approach of rule induction systems. In a main loop, a covering algorithm constructs a set of clauses. Starting from an empty set of clauses, it constructs a clause explaining some of the positive examples, adds this clause to the hypothesis, and removes the positive examples explained. These steps are repeated until all positive examples have been explained (the hypothesis is complete).</p><p>In the inner loop of the covering algorithm, individual clauses are constructed by (heuristically) searching the space of possible clauses, structured by a specialization or generalization operator. Typically, search starts with a very general rule (clause with no conditions in the body), then proceeds to add literals (conditions) to this clause until it only covers (explains) positive examples (the clause is consistent).</p><p>When dealing with incomplete or noisy data, which is most often the case, the criteria of consistency and completeness are relaxed. Statistical criteria are typically used instead. These are based on the number of positive and negative examples explained by the definition and the individual constituent clauses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Structuring the space of clauses</head><p>Having described how to learn sets of clauses by using the covering algorithm for clause/rule set induction, let us now look at some of the mechanisms underlying single clause/rule induction. In order to search the space of relational rules (program clauses) systematically, it is useful to impose some structure upon it, e.g., an ordering. One such ordering is based on ¦È-subsumption, defined below.</p><p>A substitution ¦È = {V1/t1, ..., Vn/tn} is an assignment of terms ti to variables Vi. Applying a substitution ¦È to a term, atom, or clause F yields the instantiated term, atom, or clause F ¦È where all occurrences of the variables Vi are simultaneously replaced by the term ti. Let c and c be two program clauses. Clause c ¦È-subsumes c if there exists a substitution ¦È, such that c¦È ? c <ref type="bibr" target="#b34">[41]</ref>. To illustrate the above notions, consider the clause c = daughter(X, Y ) ¡û parent(Y, X). Applying the substitution ¦È = {X/mary, Y /ann} to clause c yields have a least upper bound (lub) and a greatest lower bound (glb). Both the lub and the glb are unique up to equivalence (renaming of variables) under ¦È-subsumption. Reduced clauses are the minimal representatives of the equivalence classes of clauses defined by ¦È-subsumption. For example, the clauses daughter(X, Y ) ¡û parent(Y, X), parent(W, V ) and daughter(X, Y ) ¡û parent(Y, X) ¦È-subsume one another and are thus equivalent. The latter is reduced, while the former is not. c¦È = daughter(mary, ann) ¡û parent(ann, mary).</p><p>Clauses can be viewed as sets of literals: the clausal notation daughter(X, Y ) ¡û parent(Y, X) thus stands for {daughter(X, Y ), parent(Y, X)} where all variables are assumed to be universally quantified, overline denotes logical negation, and the commas denote disjunction. According to the definition, clause c ¦È-subsumes c if there is a substitution ¦È that can be applied to c such that every literal in the resulting clause occurs in c . Clause c ¦È-subsumes the clause</p><p>The second property of ¦È-subsumption leads to the following definition: The least general generalization (lgg) of two clauses c and c , denoted by lgg(c, c ), is the least upper bound of c and c in the ¦È-subsumption lattice <ref type="bibr" target="#b34">[41]</ref>. The rules for computing the lgg of two clauses are outlined later in this chapter.</p><p>Note that ¦È-subsumption and least general generalization are purely syntactic notions since they do not take into account any background knowledge. Their computation is therefore simple and easy to be implemented in an ILP system. The same holds for the notion of generality based on ¦È-subsumption. On the other hand, taking background knowledge into account would lead to the notion of semantic generality <ref type="bibr">[39; 6]</ref>, defined as follows: Clause c is at least as general as clause c with respect to background theory B if B ¡È {c} |= c . The syntactic, ¦È-subsumption based, generality is computationally more feasible. Namely, semantic generality is in general undecidable and does not introduce a lattice on a set of clauses. Because of these problems, syntactic generality is more frequently used in ILP systems.</p><p>¦È-subsumption is important for inductive logic programming for the following reasons:</p><formula xml:id="formula_3">c = daughter(X, Y ) ¡û f emale(X), parent(Y, X)</formula><p>under the empty substitution ¦È = ?, since {daughter(X, Y ), parent(Y, X)} is a proper subset of {daughter(X, Y ), f emale(X), parent(Y, X)}. Furthermore, under the substitution ¦È = {X/mary, Y /ann}, clause c ¦È-subsumes the clause c = daughter(mary, ann) ¡û f emale(mary), parent(ann, mary), parent(ann, tom). ¦È-subsumption introduces a syntactic notion of generality. Clause c is at least as general as clause c</p><formula xml:id="formula_4">(c ¡Ü c ) if c ¦È- subsumes c . Clause c is more general than c (c &lt; c ) if c ¡Ü c</formula><p>holds and c ¡Ü c does not. In this case, we say that c is a specialization of c and c is a generalization of c . If the clause c is a specialization of c then c is also called a refinement of c. The only clause refinements usually considered by ILP systems are the minimal (most general) specializations of the clause.</p><p>There are two important properties of ¦È-subsumption:</p><p>? As shown above, it provides a generality ordering for hypotheses, thus structuring the hypothesis space. It can be used to prune large parts of the search space.</p><p>? ¦È-subsumption provides the basis for the following important ILP techniques:</p><p>-clause construction by top-down searching of refinement graphs,</p><p>-bounding the search of refinement graphs from below by the bottom clause constructed as * the least general generalization of two (or more) training examples, relative to the given background knowledge <ref type="bibr" target="#b27">[37]</ref>, or * the most specific inverse resolvent of an example with respect to the given background knowledge <ref type="bibr" target="#b50">[34]</ref>.</p><p>? If c ¦È-subsumes c then c logically entails c , c |= c . The reverse is not always true. As an example, <ref type="bibr" target="#b28">[23]</ref> gives the following two clauses c = list(cons(V, W )) ¡û list(W ) and c = list(cons(X, cons(Y, Z))) ¡û list(Z). Given the empty list, c constructs lists of any given length, while c constructs lists of even length only, and thus c |= c . However, no substitution ¦È exists such that c¦È = c , since ¦È should map W both to cons(Y, Z) and to Z, which is impossible. Therefore, c does not ¦È-subsume c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Searching the space of clauses</head><p>? The relation ¡Ü introduces a lattice on the set of reduced clauses <ref type="bibr" target="#b34">[41]</ref>. This means that any two reduced clauses Most ILP approaches search the hypothesis space of program clauses in a top-down manner, from general to specific hypotheses, using a ¦È-subsumption-based specialization operator. A specialization operator is usually called a refinement operator <ref type="bibr" target="#b42">[46]</ref>. Given a hypothesis language L, a refinement operator ¦Ñ maps a clause c to a set of clauses ¦Ñ(c) which are specializations (refinements) of c: ¦Ñ(c) = {c | c ¡Ê L, c &lt; c }. A refinement operator typically computes only the set of minimal (most general) specializations of a clause under ¦È-subsumption. It employs two basic syntactic operations:</p><formula xml:id="formula_5">daughter(X, Y ) ¡û A ? ? ? ? ? ? ? ? ? d d ? ? ? z d ? ¡¤ ¡¤ ¡¤ daughter(X, Y ) ¡û parent(X, Z) daughter(X, Y ) ¡û X = Y daughter(X, Y ) ¡û parent(Y, X) daughter(X, Y ) ¡û f emale(X) ? ? ? ? ? ? ? ? ? ? ¡§ ¡§ ¡§ ¡§ ¡§ ¡§ % r r r r r r j ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ daughter(X, Y ) ¡û f emale(X) f emale(Y ) daughter(X, Y ) ¡û f emale(X) parent(Y, X)</formula><p>The search for a clause starts at the top of the lattice, with the clause d(X, Y ) ¡û that covers all example (positive and negative). Its refinements are then considered, then their refinements in turn, an this is repeated until a clause is found which covers only positive examples. In the example above, the clause daughter(X, Y ) ¡û f emale(X), parent(Y, X) is such a clause. Note that this clause can be reached in several ways from the top of the lattice, e.g., by first adding f emale(X), then parent(Y, X) or vice versa.</p><p>The refinement graph is typically searched heuristically level-wise, using heuristics based on the number of positive and negative examples covered by a clause. As the branching factor is very large, greedy search methods are typically applied which only consider a limited number of alternatives at each level. Hill-climbing considers only one best alternative at each level, while beam search considers n best alternatives, where n is the beam width. Occasionally, complete search is used, e.g., A * best-first search or breadth-first search. This search can be bound from below by using socalled bottom clauses, which can be constructed by least general generalization <ref type="bibr" target="#b27">[37]</ref> or inverse resolution/entailment <ref type="bibr" target="#b52">[36]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Transforming ILP problems to propositional form</head><p>? apply a substitution to the clause, and</p><p>? add a literal to the body of the clause.</p><p>The hypothesis space of program clauses is a lattice, structured by the ¦È-subsumption generality ordering. In this lattice, a refinement graph can be defined as a directed, acyclic graph in which nodes are program clauses and arcs correspond to the basic refinement operations: substituting a variable with a term, and adding a literal to the body of a clause. <ref type="figure" target="#fig_0">Figure 1</ref> depicts a part of the refinement graph for the family relations problem defined in <ref type="table">Table 3</ref>, where the task is to learn a definition of the daughter relation in terms of the relations f emale and parent.</p><p>At the top of the refinement graph (lattice) is the clause</p><formula xml:id="formula_6">c = daughter(X, Y ) ¡û</formula><p>where an empty body is written instead of the body true. The refinement operator ¦Ñ generates the refinements of c, which are of the form</p><p>One of the early approaches to ILP, implemented in the ILP system LINUS <ref type="bibr" target="#b39">[29]</ref>, is based on the idea that the use of background knowledge can introduce new attributes for learning. The learning problem is transformed from relational to attribute-value form and solved by an attributevalue learner. An advantage of this approach is that data mining algorithms that work on a single table (and this is the majority of existing data mining algorithms) become applicable after the transformation.</p><p>This approach, however, is feasible only for a restricted class of ILP problems. Thus, the hypothesis language of LI-NUS is restricted to function-free program clauses which are typed (each variable is associated with a predetermined set of values), constrained (all variables in the body of a clause also appear in the head) and nonrecursive (the predicate symbol the head does not appear in any of the literals in the body).</p><p>The LINUS algorithm which solves ILP problems by transforming them into propositional form consists of the following three steps:</p><formula xml:id="formula_7">¦Ñ(c) = {daughter(X, Y ) ¡û L}</formula><p>? The learning problem is transformed from relational to attribute-value form.</p><p>where L is one of following literals:</p><p>? literals having as arguments the variables from the head of the clause: X = Y (this corresponds to applying a substitution</p><formula xml:id="formula_8">X/Y ), f emale(X), f emale(Y ), parent(X, X), parent(X, Y ), parent(Y, X)</formula><p>, and parent(Y, Y ), and</p><p>? The transformed learning problem is solved by an attributevalue learner.</p><p>? The induced hypothesis is transformed back into relational form.</p><p>? literals that introduce a new distinct variable Z (Z = X and Z = Y ) in the clause body: parent(X, Z), parent(Z, X), parent(Y, Z), and parent(Z, Y ).</p><p>This assumes that the language is restricted to definite clauses, hence literals of the form not L are not considered, and nonrecursive clauses, hence literals with the predicate symbol daughter are not considered.</p><p>The above algorithm allows for a variety of approaches developed for propositional problems, including noise-handling techniques in attribute-value algorithms, such as CN2 <ref type="bibr">[8]</ref>, to be used for learning relations. It is illustrated on the simple ILP problem of learning family relations. The task is to define the target relation daughter(X, Y ), which states that person X is a daughter of person Y , in terms of the background knowledge relations f emale, male and parent. </p><note type="other">: Non-ground background knowledge for learning the daughter relation. Training examples</note><p>Background knowledge daughter(mary, ann). ? parent(X, Y ) ¡û mother(ann, mary). f emale(ann). daughter(eve, tom).</p><p>? mother(X, Y ). mother(ann, tom). f emale(mary). daughter(tom, ann). parent(X, Y ) ¡û f ather(tom, eve). f emale(eve). daughter(eve, ann).</p><p>f ather(X, Y ). f ather(tom, ian). <ref type="table">Table 5</ref>: Propositional form of the daughter relation problem.</p><formula xml:id="formula_9">Variables Propositional features C X Y f (X) f (Y ) m(X) m(Y ) p(X, X) p(X, Y ) p(Y, X) p(Y, Y ) ?</formula><note type="other">mary ann true true f alse f alse f alse f alse true f alse ? eve tom true f alse f alse true f alse f alse true f alse tom ann f alse true true f alse f alse f alse true f alse eve ann true true f alse f alse f alse f alse f alse f alse</note><p>All the variables are of the type person, defined as person = {ann, eve, ian, mary, tom}. There are two positive and two negative examples of the target relation. The training examples and the relations from the background knowledge are given in <ref type="table">Table 3</ref>. However, since the LINUS approach can use non-ground background knowledge, let us assume that the background knowledge from <ref type="table" target="#tab_1">Table 4</ref> is given.</p><p>The first step of the algorithm, i.e., the transformation of the ILP problem into attribute-value form, is performed as follows. The possible applications of the background predicates on the arguments of the target relation are determined, taking into account argument types. Each such application introduces a new attribute. In our example, all variables are of the same type person. The corresponding attribute-value learning problem is given in <ref type="table">Table 5</ref>, where f stands for f emale, m for male and p for parent. The attribute-value tuples are generalizations (relative to the given background knowledge) of the individual facts about the target relation.</p><p>In <ref type="table">Table 5</ref>, variables stand for the arguments of the target relation, and propositional features denote the newly constructed attributes of the propositional learning task. When learning function-free clauses, only the new attributes (propositional features) are considered for learning.</p><p>In the second step, an attribute-value learning program induces the following if-then rule from the tuples in <ref type="table">Table 5</ref>:</p><formula xml:id="formula_10">Class = ? if [f emale(X) = true]¡Ä[parent(Y, X) = true]</formula><p>In the last step, the induced if-then rules are transformed into clauses. In our example, we get the following clause:</p><formula xml:id="formula_11">daughter(X, Y ) ¡û f emale(X), parent(Y, X).</formula><p>The LINUS approach has been extended to handle determinate clauses <ref type="bibr">[17; 30]</ref>, which allow the introduction of determinate new variables (which have a unique value for each training example). There also exist a number of other approaches to propositionalization, some of them very recent: an overview is given by Kramer et al. <ref type="bibr" target="#b38">[28]</ref>.</p><p>Let us emphasize again, however, that it is in general not possible to transform an ILP problem into a propositional (attribute-value) form efficiently. De Raedt <ref type="bibr">[13]</ref> treats the relation between attribute-value learning and ILP in detail, showing that propositionalization of some more complex ILP problems is possible, but results in attribute-value problems that are exponentially large. This has also been the main reason for the development of a variety of new RDM/ILP techniques by upgrading propositional approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Upgrading propositional approaches</head><p>ILP/RDM algorithms have many things in common with propositional learning algorithms. In particular, they share the learning as search paradigm, i.e., they search for patterns valid in the given data. The key differences lie in the representation of data and patterns, refinement operators/generality relationships, and testing coverage (i.e., whether a rule explains an example).</p><p>Van Laer and De Raedt <ref type="bibr" target="#b48">[49]</ref> explicitly formulate a recipe for upgrading propositional algorithms to deal with relational data and patterns. The key idea is to keep as much of the propositional algorithm as possible and upgrade only the key notions. For rule induction, the key notions are the refinement operator and coverage relationship. For distancebased approaches, the notion of distance is the key one. By carefully upgrading the key notions of a propositional algorithm, a RDM/ILP algorithm can be developed that has the original propositional algorithm as a special case.</p><p>The recipe has been followed (more or less exactly) to develop ILP systems for rule induction, well before it was formulated explicitly. The well known FOIL <ref type="bibr" target="#b36">[42]</ref> system can be seen as an upgrade of the propositional rule induction program CN2 <ref type="bibr">[8]</ref>. Another well known ILP system, PRO-GOL <ref type="bibr" target="#b52">[36]</ref> can be viewed as upgrading the AQ approach <ref type="bibr" target="#b47">[33]</ref> to rule induction.</p><p>More recently, the upgrading approach has been used to develop a number of RDM approaches that address data mining tasks other than binary classification. These include the discovery of frequent Datalog patterns and relational association rules <ref type="bibr" target="#b8">[11]</ref> (Chapter 8 of <ref type="bibr">[16]</ref>) <ref type="bibr" target="#b7">[10]</ref>, the induction of relational decision trees (structural classification and regression trees <ref type="bibr" target="#b35">[27]</ref> and first-order logical decision trees <ref type="bibr" target="#b19">[3]</ref>), and relational distance-based approaches to classification and clustering ( <ref type="bibr" target="#b32">[25]</ref>, Chapter 9 of <ref type="bibr">[16]</ref>; <ref type="bibr" target="#b25">[22]</ref>). The algorithms developed have as special cases well known propositional algorithms, such as the APRIORI algorithm for finding frequent patterns; the CART and C4.5 algorithms for learning decision trees; k-nearest neighbor classification, hierarchical and k-medoids clustering. In the following three sections, we briefly review how each of the propositional approaches has been lifted to a relational framework, highlighting the key differences between the relational algorithms and their propositional counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RELATIONAL ASSOCIATION RULES</head><p>The discovery of frequent patterns and association rules is one of the most commonly studied tasks in data mining. Here we first describe frequent relational patterns (frequent Datalog patterns) and relational association rules (query extensions). We then look into how a well-known algorithm for finding frequent itemsets has been upgraded do discover frequent relational patterns.</p><p>answer substitutions ¦È for the variables in the key atom for which the query Q¦È succeeds in the given database, i.e., a(Q, r, key) = |{¦È ¡Ê answerset(key, r)|Q¦È succeds w.r.t. r}|. The relative frequency (support) can be calculated as f (Q, r, key) = a(Q, r, key)/|{¦È ¡Ê answerset(key, r)}|. Assuming the key is person(X), the absolute frequency for our query involving parents, children and pets can be calculated by the following SQL statement:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Frequent Datalog queries and query extensions</head><p>Dehaspe and Toivonen <ref type="bibr" target="#b7">[10]</ref>, <ref type="bibr" target="#b8">[11]</ref> (Chapter 8 of <ref type="bibr">[16]</ref>) consider patterns in the form of Datalog queries, which reduce to SQL queries.</p><p>A Datalog query has the form ? ? A1, A2, . . . An, where the Ai's are logical atoms.</p><p>An example Datalog query is Datalog queries can be viewed as a relational version of itemsets (which are sets of items occurring together). Consider the itemset {person, parent, child, pet}. The marketbasket interpretation of this pattern is that a person, a parent, a child, and a pet occur together. This is also partly the meaning of the above query. However, the variables X, Y , and Z add extra information: the person and the parent are the same, the parent and the child belong to the same family, and the pet belongs to the child. This illustrates the fact that queries are a more expressive variant of itemsets.</p><p>To discover frequent patterns, we need to have a notion of frequency. Given that we consider queries as patterns and that queries can have variables, it is not immediately obvious what the frequency of a given query is. This is resolved by specifying an additional parameter of the pattern discovery task, called the key. The key is an atom which has to be present in all queries considered during the discovery process. It determines what is actually counted. In the above query, if person(X) is the key, we count persons, if parent(X, Y ) is the key, we count (parent,child) pairs, and if hasP et(Y, Z) is the key, we count (owner,pet) pairs. This is described more precisely below.</p><p>Submitting a query Q =? ? A1, A2, . . . An with variables {X1, . . . Xm} to a Datalog database r corresponds to asking whether a grounding substitution exists (which replaces each of the variables in Q with a constant), such that the conjunction A1, A2, . . . An holds in r. The answer to the query produces answering substitutions ¦È = {X1/a1, . . . Xm/am} such that Q¦È succeeds. The set of all answering substitutions obtained by submitting a query Q to a Datalog database r is denoted answerset(Q, r).</p><p>The absolute frequency of a query Q is the number of Association rules have the form A ¡ú C and the intuitive market-basket interpretation "customers that buy A typically also buy C". Given A and C have supports fA and fC , respectively, the confidence of the association rule is defined to be cA¡úC = fC /fA. The task of association rule discovery is to find all association rules A ¡ú C, where fC and cA¡úC exceed prespecified thresholds (minsup and minconf).</p><p>Association rules are typically obtained from frequent itemsets. Suppose we have two frequent itemsets A and C, such that A ? C, where C = A¡ÈB. If the support of A is fA and the support of C is fC , we can derive an association rule A ¡ú B, which has confidence fC /fA. Treating the arrow as implication, note that we can derive A ¡ú C from A ¡ú B (A ¡ú A and A ¡ú B implies A ¡ú A ¡È B, i.e., A ¡ú C).</p><p>Relational association rules can be derived in a similar manner from frequent Datalog queries. From two frequent queries Q1 =? ? l1, . . . lm and Q2 =? ? l1, . . . lm, lm+1, . . . ln, where Q2 ¦È-subsumes Q1, we can derive a relational association rule Q1 ¡ú Q2. Since Q2 extends Q1, such a relational association rules is named a query extension.</p><p>A query extension is thus an existentially quantified implication of the form ? ? l1, . . . lm ¡ú? ? l1, . . . lm, lm+1, . . . ln (since variables in queries are existentially quantified). A shorthand notation for the above query extension is ? ? l1, . . . lm ; lm+1, . . . ln. We call the query ? ? l1, . . . lm the body and the sub-query lm+1, . . . ln the head of the query extension. Note, however, that the head of the query extension does not correspond to its conclusion (which is ? ? l1, . . . lm, lm+1, . . . ln).</p><p>Assume  <ref type="figure">Z)</ref>) is trivially true if at least one person in the database has a pet. The correct interpretation of the query extension E is: "if a person has a child, then this person also has a child that has a pet."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discovering frequent queries: WARMR</head><p>The task of discovering frequent queries is addressed by the RDM system WARMR <ref type="bibr" target="#b7">[10]</ref>. WARMR takes as input a database r, a frequency threshold minf req, and declarative language bias L. The latter specifies a key atom and inputoutput modes for predicates/relations, discussed below.</p><p>WARMR upgrades the well-known APRIORI algorithm for discovering frequent patterns, which performs levelwise search <ref type="bibr" target="#b16">[2]</ref> through the lattice of itemsets. APRIORI starts with the empty set of items and at each level l considers sets of items of cardinality l. The key to the efficiency of APRIORI lies in the fact that a large frequent itemset can only be generated by adding an item to a frequent itemset. Candidates at level l + 1 are thus generated by adding items to frequent itemsets obtained at level l. Further efficiency is achieved using the fact that all subsets of a frequent itemset have to be frequent: only candidates that pass this tests get their frequency to be determined by scanning the database.</p><p>In analogy to APRIORI, WARMR searches the lattice of Datalog queries for queries that are frequent in the given database r. In analogy to itemsets, a more complex (specific) frequent query Q2 can only be generated from a simpler (more general) frequent query Q1 (where Q1 is more general than Q2 if Q1 ¦È-subsumes Q2; see Section 2.3 for a definition of ¦È-subsumption). WARMR thus starts with the query ? ? key at level 1 and generates candidates for frequent queries at level l + 1 by refining (adding literals to) frequent queries obtained at level l.</p><p>is not allowed, as it violates the declarative bias constraint that the first argument of hasP et has to appear earlier in the query. This causes some complications in pruning the generated candidates for frequent queries: WARMR keeps a list of infrequent queries and checks whether the generated candidates are subsumed by a query in this list. The WARMR algorithm is given in <ref type="table">Table 7</ref>. <ref type="table">Table 7</ref> Find frequency of all queries Q ¡Ê Q d</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.</head><p>Move those with frequency below minfreq to I</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.</head><p>Update</p><formula xml:id="formula_12">F := F ¡È Q d 8.</formula><p>Compute new candidates: <ref type="table">Table 6</ref>: An example specification of declarative language bias settings for WARMR.</p><formula xml:id="formula_13">Q d+1 = WARMRgen(L; I; F; Q d ) )</formula><p>warmode key(person(-)). warmode(parent(+, -)). warmode(hasPet(+, cat)). warmode(hasPet(+, dog)). warmode(hasPet(+, lizard)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Increment d</head><p>10. Return F Function WARMRgen(L; I; F; Q d );</p><p>1. Initialize Q d+1 := ? 2. For each Qj ¡Ê Q d , and for each refinement Q Suppose we are given a Prolog database containing the predicates person, parent, and hasP et, and the declarative bias in <ref type="table">Table 6</ref>. The latter contains the key atom parent(X) and input-output modes for the relations parent and hasP et. Input-output modes specify whether a variable argument of an atom in a query has to appear earlier in the query (+), must not (-) or may, but need not to (¡À). Inputoutput modes thus place constraints on how queries can be refined, i.e., what atoms may be added to a given query.</p><p>Given the above, WARMR starts the search of the refinement graph of queries at level 1 with the query ? ? person(X).</p><p>At level 2, the literals parent(X, Y ), hasP et(X, cat), hasP et(X, dog) and hasP et(X, lizard) can be added to this query, yielding the candidate queries ??person(X), parent(X, Y ), ??person(X), hasP et(X, cat), ? ? person(X), hasP et(X, dog), and ? ? person(X), hasP et(X, lizard). Taking the first of the level 2 queries, we the following literals are added to obtain level 3 queries: parent(Y, Z) (note that parent(Y, X) cannot be added, because X already appears in the query being refined), hasP et(Y, cat), hasP et(Y, dog) and hasP et <ref type="bibr">(Y, lizard)</ref>.</p><p>While all subsets of a frequent itemset must be frequent in APRIORI, not all sub-queries of a frequent query need be frequent queries in WARMR. Consider the query ? ? person(X), parent(X, Y ), hasP et(Y, cat) and assume it is frequent. The sub-query ? ? person(X), hasP et <ref type="bibr">(Y, cat)</ref> j ¡Ê L of Qj: Add Q j to Q d+1 , unless: (i) Q j is more specific than some query ¡Ê I, or (ii) Q j is equivalent to some query ¡Ê Q d+1 ¡È F 3. Return Q d+1 WARMR upgrades APRIORI to a multi-relational setting following the upgrading recipe (see Section 2.6). The major differences are in finding the frequency of queries (where we have to count answer substitutions for the key atom) and the candidate query generation (by using a refinement operator and declarative bias). WARMR has APRI-ORI as a special case: if we only have predicates of zero arity (with no arguments), which correspond to items, WARMR can be used to discover frequent itemsets.</p><p>More importantly, WARMR has as special cases a number of approaches that extend the discovery of frequent itemsets with, e.g., hierarchies on items <ref type="bibr" target="#b44">[47]</ref>, as well as approaches to discovering sequential patterns <ref type="bibr" target="#b15">[1]</ref>, including general episodes <ref type="bibr" target="#b45">[32]</ref>. The individual approaches mentioned make use of the specific properties of the patterns considered (very limited use of variables) and are more efficient than WARMR for the particular tasks they address. The high expressive power of the language of patterns considered has its computational costs, but it also has the important advantage that a variety of different pattern types can be explored without any changes in the implementation. WARMR can be (and has been) used to perform propositionalization, i.e., to transform MRDM problems to propositional <ref type="table">(single table)</ref> form. WARMR is first used to discover frequent queries. In the propositional form, examples correspond to answer substitutions for the key atom and the binary attributes are the frequent queries discovered. An attribute is true for an example if the corresponding query succeeds for the corresponding answer substitution. This approach has been applied with considerable success to the tasks of predictive toxicology <ref type="bibr" target="#b6">[9]</ref> and genome-wide prediction of protein functional class <ref type="bibr" target="#b29">[24]</ref>. <ref type="table">Table 8</ref>: A decision list representation of the relational decision tree in <ref type="figure" target="#fig_4">Figure 3</ref>. maintenance(M, A) ¡û haspart(M, X), worn(X), irreplaceable(X) !, A = send back maintenance(M, A) ¡û haspart(M, X), worn(X), !, A = repair in house maintenance(M, A) ¡û A = no maintenance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RELATIONAL DECISION TREES</head><p>Decision tree induction is one of the major approaches to data mining. Upgrading this approach to a relational setting has thus been of great importance. In this section, we first look into what relational decision trees are, i.e., how they are defined, then discuss how such trees can be induced from multi-relational data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Relational Classification, Regression, and Model Trees</head><p>Without loss of generality, we can say the task of relational prediction is defined by a two-place target predicate target(ExampleID, ClassV ar), which has as arguments an example ID and the class variable, and a set of background knowledge predicates/relations. Depending on whether the class variable is discrete or continuous, we talk about relational classification or regression. Relational decision trees are one approach to solving this task.</p><p>An example relational decision tree is given in <ref type="figure" target="#fig_4">Figure 3</ref>. It predicts the maintenance action A that should be taken on machine M (maintenance <ref type="figure">(M, A)</ref>), based on parts the machine contains (haspart(M, X)), their condition (worn(X)) and ease of replacement (irreplaceable(X)). The target predicate here is maintenance(M, A), the class variable is A, and background knowledge predicates are haspart(M, X), worn(X) and irreplaceable(X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>haspart(M, X), worn(X)</head><p>yes no irreplaceable(X)</p><p>A=no maintenance yes no</p><p>A=send back A=repair in house Relational decision trees have much the same structure as propositional decision trees. Internal nodes contain tests, while leaves contain predictions for the class value. If the class variable is discrete/continuous, we talk about relational classification/regression trees. For regression, linear equations may be allowed in the leaves instead of constant class-value predictions: in this case we talk about relational model trees.</p><p>The tree in <ref type="figure" target="#fig_4">Figure 3</ref> is a relational classification tree, while the tree in <ref type="figure">Figure 2</ref> is a relational regression tree.</p><p>The latter predicts the degradation time (the logarithm of the mean half-life time in water <ref type="bibr">[19]</ref>) of a chemical compound from its chemical structure, where the latter is represented by the atoms in the compound and the bonds between them. The target predicate is degrades(C, LogHLT ), the class variable LogHLT , and the background knowledge predicates are atom(C, AtomID, Element) and bond(C, A1, A2, BondT ype). The test at the root of the tree atom(C, A1, cl) asks if the compound C has a chlorine atom A1 and the test along the left branch checks whether the chlorine atom A1 is connected to a nitrogen atom A2.</p><p>As can be seen from the above examples, the major difference between propositional and relational decision trees is in the tests that can appear in internal nodes. In the relational case, tests are queries, i.e., conjunctions of literals with existentially quantified variables, e.g., atom(C, A1, cl) and haspart(M, X), worn(X). Relational trees are binary: each internal node has a left (yes) and a right (no) branch. If the query succeeds, i.e., if there exists an answer substitution that makes it true, the yes branch is taken.</p><p>It is important to note that variables can be shared among nodes, i.e., a variable introduced in a node can be referred to in the left (yes) subtree of that node. For example, the X in irreplaceable(X) refers to the machine part X introduced in the root node test haspart(M, X), worn(X). Similarly, the A1 in bond(C, A1, A2, BT ) refers to the chlorine atom introduced in the root node atom(C, A1, cl). One cannot refer to variables introduced in a node in the right (no) subtree of that node. For example, referring to the chlorine atom A1 in the right subtree of the tree in <ref type="figure">Figure 2</ref> makes no sense, as going along the right (no) branch means that the compound contains no chlorine atoms.</p><p>The actual test that has to be executed in a node is the conjunction of the literals in the node itself and the literals on the path from the root of the tree to the node in question. For example, the test in the node irreplaceable(X) in <ref type="figure" target="#fig_4">Fig- ure 3</ref> is actually haspart(M, X), worn(X), irreplaceable(X). In other words, we need to send the machine back to the manufacturer for maintenance only if it has a part which is both worn and irreplaceable. Similarly, the test in the node bond(C, A1, A2, BT ), atom(C, A2, n) in <ref type="figure">Figure 2</ref> is in fact atom(C, A1, cl), bond(C, A1, A2, BT ), atom(C, A2, n). As a consequence, one cannot transform relational decision trees to logic programs in the fashion "one clause per leaf" (unlike propositional decision trees, where a transformation "one rule per leaf" is possible).</p><p>Relational decision trees can be easily transformed into first-order decision lists, which are ordered sets of clauses (clauses in logic programs are unordered). When applying a decision list to an example, we always take the first clause that applies and return the answer produced. When applying a logic program, all applicable clauses are used and a set of answers can be produced. First-order decision lists can be represented by Prolog programs with cuts (!) <ref type="bibr" target="#b20">[4]</ref>: cuts ensure that only the first applicable clause is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>atom(C, A1, cl)</head><p>true false bond(C, A1, A2, BT ), atom(C, A2, n) atom(C, A3, o)</p><p>true false true false</p><p>LogHLT=7.82 LogHLT=7.51 LogHLT=6.08 LogHLT=6.73</p><p>Figure 2: A relational regression tree for predicting the degradation time LogHLT of a chemical compound C (target predicate degrades(C, LogHLT )). <ref type="table">Table 9</ref>: A decision list representation of the relational regression tree for predicting the biodegradability of a compound, given in <ref type="figure">Figure 2</ref>.</p><formula xml:id="formula_14">degrades(C, LogHLT ) ¡û atom(C, A1, cl), bond(C, A1, A2, BT ), atom(C, A2, n), LogHLT = 7.82, ! degrades(C, LogHLT ) ¡û atom(C, A1, cl), LogHLT = 7.51, ! degrades(C, LogHLT ) ¡û atom(C, A3, o), LogHLT = 6.08, ! degrades(C, LogHLT ) ¡û LogHLT = 6.73.</formula><p>the repair in house leaf, the left (yes) branch out of the root has been followed, but the right (no) branch out of the irreplaceable(X) node has been followed. A decision list produced from the relational regression tree in <ref type="figure">Figure 2</ref> is given in <ref type="table">Table 9</ref>.</p><p>Generating a logic program from a relational decision tree is more complicated. It requires the introduction of new predicates. We will not describe the transformation process in detail, but rather give an example. A logic program, corresponding to the tree in <ref type="figure" target="#fig_4">Figure 3</ref> is given in <ref type="table" target="#tab_0">Table 10</ref>. <ref type="table" target="#tab_0">Table 10</ref>: A logic program representation of the relational decision tree in <ref type="figure" target="#fig_4">Figure 3</ref>.</p><formula xml:id="formula_15">a(M ) ¡û haspart(M, X), worn(X), irreplaceable(X) b(M ) ¡û haspart(M, X), worn(X) maintenance(M, A) ¡û not a(M ), A = no aintenance maintenance(M, A) ¡û b(M ), A = repair in house maintenance(M, A) ¡û a(M ), not b(M ), A = send back</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Induction of Relational Decision Trees</head><p>A decision list is produced by traversing the relational regression tree in a depth-first fashion, going down left branches first. At each leaf, a clause is output that contains the prediction of the leaf and all the conditions along the left (yes) branches leading to that leaf. A decision list obtained from the tree in <ref type="figure" target="#fig_4">Figure 3</ref> is given in <ref type="table">Table 8</ref>. For the first clause (send back), the conditions in both internal nodes are output, as the left branches out of both nodes have been followed to reach the corresponding leaf. For the second clause, only the condition in the root is output: to reach</p><p>The two major algorithms for inducing relational decision trees are upgrades of the two most famous algorithms for inducting propositional decision trees. SCART <ref type="bibr">[26; 27]</ref> is an upgrade of CART <ref type="bibr" target="#b21">[5]</ref>, while TILDE <ref type="bibr">[3; 14]</ref> is an upgrade of C4.5 <ref type="bibr" target="#b37">[43]</ref>. According to the upgrading recipe, both SCART and TILDE have their propositional counterparts as special cases. The actual algorithms thus closely follow CART and C4.5. Here we illustrate the differences between SCART and CART by looking at the TDIDT (top-down induction of decision trees) algorithm of SCART <ref type="table" target="#tab_0">(Table 11)</ref>.</p><p>Given a set of examples, the TDID algorithm first checks if a termination condition is satisfied, e.g., if all examples belong to the same class c. If yes, a leaf is constructed with an appropriate prediction, e.g., assigning the value c to the class variable. Otherwise a test is selected among the possible tests for the node at hand, examples are split into subsets according to the outcome of the test, and tree construction proceeds recursively on each of the subsets.  A tree is thus constructed with the selected test at the root and the subtrees resulting from the recursive calls attached to the respective branches. The major difference in comparison to the propositional case is in the possible tests that can be used in a node. While in CART these remain (more or less) the same regardless of where the node is in the tree (e.g., A = v or A &lt; v for each attribute and attribute value), in SCART the set of possible tests crucially depend on the position of the node in the tree. In particular, it depends on the tests along the path from the root to the current node, more precisely the variables appearing in those tests and the declarative bias. To emphasize this, we can think of a GenerateTests procedure being separately employed before evaluating the tests. The inputs to this procedure are the tests on positive branches from the root to the current node and the declarative bias. These are also inputs to the top level TDIDT procedure.</p><p>The declarative bias in SCART contains statements of the form schema(CofL,TandM), where CofL is a conjunction of literals and TandM is a list of type and mode declarations for the variables in those literals. Two such schema statements, used in the induction of the regression tree in <ref type="figure">Figure 2</ref> are as follows: schema(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(bond(V, W, X, Y), atom(V, X, Z)), [V:chemical:'+', W:atomid:'+', X:atomid:'-', Y:bondtype:'-', Z:element: '=']) and schema(bond(V, W, X, Y), [V: chemical:'+', W:atomid:'+', X:atomid:'-', Y:bondtype: '=']).</head><p>In the lists, each variable in the conjunction is followed by its type and mode declaration: '+' denotes that the variable must be bound (i.e., appear in TestsOnYesBranchesSofar), -that it must not be bound, and = that it must be replaced by a constant value.</p><p>Assuming we have taken the left branch out of the root in <ref type="figure">Figure 2</ref>, TestsOnYesBranchesSofar = atom(C, A1, cl). Taking the declarative bias with the two schema statements above, the only choice for replacing the variables V and W in the schemata are the variables c and A1, respectively. The possible tests at this stage are thus of the form bond(C, A1, A2, BT ), atom(C, A2, E), where E is replaced with an element (such as cl -chlorine, s -sulphur, or nnitrogen), or of the form bond(C, A1, A2, BT ), where BT is replaced with a bond type (such as single, double, or aromatic).</p><p>Among the possible tests, the test bond(C, A1, A2, BT ), atom(C, A2, n) is chosen.</p><p>The approaches to relational decision tree induction are among the fastest MRDM approaches. They have been successfully applied to a number of practical problems. These include learning to predict the biodegradability of chemical compounds <ref type="bibr">[19]</ref> and learning to predict the structure of diterpene compounds from their NMR spectra <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The RIBL distance measure</head><p>Propositional distance measures are defined between examples that have the form of vectors of attribute values. They essentially sum up the differences between the examples' values along each of the dimensions of the vectors. Given two examples x = (x1, . . . , xn) and y = (y1, . . . , yn), their distance might be calculated as</p><formula xml:id="formula_16">n distance(x, y) = difference(xi, yi)/n i=1</formula><p>where the difference between attribute values is defined as</p><formula xml:id="formula_17">|xi ? yi| if continuous difference(xi, yi) = ? ? ? ? ? 0 if discrete and xi = yi ? ? ? ? 1 otherwise</formula><p>In a relational representation, an example (also called instance or case) can be described by a set of facts about multiple relations. A fact of the target predicate of the form target(ExampleID, A1, ..., An) specifies an instance through its ID and properties, and additional information can be specified through background knowledge predicates. In Table 12, the target predicate member(PersonID,A,G,I,MT) specifies information on members of a particular club, which includes age, gender, income and membership type. The background predicates car(OwnerID, CT, T S, M ) and house(OwnerID, DistrictID, Y, S) provide information on property owned by club members: for cars this includes car type, top speed and manufacturer, for houses the district, construction year and size. Additional information is available on districts through the predicate district(DistrictID, P, S, C), i.e., the popularity, size, and country of the district. <ref type="table" target="#tab_0">Table 12</ref>: Two examples on which to study a relational distance measure. member <ref type="bibr">(person1 , 45 , male, 20 , gold ) member(person2 , 30 , female, 10 , platinum)</ref> car <ref type="bibr">(person1 , wagon, 200 , volkswagen ) car(person1 , sedan, 220 , mercedesbenz ) car(person2 , roadster , 240 , audi ) car(person2 , coupe, 260 , bmw )</ref> house <ref type="bibr">(person1 , murgle, 1987 , 560 ) house(person1 , montecarlo, 1990 , 210 ) house(person2 , murgle, 1999 , 430 )</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATIONAL DISTANCE-BASED APPROACHES</head><p>district(montecarlo, famous, large, monaco) district(murgle, famous, small , slovenia)</p><p>To upgrade distance-based approaches to learning, including prediction and clustering, it is necessary to upgrade the key notion of a distance measure from the propositional to the relational case. Such a measure could then be used within standard statistical approaches, such as nearest-neighbor prediction or hierarchical agglomerative clustering. In their system RIBL, Emde and Wettschereck <ref type="bibr" target="#b25">[22]</ref> propose a relational distance measure. Below we first briefly discuss this measure, then outline how it has been used for relational classification and clustering <ref type="bibr" target="#b32">[25]</ref>.</p><p>The basic idea behind the RIBL <ref type="bibr" target="#b25">[22]</ref> distance measure is as follows. To calculate the distance between two objects/examples, their properties are taken into account first (at depth 0). Next (at depth 1), objects immediately related to the two original objects are taken into account, or more precisely, the distances between the corresponding related objects. At depth 2, objects related to those at depth 1 are taken into account, and so on, until a user-specified depth limit is reached. member <ref type="bibr">(person1 , 45 , male, 20 , gold )</ref> car <ref type="bibr">(person1 , wagon, 200 , volkswagen )</ref> E car(person1 , sedan, 220 , mercedesbenz ) E house <ref type="bibr">(person1 , murgle, 1987 , 560 )</ref> E district(murgle, famous, small , slovenia) E house <ref type="bibr">(person1 , montecarlo, 1990 , 210 )</ref> E district(montecarlo, famous, large, monaco) E <ref type="figure">Figure 4</ref>: The case of (all facts related to) member(person1 , 45 , male, 2000000 , gold ) constructed with respect to the background knowledge in <ref type="table" target="#tab_0">Table 12</ref> and a depth limit of 2.</p><p>In our example, when calculating the distance between e1 = member(person1, 45, male, 20, gold) and e2 = member(person2, 30, f emale, 10, platinum), the properties of the persons (age, gender, income, membership type) are first compared and differences between them calculated and summed (as in the propositional case). At depth 1, cars and houses owned by the two persons are compared, i.e., distances between them are calculated. At depth 2, the districts where the houses reside are taken into account when calculating the distances between houses. Before beginning to calculate distances, RIBL collects all facts related to a person into a so-called case. The case for person1 generated with a depth limit of 2 is given in <ref type="figure">Figure 4</ref>.</p><p>Let us calculate the distance between the two club members according to the distance measure. d(e1, e2) = 1/5¡¤ (d(person1, person2) + d(45, 30) + d(male, f emale)+ d <ref type="bibr">(20, 10) + d(gold, platinum)</ref>). With a depth limit of 0, the identifiers person1 and person2 are treated as discrete values, d(person1, person2) = 1 and we have d(e1, e2) = (1 + (45 ? 30)/100 + 1 + (20 ? 10)/50 + 1)/5 = 0.67; the denominators 100 and 50 denote the highest possible differences in age and income.</p><p>To calculate d(person1, person2) at level 1, we collect the facts directly related to the two persons and partition them according to the predicates: we thus have F1, car = { car <ref type="bibr">(person1 , wagon, 200 , volkswagen)</ref>, car(person1 , sedan, 220 , mercedesbenz )} ; F2, car = { car(person2 , roadster , 240 , audi ), car(person2 , coupe, 260 , bmw )}; F1, house = { house <ref type="bibr">(person1 , murgle, 1987 , 560 )</ref>, house(person1 , montecarlo, 1990 , 210 )}; and F2, house = { house(person2 , murgle, 1999 , 430 )}. Then d(person1, person2) = (d(F1, car, F2, car)+ d(F1, house, F2, house))/2. Distances between sets of facts are calculated as follows. We take the smaller set of facts (or the first, if they are of the same size): for d(F1, house, F2, house), we take F2, house. For each fact in this set, we calculate its distance to the nearest element of the other set, e.g., F1, house, summing up these distances (the house of person2 is closer to the house of person1 in murgle then to the one in montecarlo). We add a penalty for the possible mismatch in cardinality and normalize with the cardinality of the larger set: <ref type="bibr">(house(person2, murgle, 1999, 430)</ref>, house(person1, murgle, 1987, 560)), d(house <ref type="bibr">(person2, murgle, 1999, 430)</ref>, house(person1, montecarlo, 1990, 210))]/2 = 0.5 ¡¤ <ref type="bibr">[1 + min((0 + (1999 ? 1987)</ref> Finally, at level 2, the distance between the two districts is taken into account when calculating d(F1, house, F2, house). We have d(murgle, montecarlo) = (0 + 1 + 1)/3 = 2/3. However, since the house of person2 is closer to the house of person1 in murgle then to the one in montecarlo, the value of d(F1, house, F2, house) does not change as it equals 0.5 ¡¤ <ref type="bibr">[1 + min((0 + (1999 ? 1987)</ref> We should note here that the RIBL distance measure is not a metric <ref type="bibr">[44]</ref>. However, some relational distance measures that are metrics have been proposed recently <ref type="bibr" target="#b40">[45]</ref>. Designing distance measures for relational data is still a largely open and lively research area. Since distances and kernels are strongly related, this area is also related to designing kernels for structured data (Gaertner; this issue).</p><formula xml:id="formula_18">d(F1, house, F2, house) = [1 + min(d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Relational distance-based learning</head><p>Once we have a relational distance measure, we can easily adapt classical statistical approaches to prediction and clustering, such as the nearest-neighbor method and hierarchical agglomerative clustering, to work on relational data. This is precisely what has been done with the RIBL distance measure.</p><p>The original RIBL <ref type="bibr" target="#b25">[22]</ref> addresses the problem of prediction, more precisely classification. It uses the k-nearest neighbor (kNN) method in conjunction with the RIBL distance measure to solve the problem addressed. RIBL was successfully applied to the practical problem of diterpene structure elucidation <ref type="bibr" target="#b17">[18]</ref>, where it outperformed propositional approaches as well as a number of other relational approaches.</p><p>RIBL2 <ref type="bibr" target="#b32">[25]</ref> upgrades the RIBL distance measure by considering lists and terms as elementary types, much like discrete and numeric values. Edit distances are used for these, while the RIBL distance measure is followed otherwise. RIBL2 has been used to predict mRNA signal structure and to automatically discover previously uncharacterized mRNA signal structure classes <ref type="bibr" target="#b32">[25]</ref>.</p><p>Two clustering approaches have been developed that use the RIBL distance measure <ref type="bibr" target="#b32">[25]</ref>. RDBC uses hierarchical agglomerative clustering, while FORC adapts the k-means approach. The latter relies on finding cluster centers, which is easy for numeric vectors but far from trivial in the relational case. FORC thus uses the k-medoids method, which defines a cluster center as the existing case/example that has the smallest sum of squared distances to all other cases in the cluster and only uses distance information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">MRDM LITERATURE AND INTERNET RESOURCES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">SUMMARY</head><p>As one of the position statements in this special issue states (Domingos; this issue), (multi-)relational data mining (MRDM) is a field whose time has come. The present article provides an entry point into this lively and exciting research area. Since many of the MRDM techniques around have their origin in logic-based approaches to learning and inductive logic programming (ILP), an introduction to ILP was given first. Three major approaches to MRDM were covered next: relational association rules, relational decision trees and relational distance-based approaches. Finally, a brief overview of the literature and Internet resources in this area were provided for those looking for more detailed information. While a variety of successful applications of MRDM exist, these are not covered in this article: we refer the reader to <ref type="bibr" target="#b22">[20]</ref>, as well as <ref type="bibr">Page and Craven; Domingos; and Getoor (this issue</ref>).</p><p>The bulk of this special issue is devoted to hot topics and recent advances in MRDM. To some extent, hot topics in MRDM mirror hot topics in data mining and machine learning. These include ensemble methods (not covered here, see, e.g., Chapter 12 of <ref type="bibr">[16]</ref>), kernel methods (Gaertner; this issue), probabilistic methods (De Raedt and Kersting; this issue), and scalability issues (Blockeel and Sebag; this issue). The same goes for application areas, with computational biology and bioinformatics being the most popular (Page and Craven; this issue). Web mining and link mining (link analysis/link discovery) follow suit (Domingos; this issue; Getoor; this issue).</p><p>Mining data that is richer in structure than a single table is rightfully attracting an ever increasing amount of research effort. It is important to realize that the formulation of multi-relational data mining is very general and has as special cases mining of data in the form of sequences, trees, and graphs. A survey article (Washio and Motoda; this issue) and a position statement (Holder and Cook; this issue) on mining data in the form of graphs are included in this special issue. Exploring representations that are richer than propositional and poorer than full first-order logic may be well worthwhile, because of the possibility to design more efficient algorithms.</p><p>In summary, the issue provide an introduction to the important and exciting research field of multi-relational data mining. It also outlines recent advances and interesting open questions. (The latter include, for example, the seamless integration of MRDM approaches within actual database management systems (DBMSs) and using the query optimization techniques of the DBMSs to improve the efficiency of MRDM approaches.) In this way, we hope to attract the attention of data mining researchers and stimulate new research and solutions to open problems in this area, which can have practical applications and far reaching implications for the entire field of data mining.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Part of the refinement graph for the family relations problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>select</head><label></label><figDesc>count(distinct *) from select Person.Id from Person, Parent, HasPet where Person.Id = Parent.Pid and Parent.Kid = HasPet.Pid ? ? person(X), parent(X, Y ), hasP et(Y, Z) This query on a Prolog database containing predicates person, parent, and hasP et is equivalent to the SQL query select Person.Id, Parent.Kid, HasPet.Aid from Person, Parent, HasPet where Person.Id = Parent.Pid and Parent.Kid = HasPet.Pid on a database containing relations Person with argument Id, Parent with arguments Pid and Kid, and HasPet with arguments Pid and Aid. This query finds triples (x, y, z), where child y of person x has pet z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the queries Q1 =? ? person(X), parent(X, Y ) and Q2 =??person(X), parent(X, Y ), hasP et(Y, Z) are fre- quent, with absolute frequencies of 40 and 30, respectively. The query extension E =? ? person(X), parent(X, Y ) ; hasP et(Y, Z) can be considered a relational association rule with a support of 30 and confidence of 30/40 = 75%. Note the difference in meaning between the query extension E and two obvious, but incorrect, attempts at defining relational association rules. The clause person(X), parent(X, Y ) ¡ú hasP et(Y, Z) (which stands for the logical formula ?XY Z : person(X) ¡Ä parent(X, Y ) ¡ú hasP et(Y, Z)) would be inter- preted as follows: "if a person has a child, then this child has a pet". The implication ? ? person(X), parent(X, Y ) ¡ú ? ? hasP et(Y, Z), which stands for (?XY : person(X) ¡Ä parent(X, Y )) ¡ú (?Y Z : hasP et(Y,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>: The WARMR algorithm for discovering frequent Datalog queries. Algorithm WARMR( r, L, key, minfreq; Q) Input: Database r; Declarative language bias L and key ; threshold minfreq; Output: All queries Q ¡Ê L with frequency ¡Ý minfreq 1. Initialize level d := 1 2. Initialize the set of candidate queries Q1 := {?-key} 3. Initialize the set of (in)frequent queries F := ?; I := ? 4. While Q d not empty 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A relational decision tree, predicting the class variable A in the target predicate maintenance(M, A).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>procedure</head><label></label><figDesc>DivideAndConquer(TestsOnYesBranchesSofar, DeclarativeBias, Examples) if TerminationCondition(Examples) then N ewLeaf = CreateNewLeaf(Examples) return N ewLeaf else PossibleTestsNow = GenerateTests(TestsOnYesBranchesSofar, DeclarativeBias) BestTest = FindBestTest(PossibleTestsNow, Examples) (Split1, Split2) = SplitExamples(Examples, TestsOnYesBranchesSofar, BestTest) Lef tSubtree = DivideAndConquer(T estsOnY esBranchesSof ar ¡Ä BestT est, Split1) RightSubtree = DivideAndConquer(T estsOnY esBranchesSof ar, Split2) return [BestT est, Lef tSubtree, RightSubtree]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>/ 100 +</head><label>100</label><figDesc>|430 ? 560|/1000)/3, (1+(1999?1990)/100+(430?210)/1000)/3)] = 0.5 + 0.5 ¡¤ min(0.25/3, 1.31/3) = 13/24. For calculating d(F1, car, F2, car), we take F1, car and note that both cars of person1 are closer to the audi of per- son2 than to the bmw. We thus have d(F1, car, F2, car) = 0.5¡¤[minc¡ÊF 2 ,car d(car(person1 , wagon, 200 , volkswagen ), c)+ minc¡ÊF 2 ,car d(car(person1 , sedan, 220 , mercedesbenz ), c)] = 0.5 ¡¤ [(1 + |200 ? 240|/100 + 1)/3, (1 + |220 ? 240|/100 + 1)/3] = 11/15. Thus, at level 1, d(person1, person2) = 0.5 ¡¤ (13/24 + 11/15) = 0.6375 and d(e1, e2) = (0.6375 + (45 ? 30)/100 + 1 + (20 ? 10)/50 + 1)/5 = 0.5975.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>/ 100 +</head><label>100</label><figDesc>|430 ? 560|/1000)/3, (2/3+(1999?1990)/100+(430?210)/1000)/3)] = 0.5 + 0.5 ¡¤ min(0.25/3, (2/3 + 0.31)/3) = 13/24. d(e1, e2) is thus the same at level 1 and level 2 and is equal to 0.5975.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>A relational database with two tables and two 
classification rules: a propositional and a relational. 
Customer table 
ID Gender Age Income TotalSpent BigS 
c1 
Male 
30 214000 
18800 Yes 
c2 Female 
19 139000 
15100 Yes 
c3 
Male 
55 
50000 
12400 No 
c4 Female 
48 
26000 
8600 No 
c5 
Male 
63 191000 
28100 Yes 
c6 
Male 
63 114000 
20400 Yes 
c7 
Male 
58 
38000 
11800 No 
c8 
Male 
22 
39000 
5700 No 
... 
... 
... 
... 
... ... 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 4</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 11 :</head><label>11</label><figDesc>The TDIDT part of the SCART algorithm for inducing relational decision trees.</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">report on this event by D?eroski and?enkoand¡¦and?enko is included in this special issue. Recent developments in MRDM are presented at the annual workshop on Multi-Relational Data Mining. The first European event on this topic was</title>
		<idno>preceding ECML/PKDD 2002</idno>
		<ptr target="http://www.ar.sanken.osaka-u.ac.jp/MGTS-2003CFP.html" />
	</analytic>
	<monogr>
		<title level="m">This introductory article is largely based on material from that book. The RDM book originated from the International Summer School on Inductive Logic Programming and Knowledge Discovery in Databases (ILP&amp;KDD-97), held 15-17 September 1997 in</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>MRDM2003/). Of interest are the workshops on Learning Statistical Models from Relational Data held at AAAI-2000. http://kdl.cs.umass.edu/events/srl2003/) and the workshop on Mining Graphs, Trees and Sequences, held at ECML/PKDD-2003</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The present issue is the first special issue devoted to the topic of multi-relational data mining. Two journal special issues address the related topic of using ILP for KDD</title>
	</analytic>
	<monogr>
		<title level="m">and Data Mining and Knowledge Discovery</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>Many papers related to MRDM appear in the ILP literature. For an overview of the ILP literature, see Chapter 3 of the RDM book [16</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Since 1996, the proceedings of the ILP workshops are published by Springer within the Lecture Notes in Artificial Intelligence/Lecture Notes in Computer Science series. Papers on ILP appear regularly at major data mining, machine learning and artificial intelligence conferences. The same goes for a number of journals, including Journal of Logic Programming, Machine Learning, and New Generation Computing. Each of these has published several special issues on ILP. Special issues on ILP containing extended versions of selected papers from ILP workshops appear regularly in the Machine Learning journal. Selected papers from the ILP-91 workshop appeared as a book Inductive Logic Programming</title>
		<ptr target="http://www-ai.ijs.si/" />
	</analytic>
	<monogr>
		<title level="m">The major publication venue for ILP-related papers is the annual ILP workshop. The first International Workshop on Inductive Logic Programming (ILP-91) was organized in 1991</title>
		<editor>De Raedt</editor>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>ilpnet2/) is of special interest. It contains an overview of ILP related resources in several categories. These include a list of and pointers to ILP-related educational materials, ILP applications and datasets, as well as ILP systems. related Web resources we refer the reader to Chapter 16 of the RDM book [16</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rule induction with CN2: Some recent improvements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Boswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth European Working Session on Learning</title>
		<meeting>the Fifth European Working Session on Learning</meeting>
		<imprint>
			<biblScope unit="page" from="151" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The CN2 induction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Niblett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="261" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding frequent substructures in chemical compounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dehaspe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Fourth International Conference on Knowledge Discovery and Data Mining<address><addrLine>Menlo Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="30" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discovery of frequent datalog patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dehaspe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="36" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Discovery of Relational Association Rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dehaspe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="189" to="212" />
		</imprint>
	</monogr>
	<note>In [16</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<title level="m">Advances in Inductive Logic Programming</title>
		<editor>L. De Raedt</editor>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attribute-value learning versus inductive logic programming: the missing links (extended abstract)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Inductive Logic Programming</title>
		<meeting>the Eighth International Conference on Inductive Logic Programming</meeting>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Three Companions for Data Mining in First Order Logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blockeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dehaspe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Laer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="105" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">First order jk-clausal theories are PAC-learnable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>D?eroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="375" to="392" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">Relational Data Mining</title>
		<editor>16] S. D?eroski and N. Lavra?</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PAClearnability of determinate logic programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">S</forename><surname>D?eroski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muggleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Data Engineering</title>
		<meeting>the Eleventh International Conference on Data Engineering<address><addrLine>Los Alamitos, CA; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
	<note>Proceedings of the Fifth ACM Workshop on Computational Learning Theory</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast discovery of association rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Verkamo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<editor>U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy</editor>
		<meeting><address><addrLine>Menlo Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="307" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Diterpene structure elucidation from</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>D?eroski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulze-Kremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heidtke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wettschereck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blockeel</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">C NMR spectra with Inductive Logic Programming. Applied Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="363" to="383" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Top-down induction of first order logical decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blockeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="285" to="297" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Prolog Programming for Artificial Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bratko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">S</forename><surname>D?eroski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blockeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kompare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Laer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Inductive Logic Programming</title>
		<meeting>the Ninth International Workshop on Inductive Logic Programming<address><addrLine>Harlow, England; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="80" to="91" />
		</imprint>
	</monogr>
	<note>Experiments in Predicting Biodegradability</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<pubPlace>Wadsworth, Belmont</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Relational Data Mining Applications: An Overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>D?eroski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="339" to="364" />
		</imprint>
	</monogr>
	<note>In [16</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generalized subsumption and its applications to induction and redundancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="176" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>D?eroski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
		<title level="m">Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Proceedings of the First International Workshop on MultiRelational Data Mining</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Relational instancebased learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Emde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wettschereck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Machine Learning</title>
		<meeting>the Thirteenth International Conference on Machine Learning</meeting>
		<imprint>
			<biblScope unit="page">122</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Kaufmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<pubPlace>San Mateo, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient induction of logic programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muggleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Algorithmic Learning Theory</title>
		<meeting>the First Conference on Algorithmic Learning Theory<address><addrLine>Tokyo</addrLine></address></meeting>
		<imprint>
			<publisher>Ohmsha</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="368" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Logical approaches to machine learning -an overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">THINK</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Genome scale prediction of protein functional class from sequence using data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dehaspe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Sixth International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="384" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Declarative bias in inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nedellec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rouveirol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bergadano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tausend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Inductive Logic Programming</title>
		<editor>L. De Raedt</editor>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="82" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A study of generalisation in logic programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Niblett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third European Working Session on Learning</title>
		<meeting>the Third European Working Session on Learning<address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Pitman</publisher>
			<date type="published" when="1988" />
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distance Based Approaches to Relational Learning and Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kirsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Horv¨¢th ; R. De Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Inductive Logic Programming</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="213" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Structural regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth National Conference on Artificial Intelligence</title>
		<meeting>the Thirteenth National Conference on Artificial Intelligence<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="812" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A note on inductive generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Plotkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Intelligence 5</title>
		<editor>B. Meltzer and D. Michie</editor>
		<meeting><address><addrLine>Edinburgh</addrLine></address></meeting>
		<imprint>
			<publisher>Edinburgh University Press</publisher>
			<date type="published" when="1969" />
			<biblScope unit="page" from="153" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Inducing Classification and Regression Trees in First Order Logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="140" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning logical definitions from relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="239" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">C4.5: Programs for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Mateo, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Clustering and instance based learning in first order logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lavra?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">J</forename><surname>Ramon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="262" to="291" />
		</imprint>
	</monogr>
<note type="report_type">PhD Thesis</note>
	<note>Propositionalization Approaches to Relational Data Mining</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning nonrecursive definitions of relations with LINUS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lavra?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>D?eroski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grobelnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth European Working Session on Learning</title>
		<meeting>the Fifth European Working Session on Learning<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="265" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A polynomial time computable metric between point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bruynooghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Informatica</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="765" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Inductive Logic Programming: Techniques and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lavra?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>D?eroski</surname></persName>
		</author>
		<ptr target="http://www-ai.ijs.si/SasoDzeroski/ILPBook/" />
		<imprint>
			<date type="published" when="1994" />
			<publisher>Ellis Horwood</publisher>
			<pubPlace>Chichester</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Algorithmic Program Debugging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Foundations of Logic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lloyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mining generalized association rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-first International Conference on Very Large Data Bases</title>
		<meeting>the Twenty-first International Conference on Very Large Data Bases<address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="407" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Discovering generalized episodes using minimal occurrences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Second International Conference on Knowledge Discovery and Data Mining<address><addrLine>Menlo Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="146" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Principles of Database and Knowledge Base Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Computer Science Press</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Rockville, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The multi-purpose incremental learning system AQ15 and its testing application on three medical domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mozeti?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lavra?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth National Conference on Artificial Intelligence</title>
		<meeting>the Fifth National Conference on Artificial Intelligence<address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="1041" to="1045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">How to Upgrade Propositional Learners to First Order Logic: A Case Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Van Laer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="235" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Inductive Logic Programming for Knowledge Discovery in Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="74" to="101" />
		</imprint>
	</monogr>
	<note>In [16</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muggleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Generation Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="295" to="318" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<title level="m">Inductive Logic Programming</title>
		<editor>S. Muggleton</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Inverse entailment and Progol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muggleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Generation Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="245" to="286" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
