[
     {
          "head": {
               "n": "0",
               "text": "abstract",
               "type": "abstract"
          },
          "paragraphs": [
               "The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a method for rating Web pages objectively and mechanically, eeectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to eeciently compute PageRank for large numbers of pages. And, we show h o w to apply PageRank to search and to user navigation."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 0,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "1",
               "text": "Introduction and Motivation",
               "type": "introduction"
          },
          "paragraphs": [
               "The World Wide Web creates many new challenges for information retrieval. It is very large and heterogeneous. Current estimates are that there are over 150 million web pages with a doubling life of less than one year. More importantly, the web pages are extremely diverse, ranging from \"What is Joe having for lunch t o d a y?\" to journals about information retrieval. In addition to these major challenges, search engines on the Web must also contend with inexperienced users and pages engineered to manipulate search engine ranking functions.",
               "However, unlike \"\"at\" document collections, the World Wide Web is hypertext and provides considerable auxiliary information on top of the text of the web pages, such as link structure and link text. In this paper, we take advantage of the link structure of the Web to produce a global ranking of every web page. This ranking, called PageRank, helps search engines and users quickly make sense of the vast heterogeneity of the World Wide Web."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 1,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "1.1",
               "text": "Diversity of Web Pages",
               "type": "introduction"
          },
          "paragraphs": [
               "Although there is already a large literature on academic citation analysis, there are a number of signiicant diierences between web pages and academic publications. Unlike academic papers which are scrupulously reviewed, web pages proliferate free of quality control or publishing costs. With a simple program, huge numbers of pages can be created easily, artiicially innating citation counts. Because the Web environment contains competing proot seeking ventures, attention getting strategies evolve in response to search engine algorithms. For this reason, any e v aluation strategy which counts replicable features of web pages is prone to manipulation. Further, academic papers are well deened units of work, roughly similar in quality and numberof citations, as well as in their purpose to extend the bodyof knowledge. Web pages vary on a much wider scale than academic papers in quality, usage, citations, and length. A random archived message posting asking an obscure question about an IBM computer is very diierent from the IBM home page. A research article about the eeects of cellular phone use on driver attention is very diierent from an advertisement for a particular cellular provider. The average web page quality experienced by a user is higher than the quality of the average web page. This is because the simplicity of creating and publishing web pages results in a large fraction of low quality w eb pages that users are unlikely to read.",
               "There are many axes along which web pages may be diierentiated. In this paper, we deal primarily with one -an approximation of the overall relative importance of web pages."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 2,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "1.2",
               "text": "PageRank",
               "type": "introduction"
          },
          "paragraphs": [
               "In order to measure the relative importance of web pages, we propose PageRank, a method for computing a ranking for every web page based on the graph of the web. PageRank has applications in search, browsing, and traac estimation.",
               "Section 2 gives a mathematical description of PageRank and provides some intuitive justiication. In Section 3, we show how we eeciently compute PageRank for as many as 518 million hyperlinks. To test the utility o f P ageRank for search, we built a web search engine called Google Section 5. We also demonstrate how P ageRank can be used as a browsing aid in Section 7.3."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 3,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "2.1",
               "text": "Related Work",
               "type": "relatedwork"
          },
          "paragraphs": [
               "There has been a great deal of work on academic citation analysis Gooman has published an interesting theory of how information ow in a scientiic community is an epidemic process.",
               "There has been a fair amount of recent activity on how to exploit the link structure of large hypertext systems such as the web. Pitkow recently completed his Ph.D. thesis on World Wide Web Ecologies\" PPR96 with a wide variety of link based analysis. Weiss discuss clustering methods that take the link structure into account WVS + 96 Spertus discusses information that can beobtained from the link structure for a variety of applications. Good visualization demands added structure on the hypertext and is discussed in MF95 Recently, Kleinberg has developed an interesting model of the web as Hubs and Authorities, based on an eigenvector calculation on the co-citation matrix of the web.",
               "Finally, there has been some interest in what quality\" means on the net from a library community",
               "It is obvious to try to apply standard citation analysis techniques to the web's hypertextual citation structure. One can simply think of every link as being like an academic citation. So, a major page like http:::www.yahoo.comm will have tens of thousands of backlinks or citations pointing to it. This fact that the Yahoo home page has so many backlinks generally imply that it is quite important. Indeed, many of the web search engines have used backlink count a s a w ay to try to bias their databases in favor of higher quality or more important pages. However, simple backlink counts have a n umber of problems on the web. Some of these problems have to do with characteristics of the web which are not present in normal academic citation databases."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 4,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "2.2",
               "text": "Link Structure of the Web",
               "type": "relatedwork"
          },
          "paragraphs": [
               "While estimates vary, the current graph of the crawlable Web has roughly 150 million nodes pages and 1.7 billion edges links. Every page has some number of forward links outedges and backlinks inedges see . We can never know whether we h a ve found all the backlinks of a particular page but if we h a ve d o wnloaded it, we know all of its forward links at that time. Web pages vary greatly in terms of the number of backlinks they have. For example, the Netscape home page has 62,804 backlinks in our current database compared to most pages which have just a few backlinks. Generally, highly linked pages are more than pages with few links. Simple citation counting has been used to speculate on the future winners of the Nobel Prize PageRank provides a more sophisticated method for doing citation counting.",
               "The reason that PageRank is interesting is that there are many cases where simple citation counting does not correspond to our common sense notion of importance. For example, if a web page has a link oo the Yahoo home page, it may bejust one link but it is a very important one. This page should be ranked higher than many pages with more links but from obscure places. PageRank is an attempt to see how good an approximation to can be obtained just from the link structure."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 5,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "2.3",
               "text": "Propagation of Ranking Through Links",
               "type": "relatedwork"
          },
          "paragraphs": [
               "Based on the discussion above, we give the following intuitive description of PageRank: a page has high rank if the sum of the ranks of its backlinks is high. This covers both the case when a page has many backlinks and when a page has a few highly ranked backlinks."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 6,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "2.4",
               "text": "Deenition of PageRank",
               "type": "relatedwork"
          },
          "paragraphs": [
               "Let u beaweb page. Then let F u be the set of pages u points to and B u be the set of pages that point t o u. Let N u = jF u j be the number of links from u and let c be a factor used for normalization so that the total rank of all web pages is constant.",
               "We begin by deening a simple ranking, R which is a slightly simpliied version of PageRank: This formalizes the intuition in the previous section. Note that the rank of a page is divided among its forward links evenly to contribute to the ranks of the pages they point to. Note that c c 1 because there are a number of pages with no forward links and their weight is lost from the system see section 2.7. The equation is recursive but it may be computed by starting with any set of ranks and iterating the computation until it converges. demonstrates the propagation of rank from one pair of pages to another. shows a consistent steady state solution for a set of pages.",
               "Stated another way, let A be a square matrix with the rows and column corresponding to web pages. Let A u;v = 1=N u if there is an edge from u to v and A u;v = 0 if not. If we treat R as a vector over web pages, then we h a ve R = cAR. So R is an eigenvector of A with eigenvalue c. In fact, we w ant the dominant eigenvector of A. It may be computed by repeatedly applying A to any nondegenerate start vector.",
               "There is a small problem with this simpliied ranking function. Consider two web pages that point to each other but to no other page. And suppose there is some web page which points to one of them. Then, during iteration, this loop will accumulate rank but never distribute any rank since there are no outedges. The loop forms a sort of trap which w e call a rank sink.",
               "To o vercome this problem of rank sinks, we i n troduce a rank source: "
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 7,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "2.5",
               "text": "Random Surfer Model",
               "type": "modelling"
          },
          "paragraphs": [
               "The deenition of PageRank above has another intuitive basis in random walks on graphs. The simpliied version corresponds to the standing probability distribution of a random walk on the graph of the Web. Intuitively, this can be thought of as modeling the behavior of a random surfer\". The random surfer\" simply keeps clicking on successive links at random. However, if a real Web surfer ever gets into a small loop of web pages, it is unlikely that the surfer will continue in the loop forever. Instead, the surfer will jump to some other page. The additional factor E can be viewed as a way of modeling this behavior: the surfer periodically bored\" and jumps to a random page chosen based on the distribution in E.",
               "So far we have left E as a user deened parameter. In most tests we let E be uniform over all web pages with value . However, in Section 6 we show how diierent values of E can generate page ranks."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 8,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "2.6",
               "text": "Computing PageRank",
               "type": "modelling"
          },
          "paragraphs": [
               "The computation of PageRank is fairly straightforward if we ignore the issues of scale. Let S be almost any v ector over Web pages for example E. Then PageRank may be computed as follows:",
               "Note that the d factor increases the rate of convergence and maintains jjRjj 1 . An alternative normalization is to multiply R by the appropriate factor. The use of d may have a small impact on the innuence of E."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 9,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "2.7",
               "text": "Dangling Links",
               "type": "modelling"
          },
          "paragraphs": [
               "One issue with this model is dangling links. Dangling links are simply links that point t o a n y page with no outgoing links. They aaect the model because it is not clear where their weight should be distributed, and there are a large number of them. Often these dangling links are simply pages that we h a ve not downloaded yet, since it is hard to sample the entire web in our 24 million pages currently downloaded, we h a ve 51 million URLs not downloaded yet, and hence dangling. Because dangling links do not aaect the ranking of any other page directly, we simply remove them from the system until all the PageRanks are calculated. After all the PageRanks are calculated, they can be added back in, without aaecting things signiicantly. Notice the normalization of the other links on the same page as a link which w as removed will change slightly, but this should not have a large eeect."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 10,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "3",
               "text": "Implementation",
               "type": "modelling"
          },
          "paragraphs": [
               "As part of the Stanford WebBase project we h a ve built a complete crawling and indexing system with a current repository of 24 million web pages. Any w eb crawler needs to keep a database of URLs so it can discover all the URLs on the web. To implement PageRank, the web crawler simply needs to build an index of links as it crawls. While a simple task, it is non-trivial because of the huge volumes involved. For example, to index our current 24 million page database in about ve d a ys, we need to process about 50 web pages per second. Since there about about 11 links on an average page depending on what you count as a link we need to process 550 links per second. Also, our database of 24 million pages references over 75 million unique URLs which each link must be compared against.",
               "Much time has been spent making the system resilient in the face of many deeply and intricately awed web artifacts. There exist innnitely large sites, pages, and even URLs. A large fraction of web pages have incorrect HTML, making parser design diicult. Messy heuristics are used to help the crawling process. For example, we do not crawl URLs with cgi-binn in them. Of course it is impossible to get a correct sample of the \"entire web\" since it is always changing. Sites are sometimes down, and some people decide to not allow their sites to be indexed. Despite all this, we believe w e h a ve a reasonable representation of the actual link structure of publicly accessible web."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 11,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "3.1",
               "text": "PageRank Implementation",
               "type": "modelling"
          },
          "paragraphs": [
               "We convert each URL into a unique integer, and store each h yperlink in a database using the integer IDs to identify pages. Details of our implementation are in In general, we h a ve implemented PageRank in the following manner. First we sort the link structure by Parent ID. Then dangling links are removed from the link database for reasons discussed above a few iterations removes the vast majority of the dangling links. We need to make an initial assignment of the ranks. This assignment can be made by one of several strategies. If it is going to iterate until convergence, in general the initial values will not aaect values, just the rate of convergence. But we can speed up convergence by c hoosing a good initial assignment. We believe that careful choice of the initial assignment and a small number of iterations may result in excellent or improved performance.",
               "Memory is allocated for the weights for every page. Since we use single precision point values at 4 bytes each, this amounts to 300 megabytes for our 75 million URLs. If insuucient RAM is available to hold all the weights, multiple passes can bemade our implementation uses half as much memory and two passes. The weights from the current time step are kept in memory, and the previous weights are accessed linearly on disk. Also, all the access to the link database, A, is linear because it is sorted. Therefore, A can bekept on disk as well. Although these data structures are very large, linear disk access allows each iteration to be completed in about 6 minutes o n a t ypical workstation. After the weights have converged, we add the dangling links back in and recompute the rankings. Note after adding the dangling links back in, we need to iterate as many times as was required to remove the dangling links. Otherwise, some of the dangling links will have a zero weight. This whole process takes about hours in the current implementation. With less strict convergence criteria, and more optimization, the calculation could be much faster. Or, more eecient techniques for estimating eigenvectors could be used to improve performance. However, it should be noted that the cost required to compute the PageRank is insigniicant compared to the cost required to build a full text index."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 12,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "4",
               "text": "Convergence Properties",
               "type": "modelling"
          },
          "paragraphs": [
               "As can be seen from the graph in PageRank on a large 322 million link database converges to a reasonable tolerance in roughly 52 iterations. The convergence on half the data takes roughly 45 iterations. This graph suggests that PageRank will scale very well even for extremely large collections as the scaling factor is roughly linear in log n.",
               "One of the interesting ramiications of the fact that the PageRank calculation converges rapidly is that the web is an expander-like graph. To understand this better, we give a brief overview of the theory of random walks on graphs; refer to Motwani-Raghavan for further details. A random walk on a graph is a stochastic process where at any given time step we are at a particular node of the graph and choose an outedge uniformly at random to determine the node to visit at the next time step. A graph is said to bean expander if it is the case that every not too large subset of nodes S has a neighborhood set of vertices accessible via outedges emanating from nodes in S that is larger than some factor times jSj; here, is called the expansion factor. It is the case that a graph has a goodexpansion factor if and only if the largest eigenvalue is suuciently larger than the second-largest eigenvalue. A random walk on a graph is said to be rapidly-mixing if it quickly time logarithmic in the size of the graph converges to a limiting distribution on the set of nodes in the graph. It is also the case that a random walk is rapidly-mixing on a graph if and only if the graph is an expander or has an eigenvalue separation.",
               "To relate all this to the PageRank computation, note that it is essentially the determination of the limiting distribution of a random walk on the Web graph. The importance ranking of a node is essentially the limiting probability that the random walk will be at that node after a suuciently large time. The fact that the PageRank computation terminates in logarithmic time is equivalent t o saying that the random walk is rapidly mixing or that the underlying graph has a good expansion factor. Expander graphs have many desirable properties that we may be able to exploit in the future in computations involving the Web graph. The beneets of PageRank are the greatest for underspeciied queries. For example, a query for Stanford University\" may return any numberof web pages which mention Stanford such as publication lists on a conventional search engine, but using PageRank, the university home page is listed"
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 13,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "5.1",
               "text": "Title Search",
               "type": "modelling"
          },
          "paragraphs": [
               "To test the usefulness of PageRank for search w e implemented a search engine that used only the titles of 16 million web pages. To answer a query, the search engine all the web pages whose titles contain all of the query words. Then it sorts the results by PageRank. This search engine is very simple and cheap to implement. In informal tests, it worked remarkably well. As can be seen in , a search for yields a list of top universities. This shows our MultiQuery system which allows a user to query two search engines at the same time. The search engine on the left is our PageRank based title search engine. The bar graphs and percentages shown are a log of the actual PageRank with the top page normalized to 100, not a percentile which i s used everywhere else in this paper. The search engine on the right is Altavista. You can see that Altavista returns random looking web pages that match the query University\" and are the root page of the server Altavista seems to be using URL length as a quality heuristic.  "
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 14,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "5.2",
               "text": "Rank Merging",
               "type": "modelling"
          },
          "paragraphs": [
               "The reason that the title based PageRank system works so well is that the title match ensures high precision, and the PageRank ensures high quality. When matching a query like on the web, recall is not very important because there is far more than a user can look at. For more speciic searches where recall is more important, the traditional information retrieval scores over full-text and the PageRank should be combined. Our Google system does this type of rank merging. Rank merging is known to be a very diicult problem, and we need to spend considerable additional eeort before we will be able to do a reasonable evaluation of these types of queries. However, we do believe that using PageRank as a factor in these queries is quite beneecial."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 15,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "5.3",
               "text": "Some Sample Results",
               "type": "modelling"
          },
          "paragraphs": [
               "We have experimented considerably with Google, a full-text search engine which uses PageRank. While a full-scale user study is beyond the scope of this paper, we provide a sample query in Appendix A. For more queries, we encourage the reader to test Google themselves shows the top 15 pages based on PageRank. This particular listing was generated in July 1996. In a more recent calculation of PageRank, Microsoft has just edged out Netscape for the highest PageRank."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 16,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "5.4",
               "text": "Common Case",
               "type": "modelling"
          },
          "paragraphs": [
               "One of the design goals of PageRank was to handle the common case for queries well. For example, a user searched for wolverine\", remembering that the University o f Michigan system used for all administrative functions by students was called something with a wolverine in it. Our PageRank based title search system returned the answer Access\" as the result. This is sensible since all the students regularly use the Wolverine Access system, and a random user is quite likely to be looking for it given the query The fact that the Wolverine Access site is a good common case is not contained in the HTML of the page. Even if there were a way of deening good meta-information of this form within a page, it would beproblematic since a page author could not be trusted with this kind of evaluation. Many w eb page authors would simply claim that their pages were all the best and most used on the web.",
               "It is important to note that the goal of a site that contains a great deal of information about wolverines is a very diierent task than the common case wolverine site. There is an interesting system that attempts to sites that discuss a topic in detail by propagating the textual matching score through the link structure of the web. It then tries to return the page on the most central path. This results in good results for queries like ower\"; the system will return good navigation pages from sites that deal with the topic of in detail. Contrast that with the common case approach which might simply return a commonly used commercial site that had little information except how to buy owers. It is our opinion that both of these tasks are important, and a general purpose web search engine should return results which fullll the needs of both of these tasks automatically. In this paper, we are concentrating only on the common case approach."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 17,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "5.5",
               "text": "Subcomponents of Common Case",
               "type": "modelling"
          },
          "paragraphs": [
               "It is instructive to consider what kind of common case scenarios PageRank can help represent. Besides a page which has a high usage, like the Wolverine Access cite, PageRank can also represent a collaborative notion of authority or trust. For example, a user might prefer a news story simply because it is linked is linked directly from the New York Times home page. Of course such a story will receive quite a high PageRank simply because it is mentioned by a v ery important page. This seems to capture a kind of collaborative trust, since if a page was mentioned by a trustworthy or authoritative source, it is more likely to betrustworthy or authoritative. Similarly, quality or importance seems to within this kind of circular deenition."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 18,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "6",
               "text": "Personalized PageRank",
               "type": "modelling"
          },
          "paragraphs": [
               "An important component of the PageRank calculation is E a vector over the Web pages which is used as a source of rank to make up for the rank sinks such as cycles with no outedges see Section 2.4. However, aside from solving the problem of rank sinks, E turns out to be a powerful parameter to adjust the page ranks. Intuitively the E vector corresponds to the distribution of web pages that a random surfer periodically jumps to. As we see below, it can beused to give broad general views of the Web or views which are focussed and personalized to a particular individual.",
               "We h a ve performed most experiments with an E vector that is uniform over all web pages with jjEjj 1 = 0:15. This corresponds to a random surfer periodically jumping to a random web page. This is a very democratic choice for E since all web pages are valued simply because they exist.",
               "Although this technique has been quite successful, there is an important problem with it. Some Web pages with many related links receive an overly high ranking. Examples of these include copyright w arnings, disclaimers, and highly interlinked mailing list archives.",
               "Another extreme is to have E consist entirely of a single web page. We tested two such E's the Netscape home page, and the home page of a famous computer scientist, John McCarthy. For the Netscape home page, we attempt to generate page ranks from the perspective o f a n o vice user who has Netscape set as the default home page. In the case of John McCarthy's home page we want to calculate page ranks from the perspective o f an individual who has given us considerable contextual information based on the links on his home page.",
               "In both cases, the mailing list problem mentioned above did not occur. And, in both cases, the respective home page got the highest PageRank and was followed by its immediate links. From  : Page Ranks for Two Diierent Views: Netscape vs. John McCarthy that point, the disparity decreased. In , we show the resulting page rank percentiles for an assortment of diierent pages. Pages related to computer science have a higher McCarthy-rank than Netscape-rank and pages related to computer science at Stanford have a considerably higher McCarthy-rank. For example, the Web page of another Stanford Computer Science Dept. faculty member is more than six percentile points higher on the McCarthy-rank. Note that the page ranks are displayed as percentiles. This has the eeect of compressing large diierences in PageRank at the top of the range.",
               "Such personalized page ranks may have a number of applications, including personal search engines. These search engines could save users a great deal of trouble by eeciently guessing a large part of their interests given simple input such as their bookmarks or home page. We show a n example of this in Appendix A with the Mitchell\" query. In this example, we demonstrate that while there are many people on the web named Mitchell, the number one result is the home page of a colleague of John McCarthy named John Mitchell."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 19,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "6.1",
               "text": "Manipulation by Commercial Interests",
               "type": "modelling"
          },
          "paragraphs": [
               "These types of personalized PageRanks are virtually immune to manipulation by commercial interests. For a page to get a high PageRank, it must convince an important page, or a lot of non-important pages to link to it. At worst, you can have manipulation in the form of buying advertisementslinks on important sites. But, this seems well under control since it costs money. This immunity to manipulation is an extremely important property. This kind of commercial manipulation is causing search engines a great deal of trouble, and making features that would be great to have very diicult to implement. For example fast updating of documents is a very desirable feature, but it is abused by people who want to manipulate the results of the search engine.",
               "A compromise between the two extremes of uniform E and single page E is to let E consist of all the root level pages of all web servers. Notice this will allow some manipulation of PageRanks. Someone who wished to manipulate this system could simply create a large numberof root level servers all pointing at a particular site."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 20,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "7.1",
               "text": "Estimating Web Traac",
               "type": "modelling"
          },
          "paragraphs": [
               "Because PageRank roughly corresponds to a random web surfer see Section 2.5, it is interesting to see how P ageRank corresponds to actual usage. We used the counts of web page accesses from NLANR proxy cache and compared these to PageRank. The NLANR data was from several national proxy caches over the period of several months and consisted of 11,817,665 unique URLs with the highest hit count going to Altavista with 638,657 hits. There were 2.6 million pages in the intersection of the cache data and our 75 million URL database. It is extremely diicult to compare these datasets analytically for a number of diierent reasons. Many of the URLs in the cache access data are people reading their personal mail on free email services. Duplicate server names and page names are a serious problem. Incompleteness and bias a problem is both the PageRank data and the usage data. However, we did see some interesting trends in the data. There seems to be a high usage of pornographic sites in the cache data, but these sites generally had low PageRanks. We believe this is because people do not want to link to pornographic sites from their own web pages. Using this technique of looking for diierences between PageRank and usage, it may be possible to things that people like to look at, but do not want t o m e n tion on their web pages. There are some sites that have a v ery high usage, but low P ageRank such as netscape.yahoo.com. We believe there is probably an important backlink which simply is omitted from our database we only have a partial link structure of the web. It may be possible to use usage data as a start vector for PageRank, and then iterate PageRank a few times. This might allow in holes in the usage data. In any case, these types of comparisons are an interesting topic for future study."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 21,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "7.2",
               "text": "PageRank as Backlink Predictor",
               "type": "modelling"
          },
          "paragraphs": [
               "One justiication for PageRank is that it is a predictor for backlinks. In we explore the issue of how to crawl the web eeciently, trying to crawl better documents We found on tests of the Stanford web that PageRank is a better predictor of future citation counts than citation counts themselves.",
               "The experiment assumes that the system starts out with only a single URL and no other information, and the goal is to try to crawl the pages in as close to the optimal order as possible. The optimal order is to crawl pages in exactly the order of their rank according to an evaluation function. For the purposes here, the evaluation function is simply the numberof citations, given complete information. The catch is that all the information to calculate the evaluation function is not available until after all the documents have been crawled. It turns out using the incomplete data, PageRank is a more eeective w ay to order the crawling than the number of known citations. In other words, PageRank is a better predictor than citation counting even when the measure is the number of citations! The explanation for this seems to be that PageRank avoids the local maxima that citation counting gets stuck in. For example, citation counting tends to get stuck i n local collections like the Stanford CS web pages, taking a long time to branch out and highly cited pages in other areas. PageRank quickly the Stanford homepage is important, and gives preference to its children resulting in an eecient, broad search.",
               "This ability o f P ageRank to predict citation counts is a powerful justiication for using PageRank. Since it is very diicult to map the citation structure of the web completely, PageRank may even be a better citation count approximation than citation counts themselves. "
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 22,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "7.3",
               "text": "User Navigation: The PageRank Proxy",
               "type": "modelling"
          },
          "paragraphs": [
               "We have developed a web proxy application that annotates each link that a user sees with its PageRank. This is quite useful, because users receive some information about the link before they click on it. In is a screen shot from the proxy. The length of the red bars is the log of the URL's PageRank. We can see that major organizations, like Stanford University, receive a very high ranking followed by research groups, and then people, with professors at the high end of the people scale. Also notice ACM has a very high PageRank, but not as high as Stanford University. Interestingly, this PageRank annotated view of the page makes an incorrect URL for one of the professors glaringly obvious since the professor has a embarrassingly low P ageRank. Consequently this tool seems useful for authoring pages as well as navigation. This proxy is very helpful for looking at the results from other search engines, and pages with large numbers of links such as Yahoo's listings. The proxy can help users decide which links in a long listing are more likely to beinteresting. Or, if the user has some idea where the link they are looking for should fall in the spectrum, they should be able to scan for it much more quickly using the proxy."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 23,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "7.4",
               "text": "Other Uses of PageRank",
               "type": "modelling"
          },
          "paragraphs": [
               "The original goal of PageRank was a way to sort backlinks so if there were a large number of backlinks for a document, the backlinks could be displayed We h a ve implemented such a system. It turns out this view of the backlinks ordered by P ageRank can be very interesting when trying to understand your competition. For example, the people who run a news site always want to keep track o f a n y signiicant backlinks the competition has managed to get. Also, PageRank can help the user decide if a site is trustworthy or not. For example, a user might be inclined to trust information that is directly cited from the Stanford homepage."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 24,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     },
     {
          "head": {
               "n": "8",
               "text": "Conclusion",
               "type": "conclusion"
          },
          "paragraphs": [
               "In this paper, we have taken on the audacious task of condensing every page on the World Wide Web into a single number, its PageRank. PageRank is a global ranking of all web pages, regardless of their content, based solely on their location in the Web's graph structure. Using PageRank, we are able to order search results so that more important and central Web pages are given preference. In experiments, this turns out to provide higher quality search results to users. Th intuition behind PageRank is that it uses information which is exernal to the Web pages themselves -their backlinks, which provide a kind of peer review. Furthermore, backlinks from pages are more signiicant than backlinks from average pages. This is encompassed in the recursive deenition of PageRank Section 2.4.",
               "PageRank could beused to separate out a small set of commonly used documents which can answer most queries. The full database only needs to be consulted when the small database is not adequate to answer a query. Finally, PageRank may bea good way to help nd representative pages to display for a cluster center.",
               "We h a ve found a number of applications for PageRank in addition to search which include traac estimation, and user navigation. Also, we can generate personalized PageRanks which can create a view of Web from a particular perspective.",
               "Overall, our experiments with PageRank suggest that the structure of the Web graph is very useful for a variety of information retrieval tasks."
          ],
          "paper_id": "244bd2d0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 25,
          "fromPaper": "The PageRank Citation Ranking: Bringing Order to the Web"
     }
]