[
     {
          "head": {
               "n": "0",
               "text": "abstract",
               "type": "abstract"
          },
          "paragraphs": [
               "This paper gives an overview of the bottom-up approach to artificial intelligence (AI), commonly referred to as behavior-oriented AI. The behavior-oriented approach, with its focus on the interaction between autonomous agents and their environments, is introduced by contrasting it with the traditional approach of knowledge-based AI. Different notions of autonomy are discussed, and key problems of generating adaptive and complex behavior are identified. A number of techniques for the generation of behavior are introduced and evaluated regarding their potential for realizing different aspects of autonomy as well as adaptivity and complexity of behavior. It is concluded that in order to realize truly autonomous and intelligent agents, the behavior-oriented approach will have to focus even more on lifelike qualities in both agents and environments."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 0,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "1.",
               "text": "Introduction",
               "type": "introduction"
          },
          "paragraphs": [
               "This paper gives an overview of the bottom-up approach to the study of artificial intelligence (AI), commonly referred to as behavior-oriented AI or autonomous agents research.",
               "Traditionally, AI has been based on the view that intelligent behavior is the result of abstract processes at the 'knowledge level' , a level that is independent of its actual implementation in biological or physical mechanisms. In particular the Physical Symbol System Hypothesis posits that the essence of intelligent behavior is rule-based manipulation of symbolic representations. Hence, traditional AI has been predominantly concerned with the synthesis of abstract capacities in representational and formal domains, such as logical problem-solving or chess-playing, rather than with robotic/sensorimotor capacities that allow interaction with the represented world. The relevance of this type of AI for virtual environments has been somewhat limited, since virtual reality (VR) is typically concerned with the modeling of the concrete/physical rather than the abstract/mental. In such domains traditional AI has had relatively little to offer since it was concerned with disembodied reasoning, rather than perception and action.",
               "Since the mid 1980s, however, traditional AI and the underlying cognitivist paradigm in cognitive science have been questioned from a number of perspectives: In the robotics field traditional AI systems were shown to have serious problem in dealing with complex environments, let alone the real world, and the alternative approach of behavior-based robotics, focusing on perception and action, was introduced . At the same time the re-emergence of connectionism and artificial neural nets 2 (ANNs) began to pose a serious threat to symbolic theories of intelligence. Furthermore, researchers interested in the situated nature of cognition and its bodily/biological basis began to argue that intelligent behavior and cognition are much more about effective interaction between agent/organism and environment, rather than an agent's capacity to handle abstract world models internally. From the combination of these influences the field of behavior-oriented AI has emerged, which unlike its traditional counterpart, is predominantly concerned with the study of so-called autonomous agents, situated in and interacting with an environment. Section 2 elaborates in further detail the fundamental differences between traditional and behavior-oriented AI. Furthermore the terminology of the latter field is introduced, and related research areas are identified.",
               "Section 3 outlines some key issues and problems of behavior-oriented AI. Firstly, different notions of autonomy are discussed, and three relevant dimensions/aspects of autonomy are identified, which will be used throughout the paper to evaluate different approaches and techniques. Secondly, adaptivity and complexity of behavior are discussed, and three key problems for the realization of adaptive control composition in autonomous agents are identified, which will be used to evaluate different approaches and techniques presented in the following sections. Thirdly, different types of autonomous agents are distinguished and their relevance and major differences are identified. Here, it will be argued that robotic agents are the most promising route from an AI perspective. Hence, the rest of this paper will focus on this type of agent.",
               "3 Section 4 reviews relevant approaches and techniques for the generation of behavior in autonomous agents, including algorithmic and dynamic approaches, subsumption architecture and other modular architectures, self-learning in different types of ANNs, as well as evolutionary methods. These techniques are discussed in general, illustrated with examples of their practical use, and evaluated regarding their suitability for the realization of autonomous, adaptive and complex behavior.",
               "Section 5 summarizes and integrates the discussion of key issues and open problems in behavior-oriented AI given in section 3 with the overview of available techniques given in section 4. This results in the discussion of a number of key issues/suggestions for future behavior-oriented AI research, including the possible roles different types of autonomous agents can play in virtual environments, as well as the opportunities virtual environments might have to offer behavior-oriented AI.",
               "Section 6 briefly summarizes the work presented here, and draws a number of conclusions concerning the future development of AI in general, and its relation to virtual environments in particular."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 1,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "2.",
               "text": "Behavior-oriented AI: Concepts and Terminology",
               "type": "introduction"
          },
          "paragraphs": [
               "The main intention of this section is to introduce behavior-oriented AI by contrasting it with traditional, knowledge-based AI (section 2.1). Furthermore the terminology of behavior-oriented AI, as far as relevant, is introduced in section 2.2, and a number of related research areas, united by a common interest in autonomous agents, are identified in section 2.3. the problem of how to 'extract' a human's (expert-) knowledge and represent it in a form appropriate for a machine to reason with. The knowledge-based approach is sometimes also referred to as 'top-down AI' (e.g. , due to the fact that it typically is concerned with the modeling of high-level cognitive capacities such as planning, problem solving, or game playing, but for a long time ignored lower-level mechanisms and competencies as they are required for interaction with physical environments, such as perception and motor skills. Knowledge-based AI succeeded in building systems with, in many cases, impressive performance/expertise within narrow domains such as medical diagnosis, theorem proving or chess.",
               "This methodology does, however, result in systems which lack grounding, i.e. do not have any direct causal connection to the 'outside world' they represent, but only an indirect connection via the interpretation of the system's user/designer . Furthermore most of these systems suffer from the problem of \"brittleness\" , i.e. the tendency to fail utterly when confronted with problems even slightly outside their domain of expertise .",
               "Behavior-oriented AI, on the other hand, is inspired by the notion of intelligence/cognition as a biological characteristic . Hence, the view that 'intelligence' should be considered an agent's capacity to interact with its environment, rather than to represent/model it internally. Most research in the field is therefore concerned with the study of systems that to some degree exhibit life-like qualities, in particular so-called autonomous agents, which Beer (1995) characterizes as follows:",
               "By autonomous agent, I mean any embodied system designed to satisfy internal or external goals by its own actions while in continuous long-term 6 interaction with the environment in which it is situated. The class of autonomous agents is thus a fairly broad one, encompassing at the very least all animals and autonomous robots.",
               "For the moment, the above broad notion can be considered a good approximation which, probably, the majority of researchers in behavior-oriented AI would agree to. Section 3 will give a more detailed discussion of different notions of autonomy, i.e. the question what exactly is to be understood as \"its own actions\" in the above definition, and different types of autonomous agents, i.e. the question what exactly is to be understood as \"embodied\" and \"situated\".",
               "In analogy to living systems, in which adaptation is not restricted to self-learning at the level of the individual, behavior-oriented AI also studies intelligent behavior as a result of adaptation at the species level (evolution) as well as at the social (group) level . Approaches to the synthesis of self-learning and evolutionary adaptation in autonomous agents will be discussed in section 4.2. The aspect of 'social intelligence' is not discussed in further detail in this paper; for an overview see .",
               "The behavior-oriented approach to the study of intelligent behavior, also referred to as 'bottom-up AI , is to start off with simple agents with multiple integrated competencies, rather than expertise in narrow domains. Typically these skills are very basic 'survival' skills of the sensory-motor coordination type (such as obstacle avoidance), based on simple underlying mechanisms on top of which more complex behavior can be developed in interaction with the environment. This is supported by the insight that behavioral complexity does not necessarily require complexity of the underlying behavior-generating mechanisms . Furthermore, the approach is to a large extent supported by theory of sensorimotor intelligence and intellectual development , as well as more recent work in developmental psychology (e.g. , which emphasize the relevance of sensorimotor development for the acquisition of more abstract cognitive capacities."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 2,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "2.2",
               "text": "Some terminology",
               "type": "introduction"
          },
          "paragraphs": [
               "As a result of the diversity/interdisciplinarity of behavior-oriented AI (cf. section 2.3), much of its basic terminology is still ill-defined. The term 'agent', for example, is unfortunately used for all sorts of systems, ranging from the most complex (humans, animals) to the very simple (programs, subroutines, etc.) (cf. . In line with Beer's above definition, in this paper an agent is to be understood as a system that can be viewed as situated in an environment, and interacting with it by means of perception and action.",
               "The use of the term 'behavior' is twofold: On the one hand it can refer to a system's 'overall (or global) behavior', on the other hand to a particular 'behavior' such as obstacle avoidance (cf. . defines the term as follows:",
               "A behavior is a regularity in the interaction dynamics between an agent and its environment, for example, maintaining a bounded distance from the wallBehaviors belong .. to the descriptive vocabulary of the observer.",
               "Accordingly, 'behavior systems' are defined as follows : 8 A behavior system is the set of all mechanisms that play a role in establishing a particular behavior.",
               "Another central concept in behavior-oriented AI is that of emergence of behavior and functionality. Emergence is of central importance to the approach since it offers a possible bridge between the necessity of complex and adaptive behavior at the global level (i.e. temporally extended and dynamically varying) and the mechanisms of distributed multiple competencies and situation-based action. Steels (1994) defines emergence as follows:",
               "A behavior is emergent if it can only be defined using descriptive categories which are not necessary to describe the behaviour of the constituent components. An emergent behaviour leads to emergent functionality if the behaviour contributes to the system's self-preservation and if the system can build further upon it.",
               "An example is given by who describes the emergence of a 'corridor following' behavior in a simple robot from three underlying particular behaviors: turning right when perceiving a wall on the left, turning left when perceiving a wall on the right, and moving straight forward otherwise.",
               "Despite its central role in behavior-based AI the mechanisms of emergence are far from being fully understood yet. Hence, Maes (1993) points out:",
               "We need a better understanding of the underlying principles of BehaviorBased AI. ... In particular, it is important to understand the mechanisms and limitations of emergent behavior."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 3,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "paragraphs": [
               "A detailed discussion of different notions of autonomy, embodiment and situatedness will be given in sections 3.1 and 3.3, since these are conceptual key issues rather than questions of terminology."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "head": {
               "type": "introduction"
          },
          "paragraphNo": 4,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "2.3",
               "text": "Related research areas",
               "type": "introduction"
          },
          "paragraphs": [
               "The area of behavior-oriented AI is in its very nature interdisciplinary, bringing together researchers from such diverse areas as AI, cognitive science, psychology, ethology, philosophy, biology, control theory, etc., each of which has particular goals, interests and reasons for studying autonomous agents. Hence, there are a number of overlapping research areas sharing a common interest in autonomous agents as tools/objects of research.",
               "In addition to 'behavior-oriented' (e.g. and 'behavior-based' (e.g. ) the autonomous agents approach, as mentioned above, is referred to 'bottom-up AI'. Moreover, behavior-oriented AI to a large extent overlaps with (a) the field of Adaptive Behavior (e.g. , which could be characterized as the study of the mechanisms of adaptive behavior in both natural and artificial agents, and (b) the Animat Approach, a term coined by , since it uses artificial animals ('animats') as tools for the study of intelligent behavior in natural ones. Furthermore these approaches, naturally, share an interest in life-like or living systems with the field of Artificial Life, which is typically defined as the study of computational models of biological phenomena and \"life-as-it-could-be\" . Moreover autonomous agents are increasingly being studied as a bottom-up approach by researchers in cognitive science, in particular those interested in Situated and/or Embodied Cognition (e.g. , who emphasize the situated nature of activity and the bodily/sensorimotor basis of cognition and intelligent behavior. Finally, the above approaches are also referred to as \"New AI\" (e.g. Dorffner, 1997a) or \"Nouvelle AI\" (e.g. )."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 5,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "3.",
               "text": "Key Issues in Behavior-oriented AI",
               "type": "introduction"
          },
          "paragraphs": [
               "This section examines a number of conceptual key issues in behavior-oriented AI. A detailed discussion of different notions of autonomy is given in section 3.1, and three key dimensions/aspects of autonomy are identified. Section 3.2 outlines the problem of adaptive control composition, and identifies three key problems for its realization in artificial agents. These problems, along with the above aspects of autonomy, will be used throughout the rest of the paper to evaluate different approaches to the realization of behavior in artificial agents. Different types of autonomous agents, and their relevance for behavior-oriented AI, are discussed in section 3.3."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 6,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "3.1",
               "text": "Autonomy",
               "type": "introduction"
          },
          "paragraphs": [
               "First of all, it should be noted that there is a wide range of definitions and notions of 'autonomy' (e.g. . Roughly, however, the spectrum can be reduced to two notions, here referred to as operational and behavioral autonomy.",
               "Both notions agree in the idea that automaticity is a necessary element of any kind of autonomy. Steels (1995) defines automaticity as follows, similar to Beer's above characterization of autonomous agents:"
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 7,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "paragraphs": [
               "To be autonomous you must first be automatic. This means that you must be able to operate in an environment, sense this environment and impact it in ways that are beneficial to yourself and to the tasks that are crucial to your further existence.",
               "Apart from this, an operational notion of autonomy requires the capacity to operate without human intervention, i.e. without being remotely controlled by a human or dependent on one to, for example, change batteries. This notion of autonomy is rather widespread, especially in the engineering-oriented robotics community, and does make sense where it is used to characterize systems as 'autonomous' which normally are controlled by humans, as in \"autonomous vacuum cleaner\" ( ) or \"autonomous land vehicle\" .",
               "This operational notion of autonomy does, however, not capture all of what the term is usually associated with when applied to humans or animals (e.g ). If you look up the term 'autonomy' in the Oxford English Dictionary, for example, you will find the following definition ):",
               "Freedom of will; the Kantian doctrine of the self-determination of the will, apart from the object willed.",
               "Obviously, this stronger notion of autonomy does not apply to today's operationally autonomous agents, whose behavior is typically to a high degree pre-programmed, rather than self-determined. It is unclear what exactly the basis of this self-determination is in natural agents, and one might be skeptical whether it ever could be synthesized in artificial ones. Hence, Bourgine and Varela (1992) point out ... the need to understand the class of processes that endows living creatures with their characteristic autonomy. Autonomy in this context refers to their basic and fundamental capacity to be, to assert their existence and to bring forth a world that is significant and pertinent without being pre-digested in advance.",
               "Dissatisfied with the engineering-oriented, operational notion of autonomy, a number of researchers have started to aim for autonomy in the stronger sense, trying to find a notion more in agreement with the cognitive science use of the term, thus sometimes referred to as \"true autonomy\" or \"real autonomy\" . points out that the term 'autonomy' is derived from the Greek words 'autos' (self) and 'nomos' (law or rule), and hence continues his above definition of autonomy/automaticity as follows:",
               "But autonomy goes beyond automaticity, because it also supposes that the basis of self-steering originates (at least partly) in the agent's own capacity to form and adapt its principles of behavior. A number of points concerning this stronger notion of autonomy, hereafter referred to as behavioral autonomy, should be noted here:",
               "? Behavioral autonomy should be considered a continuum, not an all-or-nothing",
               "property . That means, there are different degrees of autonomy, i.e. degrees to which an agent is in control of its behavior.",
               "? Agents with full behavioral autonomy do not exist in nature. This is due to the behavioral bias of physiology, innate instincts, reflexes, etc. which are 1 quoting personal communication with Tim Smithers 13 supplied to living creatures by evolution and typically not under (full) control of the individual.",
               "? Agents without any behavioral autonomy on the other hand, i.e. agents whose behavior is completely innate (instinctive, reflexive) do exist in nature. They are, however, although autonomous in the operational sense, typically considered unintelligent (as individuals) and therefore of limited interest to AI and cognitive science.",
               "Boden (1996) points out that there are different degrees and gradations of autonomy and identifies three dimensions according to which an agent's autonomy can be judged. We will in this paper use a similar list of three relevant aspects of autonomy (hereafter referred to A1, A2, A3), of which the first two are taken from , whereas the third is less demanding than Boden's 2 :",
               "? A1 -the extent to which responses to external stimuli are purely reactive or mediated by inner mechanisms partly dependent on an agent's history of sensory input and/or internal state.",
               "? A2 -the extent to which such inner control mechanisms have been selfgenerated or -organized rather than externally imposed.",
               "? A3 -the extent to which inner control mechanisms can be selectively activated, i.e. the degree to which an agent can dynamically decide what to do 'right now'.",
               "Aspects A1-3 are explained briefly in the following, but will be discussed further throughout the rest of the paper, and will be used to evaluate/assess the potential for different degrees and forms of autonomy in different approaches and techniques in behavior-oriented AI.",
               "A1 can be explained with the distinction between purely reactive agents and agents making use of an internal state. A purely reactive agent's action at time t will depend on its (sensory) input at time t alone, i.e. its behavior at each time is the result of an inputoutput mapping. To be able to cope with its environment, such an agent does, of course, strongly depend on sufficient input to be available from the environment at any time.",
               "The behavior of an agent that can make use of an internal state to encode, for example, a history of percepts and actions, is determined by a mapping from an input history to actions, which is inherently more powerful. Hence, the A1 aspect of autonomy supplied by internal state/memory is a certain freedom/independence of external memory/information supplied by or contained in the environment at a given point in time.",
               "A2 is concerned with the distinction between engineering and self-organization, and follows directly from the notion of behavioral autonomy as including the capacity of a certain behavioral self-determination. Agents which are built/engineered/programmed to solve particular tasks, and have no means of adapting their behavior themselves, obviously lack the A2 aspect of autonomy. Agents capable of forming and/or adapting their own behavior, on the other hand, will have a certain degree of A2 autonomy depending on to what extent they are capable of self-organization and -adaptation.",
               "Hence, autonomy in the A2 sense is (a) independence of constraints imposed by an 15 external designer/programmer, and (b) the freedom to override/adapt built-in or innate constraints and behavior.",
               "A3 is concerned with flexibility/adaptivity of behavior, and characterizes the extent to which inner control mechanisms can be selectively used and modified. This would require an agent not only to develop itself the principles of its behavior, but also the capacity to dynamically decide which of them to use to guide its behavior in a particular situation. Hence, A3-type autonomy provides the freedom of flexibly organized behavior and independence of the need to match all possible problems/situations with a static behavioral repertoire, i.e. it allows a divide-and-conquer approach to the acquisition and control of complex behavior.",
               "It should be noted that A1-3 are not supposed to capture all aspects of life-like autonomy, but only a number of aspects relevant in the context of behavior-oriented AI.",
               "Hence, the rest of this paper will leave the question of 'freedom of will' aside, and instead use above aspects/dimensions A1, A2 and A3 to evaluate which degrees/forms of autonomy have been realized so far in behavior-oriented AI, and how, possibly, higher degrees can be achieved in the future."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "head": {
               "type": "introduction"
          },
          "paragraphNo": 8,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "3.2",
               "text": "Complexity & adaptivity of behavior",
               "type": "introduction"
          },
          "paragraphs": [
               "An agent that strives to fulfill internal, and possibly external, goals in interaction with a dynamic environment, will typically have to face a number of different and varying situations and tasks/requirements, each of which it has to master to a degree that ensures its long-term survival and self-preservation. That means, apart from learning to master particular situations, such an agent will typically have to exhibit different behaviors in different situations. Hence, behavioral complexity requires basically two types of adaptation/learning:",
               "? Long-term adaptation or learning of particular behaviors, typically throughout an agent's lifetime (A2 autonomy). A typical example would be a robot that learns to avoid obstacles and might have to adapt this behavior later due to failing sensors, for example.",
               "? Short-term adaptation of an agent's current overall control/behavior.",
               "Typically this requires to temporarily integrate or switch between different specific behaviors/controllers (A3 autonomy). In the above example, shortterm adaptation would be required whenever the robot has to integrate/coordinate obstacle avoidance with other behaviors such as goal finding or object detection.",
               "Hence, somewhat simplified and idealized, adaptive control of such an agent could be realized as in control composition model ( ) in which the control of a simple robotic agent is realized by a set of different controllers ('control level') for different behaviors. Each of these subsystems realizes a particular mapping from sensory input to actuator output at the 'sensorimotor level'. One of them, for example, avoids obstacles, another one approaches them. The 'adaptive control' level is realized by a higher level mechanism that learns to switch between these controllers depending on the agent's current situation.",
               "<<< here >>> Hence, adaptive control composition in such an agent poses the following three problems:",
               "I. How to acquire and adapt particular behaviors, such as 'obstacle avoidance', realized as individual controllers in the above example from Araujo &",
               "Grupen. This poses a learning problem at the control level (part of the above A2 aspect of autonomy).",
               "II. How to coordinate and structure the control of a possibly large number of these specific behaviors, some of which exclude/override each other and some of which have to be integrated, such as 'goal finding' and 'obstacle avoidance'. This poses a control problem at the adaptive control level, closely related to the above A3 aspect of autonomy.",
               "III. How to learn to solve control problem II, as it would be required from an agent combining A2 and A3 autonomy. This is due to the fact that behavioral autonomy is considered to be the individual agent's own capacity not only to learn specific behaviors (problem I) but also to form and adapt its principles of behavior. Hence, this poses another learning problem, this time at the adaptive control level, namely how to acquire and adapt the overall control mechanism and structure.",
               "It should be noted that for operationally autonomous agents above problems I and II will be solved by the system's designer. Typically this is done by programming individual behaviors and their coordination beforehand (thus the lack of A2 autonomy), which is clearly a non-trivial task."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 9,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "paragraphs": [
               "In the case of behaviorally autonomous agents on the other hand the problems, at least partly, will have to be solved by the agent on its own. That means its behavior will not be fully pre-defined, but subject to self-adaptation (A2 autonomy). To allow this, the agent has to be equipped with some generic learning/adaptation mechanism by the system's designer beforehand. Hence, the designer's role here is still non-trivial, but of a different and less determining kind (cf. ).",
               "In Araujo and Grupen's (1996) above model of control composition, for example, the agent only has a limited degree of A2 and behavioral autonomy: The individual controllers/behaviors are pre-defined and not subject to adaptation, moreover their number is limited and fixed, and the overall control can only switch between discrete behaviors, but not combine or integrate them. Further approaches to control composition will be discussed in section 4. refers to embodiment and situatedness as the \"two cornerstones to the new approach to Artificial Intelligence\", and defines them as follows:"
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "head": {
               "type": "introduction"
          },
          "paragraphNo": 10,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "3.3",
               "text": "Types of autonomous agents",
               "type": "introduction"
          },
          "paragraphs": [
               "The robots are situated in the world -they do not deal with abstract descriptions but with the here and now of the world directly influencing the behavior of the system. "
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 11,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "paragraphs": [
               "As with autonomy, however, there are different notions/interpretations of embodiment and situatedness. Firstly there are those who emphasize the relevance of physical embodiment and accordingly restrict themselves to robotic forms of intelligence (e.g. , i.e. physically embodied agents, interacting with their environment by means of sensory perception and motor control. Thus, these agents, unlike traditional AI systems, can be said to have a certain degree of physical grounding .",
               "Secondly, there are those who also emphasize the relevance of sensorimotor interaction between robotic agents and their environments, but, for lack of physical resources, realizability, convenience or efficiency, use simulated embodied agents in sensorimotor interaction with more or less realistic simulated environments (e.g. ).",
               "Many researchers in the behavior-oriented AI community feel very strongly about this distinction between simulation and reality. A typical example is Steels (1994):",
               "The goal is to build artifacts that are \"really\" intelligent, that is, intelligent in the physical world, not just intelligent in a virtual world. This makes unavoidable the construction of robotic agents that must sense the environment and can physically act upon the environment, particularly if sensorimotor competences are studied. This is why behavior-oriented AI researchers insist so strongly on the construction of physical agentsPerforming simulations of agentsis, of course, an extremely valuable aid in exploring and testing out certain mechanisms, the way simulation is heavily used in the design of airplanes. But a simulation of an airplane should not be confused with the airplane itself."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "head": {
               "type": "introduction"
          },
          "paragraphNo": 12,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "paragraphs": [
               "Thirdly, there are researchers who de-emphasize the role of physical embodiment and sensorimotor interaction, and rather focus on a more abstract situatedness or 'embeddedness', i.e. the property of existing within and directly interacting with a (nonphysical environment). Typical examples are so-called 'software agents' (e.g. , interacting with software environments such as the Internet (e.g. . Unlike the above embodied agents, they lack simulated or physical body, sensors, motors, etc., but, unlike traditional AI systems, they directly interact with a complex (software) environment."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "head": {
               "type": "introduction"
          },
          "paragraphNo": 13,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "text": "Genesereth",
               "type": "introduction"
          },
          "paragraphs": [
               "There is, however, a clear distinction to be made between agents that possess a body (or a realistic simulation of it) and those which do not ( "
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 14,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "4.",
               "text": "Agent Control Architectures and Techniques",
               "type": "introduction"
          },
          "paragraphs": [
               "As mentioned in section 2.1, behavior-oriented AI typically studies the origin of complexity and adaptivity at different levels, from components and individual agents to 'species' of agents and complete eco-systems. There is however a growing consensus in the field that behavior systems should be considered the basic units of study, and that the behavior of a complete agent typically is the result of the cooperation/competition of a number of behavior systems. This section discusses different approaches to the design of behavior systems and techniques for their realization. Roughly these approaches can be categorized into two types:",
               "? Engineering approaches, basically aiming for operational autonomy by engineering/programming an agent's behavior systems more or less completely beforehand.",
               "? Learning approaches, aiming to equip the agent with a capacity for selforganization (the A2 aspect of autonomy), and thus provide it with a certain degree of behavioral autonomy."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 15,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "4.1",
               "text": "Engineering approaches",
               "type": "modelling"
          },
          "paragraphs": [
               "The problem with designing and 'hand-crafting' an agent's behavior systems is that typically not all possible situations the agent might have to face are known beforehand, especially in unpredictable environments. Hence, state that \"behaviour-based robot design has some reputation for being a black art\", since an agent's behavior is not determined by its internal behavior systems alone, but by their interaction with its environment ( ).",
               "Furthermore, even if environmental dynamics and all possible situations could be foreseen at an abstract level, e.g. obstacles have to be avoided, this would not imply that it is actually possible to specify/formalize beforehand the agent's behavior at any point in time, i.e. to define how, in detail, all possible obstacle avoidance situations should be handled."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 16,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "4.1.1",
               "text": "Algorithmic approaches / Subsumption architecture",
               "type": "modelling"
          },
          "paragraphs": [
               "Algorithmic approaches are based on pre-programming an agent's behavior systems.",
               "Following a divide-and-conquer approach, the overall control task is typically into suitable behaviors, to the designer. Thus, the agent's A2 autonomy is greater than in the original subsumption architecture, it does however remain limited.",
               "Thus, the subsumption architecture has similarities to earlier discussed control composition model. Both approaches use a fixed number of basic pre-defined behavioral modules, which in the former case combine in a selforganizing fashion to achieve bottom-up emergence of overall behavior, whereas in Araujo and Grupen's model they are switched between at a higher level of control.",
               "In summary, pre-modularized architectures in general, and the subsumption architecture in particular, can be said to suffer from the following problems:",
               "? The decomposition into modules/behavior systems has to be carried out topdown and a priori by the system's designer. It has been argued that this can be disadvantageous , and that behavioral organization/hierarchy should better be emergent instead ).",
               "? In a concrete agent, interacting with a non-trivial environment, the number of states and corresponding subsystems quickly increases, and so does their complexity .",
               "? Priority arbitration schemes to handle conflicts between behavioral layers/modules, such as the subsumption relations in the subsumption architecture, are fixed and rather arbitrary which limits the controlled agent's flexibility ( )."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 17,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "paragraphs": [
               "? The inherent commitment to finite-state automata as underlying mechanism results in a finite number of discrete states/behaviors that makes it difficult to achieve smooth, continuous behavior at the level of the overall system .",
               "Furthermore, the algorithmic framework leaves relatively little room for the capacity to learn (A2 autonomy) which here would require an agent to re-program or re-structure itself."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "head": {
               "type": "modelling"
          },
          "paragraphNo": 18,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "4.1.2",
               "text": "Dynamic approaches",
               "type": "modelling"
          },
          "paragraphs": [
               "Dynamic approaches are based on the notion of behavior systems as continuous dynamical systems instead of discrete computational systems as in the algorithmic approach . For a detailed review see .",
               "As the subsumption architecture, a dynamical systems architecture typically consists of a number of processes running in parallel. Each of these processes, by means of differential equations, establishes a continuous relationship between a set of quantities, e.g. between distance sensor readings and motor parameters in a mobile robot that turns sharper or slows down the closer it gets to an obstacle ( . are assigned to each behavior, which describe the behavior's state of activation and allow to switch it on or off, and a 'competition matrix' is used to define whether or not different behaviors can be active at the same time.",
               "An advantage of behavior systems realized by differential equations is that, at least theoretically, it makes self-adaptation of an agent's behavior through dynamic adaptation of parameters, such as motivational variables, simpler than in the algorithmic framework. Another advantage of this approach is the fact that the dynamical systems paradigm is closer to descriptions used in physics, biology, and control theory .",
               "Dynamical systems theory is, at least in theory, probably the most powerful and most appropriate framework for behavior description and explanation. It has been adopted as a framework of explanation in recurrent ANN approaches to adaptive behavior (e.g , and also as a research framework in cognitive science for an introduction and overview).",
               "The 'pure' dynamic approaches presented so far, using differential equations, however, suffer from a lack of a learning mechanism, and thus A2 autonomy. Hence, the major 27 problem is basically the same as for the algorithmic approach: It is (relatively) simple for the system's designer to increase behavioral complexity by adding further differential equations and/or motivational variables (behaviors/FSA in the algorithmic approach). It is, however, unclear how an agent could increase the complexity of its behavior on its own (A3 autonomy), let alone how it could learn to do this (A2 autonomy)."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 19,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "4.2",
               "text": "Learning approaches",
               "type": "modelling"
          },
          "paragraphs": [
               "As mentioned above, learning approaches to agent control aim to give agents a certain degree of A2 autonomy by providing agents with some capacity for learning/selforganization that allows them to acquire and/or adapt their behavior, at least to some extent, on their own. Most prominent among these are",
               "? the connectionist/ANN approach to self-learning of individuals (section"
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 20,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "text": "4.2.1), and",
               "type": "modelling"
          },
          "paragraphs": [
               "? evolutionary learning techniques based on adaptation at the population/species level, inspired by the mechanisms of natural selection (section 4.2.2).",
               "The challenge for learning approaches is to design an appropriate generic learning mechanism, that allows agents the acquisition of complex and adaptive behavior, i.e. to solve problems I-III, at least partly, themselves."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 21,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "4.2.1",
               "text": "Connectionist approaches",
               "type": "modelling"
          },
          "paragraphs": [
               "For the understanding of the following discussion it should suffice to know that an ANN consists of a number of interconnected simple processing elements/units, each of which receives a number of numerical input values from which it calculates its own activation and a resulting output value. These outputs themselves can be passed on as input to other units along weighted interconnections. Thus, the ANN as a whole computes a certain mapping from input to output units; units in between are referred to as 'hidden units'. ANNs learn, from examples, to acquire a certain input-output mapping through adaptation of the connection weights between units (therefore ANNs are also referred to as connectionist networks) according to some learning algorithm, such as error backpropagation A major advantage of the connectionist approach in the context of autonomous agents is that it incorporates mechanisms for learning behaviors which, often in combination with reinforcement learning techniques, i.e. learning from 'reward' and 'punishment', allows a bottom-up development of integrated control strategies . For a detailed introduction and overview of reinforcement learning see , or Sutton (1991) for an animat-related overview. In the simplest case, this self-learning capacity reduces the system designer's task to the choice of an appropriate ANN architecture for a given control task.",
               "Hence, connectionist networks are commonly considered to be effective mechanisms for controlling autonomous agents ( what connectionist architecture is best suited for which type of control task.",
               "Feed-forward ANNs:",
               "Because feed-forward networks compute a one-way mapping from inputs to outputs (i.e.",
               "in the case of a robot controller from sensor input to motor outputs), a feed-forward controller can only react to its current input in each time step. As discussed above, purely reactive agents lack A1 autonomy, i.e. they depend on sufficient input to be available from the environment at any point in time. The complexity of behavior that can be achieved with such an agent is of course limited. Nevertheless it has been shown that such a system can learn to acquire far more than trivial behavior. , for example, used a simple robotic vehicle controlled by a feed-forward network mapping the input from two whisker sensors at the front of the vehicle to four possible actions (left, right, forward, backward). It was shown that, using reinforcement learning, this vehicle could successfully be trained to perform tasks like corridor following or box pushing in a purely reactive fashion."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 22,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "text": "Recurrent ANNs:",
               "type": "modelling"
          },
          "paragraphs": [
               "Recurrent connectionist networks are a special case of dynamical systems (see for a detailed discussion of this aspect), and they are commonly considered more suitable for the purpose of adaptive control since they allow an implicit representation of dynamical aspects in form of feedback through an internal network state (e.g. . ). These recurrent controller networks have been shown to develop internal states that reflect the vehicle's current task/context ( ) and result in sequential, plan-like behavior that can be characterized as \"emergent planning\" .",
               "Limitations of conventional ANN approaches:",
               "A problem with the connectionist approach is that, although single ANNs might be very well suited for the realization/learning of individual behavior systems (solving problem I), a conventional monolithic network could hardly be sufficient to realize and integrate a number of complex and diverse behavior systems (problem II) as it would be required to control a complete autonomous agent that interacts with a complex and dynamic environment."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 23,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "paragraphs": [
               "More specifically, the problem is that an ANN always realizes a certain function, or an iterated function system in the case of recurrent networks , i.e. a particular mapping of input (and internal state) to output. The weights in such a network, realizing this functional mapping, usually remain constant after a certain training phase, or, in continual or 'lifelong' learning approaches, are updated only stepwise and very gradually, i.e. the sensorimotor mapping will basically remain static. That means that the complexity/diversity of behavior that can be learned by a single conventional ANN is strongly limited by the degree to which a number of behavior systems can be realized/integrated in a single functional mapping. This limits the controlled agent's A3 autonomy, i.e. its capacity for flexible short-term adaptation to a possibly rapidly changing environment."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "head": {
               "type": "modelling"
          },
          "paragraphNo": 24,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "text": "Modular ANNs:",
               "type": "modelling"
          },
          "paragraphs": [
               "There are a number of modular ANN approaches to the control of autonomous agents (e.g. ). They do, however, mostly suffer from the problems of built-in modularization, as in the subsumption architecture, which have been discussed earlier: The controlled agents are given a certain A3 autonomy, they are, however, typically not able to re-structure/organize their behavior themselves.",
               "A modular ANN approach that, at least partly, addresses problems I, II and III of adaptive control composition is Jordan and Jacobs' work on the Adaptive Mixture of Experts (AME) architecture. AME is a general learning approach, not specifically developed for autonomous agents control. , however, have applied a similar approach, referred to as Mixture of Recurrent Neural Network Experts (MRE), to 32 robot learning. present a good overview of the AME architecture; for details see or .",
               "In the AME architecture ( ) a number of ANN-modules, referred to as experts, work in parallel. Each expert typically acquires a specific competence/behavior (solving problem I). Their outputs are integrated by another ANN, the so-called gating network, which assigns weights (g i ) between 0 and 1 to each of the experts' outputs (typically the sum of all g i is 1).",
               "<<< here >>> In comparison to the control composition models discussed earlier the AME has the following advantages:",
               "? The individual controllers are learned (solving problem I), and it is up to the system how to distribute behaviors over expert networks.",
               "? Switching between behaviors/experts is carried out by the gating network (solving problem II) and it can be learned as well (problem III).",
               "? Combination/composition of behaviors is difficult, but possible through softswitching of the gating network (more than one g i greater 0).",
               "Hence, an AME-controlled agent has a certain degree of A2 and A3 autonomy, and if using recurrent networks, as in the MRE case, it will also have A1 autonomy. The number of expert networks, however, is typically pre-set. Hence, the system's structure is still partly designed instead of fully self-acquired, which leads to problems with adaptation of the behavioral organization, concerning how and when to add or remove experts.",
               "Self-Adapting Recurrent Networks:",
               "As discussed above, the problem with conventional ANNs is that their connection weights are rather static, with the result that the controlled agent, apart from the influence of the internal feedback, always maps sensory input to motor output the same way. In biological neural networks, however, connection weights are known to fluctuate.",
               "This has, for example, been pointed out by :",
               "To summarize, in biologically realistic nets the connectivity, connection weights, nodal transfer functions, and network parameters can change fluidly under chemical control: the nets are chemically tunable moment to moment, leaving no trace, only a fluctuating attunement.",
               "A connectionist control architecture that aims to provide A3 autonomy through a form of \"fluctuating attunement\" is the Self-Adapting Recurrent Network (SARN) , inspired by Pollack's (1991) work on 'dynamical recognizers'.",
               "Adaptive control composition is here achieved through a different, higher-order, type of feedback which is used to adapt the sensorimotor mapping itself. Thus, the SARN ( ) consists of two ANNs in a master-slave relationship:",
               "? The function (slave) network, as conventional ANN controllers, maps sensory inputs to motor outputs, and is responsible for acquiring particular behaviors,",
               "i.e. correct input-state-output mappings.",
               "? The context (master) network takes as input the function network's internal state, and adapts weights in the function network. Hence, the context network has to learn when to use which behavior/sensorimotor mapping, or, in AME terms, how to 'construct' a suitable expert for each particular context and internal state.",
               "<<< here >>> In a number of experiments on mobile robots (Ziemke, 1996a; Ziemke, 1996c) SARNs have proven capable to acquire structured behavior, based on emergent behavioral hierarchy/organization realized as internal state space dynamics. In these experiments",
               "SARNs were shown to realize adaptive control composition through context-dependent continual adaptation of their own behavioral bias (default behavior) and sensorimotor mapping. This, for example, allowed a controlled robot to integrate wandering and obstacle avoidance behaviors with periodically switching between seeking and avoiding light sources in a subsumption-like fashion (Ziemke, 1996a).",
               "Hence, in comparison to the models of control composition discussed above, the SARN architecture offers the following advantages:",
               "? Individual behaviors, such as obstacle avoidance, are acquired through the function network (solving learning problem I).",
               "? The overall control is composed of multiple 'virtual controllers'",
               "(instantiations/variations of the function network) 'constructed' by the context network (solving control problem II) which learns when to use which function network (solving learning problem III).",
               "? Emergent behavioral organization and 'decomposition' of the overall control task into particular behaviors and their combinations, based on internal state space dynamics, in line with the earlier discussed arguments against built-in behavioral organization/structure.",
               "Furthermore, in comparison to the AME/MRE the SARN architecture has the following characteristics:",
               "? The SARN does not switch between a limited/fixed number of experts, but 'constructs' 'virtual experts' for each context, which to some degree circumvents AME/MRE's problems when/how to add and remove experts, which could be especially useful for developing agents in dynamical environments.",
               "? Integration/combination of multiple behaviors is simpler, since behaviors are realized as 'virtual' rather than dedicated single-purpose experts.",
               "Thus, the SARN combines different aspects of autonomy in an interesting way: It provides A1 autonomy through its use of internal state, A2 autonomy through its twofold learning capacity, and A3 autonomy through its capacity to selectively modify/activate behavior in a context-dependent fashion."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 25,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "paragraphs": [
               "Problems with (connectionist) self-learning approaches:",
               "Although the use of connectionist self-learning techniques allows for A2 autonomy, it has to be noted that there are a number of problems arising from the fact that the most powerful ANN-learning techniques are supervised or reinforcement learning techniques.",
               "During supervised learning an ANN has to be supplied with the correct target output in every time step. This leads to a problem analog to that of engineering approaches (cf. section 4.1), namely that a sufficiently accurate model of the control task has to be available beforehand.",
               "During reinforcement learning an ANN typically only receives occasional feedback in terms of 'good' or 'bad'. This has the advantage that no longer a detailed model of what exactly to do in every situation is required. Instead more abstract information has to be available about negative situations (e.g. a robot hitting an obstacle) and positive ones (e.g. a robot reaching a goal). This abstract feedback is, of course, easier to provide than detailed supervision, nevertheless there are problems:",
               "? Reinforcement is typically given in abstract terms ('good' or 'bad') whereas most ANN learning algorithms require precise error measurements. There are, however, approaches addressing this problem, as the complementary reinforcement backpropagation algorithm ; for applications to robot learning see ).",
               "? ANNs typically learn from feedback in every time step. For complex tasks, however, reinforcement often is not available until, for example, a goal is achieved, i.e. typically after a possibly long sequence of actions."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "head": {
               "type": "modelling"
          },
          "paragraphNo": 26,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "paragraphs": [
               "It should be noted that these problems, especially the latter one, are not ANN-specific but have to be faced by any self-learning agent when learning from interaction with an environment."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "head": {
               "type": "modelling"
          },
          "paragraphNo": 27,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "4.2.2",
               "text": "Evolutionary approaches",
               "type": "modelling"
          },
          "paragraphs": [
               "Evolutionary learning techniques are inspired by the mechanisms of natural selection . Evolutionary algorithms typically start from a randomly initialized population of individuals/genotypes encoded as strings of bits or real numbers. Each individual usually represents a (possible) solution to the problem at hand; in the context of autonomous agent control this could, for example, be the weights in a controller ANN (of pre-defined architecture). Further generations of individuals are then typically produced through iteration of the following steps:",
               "? Evaluate each individual's fitness according to some measure that determines how good a solution the individual represents.",
               "? Select two parent individuals from the old population, biased in favor of fitter ones.",
               "? Recombine the two parent individuals to give two offspring: Both parent strings are cut in pieces at randomly chosen positions. The two offspring individuals are formed by recombining parts of parent 1 with parts of parent 2.",
               "? Mutate the offspring individuals by altering the strings at randomly chosen positions.",
               "? Insert offspring individuals into the new population."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 28,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "paragraphs": [
               "Due to the repeated selection and reproduction of fitter individuals ('survival of the fittest'), the population's average fitness, i.e. the individuals' capacity to solve the problem at hand, tends to increase over generations. For a proper introduction to evolutionary algorithms see .",
               "A number of researchers (e.g. have used evolutionary algorithms to evolve ANN connection weights. , for example, used an evolutionary algorithm to find suitable weight settings for the recurrent controller network for the robotic vehicle discussed above. Comparisons to the results achieved with conventional ANN learning showed that evolutionary algorithms can find suitable solutions more reliably in cases where no sufficient reinforcementmodel or only delayed reinforcement is available, e.g. if the controller only gets rewarded once for reaching a goal location after a possibly large number of steps. This is due to the fact that evolutionary algorithms do not require step-by-step supervision or reinforcement, since they are not based on self-learning individuals. Instead they typically give a once-in-a-lifetime reinforcement by letting individuals reproduce according to their fitness, which in Meeden's (1996) case was evaluated by counting achieved goals and errors made for each individual during a test run.",
               "Thus, evolutionary algorithms, typically evolve a large number of individuals, each representing a possible solution, over an even larger number of generations. The problem with this type of learning in an autonomous agent is that in an individual physical agent/robot the evolution and evaluation of such a large number of controllers is often not possible/feasible due to real time and memory restrictions. Therefore this learning process can often only be carried out in a simulated environment, therefore 39 referred to as \"off-line evolution\" . This is also the case in the above example from , where, however, the evolved control networks were successfully transferred to the physical robot afterwards. A counter-example of the evolution of homing behavior realized entirely on a physical robot is given by .",
               "Another interesting application of evolutionary methods is the evolution of agent morphologies (e.g. , sometimes in co-evolution with the agent's control mechanism . This approach aims to overcome call the \"chicken and egg\" problem of evolutionary robotics:",
               "Learning to control a complex body is dominated by inductive biases specific to its sensors and effectors, while building a body which is controllable is conditioned on the pre-existence of a brain.",
               "An example of this approach is the work by , who co-evolved, in simulation, the 'eyes' (optical sensors) and 'brains' (connectionist control networks) of simple agents which pursue and evade each other in a two-dimensional plane.",
               "For detailed discussions of different approaches in evolutionary robotics see or , for a discussion of the relation to other approaches see .",
               "Combinations of evolutionary adaptation and self-learning:",
               "Inspired by the variety of adaptive mechanisms in natural systems, a number of researchers have suggested the combination of different adaptation techniques (e.g."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "head": {
               "type": "modelling"
          },
          "paragraphNo": 29,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "paragraphs": [
               "Vaario & Ohsuga, 1997). Combinations of evolutionary adaptation with self-learning techniques have recently received a lot of attention (e.g. ). Such approaches are often inspired by natural systems, such as higher animals, which are typically equipped with some minimal innate 'default'-behavior (instincts, reflexes, etc.) by evolution, often required to ensure survival until more complex skills have been self-learned. An example of this is the innate capacity of many animals (zebras, antelopes, etc.) to learn to walk within minutes after birth .",
               "A typical example of such a combination is the evolution of connectionist control architectures (cf. , which, once evolved, can be trained in the conventional self-learning fashion (e.g. . The evolution of ANN architectures has the advantage that it further reduces the designer's influence on an agent's behavior systems.",
               "Another interesting example for the combination of evolutionary adaptation and selflearning is work on 'self-teaching' networks which give individual robots the capacity to adapt to changing environments, learning from evolved self-supervision. These self-teaching networks produce motor outputs, but also an equal number of teaching outputs which are used to supervise the motor output units, i.e. the teaching units' outputs are used as target outputs for the motor output units during the robot's self-learning phase."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "head": {
               "type": "modelling"
          },
          "paragraphNo": 30,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "5.",
               "text": "Discussion",
               "type": "modelling"
          },
          "paragraphs": [
               "After giving an overview of some key issues of the behavior-oriented approach to AI in section 2 and 3, and architectures and techniques for the realization of adaptive behavior 41 in section 4, the intention of this section is to discuss a few key issues/questions for future work on autonomous agents. Furthermore, it will be discussed what behaviororiented AI has to offer to virtual environments, and vice versa."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 31,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "5.1",
               "text": "How relevant are autonomous agents to AI?",
               "type": "modelling"
          },
          "paragraphs": [
               "As Pfeifer (1995) points out, throughout its history AI's interest in intelligent behavior and cognitive capacities has been twofold, and so it is in the study of autonomous agents:",
               "On the one hand we want to design robots, on the other we want to understand "
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 32,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "5.2",
               "text": "Behavioral autonomy in artifacts?",
               "type": "modelling"
          },
          "paragraphs": [
               "The question whether or not life-like levels of autonomy and intelligence can be achieved in artificial agents has to be considered an empirical issue, and time will tell how successful current approaches will be. It is clear that any technique for the realization of behavioral autonomy has to address adaptive behavior and control composition, and a number of approaches have been reviewed briefly in this paper. The key question here is how to deal with complex, but flexible structures of behavior.",
               "The lack of environmental grounding in traditional AI, i.e. the systems' inability to relate their built-in internal mechanisms and representations to their environment via perception and action, seems to suggest to reduce engineering to a minimum (cf. e.g. , ; see, however, also Albus (1996) for an opposite viewpoint), and to rely on a maximum of self-organization instead (cf. ).",
               "This would require not to build/design too much into a system beforehand, but to let agents acquire both their behavior and its behavioral organization themselves in the course of artificial evolution, development and learning; in particular aiming for evolved/emergent control hierarchies and behavior organization, as exhibited by a number of approaches discussed in this paper.",
               "As point out, the autonomy and situatedness of living systems is the result of the long, evolutionary and individual, history of agent-environment interaction reflected in their embodiment. Hence, they view cognition and intelligent behavior as embodied action, which they describe as follows:",
               "By using the term embodied we mean to highlight two points: first, that cognition depends upon the kinds of experience that come from having a body with various sensorimotor capacities, and second, that these individual sensorimotor capacities are themselves embedded in a more encompassing biological, psychological, and cultural context. By using the term action we mean to emphasize ... that sensory and motor processes, perception and action, are fundamentally inseparable in lived cognition.",
               "From this perspective, behavioral autonomy is the result of the co-evolution/codevelopment of living systems with natural environments. Hence, the synthesis of behavioral autonomy and intelligence in artifacts is likely to require more life-like qualities in both agents and environments (cf. also )."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 33,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "5.3",
               "text": "Life-like qualities in agents and environments",
               "type": "modelling"
          },
          "paragraphs": [
               "Although behavior-oriented AI is to some degree biologically inspired , so far most of its agents, environments, and learning scenarios seem rather simplistic when compared to natural agents and environments. The goal of AI is, of course, not to copy biological systems , but to understand and model the general principles underlying intelligent behavior and cognitive capacities.",
               "Some of the abstractions from biology used so far, and certainly those used by cognitivism and traditional AI, however, seem to leave out relevant details which need to be 're-introduced' into the study of intelligent behavior ( .",
               "Some aspects that have to be addressed on the agent side are the following:",
               "? Much current work still focuses on one-learning-technique-, one-stagelearning of a pre-given task in a pre-given environment. Adaptation in natural systems, however, takes place through multiple mechanisms of evolution, individual development, and self-learning (e.g. ; cf. previous section).",
               "? In particular, intelligence in natural agents is subject to development, often involving a number of developmental stages in the individual, biased by its species-dependent physiology, instincts, reflexes, etc. (cf. ). In behavior-oriented AI the aspect of individual development has been largely neglected for a long time , but has received attention more recently (e.g. ).",
               "? Agents need internal dynamics, i.e. more complex systems of varying motivations and values, including a variety of mechanisms supplying selfreinforcement for an agent's behavior (cf. 'selfteaching' networks discussed earlier).",
               "? Agents need to be more active following their internal dynamics and pursuing their goals. That is, they need capacities to perceive actively and selectively, to focus their attention, etc.; currently agents are typically rather passive 45 observers, purely driven by the dynamics of the \"environmental puppeteer\" ).",
               "As discussed above, behavior results from the interaction of an agent's actions with environmental conditions and dynamics. Hence, it is unlikely that 'intelligent' behavior can be achieved in complex agents dealing with simple environments. That means, complexity is also required on the environment side. That is, there is a need for artificial ecosystems (cf. ) with competition, collaboration and coevolution at both agent and species level, in which there are ecological niches into which agents/species can specialize."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 34,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "5.4",
               "text": "Does behavior-oriented AI need virtual environments?",
               "type": "modelling"
          },
          "paragraphs": [
               "As discussed earlier, there is a certain resentment toward simulation in much of the autonomous agents/robotics community. The 'real world', however, happens to be a very scarce resource. It has been argued in this paper, that in line with the general behavior-oriented approach to adaptation/learning/intelligence at different levels and in different forms, learning scenarios should be more complex and more realistic than they usually are today, preferably complete ecosystems, i.e. environments containing multiple agents/species of sufficient complexity. In the typical research laboratory, however, the demand for multiple robots already poses a serious problem. The required resources for complex learning scenarios are simply not physically available, and, even if they were, much of the above could not be realized within reasonable time limits, if at all. Processes in the physical world are, for example, relatively slow, and research on the evolution of hardware is only in its beginning stages. Hence, due to the impossibility/unfeasibility of much of the above in the real world, virtual environments 46 might be an alternative route (cf. e.g. ). The question of how much of the above is realizable with existing VR techniques, does of course remain.",
               "Hence, to be able to implement many of the ideas in the area of behavior-oriented AI, as well as to resolve many of its open questions, future research on autonomous agents could profit greatly from realistic simulations of autonomous agents in sensorimotor interaction with virtual environments of sufficient complexity."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 35,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "5.5",
               "text": "Do virtual environments need autonomous agents?",
               "type": "modelling"
          },
          "paragraphs": [
               "The question of possible use/roles of autonomous agents in virtual environments has to be answered with regard to different types of agents, as well as different types of virtual environments.",
               "First of all, autonomous agents, or AI techniques in general, are of course not required in all virtual environments. In architectural VR scenarios, for example, commonly only visited by avatars, representing human visitors, there probably is no need for any type of autonomous agents, unless, possibly, as a rather passive population. In many other environments operationally autonomous agents might play the role of side-actors, as, for example, in the ALIVE system ( , which allows the interaction of humans and a virtual environment inhabited by artificial agents. Many techniques for generation/simulation of adaptive behavior in operationally autonomous agents are available, and a number of examples have been given in this paper. The role of software agents, much like in existing software environments, could be that of monitors or sentinels, e.g. in medical training simulations.",
               "If, however, there is a need for virtual environments to be populated by more flexible and life-like agents, in particular where humans are supposed to cooperate or compete with artificial agents, e.g. for training purposes, then there will be a need for agents with a certain degree of behavioral autonomy and some capacity for self-organization. After all, how realistic could such a scenario be if only the humans could learn, but not their virtual collaborators/opponents? Hence, many of the AI issues discussed in this paper, such as the question of how intelligent/adaptive, and thereby how realistic, an artificial agent can ever be, are also relevant for the realization of realistic virtual environments of this type."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 36,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "head": {
               "n": "6.",
               "text": "Summary and Conclusion",
               "type": "conclusion"
          },
          "paragraphs": [
               "The intention of this paper has been to give an overview of the key issues, available",
               "techniques and remaining problems of research on adaptive behavior in autonomous agents. Section 2 introduced the behavior-oriented approach to AI by contrasting it with its knowledge-based counterpart. Some of the problems of traditional AI systems have been reviewed, in particular their incapacity to relate their abstract internal processes and representations to the 'world outside'. In the behavior-oriented approach, on the other hand, agent-environment interaction and a bottom-up approach to the modeling of adaptive behavior have been identified as key elements.",
               "Section 3 discussed a number of conceptual issues in the area of behavior-oriented AI.",
               "Different notions of autonomy were discussed, and three relevant aspects of life-like autonomy were identified. Furthermore, the generation of adaptive and complex behavior, as typical for natural agents, was introduced as a key issue in behaviororiented AI, and three key problems for its realization in artificial agents were identified.",
               "Section 4 gave an overview of architectures and techniques used in engineering and learning approaches to the realization of agents' behavior systems in general, and adaptive control composition in particular. The individual techniques were discussed in general, exemplified, and evaluated regarding their potential to realize different aspects of autonomy and to address/solve the aforementioned problems of adaptive control composition.",
               "Section 5, finally, put bits and pieces together, making a number of suggestions concerning the central importance of behavioral autonomy for autonomous agents and for AI and cognitive science research in general. In particular, it was argued, following , that in order to synthesize true intelligence and autonomy, artificial agents, their environments, and the interaction between them would have to be more like-like. It should be noted that this does not necessarily require to mimic biological systems in detail (cf. , but rather to attempt to gain further understanding of the underlying principles of intelligent behavior in living systems.",
               "Moreover, it was argued that the areas of behavior-oriented AI and virtual environments research, despite a certain resentment towards simulation in the former camp, could in fact complement each other, and that both could profit from a closer cooperation. The contribution behavior-oriented AI can make to virtual environments lies in the development of models/mechanisms for the realization of behavioral adaptivity and complexity, and thereby more realistic behavior in, for example, virtual animals or humans.",
               "For behavior-oriented AI, due to its emphasis of embodiment and agent-environment interaction, the ultimate validation of theories/models of intelligent behavior can only be provided by physical agents. The limitations of physical experimentation, however, make the synthesis of physical life-like agents unfeasible, at least in the near future."
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 37,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     },
     {
          "paragraphs": [
               "Hence, possibly as an intermediate step, virtual environments would be extremely useful tools for behavior-oriented AI experimentation if they could provide sufficiently lifelike agents, environments, and modes of interaction between them. "
          ],
          "paper_id": "24a8e6f0-97d4-11e8-9580-1f0eb29018a9",
          "head": {
               "type": "conclusion"
          },
          "paragraphNo": 38,
          "fromPaper": "Adaptive Behavior in Autonomous Agents"
     }
]