[
     {
          "head": {
               "n": "0",
               "text": "abstract",
               "type": "abstract"
          },
          "paragraphs": [
               "Inspired by recent work on convex formulations of clustering (Lashkari & Golland, 2008; Nowozin & Bakir, 2008) we investigate a new formulation of the Sparse Coding Problem (Olshausen & Field, 1997). In sparse coding we attempt to simultaneously represent a sequence of data-vectors sparsely (i.e. sparse approximation (Tropp et al., 2006)) in terms of a \"code\" defined by a set of basis elements , while also finding a code that enables such an approximation. As existing alternating optimization procedures for sparse coding are theoretically prone to severe local minima problems, we propose a convex relaxation of the sparse coding problem and derive a boosting-style algorithm, that (Nowozin & Bakir, 2008) serves as a convex \"master prob-lem\" which calls a (potentially non-convex) sub-problem to identify the next code element to add. Finally, we demonstrate the properties of our boosted coding algorithm on an image denoising task. binations of elements used to represent the raw input. However, this \"alternating optimization\" approach is non-convex with many local minima, leading to recent work on alternative, convex versions of clustering and coding (Lashkari & Golland, 2008; Nowozin & Bakir, 2008; Bach et al., 2008). This work adds several main contributions: we present a regularization function based on composi-tional norms that implements a convex version of sparse coding, we derive the Fenchel conjugate of these compositional norms, and we show how Fenchel conjugates can be used to construct an efficient boosting algorithms for convex (including non-differentiable) reg-ularization functions."
          ],
          "paper_id": "21d50670-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 0,
          "fromPaper": "Convex Coding"
     },
     {
          "head": {
               "n": "1",
               "text": "Introduction",
               "type": "introduction"
          },
          "paragraphs": [
               "A crucial part of many machine learning applications is representing the raw input in terms of a \"code\", i.e. a set of features which captures the aspects of the input examples that are relevant to prediction. Unsupervised techniques such as clustering and sparse coding ) learn codes which capture the structure of unlabeled data, and have shown to be useful for a variety of machine learning problems ( . In these techniques the input is represented as a combination of the features (also known as basis vectors or dictionary elements) that make up the code. The traditional approach to clustering and coding problems is to alternate between optimizing over the elements of the code and the comClustering and coding can be viewed as matrix factorization problems ( , which seek to approximate a set of input signals Xf (BW ) with the product of a dictionary matrix B a coefficient (or weight) matrix W and an elementwise transfer function f . When B is known and fixed and a regularization function is used to encourage W to be \"sparse\", this is the sparse approximation technique developed in engineering and the sciences. Sparse approximation relies on an optimization algorithm to infer the Maximum A-Posteriori (MAP) weights?Wweights? weights?W that best reconstruct the signal. In this notation, each input signal forms a column of an input matrix X, and is generated by multiplying the dictionary (or basis matrix) B by a column from W , and (optionally) applying a transfer function f . This relationship is only approximate, as the input data is assumed to be corrupted by random noise. Priors which produce sparse solutions for W , especially L 1 regularization, have gained attention because of their usefulness in ill-posed engineering problems ), elucidating neuro-biological phenomena, ),face recognition ( , and semi-supervised and transfer learning ( . sparse approximation by also learning the basis matrix B. Clustering can also be viewed as a restricted form of the coding matrix factorization problem ( ); a special case of coding where the basis vectors are the cluster centroids and the W matrix is the cluster membership of each example. Recently showed that the clustering problem can be made convex by considering a fixed set of possible cluster centroids. Exemplars are a natural choice for these candidate cluster centroids, but show that for some problems this can be overly restrictive, and better results can be achieved by defining the problem in terms of a convex \"master\" problem, and a subproblem where new centroid candidates are generated.",
               "Applying the Maximum A Posteriori (MAP) approximation replaces the integration over W and B in (1) with its maximum value P (X|?B?WX|? X|?BX|?B? X|?B?W )P ( ? W )P ( ? B), where the values of the latent variables at the maximum, ? W and?Band? and?B, are referred to as the MAP estimates. Finding?W Finding? Finding?W given B is an approximation problem; solving for?W for? for?W and?Band? and?B simultaneously over a set of examples is a coding problem.",
               "We extend this \"convex clustering\" approach to the coding setting. Starting from a convex but intractable version of sparse coding in Section 2, we derive in Section 3 a boosting-style approach that is convex except for a subproblem. In Section 4 we give an efficient algorithm for solving the subproblem that will only improve on a fully convex exemplar-based approach, which is demonstrated on an image denoising task in Section 5. A similar convex formulation of sparse coding was independently developed by , who present an interesting optimization approach based on convex relaxations. Our work complements theirs by providing a novel optimization strategy applicable to all convex regularization functions.",
               "We will focus on the case examined in ( where the input X is assumed to be the matrix product BW corrupted by additive i.i.d Gaussian noise on each element, the prior P (W ) is assumed to be Laplacian with mean zero, and the the columns of the basis matrix are constrained to unit length. For numerical stability we minimize the negative log probability instead of maximizing :"
          ],
          "paper_id": "21d50670-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 1,
          "fromPaper": "Convex Coding"
     },
     {
          "head": {
               "n": "2",
               "text": "Sparse Coding",
               "type": "introduction"
          },
          "paragraphs": [
               "The L 1 -norm 1 , is a common choice for the regularization function(W ) because it tends to producW which are \"sparse\"-contain a small number of non-zero elements-even when the basis matrix has infinitely many columns ( ). This preference has been shown to be useful for applications such as prediction ( ) and denoising of images and video ( )."
          ],
          "paper_id": "21d50670-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 2,
          "fromPaper": "Convex Coding"
     },
     {
          "head": {
               "n": "2.3",
               "text": "Convex Relaxation",
               "type": "introduction"
          },
          "paragraphs": [
               "Uppercase letters, X, denote matrices and lowercase letters, x, denote vectors. For matrices, superscripts and subscripts denote rows and columns respectively. X j is the jth column of X, X i is the ith row of X, and X i j is the element in the ith row and jth column. Elements of vectors are indicated by subscripts, x j . X T This formulation, and similar variants, is commonly solved by alternating between optimization over W and optimization over B, as both problems are convex when the other matrix is constant. However, a common objection to this approach is that the joint optimization problem is non-convex.",
               "is the transpose of matrix X. The Fenchel conjugate of a function f is denoted by f * . The notation (x) + means the larger of 0 or x."
          ],
          "paper_id": "21d50670-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 3,
          "fromPaper": "Convex Coding"
     },
     {
          "head": {
               "n": "2.2",
               "text": "Generative Model",
               "type": "modelling"
          },
          "paragraphs": [
               "From a probabilistic viewpoint, sparse coding fits a generative model (1) to unlabeled data, which factorizes a matrix of input examples, Xmn , in terms of latent variable matrices Bmd and Wdn . The matrix B is referred to as the basis, code, or dictionary, and is shared across all n examples (columns of X). Given B the examples are assumed to be independent of each other. The matrix W is commonly referred to as the coefficients or the activations of the basis vectors.",
               "As noted by ) for the related problem of learning neural networks, the non-convexity can be removed if B is a fixed, infinite basis matrix containing all unit-length vectors as columns, and the optimization is only with respect to W . They go on to show that if L 1 regularization is placed on W , it will have optimal solutions with only a finite number of non-zero weights, even if the number of basis vectors is infinite. Hence, the matrix B in (2) can be interpreted as the small set of basis vectors that have non-zero weight in W , and the fixed number of columns of B can be written as a compositional norm constraint on W. The Lp norm of a vector x is: = `P i |x|"
          ],
          "paper_id": "21d50670-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 4,
          "fromPaper": "Convex Coding"
     },
     {
          "head": {
               "n": "2.3.1",
               "text": "Compositional Norms",
               "type": "modelling"
          },
          "paragraphs": [
               "A compositional norm is a norm composed of norms 2 over disjoint sets of variables . A useful and notationally convenient example of a compositional norm is a block norm . Define a block norm of the matrix W , p,q to be the L q norm of the L p norms of every row W i :",
               "set from the full basis (by encouraging multiple examples to share the same bases) to form the coded representation of the input. However, by exploiting properties of the L 2 2,1 +L 2 1 regularization function it is also possible to handle an infinitely large B. We show how to solve (4) with an efficient boosting algorithm (Algorithm 1) that adds one new basis vector to the active basis matrix in each step.",
               "Since in our setting W is defined so that each row corresponds to a basis vector and each column corresponds to an example, a block norm can encourage all examples to use a subset of the basis vectors. For instance, the fixed size of B in is equivalent to a hard constraint on W in terms of the non-convex L 2,0 block semi-norm. L 2,0 is the L 0 semi-norm 4 of the L 2 norm of each row of W , which counts how many basis vectors have non-zero entries in W . This approach is motivated by the view of boosting as functional gradient descent in the space of weak learners ( ). In our case, each weak learner is a vector with unit length. Each boosting step, attempts to maximize the correlation between the negative loss gradient and a \"small\" change in W , as measured by a regularization function(W ). For infinite sets of potential basis vectors, the maximization at each step of this boosting approach is a non-convex sub-problem which must be solved (or approximated) by an oracle (Algorithm 2)."
          ],
          "paper_id": "21d50670-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 5,
          "fromPaper": "Convex Coding"
     },
     {
          "head": {
               "n": "3.1",
               "text": "Fenchel Conjugate",
               "type": "modelling"
          },
          "paragraphs": [
               "Our convex coding formulation relaxes this non-convex L 2,0 constraint by substituting regularization with the convex (but still sparse) L 2,1 constraint:",
               "A useful tool in our analysis will be the the FenchelLegendre conjugate , also known as the conjugate function ), which generalizes Legendre duality to include non-differentiable functions:",
               "The fact that the norms are squared in (4) will be mathematically convenient later, and is equivalent to scaling the regularization constant 5 .",
               "The L 2,1 block norm has been advocated recently as a regularization function for multi-task learning ( ), and the combination of the L 2,1 block norm with the L 1 norm was independently used for sparse coding by ( , although presented quite differently in terms of decomposition norms. Their work provides an interesting alternative framework and optimization strategy to the boosting approach presented here.",
               "is the conjugate of the function f (x), and the variable z is the dual variable of x. When the supremum in (5) is achieved, every maximal valux of (5) is a subgradient 6 with respect to z of the conjugate function f * (z):"
          ],
          "paper_id": "21d50670-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 6,
          "fromPaper": "Convex Coding"
     },
     {
          "head": {
               "n": "3",
               "text": "A Boosting Approach to Coding",
               "type": "modelling"
          },
          "paragraphs": [
               "With a finite basis matrix B, (4) can be solved directly with various convex optimization techniques, including, e.g., subgradient descent. . The regularization term effectively \"selects\" a small active Each step of a gradient boosting style algorithm ) seeks a descent direction which provides the greatest reduction of loss for a small increase in the regularization function. If w is a vector of weights over all of the possible weak learners, we wish to find the step ? ? w that is both maximally correlated with the negative loss gradient ?L(w), and smaller than as measured by the regularization function(w): The component norms of a compositional norm can also be compositional norms, allowing heirarchical arrangements of three or more norms.",
               "3 It can be easily verified that (3) satisfies the definition of a norm. For 0p < 1, the Lp norm of a vector x is redefined as:",
               "(?w)p i",
               "5 At the minimum?Wminimum? minimum?W of ",
               "Consider the function f = max(x1, x2). If x1 > x2, then there is a unique gradient This section will show that if the regularization functionis convex, defined on d , and the constraint is strictly feasible 7 , every subgradient of its Fenchel conjugate evaluated on the loss gradient, ? ? w",
               ", is an optimal boosting step according to . This provides a useful method for constructing boosting algorithms for a wide class of regularization functions, and we will apply it to (4).",
               "First we upper bound by the minimum of the unconstrained Lagrange dual function, assuming the step size constraint is feasible (i.e. there exists a ?w such that(?w)"
          ],
          "paper_id": "21d50670-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 7,
          "fromPaper": "Convex Coding"
     },
     {
          "head": {
               "n": "4",
               "text": "Oracles for Infinite Bases",
               "type": "modelling"
          },
          "paragraphs": [
               "Lemma 2 The dual of the L p,q block norm is a L p * ,q * block norm where",
               "In order to apply our boosting approach (Algorithm 1) to the sparse coding problem (4), we must compute a subgradient? Z* (Z) of , where the dual variable Z is the negative gradient of the loss:",
               "is generally very sparse, and we show that it equals:",
               "The convex exemplar-based approach to clustering formulated by can be applied to sparse coding to create a simple oracle from a set of exemplars. In this case, the finite set of exemplars is the set of possible basis vectors, and the subgradient (15) derived above can be found efficiently by solving the infimal convolution. However, this solution can is improved by Algorithm 2, which optimizes over an infinite set of possible basis vectors. This section defines the optimization problem that must be solved to boost from an infinite set of possible basis vectors. We start by considering the two limiting cases of L 2,1 +L 1 regularization, L 1 regularization and L 2,1 regularization. We then present Algorithm 2 as a heuristic for finding a good solution in the general case. ",
               "k wheris the magnitude of the largest element of the matrix | ? A| (derived below), which minimizes the infimal convolution in , andis equal to the maximal squared L 2 norm of any row in the matrix Z ? ? A. Deriving this subgradient requires analyzing some details of the infimal convolution, but is necessary for understanding the sub-problem involved in boosting from an infinite set of basis vectors.",
               ". In this case, a subgradient of* (Z) is a matrix with one non-zero element, corresponding to a maximal element of |Z|. Therefore, to find the best possible basis vector (with unit L 2 norm) we must solve for the basis vector b m that produces the largest element of Z. Since Z = ? The solution?Asolution? solution?A to the infimal convolution in is:",
               "l , we should be interested in the total derivative:",
               "derivative is equal to the partial derivative.",
               ", where:"
          ],
          "paper_id": "21d50670-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 8,
          "fromPaper": "Convex Coding"
     },
     {
          "head": {
               "n": "4.2",
               "text": "L 2,1 Regularization",
               "type": "modelling"
          },
          "paragraphs": [
               "If instead we regularize with the L ",
               "5: Assign b to the best candidate: b = ? B m . 6: Improve b by gradient ascent using :",
               "Again E is the reconstruction error . Although is not convex, it is well known that b m is the eigenvector associated with the maximum eigenvalue of the matrix EE T ( ). Running Algorithm 1 with this choice ofis very related to PCA, and can be interpreted as PCA with incomplete deflation.",
               "The L 2,1 +L 1 regularization used in (4) interpolates between the behavior of L 1 and L 2,1 based on the value of. The optimal new basis vector b m will maximize the infimal convolution:",
               "Algorithm 2 provides an empirically effective method for estimatingand b m . It starts by combining the solutions for the L 2,1 and L 1 regularization cases discussed above into a matrix of candidate basis vectors?B vectors? vectors?B. Then a promising choice for b m is selected by finding the basis vector i? B which produces a maximal row of the infimal convolution over Z = ? ",
               "The introduction of the minimization overmakes (21) significantly more difficult to solve than the L 2,1 case (20). However, we can solve the infimal convolution for a finite set of candidate basis vectors, and we have already seen the solution for the limiting cases of0 and. To review: as0, ?will converge to 0, and b m will be equal to the solution of the L 2,1 only case . When, ?converges to max j |b",
               "where:"
          ],
          "paper_id": "21d50670-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 9,
          "fromPaper": "Convex Coding"
     },
     {
          "head": {
               "n": "5",
               "text": "Results on Image Denoising",
               "type": "modelling"
          },
          "paragraphs": [
               "T m E j |, and L 2,1 +L 1 regularization reduces to L 1 regularization. In this case 10 , b mE j , where j 2 = max i i 2 , i.e. b m is guaranteed to be proportional to some column of the reconstruction error E. The L2 projection of any column of E with maximal L2 norm is a valid choice for bm We apply boosted coding (Algorithm 1) to the task of image denoising in order to evaluate its performance on a real-world task that is well-suited to sparse coding ), and lends itself particularly well to visualizing the behavior of the algorithm. The performance of alternating optimization and boosted coding turn out to be quite similar on this task, with a slight advantage for the boosted approach. This result provides reassuring evidence that the non-convex but simple alternating optimization algorithm is not seriously impaired by inferior local minima on this task. Additional experimental details and results are given in the accompanying tech report to this paper .",
               "of the image are contained in each patch. If W is set to zero, the result is to average each 8x8 patch of the image, which improves the Signal-to-Noise Ratio (SNR) of the low-frequency components of the image at the expense of the high-frequency details in the image (  : Results on five benchmark images show that both alternating optimization (Alt. Opt.) and boosted coding (B.C.) produce similar results for image denoising, and improve significantly on patch averaging (P.A.). The range reported for alternating optimization is the best and worst performance from 20 randomly initialized trials. Note that boosted coding is deterministic. Our approach is modeled on the K-SVD algorithm presented in ). As shown in , overlapping patches are extracted from a noisy input image. Each patch is rearranged into a vector x i , the mean ? x i of the patch is subtracted, and the result becomes a column of the data matrix X. X is factorized into the product of B and W using sparse coding. Non-zero components of W are then refit without regularization. Finally, the denoised image is reconstructed by adding back the mean of each patch, and averaging areas of the image where multiple patches overlap. For these experiments we used 8x8 pixel patches with an overlap of four pixels between neighboring patches. The alternating optimization approach has two hyperparameters-the regularization constantand d, the number of columns of B-and boosted coding has two regularization constant hyperparametersand. The hyper-parameters of both algorithms were independently tuned for maximal performance, in order to isolate the effect of the different optimization strategies.",
               "Coding X with alternating optimization or boosted coding , right side) restores high-frequency detail by finding basis vectors that describe shared patterns across all patches. In our experiments both algorithms produce roughly equivalent results in terms of SNR, with a slight advantage for the convex approach."
          ],
          "paper_id": "21d50670-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 10,
          "fromPaper": "Convex Coding"
     },
     {
          "head": {
               "n": "5.1",
               "text": "Boosted Coding",
               "type": "modelling"
          },
          "paragraphs": [
               "The effect of relaxing the non-convex rank constraint on W by substituting L 2,1 regularization changes the basis vectors selected. Boosted coding selects the most important basis vectors first, and those are used by many image patches due to the \"group discount\" provided by the L 2 norm in L 2,1 regularization ( ). This causes the signal to noise ratio to rise quickly at the beginning of the process and then level off once most of the underlying signal can be represented by the basis vectors."
          ],
          "paper_id": "21d50670-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 11,
          "fromPaper": "Convex Coding"
     },
     {
          "head": {
               "text": "Input Image",
               "type": "modelling"
          },
          "paragraphs": [
               "Alternating Opt. Boosted Coding Independent and identically distributed Gaussian noise (= 0.1) was added to five common benchmark images to match the assumed noise model of the L 2 loss function , left-most column). Subtracting the mean of each patch removes low-frequency components of the image, leaving the coding problem to focus on identifying which high-frequency components Basis vectors learned by boosted coding, displayed in the order they were selected (top to bottom and left to right).",
               "The first basis vectors chosen are smoother in appearance than basis vectors chosen at later steps. This is because each step of boosting with L 2,1 +L 1 regularization will find a basis vector that is maximally correlated with the reconstruction error on a subset of the image patches. In later rounds of boosting much of the structure of the image patches is already explained, and the reconstruction error on each patch consists largely of noise. Additionally, the basis selected by boosted coding is less coherent (i.e. the basis vectors are less correlated with each other) than the basis selected by alternating optimization (details in )."
          ],
          "paper_id": "21d50670-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 12,
          "fromPaper": "Convex Coding"
     },
     {
          "head": {
               "n": "5.2",
               "text": "Alternating Optimization",
               "type": "modelling"
          },
          "paragraphs": [
               "A common objection to the traditional, alternating optimization approach to sparse coding is that the non-convex rank constraint on B could result in the algorithm returning inferior local minima. Anecdotal evidence suggests this problem should be most acute for relatively small basis sizes, where there are only a small number of randomly-initialized basis vectors. In this case there are fewer degrees of freedom available to let alternating optimization escape a local minima. In our experience, inferior local minima, while they do occur, have a relatively small effect on the performance of the alternating optimization algorithm. quantifies this assertion by showing the results of repeatedly running the alternating optimization image denoising algorithm from different random initializations of the basis vectors. The optimization alternated between B and W 20 times. This represents a stress-test for the alternating optimization algorithm as the basis contains only eight basis vectors. Even in this challenging case alternating optimization performs reasonably consistently, although provides a detailed look at a case where one local minima was superior to the others. 6.710.1dB House 6.20 6.25 6.220.02dB : Signal-to-Noise ratio variance observed when using alternating optimization L1-regularized sparse coding to denoise grayscale images from 20 different random initializations of B."
          ],
          "paper_id": "21d50670-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 13,
          "fromPaper": "Convex Coding"
     }
]