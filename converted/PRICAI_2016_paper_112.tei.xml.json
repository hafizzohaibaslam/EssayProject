[
     {
          "head": {
               "n": "0",
               "text": "abstract",
               "type": "abstract"
          },
          "paragraphs": [
               "This paper investigates how norm emergence can be facilitated by agents' adaptive learning behaviors in networked multiagent systems. A general learning framework is proposed, in which agents can dynamically adapt their learning behaviors through social learning of their individual learning experience. Extensive verification of the proposed framework is conducted in a variety of situations, using comprehensive evaluation criteria of efficiency, effectiveness and efficacy. Experimental results show that the adaptive learning framework is robust and efficient for evolving stable norms among agents."
          ],
          "paper_id": "23806c80-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 0,
          "fromPaper": "Adaptive Learning for Efficient Emergence of Social Norms in Networked Multiagent Systems"
     },
     {
          "head": {
               "n": "1",
               "text": "Introduction",
               "type": "introduction"
          },
          "paragraphs": [
               "Coordination of agent behaviors is central in Multiagent Systems (MASs). Social norm is an effective technique to achieve coordination in MASs by placing social constraints on agent action choices . Understanding how social norms can emerge through local interactions has gained increasingly high attention in the research of MASs. Numerous investigations of norm emergence have been done in recent years under different assumptions about agent interaction protocols, societal topologies and observation capabilities .",
               "Learning from individual experience has been shown to be a robust mechanism to enable norm emergence in MASs . A great deal of work has studied norm emergence achieved through agent learning behaviors . The focus of these existing studies is to examine general mechanisms behind efficient emergence of social norms while agents interact with each other using basic learning (particularly reinforcement learning) methods. These mechanisms include the social learning strategy , the collective interaction protocol , the utilization of topological knowledge , and the observation capability of agents , etc. Although these studies provide us with a deep understanding of efficient mechanisms of norm emergence, they share the same limitation inevitably. That is, learning parameters in these studies are often fine-tuned by hand and thus cannot be adapted according to the varying norm emerging situations. A key question then arises that how agents can adapt their learning behaviors dynamically during the process of norm emergence, and how this kind of adaptiveness can influence the final emergence performance?",
               "To this end, this paper provides another perspective in the research of norm emergence by simply focusing on the role of learning itself in affecting the process of norm emergence. A double-layered adaptive learning framework is proposed, in which agents interact with each other using basic Reinforcement Learning (RL) methods in the local layer learning, and generate supervision policies by exploiting historical learning experience in the upper layer learning. The supervision policies are then passed down to the local layer learning in order to adapt agents' learning behaviors based on the consistency between agents' behaviors and the supervision policies. In the framework, two challenging technical issues are needed to be carefully resolved: (1) how to generate supervision policies simply based on agents' historical learning experience? and (2) how to adapt agents' local learning behaviors based on the supervision policies from the upper layer learning? To solve the former problem, the historical learning experience of each agent is synthesized into a strategy that competes with other strategies in the population. The strategies that have better performance are more likely to survive and thus be accepted by other agents. The competing process of the agents' strategies then can be carried out through a social learning process based on the principle of Evolutionary Game Theory (EGT) . For the latter, the concept of \"winning\" or \"losing\" in the well-known Multi-Agent Learning (MAL) algorithm WoLF (Win-or-Learn-Fast) is elegantly borrowed to indicate whether an agent's behavior is consistent with the supervision policy. According to the \"winning\" or \"losing\" situation, agents then can dynamically adapt their learning behaviors in local layer learning. Experiments show that the proposed framework enables norms to emerge more efficiently and with higher convergence levels than the static learning framework, and some critical factors such as size of norm space and network topologies can have significant influences on norm emergence.",
               "The remainder of the paper is organized as follows. Section 2 briefly introduces social norms and RL. Section 3 describes the proposed framework. Section 4 presents experimental studies. Finally, Section 5 concludes the paper."
          ],
          "paper_id": "23806c80-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 1,
          "fromPaper": "Adaptive Learning for Efficient Emergence of Social Norms in Networked Multiagent Systems"
     },
     {
          "head": {
               "n": "2",
               "text": "Social Norms and RL",
               "type": "introduction"
          },
          "paragraphs": [
               "As most previous studies do, this paper also uses learning \"rules of the road\" as a metaphor to study emergence of norms, which can be viewed as a Coordination Game (CG) in . The abstraction of coordination given by the CG covers a number of practical scenarios, such as distributed robots coordinating on which object to work on together, and wireless nodes coordinating on which channel to transmit message . The problem to deal with the CG is that there is nothing in the structure of the game itself that allows players (even purely rational players) to infer what they ought to do . Therefore, social norms can be used to guide agent behaviors towards specific ones when moral or rational reasoning does not provide a clear guidance of how to behave. To study the impact of norm space on norm emergence, the CG can be extended to a general form by considering actions in the norm space . ",
               "In this paper, agent interactions are purely local and are constrained by the network structures. Three different kinds of topologies are considered: grid networks, small-world networks and scale-free networks .",
               "RL algorithms has been widely used for agent interactions in previous work on norm emergence, and Q-learning is the most adopted one. In Qlearning, an agent makes a decision through the estimation of a set of Q-values, which can be updated by:",
               "where(0, 1] is a learning rate of agent and[0, 1) is a discount factor, and are the immediate and expected reward of choosing action in state at time step respectively, and Q(s, a) is the expected discounted reward of choosing actionin stateat time step + 1. Q-values of each state-action pair are stored in a table for a discrete stateaction space. At each time step, agent chooses the best-response action with the highest Q-value with a probability of 1 ? (i.e., exploitation), or chooses other actions randomly with a probability of (i.e., exploration)."
          ],
          "paper_id": "23806c80-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 2,
          "fromPaper": "Adaptive Learning for Efficient Emergence of Social Norms in Networked Multiagent Systems"
     },
     {
          "head": {
               "n": "3",
               "text": "The Proposed Learning Framework",
               "type": "modelling"
          },
          "paragraphs": [
               "In the traditional RL framework, the critical parameters such as learning rate and exploration rate are usually set to fixed values or changed according to a fixed schedule (e.g., making the agent explore less as time goes by). In the context of learning for norm emergence, as all the agents are adapting their opinions at the same time, the norm emerging process can be quite dynamic and unpredictable. Therefore, agents should learn in an adaptive way in order to properly react to the varying norm emerging situations, rather than simply follow a fixed learning schedule.",
               "To better reflect the nature of adaptive learning behaviors during norm emergence, a general adaptive RL framework is proposed, which extends the traditional RL framework by equipping an agent with a double-layered learning ability. In the framework, the local layer learning represents an individual learning process, in which the agent interacts with the environment using traditional RL algorithms. The upper layer learning represents a social learning process, in which the agent synthesizes its learning experience from the local layer learning and generates a supervision policy through exchanging its learning experience with other agents. The supervision policy is then utilized to guide the local layer learning process by tuning the critical learning parameters heuristically."
          ],
          "paper_id": "23806c80-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 3,
          "fromPaper": "Adaptive Learning for Efficient Emergence of Social Norms in Networked Multiagent Systems"
     },
     {
          "head": {
               "n": "3.1",
               "text": "Generating Supervision Policies",
               "type": "modelling"
          },
          "paragraphs": [
               "In the proposed learning framework, each agent is equipped with a capability to memorize a certain period of learning experience in terms of the chosen action and the corresponding reward. Assuming a memory capability of agent is well justified and quite common in the MAS research . Formally, let denote an agent's memory length. At step the agent can memorize the historical information in the period of steps prior to A memory table of agent at time step , then can be denoted as ",
               "where ) is 1 if = and 0 otherwise.",
               "( stores the historical information of how often action has been chosen in the past. To exclude those actions that have never been chosen, set ) is defined to contain all the actions that have been taken at least once in the last steps by agent i.e., ) = { ( > 0}. The average reward of choosing action ( then can be given by:",
               ") is if = and 0 otherwise. The past learning experience in terms of table ( and ( indicates how successful the strategy of choosing action is in the past. The upper layer learning makes use of this information in order to generate a supervision policy for local layer learning. To realize the supervision policy generation, each agent learns from other agents by comparing their learning experience. The motivation of this comparison comes from the EGT, which provides a powerful methodology to model how strategies evolve overtime based on their performance. In the context of EGT, an individual's payoff represents its fitness or social success. The dynamics of strategy change in a population is governed by social learning, that is, the most successful agents will tend to be imitated by the others. Two different approaches are proposed in this paper to realize the EGT concept in the upper layer learning process, depending on how to define the competing strategy and the corresponding performance evaluation criteria (i.e., fitness) in EGT.",
               "Reward-based approach: This approach is a performance-driven approach in that agents are aiming at maximizing their own rewards. If an action has brought about the highest reward among all the actions in the past, this action is the most profitable one and thus should be more likely to be imitated by the others in the population. Therefore, the strategy in EGT is represented by the most profitable action, and the fitness is represented by the corresponding reward of that action. More formally, letdenote the most profitable action. It can be given by Equation 4:",
               "Action-based approach: Unlike the reward-based approach, the action-based approach is a behavior-driven approach. If an agent has chosen the same action all the time, it considers this action to be the most successful one (being the norm accepted by the population). Therefore, action-based approach considers the action which has been most adopted in the past to be the strategy in EGT, and the corresponding reward of that action to be the fitness in EGT. Letdenote the most adopted action. It can be given by:",
               "After synthesizing the historical learning experience, agent then gets a strategyand its corresponding fitness of ). It then interacts with other agents through social learning based on the pairwise comparison rule, i.e., learning from a randomly selected neighbor in the neighborhood. Imitation rules in EGT then model how the strategies of these two agents evolve overtime based on their performance, which can be realized by the famous Fermi function:",
               "where denotes the probability that agent switches to the strategy of agent and is the selection bias."
          ],
          "paper_id": "23806c80-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 4,
          "fromPaper": "Adaptive Learning for Efficient Emergence of Social Norms in Networked Multiagent Systems"
     },
     {
          "head": {
               "n": "3.2",
               "text": "Adapting Local Learning Behaviors",
               "type": "modelling"
          },
          "paragraphs": [
               "Based on the principle of EGT, the upper layer learning process generates a supervision policy represented as the new strategy",
               "The new strategy indicates the most successful strategy in the neighborhood and therefore should be integrated into the local layer learning in order to entrench its influence. By comparing its action at time step, with the supervision strategy , agent can evaluate whether it is performing well or not so that its learning behavior can be dynamically adapted to fit the supervision strategy. Depending on the consistency between the agent's behavior and the supervision policy, the local layer learning can be adapted based on the following mechanisms:",
               "Supervision-In RL, the learning performance heavily depends on the learning rate parameter, which is difficult to tune. This mechanism adapts the learning rate in the local layer learning. When agent has chosen the same action with the supervision policy, it decreases its learning rate to maintain its current state, otherwise, it increases its learning rate to learn faster from its interaction experience. Formally, learning rate can be adjusted as follows:",
               "where is the changing rate of and can be given by:",
               "where[0, 1] is a variable to control the adaption rate. Supervision-Exploration-exploitation trade-off has a crucial impact on the learning process. Therefore, this mechanism adapts the exploration rate in the local layer learning. The motivation of this mechanism is that an agent needs to explore more of the environment when it is performing poorly and explore less otherwise. Similarly, the exploration rate can be adjusted according to Equation 9:",
               "in which is if",
               ", and ? ) otherwise. In RL, exploration rate is always set to a small value in order to indicate a small probability of exploration. Thus, a variable is introduced to confine the exploration rate to a maximal value of . Supervision-both: This mechanism adapts the learning rate and the exploration rate at the same time based on Supervision-and Supervision-",
               "The above mechanisms are based on the concept of \"winning\" and \"losing\" in the well-known MAL algorithm WoLF. Although the original meaning of \"winning\" or \"losing\" in WoLF and its variants is to indicate whether an agent is doing better or worse than its Nash-Equilibrium policy, this heuristic is gracefully introduced into the proposed framework to evaluate an agent's performance against the supervision policy. Specifically, an agent is considered to be winning (i.e., performing well) if its action is the same with the supervision strategy and losing (i.e., performing poorly) otherwise. The different situations of \"winning\" or \"losing\" thus indicate whether the agent's behavior is complying with the norm in the society. If an agent is in a losing state (i.e., its action is against the norm in the society), it needs to learn faster or explores more of the environment in order to escape from this adverse situation. On the contrary, it should decrease its learning and/or exploration rate to stay in the winning state."
          ],
          "paper_id": "23806c80-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 5,
          "fromPaper": "Adaptive Learning for Efficient Emergence of Social Norms in Networked Multiagent Systems"
     },
     {
          "head": {
               "n": "4",
               "text": "Experiments and Results",
               "type": "experiment"
          },
          "paragraphs": [
               "Experiments are carried out to demonstrate the performance of the proposed framework, based on metrics of effectiveness (i.e., possibility of convergence), efficiency (i.e., speed of convergence) and efficacy (i.e., level of convergence). Effectiveness indicates the percentage of runs in which a norm can successfully emerge, efficiency represents how many steps are needed for a norm to emerge, and efficacy denotes the ratio of agents in the system that can achieve the norm emergence. The Watts-Strogatz model is used to generate a small-world network and the Barabasi-Albert model is used to generate a scale-free network. Stateless version of Q-learning algorithm is used, thus = 0 in Equation 1. Unless specified otherwise, the rewiring probability in Watts-Strogatz model is set to 0.1 and average number of neighbors in the small-world network is set to 12; learning rate is set to 0.1 initially, exploration rate is set to 0.01 initially and is 0.3 by default; each agent can choose from 4 actions and have a moderate memory length of 4 steps; moreover, the parameter and are both set to 0.1. -1(c) plots the learning curves of the three adaptive learning approaches under the proposed framework, and the static learning approach under the social learning framework . In the social learning framework, each agent interacts with one of its neighbors and learns directly from this interaction. Comparing with this approach thus enables to demonstrate the merits of agents' adaptive learning behaviors in influencing the norm emerging process. The results show that the three adaptive learning approaches outperform the static social learning approach in all three networks in terms of a higher level of convergence and a faster convergence speed. Through dynamically adapting their learning behaviors during the norm emerging process, agents are able to reach an agreement more easily, and thus norms can emerge more quickly to a higher level of convergence in the adaptive learning framework. The distinction of performance between the adaptive learning and the social learning approaches is less apparent in the scale-free network because this kind of network is easier to emerge a norm compared with other networks. This is due to the small graph diameter of scale-free networks, as reported in various previous studies . shows the performance of the two different kinds of approaches adopted in the upper layer learning. As can be seen, norms emerge faster with the action-based approach at the beginning. As the process proceeds, the reward-based approach catches up with the action-based approach, and then brings about a higher level of norm emergence afterwards. This result implies that it is more reasonable to use the most profitable action rather than the most adopted action in the past as the competing strategy in EGT.  summarizes the final performance of the different approaches in 1000 independent runs. In order to better demonstrate the different performance of these approaches, we also include the results when 100% agents have chosen the same action as the norm. Achieving 100% norm emergence is an extremely challenging issue due to the widely recognized existence of subnorms . Clearly, the proposed learning approaches outperform the social learning approach in all aspect of comparison. For example, in the grid network, social learning can only enable averagely 86.1% agents in the population to achieve a consensus over the social norm. This performance is upgraded to as high as 92.2%, 91.9% and 95.7% using the three proposed approaches, respectively. Regarding effectiveness, the possibility that a norm can successfully emerge using social learning is quite low (i.e., 55.0% for 90% convergence, and 46.6% for 100% convergence). The adaptive learning approaches, however, can greatly increase the possibility of norm emergence (e.g., 86.7% for 90% and 100% convergence using supervision-both). As for efficiency, it takes averagely 4288 steps for 100% convergence using social learning approach, against 4113, 1180 and 1029 steps using the three adaptive learning approaches, respectively. To sum up, the adaptive learning approaches can promote the emergence of social norms to higher levels of convergence with fewer steps."
          ],
          "paper_id": "23806c80-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 6,
          "fromPaper": "Adaptive Learning for Efficient Emergence of Social Norms in Networked Multiagent Systems"
     },
     {
          "head": {
               "text": "Emergence of social norms",
               "type": "experiment"
          },
          "paragraphs": [
               "Learning dynamics of and To have a better understanding of the learning dynamics under the proposed framework, it is necessary to see how the critical learning parameters of learning rate and exploration rate evolve during the norm emerging process. The dynamics of and with different sizes of norm space are shown in . In both action sizes, the values of and increase sharply at the beginning, and then drop to nearly zero gradually. This is because the whole agent system is still in chaos at the beginning of the norm emerging process and it is more likely that the stochastic learning process cause the agents to be in a \"losing\" state. Therefore, agents increase their learning rate and/or exploration rate to learn faster and/or explore more, so as to get over the \"losing\" state. As the norm is emerging, the action choice is more and more consistent with supervision policy. Thus, and decrease accordingly to indicate a \"winning\" state of the agents. Comparing with (b), we can also see that, in 10-action scenario, the values change more dramatically at first and then it takes a longer time for these values to decrease to zero. This is because agents are more likely to choose the same action as the norm with a smaller action size. When the norm space gets larger, the probability to find the right action as the norm is greatly reduced. The large number of conflicts among the agents thus causes the agents to be in a \"losing\" state more often in a large norm space, and thus the norm emerging process is greatly prolonged.",
               "Influence of norm space and population size The influence of norm space on norm emergence is given by . We can see that a larger number of available actions results in a delayed convergence of norms. This is because a larger number of actions are more likely to produce local sub-norms, leading to diversity across the society. It thus takes a longer time for the agents to eliminate this diversity and achieve a final consensus. In all cases, the adaptive learning approach performs better than the social learning approach in terms of a faster convergence speed and a higher convergence level. This result shows that the proposed adaptive learning framework is indeed effective for norm emergence in a large norm space. The influence of population size on norm emergence is shown in . In both learning approaches, the norm emergence process is hindered as the population is growing larger. This result occurs because the larger the society, the more difficult to diffuse the effect of local learning to the whole society. This phenomenon can be observed in human societies where small groups can more easily establish social norms than larger groups . The proposed adaptive learning approach, however, can greatly facilitate norm emergence in different population sizes. In cases of 100, 500 and 1000 population size, the adaptive learning approach can achieve almost 100% convergence, which is a great promotion from the low convergence levels using the static social learning approach. In a population of 5000 agents, the norm emerging process is steadily facilitated to a level of 90% during 10000 steps using the adaptive learning approach, against a convergence level close to 70% using the social learning approach. summarizes the performance of 100% norm emergence with various network topologies in small-world networks. The  rewiring possibility indicates different levels of network randomness. The results show that it is more efficient for a norm to emerge in a network with higher randomness. This is because the increase in randomness can reduce the network diameter (i.e., the largest number of hops in order to traverse from one vertex to another ), and the smaller a network diameter, the more efficient for the network to evolve a social norm . The results also show that a minor increase of the rewiring possibility (from 0 to 0.1, especially from 0.01 to 0.1) can bring about significant improvement of norm emergence, while further increasing the rewiring possibility (from 0.2 to 1.0) cannot cause a further significant improvement. This is due to the fact that the network randomness is already quite high when the rewiring possibility is in-between [0.01, 0.1]. In all scenarios, the proposed learning approaches outperform the social learning approach in all three comparison criteria. Specially, when the randomness is high, approach supervision-and supervision-both can achieve norm convergence with 100% possibility. This robust norm emergence, however, only takes very short time (e.g., 117 and 112 steps for supervision-and supervision-both, respectively, againt 2984 steps for social learning, when = 1.0.)."
          ],
          "paper_id": "23806c80-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 7,
          "fromPaper": "Adaptive Learning for Efficient Emergence of Social Norms in Networked Multiagent Systems"
     },
     {
          "head": {
               "text": "Influence of network topology",
               "type": "experiment"
          },
          "paragraphs": [
               "As for neighborhood size, norm emergence is steadily promoted when the average number of neighbors is increased. This effect is due to the clustering coefficient of the network . When the average number of neighbors increases, the clustering coefficient also increases. Therefore, agents located in different parts of the network only need a smaller number of interactions to reach a consensus. On the other hand, when agents have a small neighborhood size, they only interact with their neighbors, which account for a small proportion of the whole population. This results in diverse sub-norms formed at different regions of the network. Such sub-norms conflict with each other in the network, and thus more interactions are needed to solve these conflicts and achieve a uniform norm for the whole society. In all cases of neighborhood sizes, the adaptive learning approaches can bring about more robust norm emergence with a faster convergence speed and a higher convergence level than the social learning approach."
          ],
          "paper_id": "23806c80-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 8,
          "fromPaper": "Adaptive Learning for Efficient Emergence of Social Norms in Networked Multiagent Systems"
     },
     {
          "head": {
               "n": "5",
               "text": "Conclusion",
               "type": "conclusion"
          },
          "paragraphs": [
               "In this paper, a novel learning framework was proposed to investigate how agents' adaptive learning behaviors can facilitate the process of norm emergence in network MASs. The highlight of the proposed model is the integration of social learning into the local individual learning in order to dynamically adapt agents' learning behaviors for a better performance of norm emergence. Our work thus bridges the gap between the two distinct research paradigms for learning of norm emergence by coupling a social learning process (through imitation in EGT) with a local individual learning process (i.e., RL). Although it can be expected that requiring communication among agents or additional information through social learning can facilitate norm emergence, this is not straightforward in the proposed model as the synthesized information used in social learning is generated from trail-and-error individual learning interactions, and this information is then utilized as a guide to heuristically adapt the local learning further. Tight coupling between these two learning processes can make the whole learning system rather dynamic. However, by synthesizing the individual learning experience into competing strategies in EGT and adapting local learning behaviors based on the principle of \"Win-or-Learn-Fast\", our work has illustrated that this kind of interplay between individual learning and social learning is indeed helpful in facilitating the emergence of social norms among agents."
          ],
          "paper_id": "23806c80-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 9,
          "fromPaper": "Adaptive Learning for Efficient Emergence of Social Norms in Networked Multiagent Systems"
     }
]