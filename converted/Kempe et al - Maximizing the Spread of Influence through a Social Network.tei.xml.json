[
     {
          "head": {
               "n": "0",
               "text": "abstract",
               "type": "abstract"
          },
          "paragraphs": [
               "Models for the processes by which ideas and influence propagate through a social network have been studied in a number of domains, including the diffusion of medical and technological innovations, the sudden and widespread adoption of various strategies in game-theoretic settings, and the effects of \"word of mouth\" in the promotion of new products. Motivated by the design of viral marketing strategies, Domingos and Richardson posed a fundamental algorithmic problem for such social network processes: if we can try to convince a subset of individuals to adopt a new product or innovation, and the goal is to trigger a large cascade of further adoptions, which set of individuals should we target? We consider this problem in several of the most widely studied models in social network analysis. The optimization problem of selecting the most influential nodes is NP-hard here. The two conference papers upon which this article is based (KDD 2003 and ICALP 2005) provide the first provable approximation guarantees for efficient algorithms. Using an The present article is an expanded version of two conference papers [51, 52], which appeared in KDD 2003 and ICALP 2005, respectively. * analysis framework based on submodular functions, we show that a natural greedy strategy obtains a solution that is provably within 63% of optimal for several classes of models; our framework suggests a general approach for reasoning about the performance guarantees of algorithms for these types of influence problems in social networks. We also provide computational experiments on large collaboration networks, showing that in addition to their provable guarantees, our approximation algorithms significantly out-perform node-selection heuristics based on the well-studied notions of degree centrality and distance centrality from the field of social networks. 1 Introduction A social network-the graph of relationships and interactions within a group of individuals-plays a fundamental role as a medium for the spread of information, ideas, and influence among its members. An idea or innovation will appear-for example, the use of cell phones among college students, the adoption of a new drug within the medical profession, or the rise of a political movement in an unstable society-and it can either die out quickly or make significant inroads into the population. If we want to understand the extent to which such ideas are adopted, it can be important to understand how the dynamics of adoption are likely to unfold within the underlying social network: the extent to which people are likely to be affected by decisions of their friends and colleagues, or the extent to which \"word-of-mouth\" effects will take hold. Such network diffusion processes have a long history of study in the social sciences. Some of the earliest systematic investigations focused on data pertaining to the adoption of medical and agricultural innovations in both developed and developing parts of the world [23, 69, 79]; in other contexts, research has investigated diffusion processes for \"word-of-mouth\" and \"viral marketing\" effects in the success of new products [8, 14, 26, 33, 32, 59, 68], the sudden and widespread adoption of various strategies in game-theoretic settings [11, 29, 61, 85, 86], and the problem of cascading failures in power systems [7, 6]. Motivated by applications to marketing, Domingos and Richardson posed a fundamental algorithmic problem for such systems [26, 68]. Suppose that we have data on a social network, with estimates for the extent to which individuals influence one another, and we would like to market a new product that we hope will be adopted by a large fraction of the network. The premise of viral marketing is that by initially targeting a few \"influential\" members of the network-say, giving them free samples of the product-we can trigger a cascade of influence by which friends will recommend the product to other friends, and many individuals will ultimately try it. But how should we choose the few key individuals to use for seeding this process? In [26, 68], this question was considered in a probabilistic model of interaction; heuristics were given for choosing customers with a large overall effect on the network, and methods were also developed to infer the influence data necessary for posing these types of problems. In this article, we consider the issue of choosing influential sets of individuals as a problem in discrete optimization. The optimal solution is NP-hard for most models that have been studied, including the model of [26]. The framework proposed in [68], on the other hand, is based on a simple linear model where the solution to the optimization problem can be obtained by solving a system of linear equations. Here we focus on a collection of related, NP-hard models that have been extensively studied in the social networks community, and obtain the first provable approximation guarantees for efficient THEORY"
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 0
     },
     {
          "head": {
               "n": "1.1",
               "text": "Our results",
               "type": "abstract"
          },
          "paragraphs": [
               "Our first main result is that the optimal solution for influence maximization can be efficiently approximated to within a factor of (1 ? 1/e ?). Theorem 1.1. In the Linear Threshold and Independent Cascade models (defined formally in Section 2), there is a polynomial-time algorithm approximating the maximum influence to within a factor of (1 ? 1/e ?), where e is the base of the natural logarithm andis any positive real number. This is a performance guarantee slightly better than 63%. The algorithm that achieves this performance guarantee is a natural greedy hill-climbing strategy, and so the main content of this result is the analysis framework needed for obtaining a provable performance guarantee, and the fairly surprising fact that hill-climbing is always within a factor of at least 63% of optimal for this problem. We prove this result in Section 4 using techniques from the theory of submodular functions . The key ingredient of our proofs is to exhibit, for both models, distributions over graphs with the following property: the expected activation(A) equals the expected number of nodes reachable from A if a graph G is chosen according to the distribution. We call this technique the triggering set technique.",
               "While the triggering set technique leads to approximation guarantees for several models studied in mathematical sociology, we show in Section 5 that the Decreasing Cascade model (a natural generalization THEORY OF COMPUTING, Volume 11 (4), of the Independent Cascade model, also defined in Section 2) leads to a submodular function(A), yet does not admit a proof using triggering sets. Instead, we present a more elaborate proof, based on a step-by-step analysis of the activation process.",
               "The analysis framework of submodular functions also allows us to design and prove guarantees for approximation algorithms in much richer and more realistic models of the processes by which we market to nodes. The deterministic activation of individual nodes is a highly simplified model; an issue also considered in is that we may in reality have a large number of different marketing actions available, each of which may influence nodes in different ways. The available budget can be divided arbitrarily between these actions. In Section 7, we show how to extend the analysis to this substantially more general framework. Our main result here is that a generalization of the hill-climbing algorithm still provides approximation guarantees arbitrarily close to (1 ? 1/e).",
               "Our theoretical results are complemented by an experimental evaluation of our algorithms. In Section 8, we report on the results of computational experiments with both the Linear Threshold and Independent Cascade models. These experiments show that in addition to its provable guarantees, the hill-climbing algorithm significantly out-performs strategies based on targeting nodes of high degree or distance centrality , i. e., the nodes with high degrees or small average distances to all other nodes."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 1
     },
     {
          "head": {
               "n": "1.2",
               "text": "Subsequent work",
               "type": "abstract"
          },
          "paragraphs": [
               "Since the original publication of the conference papers that this article is based on , there has been a large amount of follow-up work. We review some of the main directions and several representative papers for each; a complete review of subsequent work is beyond the scope of this article. Several papers addressing concrete questions directly related to the present discussions are also discussed in those contexts. An interested reader can find a more detailed survey in the recent monograph of Chen, Lakshmanan and Castillo .",
               "The paper by Mossel and Roch positively resolves a conjecture from , showing that for a broader class of local influence functions, the resulting objective function is submodular. (See the discussion in Section 2.1.2.) Their result implies that the (1 ? 1/e)-approximation guarantee for the greedy algorithm extends to a broader class of influence models. On the other hand, Even-Dar and Shapira showed that for the much more restricted Voter model , the objective function decomposes linearly across nodes, allowing an optimal algorithm or FPTAS, depending on the precise problem definition.",
               "A large number of papers have been written with the aim of reducing the running time for the influence maximization problem. These broadly fall into two categories: (1) heuristics which retain the (1 ? 1/e) approximation guarantee of the greedy algorithm while speeding up the computation in practice (without any worst-case running time improvements, however), and (2) heuristics which guarantee a faster running time, but sacrifice the (1 ? 1/e) approximation guarantee, either providing no guarantees or significantly weaker ones. We give a review of some papers representative of the directions pursued in this domain-a complete review of this literature is impossible due to its sheer volume.",
               "In the first category, the paper by proposed the CELF heuristic, based on a lazy evaluation of the objective function. The main insight is the following. If a node's marginal contribution to the objective function in the previous iteration of the greedy algorithm was already smaller than the current best node's, then it need not be reevaluated: by submodularity, its contribution in the current iteration can only be lower. proposed several additional heuristic optimizations to add to CELF, calling the resulting algorithm CELF++.",
               "In the second category, much of the focus is on speeding up the computation of the objective function via approximations. Several papers use approximations of the influence in terms of the influence only among simple local structures, such as shortest paths , local arborescences , local neighborhoods or local DAGs . Chen et al. propose reusing previous randomly drawn structures (and the computations on them), as well as a discounted high-degree heuristic. Jung et al. speed up computation of influence by setting up a recurrence relation between the influence of different nodes and linearizing it. Algorithms differing from the greedy addition of one node at a time have been investigated as well. Jiang et al. report significant speedups by using simulated annealing, while propose a pre-processing step breaking the graph into communities, which can then be treated practically separately.",
               "A heuristic which is in spirit similar to those of , but comes with provable guarantees, was recently proposed by Borgs et al. . In order to speed up the repeated computation of the influence of sets of nodes, they propose a preprocessing step which generates a random hypergraph sampled according to reverse reachability probabilities in the original graph. Subsequently, the greedy algorithm can be run to solve the maximum coverage problem on the sampled hypergraph, which leads to a near-linear running time of O((m + n)?3 log n) to achieve a 1 ? 1/e ?approximation. The (constant) correctness probability can be boosted with repeated invocations. also gives a quantitative tradeoff between faster (sublinear) running time and the deterioration of the approximation guarantee.",
               "In the present article (Section 7), we consider more general marketing strategies than simply selecting a seed set of vertices to activate. In general, a company intent on exploiting social network effects in marketing can combine such effects with differential pricing, as well as possibly offering products to individuals at specific times. This approach has been pursued by several recent papers .",
               "The first work along this line was by Hartline et al. . They show that the following \"Influence and Exploit\" strategies are within a factor 1/4 of optimal: first, give the product for free to a most influential subset; then, choose prices for the remaining bidders in random order to maximize revenue, ignoring their network effects. By showing that the most influential set in this sense can be approximated within a constant factor, they overall obtain a constant-factor approximation. A slightly modified model was studied by Arthur et al. : in their model, the seller cannot choose arbitrarily whom to offer the product to. Instead, the product spreads by recommendations, but the seller can decide on a price for each individual once a recommendation occurs. In this model, Arthur et al. prove competitive guarantees based on the type of buyer model. Akhlaghpour et al. study a Bayesian model wherein a price trajectory is offered to all buyers (whose values are private), and each individual buys myopically the first time his value for the item exceeds its current price. Under some restrictions, such as about possible price trajectories, they provide algorithms with provable guarantees. Haghpanah et al. consider the design not just of fixed prices, but general auction schemes, and provide a (Bayesian) auction over social networks which achieves a constant approximation to optimal revenue under restricted (all-or-nothing) valuations among the buyers.",
               "The related question of extracting truthfully from individuals how much incentivization they would require to become early adopters and recommenders was studied by Singer . He provides a truthful constant-factor approximation for the problem of choosing and paying individuals to start a cascade.",
               "THEORY OF COMPUTING, Volume 11 (4), 2015, pp. 105-147 A different generalization from pricing is suggested in a recent paper by Seeman and Singer . They base their model on the well-known observation that friends of random nodes tend to have higher degrees in expectation than the nodes themselves. Based on this observation, they suggest a two-stage process: first choose several nodes to invite; then, choose a seed set among their friends if they actually are available. They give a constant-factor approximation for this two-stage adaptive optimization process.",
               "The motivations for studying influence maximization also naturally suggest that there will frequently be competition between influences. This competition could occur between companies marketing similar products or between different political movements or ideologies seeking societal acceptance. Indeed, several papers have proposed and analyzed models for competition between multiple cascades.",
               "To our knowledge, Dubey et al. were the first to explicitly propose a model for competing cascades, extending the simple linear model of to competing cascades, and characterizing its equilibria. Subsequently, Carnes et al. and proposed extensions of the Independent Cascade model. The main challenge in extending diffusion models to multiple influences is deciding which of the influences will convince a node if multiple influences try at once. Carnes et al. propose tie breaking rules under which the best response of the last player remains submodular. Bharathi et al. side-step the issue by introducing a continuous timing component into diffusion, which guarantees that with probability 1, no two influences reach a node at the exact same time. The result is again submodularity of best responses; Bharathi et al. also prove that this makes the competitive cascade game a valid utility game , leading to a low Price of Anarchy.",
               "Borodin et al. show that such an extension is not as straightforward for the Linear Threshold model: most natural extensions to multiple influences do not preserve submodularity of the objective function. By adding a fixed sequence in which nodes are considered for state updates, and having them draw new independent thresholds each time, define a threshold-like model that retains many of the desirable properties; in particular, they achieve a Price of Anarchy of 4 for 2 players. He and Kempe subsequently showed that a Price of Anarchy of 2 for arbitrarily many players follows analogously to the result of Bharathi et al. , by reducing the game to a valid utility game . Alon et al. and focus on the existence of pure Nash Equilibria in different versions of the game, with 2 or Nplayers. Tzoumas et al. also consider the Price of Anarchy, but in their (deterministic) version of the game, it is unbounded in general, even for 2 players.",
               "Motivated by applications in which the competing cascade is considered \"harmful\" (such as a harmful rumor, computer virus, or dangerous conviction), several papers have considered the objective of minimizing the spread of a cascade, or maximizing the number of uninfluenced nodes. In this context, like in the previous one, the algorithmic questions studied include the efficient heuristic computation of equilibrium strategies as well as the optimization problem for a single first mover . A slightly different, but related situation arises when negative opinions about a product may emerge on their own as a result of the adoption of a product. The optimization problem under this variant of the model has been studied by Chen et al. ."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 2
     },
     {
          "head": {
               "n": "2",
               "text": "Models for the diffusion of an innovation",
               "type": "modelling"
          },
          "paragraphs": [
               "The social network is represented by a directed graph G, and we write uv to denote the existence of a directed edge from u to v. In considering operational models for the spread of an idea or innovation, we will speak of each individual node as being either active (an adopter of the innovation) or inactive. We will focus on settings, guided by the motivation discussed above, in which each node's tendency to become active increases monotonically as more of its neighbors become active.",
               "Also, we will first focus on the progressive case in which nodes can switch from being inactive to being active, but do not switch in the other direction. (In Section 6, we show how this assumption can be lifted.) Thus, the process will look roughly as follows from the perspective of an initially inactive node v: as time unfolds, more and more of v's neighbors become active; at some point, this may cause v to become active, and v's decision may in turn trigger further decisions by nodes to which v is connected. We let A t denote the set of nodes active at time t."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 3
     },
     {
          "head": {
               "n": "2.1",
               "text": "The Threshold model",
               "type": "modelling"
          },
          "paragraphs": [
               "Granovetter and Schelling were among the first to propose models that capture such a process; their approach was based on the use of node-specific thresholds . Many models of this flavor were subsequently investigated (see, e. g., ), but the following Linear Threshold model lies at the core of most generalizations. In this model, a node v is influenced by each incoming neighbor w according to a weight b v,w[0, 1]. Each node v has a thresholdv in the interval ; this represents the total weight which has to be exerted upon v by its active neighbors in order for v to become active. The implications of how these thresholds are chosen will be discussed momentarily.",
               "Given a choice of all nodes' thresholds, and an initial set of active nodes A 0 (with all other nodes inactive), the diffusion process unfolds deterministically in discrete steps: in step t, all nodes that were active in step t ? 1 remain active; furthermore, each currently inactive node v becomes active if and only if the total weight of its active neighbors is at leastv :",
               "wv,w active Thus, the thresholdsv intuitively represent the different latent tendencies of nodes to adopt the innovation when their neighbors do.",
               "There are three natural ways to model nodes' thresholds. They could be either hard-wired at a known value (such as 1/2), be part of the input to the optimization problem, or assumed to be random. Models with hard-wired thresholds were studied by Berger , Morris , and Peleg , for example. Unfortunately, we show in Section 3.2 that hard-wired thresholds make the optimization problem hard to approximate to within a multiplicative factor of n 1?for any> 0; the same hardness results naturally apply to the case in which the thresholds are part of the input. For this reason, and to model our lack of knowledge of the thresholds, we instead assume that the thresholdsv are chosen independently and uniformly at random from the interval ; thus, in effect, we average over all possible threshold values.",
               "Given the Linear Threshold model with random node thresholds, the influence maximization problem is now defined as in Section 1. Even with random node thresholds, we can prove that finding the best set A to target is NP-hard. Proof. Consider an instance of the NP-complete VERTEX COVER problem, defined by an undirected n-node graph G = (V, E) and an integer k; we want to know if there is a set S of k nodes in G so that every edge has at least one endpoint in S. We show that this can be viewed as a special case of the influence maximization problem.",
               "We define a corresponding instance of the influence maximization problem by directing all edges of G in both directions, assigning each directed edge e = (u, v) a weight of 1/ deg v. If there is a vertex cover S of size k in G, then one can deterministically make(A) = n by targeting the nodes in the set A = S, since each node is either in A or has all its neighbors in A. Conversely, this is the only way to get a set A with(A) = n; for if a pair of adjacent nodes u, v had neither node in A, then with random thresholds close enough to 1, neither node would become active."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 4
     },
     {
          "head": {
               "n": "2.1.1",
               "text": "A General Threshold model",
               "type": "modelling"
          },
          "paragraphs": [
               "The Linear Threshold model can be naturally generalized, by lifting the assumption that influences from individual nodes can only be aggregated linearly. In the general Threshold model, a node v's decision to become active can be based on an arbitrary monotone function of the set of neighbors of v that are already active. Each node v has associated with it a monotone threshold function f v mapping subsets of v's neighbor set to real numbers in , subject to the condition that f v ( / 0) = 0. The diffusion process follows the general structure of the Linear Threshold model. Each node v initially choosesv uniformly at random from the interval . Now, however, v becomes active in step t iff f v (S)v , where S is the set of neighbors of v that are active in step t ? 1. Note that the Linear Threshold model is the special case in which each threshold function has the form"
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 5
     },
     {
          "head": {
               "n": "2.1.2",
               "text": "The Submodular Threshold model",
               "type": "modelling"
          },
          "paragraphs": [
               "As we will see below, the influence maximization problem under the general Threshold model is highly intractable. A natural restriction is to require the local influence functions at nodes to have diminishing influence as the size of the influencing set increases. Diminishing influence of individual additions is naturally modeled by submodularity, which also plays a central role in our proofs of approximation guarantees. Recall that a function f is submodular if it satisfies the following property: the marginal gain from adding an element to a set S is at least as high as the marginal gain from adding the same element to a superset of S. Formally, a submodular function satisfies",
               "for all elements v and all pairs of sets S ? T . The Submodular Threshold model is obtained from the general Threshold model by requiring that each f v (S) be submodular. In the original version of this paper , we conjectured that the Submodular "
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 6
     },
     {
          "head": {
               "n": "2.2",
               "text": "The Cascade model",
               "type": "modelling"
          },
          "paragraphs": [
               "Based on work in interacting particle systems from probability theory, we can also consider dynamic cascade models for diffusion processes. The conceptually simplest model of this type is what one could call the Independent Cascade model, investigated in the context of marketing by . We again start with an initial set of active nodes A 0 , and the process unfolds in discrete steps according to the following randomized rule. When node v first becomes active in step t, it is given a single chance to activate each currently inactive neighbor w; it succeeds with a probability p v,w -a parameter of the system-independently of the history thus far. (If w has multiple newly activated neighbors, their attempts are sequenced in an arbitrary order.) If v succeeds, then w will become active in step t + 1; but whether or not v succeeds, it cannot make any further attempts to activate w in subsequent rounds. Again, the process runs until no more activations are possible.",
               "For the Independent Cascade model, too, the problem of finding the best set to target is NP-hard. In fact, for this model, we can even prove an approximation hardness result.",
               "Theorem 2.2. For the Independent Cascade model, it is NP-hard to approximate the most influential set to within better than a factor 1 ? 1/e.",
               "Proof. Consider an instance of the MAXIMUM COVERAGE problem, defined by a collection of subsets S 1 , S 2 , . . . , S m of a ground set U = {u 1 , u 2 , . . . , u n }; the goal is to select k of the subsets to maximize the size of their union. (We can assume that k < n < m.) We show that this can be viewed as a special case of the influence maximization problem.",
               "We define a corresponding directed bipartite graph with m + n 2 nodes: for each set S i , there is a corresponding node i, and for each element u j , there are n corresponding nodes j 1 , . . . , j n . Whenever u jS i , there is a directed edge (i, j ) with activation probability p i, j = 1 for all = 1, . . . , n.",
               "If X is a set of k of the subsets S i , and T ? U the union of the elements covered by the S iX, then choosing the k nodes corresponding to X will deterministically activate those nodes and the nodes corresponding to elements in T , for a total of k + n|T | active nodes.",
               "Next, we consider the converse direction, and an initially active set A. Without loss of generality, the set A contains only nodes i corresponding to sets S i ; otherwise, choosing any set covering element j would activate strictly more nodes. Now, let X be the set of sets S i corresponding to the nodes in A, and T ? U the set of elements covered by X. The number of activated nodes in the IC instance is k + n|T |.",
               "In summary, we have shown that in the MAXIMUM COVERAGE instance, r nodes can be covered by k sets if and only if k + nr nodes can be activated by a size-k seed set in the IC instance. As kn, we obtain (up to lower order terms) an approximation preserving reduction from MAXIMUM COVERAGE. Since MAXIMUM COVERAGE is known to be hard to approximate better than 1 ? 1/e , we obtain the same approximation hardness for influence maximization.",
               "THEORY OF COMPUTING, Volume 11 (4), 2015, pp. 105-147"
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 7
     },
     {
          "head": {
               "n": "2.2.1",
               "text": "A General Cascade model",
               "type": "modelling"
          },
          "paragraphs": [
               "The Independent Cascade model, too, can be naturally generalized, by allowing the probability that u succeeds in activating a neighbor v to depend on the set of v's neighbors that have already tried. Thus, we define an incremental function p v (u, S)[0, 1], where S is a subset of v's neighbors, and u /S. A general cascade process works by analogy with the independent case: when u attempts to activate v, it succeeds with probability p v (u, S), where S is the set of neighbors that have already tried (and failed) to activate v. The Independent Cascade model is the special case where p v (u, S) is a constant p u,v , independent of S. We will only be interested in cascade models defined by incremental functions that are order-independent in the following sense: if neighbors u 1 , u 2 , . . . , u try to activate v, then the probability that v is activated at the end of these attempts does not depend on the order in which the attempts are made. More formally, let S = {u 1 , . . . , u |S| }, and,two arbitrary permutations of {1, . . . , |S|}. Then, we require that",
               "Otherwise, even the definition of the outcome of the activation process would depend on the specific tie-breaking rules for multiple simultaneous activations."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 8
     },
     {
          "head": {
               "n": "2.2.2",
               "text": "The Decreasing Cascade model",
               "type": "modelling"
          },
          "paragraphs": [
               "Much like the general Threshold model, the general Cascade model makes influence maximization highly intractable-indeed, we will show in Section 3 that the two general models are equivalent. However, as for the threshold model, a natural \"diminishing returns\" condition leads to a tractable model again.",
               "In the Decreasing Cascade model, the probability of a node u influencing v is non-increasing as a function of the set of nodes that have previously tried to influence v. This means that p v (u, S)p v (u, T ) whenever S ? T . We call this the diminishing influence condition. Notice that the Decreasing Cascade model is a generalization of the Independent Cascade model. (On the other hand, in Section 3.1, we show that it is a special case of the Submodular Threshold model.)",
               "Whenever the incremental functions satisfy the diminishing influence condition, we will be able to show that the greedy hill climbing algorithm is a 1 ? 1/e approximation. This is despite the fact that there are functions p v (u, S) satisfying the diminishing influence condition which do not admit an equivalent formulation in terms of the Triggering Set model at the heart of most of our proofs."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 9
     },
     {
          "head": {
               "n": "2.3",
               "text": "Node weights",
               "type": "modelling"
          },
          "paragraphs": [
               "The discussion so far has been treating all nodes as equal, in that the stated goal was to simply maximize the number of nodes that are eventually activated in expectation. The model is naturally extended by associating with each node v a non-negative weightv , capturing how important it is that v be activated in the final outcome. (For instance, if we are marketing textbooks to college teachers, then the weight could be the number of students in the teacher's class, resulting in a larger or smaller number of sales.) If we let t be the time at which the diffusion process quiesces, we can define the weighted influence function defined previously is the special case obtained by settingv = 1 for all nodes v. As we will see below, most results we obtain for the influence maximization problem carry over directly to the weighted version."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 10
     },
     {
          "head": {
               "n": "3",
               "text": "Equivalence of models and hardness",
               "type": "modelling"
          },
          "paragraphs": [
               "The Linear Threshold and Independent Cascade models are two specific, widely studied models for the diffusion of influence. Above, we proposed natural ways of generalizing the models. The goal of such generalizations is to prove approximation guarantees for a broader class of influence models, and to explore the limits of models in which strong approximation guarantees can be obtained. Perhaps somewhat surprisingly, the general Threshold model and the general Cascade model are in fact equivalent. Not only does this equivalence imply that both models can be considered \"natural\" descriptions of real-world processes, but the two different views of the same random process will also be a key ingredient in our proof of approximation guarantees in Section 5.",
               "We give an explicit method to convert between the two models. Given success probabilities p v (u, S), we define the activation functions",
               "where S = {u 1 , u 2 , . . . , u r }, and S i = {u 1 , . . . , u i?1 }. That f v is well-defined follows from the orderindependence assumption on the p v (u, S). Conversely, given activation functions f v , we define success probabilities",
               "It is straightforward to verify that the activation functions defined via Equation (3.1) satisfy Equation (3.2), and the success probabilities defined via Equation (3.2) satisfy Equation (3.1). The equivalence of the models is then captured by the following lemma.",
               "Lemma 3.1. Assume that the success probabilities p v (u, S) and activation functions f v (S) satisfy Equation (3.2). Then, for each node set T and each time t, the probability that exactly the nodes of set T are active at time t is the same under the order-independent cascade process with success probabilities p v (u, S) and the general threshold process with activation functions f v (S).",
               "Proof. We show, by induction, a slightly stronger statement: namely that for each time t and any pair (T, T ), the probability that exactly the nodes of T are active at time t ? 1, and exactly those of T are active at time t, is the same under both views. By summing over all sets T , this clearly implies the lemma. At time t = 0, the inductive claim holds trivially, as the probability is 1 for the pair ( / 0, A 0 ) and 0 for all other pairs, for both processes. For the inductive step to time t, we first condition on the event that the nodes of T are active at time t ? 1, and those of T at time t.",
               "Consider a node v /T . Under the cascade process, v will become active at time t + 1 with probability",
               "where we write T \\ T = {u 1 , . . . , u r } and T i = {u 1 , . . . , u i?1 }. Under the threshold process, node v becomes active at time t + 1 iff f v (T ) <vf v (T ). Because node v is not active at time t, and by the Principle of Deferred Decisions,v is uniformly distributed in ( f v (T ), 1] at time t, so the probability that v becomes active is",
               "Substituting Equation (3.1) for f v (T ) and f v (T ), a simple calculation shows that",
               "Thus, each individual node becomes active with the same probability under both processes. As both the thresholdsv and activation attempts are independent for distinct nodes, the probability for any set T to be the set of active nodes at time t + 1 is the same under both processes. Finally, as the probability distribution over active sets T is the same conditioned on any pair (T, T ) of previously active sets, the overall distribution over pairs (T , T ) is the same in both the cascade and threshold processes.",
               "Lemma 3.1 shows that the general Threshold model is a non-trivial reparametrization of the general Cascade model. In a natural way, it allows us to make all random choices at time 0, before the process starts. An alternate way of attempting to pre-flip all coins, for instance by providing a sequence of random numbers from for use in deciding the success of activation attempts, would not preserve order-independence.",
               "For our later proofs, it will be useful to allow delaying the activation of a node whose activation criterion has been met. We therefore generalize the Threshold and Cascade models as follows: each node v has a finite waiting timev , meaning that when v's criterion for activation has been met at time t (i. e., an influence attempt was successful in the Cascade model, or f v (S)v in the Threshold model), v only becomes active at time t +v . Notice that whenv = 0 for all nodes, this is the original Threshold/Cascade model. The following lemma shows that delaying the activation of nodes does not change the eventual outcome under the general Threshold model. Because Lemma 3.1 can be extended straightforwardly to include delays in activation, we obtain the same result for the general Cascade model."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 11
     },
     {
          "head": {
               "text": "Lemma 3.2.",
               "type": "modelling"
          },
          "paragraphs": [
               "Under the general Threshold model, the distribution ?(A) over active sets at the time of quiescence is the same regardless of the waiting timesv . This even holds conditioned upon any random event E.",
               "Proof. We prove the stronger statement that for every choice of thresholdsv , and every vectorof waiting timesv , the set Sof nodes active at the time of quiescence is the same as the set S 0 0 0 of nodes active at quiescence when all waiting times are 0. This will clearly imply the claim, by integrating over all thresholds that form the event E. So from now on, fix the thresholdsv .",
               "Let A 0,t denote the set of nodes active at time t when all waiting times are 0, and A,t the set of nodes active at time t with waiting times. A simple inductive proof using the monotonicity of the activation functions f v shows that A,t ? A 0,t for all times t, which, by setting t to be the time of quiescence of the process with waiting times, implies that S? S 0 0 0 .",
               "Assume now that S= S 0 0 0 , and let T = S 0 0 0 \\ S= / 0. Among the nodes in T , let v be one that was activated earliest in the process without waiting times, i. e., TA 0,t = / 0, and vTA 0,t+1 for some time t. Because v was activated, we know thatvf v (A 0,t ), and by definition of v, no previously active nodes are in T , i. e., A 0,t ? S. But then, the monotonicity of f v implies thatvf v (S), so v should be active in the process with waiting times, a contradiction."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 12
     },
     {
          "head": {
               "n": "3.1",
               "text": "Decreasing Cascade model and Submodular Threshold model",
               "type": "modelling"
          },
          "paragraphs": [
               "We use the insights of the preceding reduction to show that the Decreasing Cascade model is a special case of the Submodular Threshold model. By Lemma 3.1, the influence function defined via Equation (3.1) gives rise to an instance of the general Threshold model, and the success probabilities p v (u, S) and activation functions f v satisfy Equation (3.2), i. e.,",
               "The condition that p v (u, S)p v (u, T ) whenever S ? T now translates to",
               "whenever S ? T and u /T . This is in a sense a \"normalized submodularity\" property; it is stronger than submodularity, which would consist of the same inequality on just the numerators. (Note that by monotonicity of f v , the denominator on the left is larger.) Hence, the Decreasing Cascade model is a special case of the Submodular Threshold model."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 13
     },
     {
          "head": {
               "n": "3.2",
               "text": "Inapproximability results and discussion",
               "type": "modelling"
          },
          "paragraphs": [
               "The general model proposed above includes large families of instances for which the influence maximization problem is not tractable. Indeed, it may become NP-hard to approximate the optimization problem to within any non-trivial factor. Theorem 3.3. In general, it is NP-hard to approximate the influence maximization problem to within a factor of n 1?, for any> 0.",
               "Proof. To prove this result, we reduce from the SET COVER problem. We start with the construction from the proof of Theorem 2.2; however, we do not need duplicates of the element nodes. Specifically, we let u 1 , . . . , u n denote the nodes corresponding to the n elements; i. e., u i becomes active when at least one of the nodes corresponding to sets containing u i is active. Next, for an arbitrarily large constant c, we add N = n c more nodes x 1 , . . . , x N ; each x j is connected to all of the nodes u i , and it becomes active only when all of the u i are.",
               "If there are at most k sets that cover all elements, then activating the nodes corresponding to these k sets will activate all of the nodes u i , and thus also all of the x j . In total, at least N + n + k nodes will be THEORY OF COMPUTING, Volume 11 (4), 2015, pp. 105-147 active. Conversely, assume that there is no set cover of size k, and consider a seed set A. Without loss of generality, A contains no node u i , since such a node can always be replaced with the node corresponding to any set containing element i, which will still result in activating u i , and possibly other nodes as well. Because there is no set cover of size k, A cannot activate all of the u i , and hence none of the x j will become active (unless targeted). In particular, fewer than n + k nodes are active in the end. If an algorithm could approximate the problem within n 1?for any, it could distinguish between the cases where N + n + k nodes are active in the end, and where fewer than n + k are. But this would solve the underlying instance of SET COVER, and therefore is impossible assuming P = NP.",
               "Note that our inapproximability result holds in a very simple model, in which each node is \"hardwired\" with a fixed threshold. 2 It is thus worth briefly considering the general issue of performance guarantees for algorithms under the above models. For both the Linear Threshold and the Independent Cascade models, the influence maximization problem is NP-complete, but, as we prove in the next section, it can be approximated well. In the linear model of , on the other hand, both the propagation of influence as well as the effect of the initial targeting are linear. Initial marketing decisions here are thus limited in their effect on node activations; each node's probability of activation is obtained as a linear combination of the effect of targeting and the effect of the neighbors. In this fully linear model, the influence can be maximized by solving a system of linear equations.",
               "In contrast, generalizing Theorem 3.3, we can show that general models like that of Domingos and Richardson , and even simple models that build in a fixed threshold (like 1/2) at all nodes , lead to influence maximization problems that cannot be approximated to within any non-trivial factor, assuming P = NP. Our analysis of approximability thus suggests a way of tracing out a more delicate boundary of tractability through the set of possible models, by helping to distinguish among those for which simple heuristics provide strong performance guarantees and those for which they can be arbitrarily far from optimal. This in turn can suggest the development of both more powerful algorithms, and the design of accurate models that simultaneously allow for tractable optimization."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 14
     },
     {
          "head": {
               "text": "4:",
               "type": "modelling"
          },
          "paragraphs": [
               "Add the node with largest estimate for(A{x}) to A. 5: end while 6: Output the set A of nodes.",
               "To obtain good approximation guarantees for the influence maximization problem in a particular model of diffusion, we use a two-step strategy. First, we show that the influence function() is a submodular function of the initial set of active nodes A. (Recall the definition of submodularity from Section 2.1.2.)",
               "Once we have proved submodularity, we can apply the following theorem of Nemhauser, Wolsey and Fisher .",
               "Theorem 4.2 ( ). Let() be a non-negative monotone submodular function. Then the greedy algorithm that (for k iterations) adds an element with the largest marginal increase in() produces a k-element set A such that(A)(1 ? 1/e)max |B|=k(B).",
               "Due to its generality, this result has found applications in a number of areas of discrete optimization (see, e. g., ). In particular, it has become a very popular technique for optimization with provable approximation guarantees since the original publication of the present article; see the survey by Krause and Golovin for a recent overview.",
               "Theorem 4.2 assumes that the function() to be optimized can be evaluated exactly at any point. As we argued above,() can in general not be evaluated exactly in polynomial time. However, as shown above, by simulating the diffusion process and sampling the resulting active sets, we are able to obtain arbitrarily close approximations to(A), with high probability. It is a fairly straightforward extension of Theorem 4.2 that by using (1)-approximate values for the function to be optimized, we obtain a (1 ? 1/e ?)-approximation, wheredepends onand goes to 0 as0. To finish the proof of Theorem 1.1, it thus remains to show that the objective function() is non-negative, monotone and submodular."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 15
     },
     {
          "head": {
               "n": "4.1",
               "text": "The triggering set technique",
               "type": "modelling"
          },
          "paragraphs": [
               "Primarily for the purpose of analysis, we define the following Triggering model: Definition 4.3 (Triggering model). Each node v independently chooses a random \"triggering set\" T v according to some distribution over subsets of its incoming neighbors. Initially, a set A is activated. Subsequently, an inactive node v becomes active in step t if it has a neighbor in its chosen triggering set T v that is active in step t ? 1.",
               "Thus, compared to, say, the Threshold model, v's threshold has been replaced by a latent subset T v of neighbors whose behavior actually affects v. It is useful to think of the triggering sets T v in terms of \"live\" and \"blocked\" edges: if node u belongs to the triggering set T v of v, then we declare the edge (u, v) to be live, and otherwise we declare it to be blocked. Since we are only interested in the node set active at the end of the process, Definition 4.3 is equivalent to stating that A n is the set of nodes v such that v is reachable from A via a path consisting entirely of live edges.",
               "The key fact about the Triggering model is that any of its instances will directly lead to a submodular influence function:",
               "Lemma 4.4. In every instance of the Triggering model, the influence function() is submodular.",
               "Proof. For any outcome X = (T v ) v of the choices of live and blocked edges, letX () denote the number of nodes reached with this particular outcome; note thatX (S) is a deterministic quantity. First, we claim that for each fixed outcome X, the functionX () is submodular. To see this, let S and T be two sets of nodes such that S ? T , and consider the quantityX (S{v}) ?X (S). Let R(v, X) be the set of nodes that are reachable from v given the edge choices X. Then,X (S{v}) ?X (S) is the number of elements in R(v, X) that are not already in the union uS R(u, X); it is at least as large as the number of elements in R(v, X) that are not in the (bigger) union",
               ", which is the defining inequality for submodularity. When we instead consider distributions over outcomes X, we can write",
               "since the expected number of nodes activated is just the weighted average over all outcomes. But a nonnegative linear combination of submodular functions is also submodular, and hence() is submodular, which concludes the proof.",
               "With Lemma 4.4 in hand, our strategy for establishing submodularity of the Independent Cascade and Linear Threshold models will be to exhibit instances of the Triggering model which have identical distributions over activated sets. We will also see other natural models that can be analyzed using this technique, but (in Section 5), we will see that for the Decreasing Cascade model, no equivalent instance of the Triggering model exists, even though the activation functions are submodular."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 16
     },
     {
          "head": {
               "n": "4.2",
               "text": "Independent Cascade",
               "type": "modelling"
          },
          "paragraphs": [
               "In this section, we will establish the following theorem:",
               "Theorem 4.5. For an arbitrary instance of the Independent Cascade model, the resulting influence function() is submodular.",
               "Proof. We establish this theorem by giving an equivalent instance of the Triggering model; the result then follows directly from Lemma 4.4. To derive the Triggering model instance, consider a point in the cascade process when node v has just become active, and it attempts to activate its neighbor w, succeeding with probability p v,w . We can view the outcome of this random event as being determined by flipping a coin of bias p v,w . From the point of view of the process, it clearly does not matter whether the coin was flipped at the moment that v became active, or whether it was flipped at the very beginning of the whole process and is only being revealed now. Continuing this reasoning, we can in fact assume that for each pair of neighbors (v, w) in the graph, a coin of bias p v,w is flipped at the very beginning of the process (independently of the coins for all other pairs of neighbors), and the result is stored so that it can be later checked in the event that v is activated while w is still inactive.",
               "Thus, an equivalent view of the process is as follows: each edge (v, w) is present in the graph G of live edges independently with probability p v,w , and absent otherwise. The preceding argument shows that conditioned on the outcome of the coin flips for each edge (v, w), a node u is reachable from A in the resulting graph if and only if it is activated by the Independent Cascade process with the same coin flip outcomes. Since the outcomes are pointwise the same, the expected number (or weight) of nodes reachable from A in the Triggering model is the same as the expected number of nodes activated in the Independent Cascade model."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 17
     },
     {
          "head": {
               "n": "4.3",
               "text": "Linear Threshold",
               "type": "modelling"
          },
          "paragraphs": [
               "We now prove an analogous result for the Linear Threshold model. Proof. We again use Lemma 4.4, exhibiting an equivalent instance of the Triggering model. In this case, the analysis is a bit more intricate than in the proof of Theorem 4.5, because there is no clear \"pointwise\" equivalent process.",
               "Recall that each node v has an influence weight b v,w0 from each of its neighbors w, and we are assuming thatw b v,w1. (We can extend the notation by writing b v,w = 0 when w is not a neighbor of v.) The equivalent instance of the Triggering model we analyze is the following: v picks at most one of its incoming edges at random, selecting the edge from w with probability b v,w and selecting no edge with probability 1 ?w b v,w . Thus, each set T v is either empty or contains exactly one edge. Note the contrast with the proof of Theorem 4.5: there, we determined whether an edge was live independently of the decision for each other edge; here, we negatively correlate the decisions so that at most one live edge enters each node.",
               "To show the equivalence of the models formally, we will establish that for any given targeted set A, the following two distributions over sets of nodes are the same:",
               "1. The distribution over active sets obtained by running the Linear Threshold process to completion starting from A; and 2. The distribution over sets reachable from A via live-edge paths, under the random selection of live edges defined above.",
               "To obtain some intuition, it is useful to first analyze the special case in which the underlying graph G is directed and acyclic. In this case, we can fix a topological ordering of the nodes v 1 , v 2 , . . . , v n (so that all edges go from earlier nodes to later nodes in the order), and build up the distribution of active sets by following this order. For each node v i , suppose that we have already determined the distribution over active subsets of its neighbors. Then under the Linear Threshold process, the probability that v i will become active, given that a subset S i of its neighbors is active, iswS i b v i ,w . This is precisely the probability that the live incoming edge selected by v i lies in S i , and so inductively, we see that the two processes define the same distribution over active sets.",
               "To prove the claim generally, consider a graph G that is not acyclic. It becomes trickier to show the equivalence, because there is no natural ordering of the nodes over which to perform induction. Instead, we argue by induction over the iterations of the Linear Threshold process. We define A t to be the set of active nodes at the end of iteration t, for t = 0, 1, 2, . . .. (Note that A 0 is the set initially targeted.) If node v has not become active by the end of iteration t, then the probability that it becomes active in iteration t + 1 is equal to the probability that the influence weights in A t \\ A t?1 push it over its threshold, given that its threshold was not exceeded already; this probability is",
               "THEORY OF COMPUTING, Volume 11 (4), 2015, pp. 105-147",
               "On the other hand, we can run the Triggering process by revealing the identities of the live edges gradually as follows. We start with the targeted set A. For each node v with at least one edge from the set A, we determine whether v's live edge comes from A. If so, then v is reachable; but if not, we keep the source of v's live edge unknown, subject to the condition that it comes from outside A. Having now exposed a new set of reachable nodes A 1 in the first stage, we proceed to identify further reachable nodes by performing the same process on edges from A 1 , and in this way produce sets A 2 , A 3 , . . .. If node v has not been determined to be reachable by the end of stage t, then the probability that it is determined to be reachable in stage t + 1 is equal to the probability that its live edge comes from A t \\ A t?1 , given that its live edge has not come from any of the earlier sets. But this is",
               "which is the same as in the Linear Threshold process of the previous paragraph. Thus, by induction over these stages, we see that the Triggering process produces the same distribution over active sets as the Linear Threshold process.",
               "Even whenw:wv b v,w > 1 for some nodes v, the result of Mossel and Roch implies that the objective function() is submodular, albeit not via a Triggering process proof. The reason is that the function",
               "wS:wv is submodular for every node v. Mossel and Roch proved as their main result that this is sufficient for() to be submodular. Thus, we obtain submodularity for all instances of the Linear Threshold model."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 18
     },
     {
          "head": {
               "n": "4.4",
               "text": "Other models",
               "type": "modelling"
          },
          "paragraphs": [
               "Beyond the Independent Cascade and Linear Threshold, there are other natural special cases of the Triggering model. One example is the \"Only-Listen-Once\" model. Here, each node v has a parameter p v so that the first neighbor of v to be activated causes v to become active with probability p v , and all subsequent attempts to activate v deterministically fail. (In other words, v only listens to the first neighbor that tries to activate it.) This process has an equivalent formulation in the Triggering Set model, with an edge distribution defined as follows: for any node v, the triggering set T v is either the entire neighbor set of v (with probability p v ) or the empty set. As a result, the influence function in the Only-Listen-Once model is also submodular, and we can obtain a (1 ? 1/e ?)-approximation here as well."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 19
     },
     {
          "head": {
               "n": "5",
               "text": "The Decreasing Cascade model",
               "type": "modelling"
          },
          "paragraphs": [
               "In this section, we investigate the Decreasing Cascade model (defined in Section 2.2.2) in detail. First, we show that the Decreasing Cascade model is not an instance of the Triggering model, implying that the Triggering Set technique cannot be used to establish the submodularity of the objective function in this case. Nonetheless, using a more involved coupling argument and the Principle of Deferred Decisions, we THEORY OF COMPUTING, Volume 11 (4), 2015, pp. 105-147 will be able to show that the objective function is in fact submodular, and hence, the 1?1/e approximation guarantee of the greedy algorithm applies to the Decreasing Cascade model as well."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 20
     },
     {
          "head": {
               "n": "5.1",
               "text": "Relationship to the Triggering Model",
               "type": "modelling"
          },
          "paragraphs": [
               "We first show an instance of the Decreasing Cascade model that is not equivalent to any instance of the Triggering model. This example shows that the Triggering Set Technique cannot be applied to show submodularity of the Decreasing Cascade model in general.",
               "Our example has five nodes. Node v could potentially be influenced by four nodes u 1 , . . . , u 4 . The first two nodes to try activating v have a probability of 1/2 each to succeed, whereas all subsequent attempts fail. The influences are thus p v (u i , S) = 1/2 whenever |S| < 2, and p v (u i , S) = 0 otherwise. Notice that this is indeed an instance of the Decreasing Cascade model, and order-independent.",
               "Assume, for contradiction, that there is a distribution on graphs such that node v is reachable from a set S with the same probability that S will activate v in the Cascade model. For any set S ? {1, 2, 3, 4}, let q S denote the probability that in this distribution over graphs, exactly the edges from u i to v for iS are present. Because with probability 1/4, v does not become active even if all u i are, we know that q / 0 = 1/4. If u 1 , u 2 , u 3 are active, then v is also active with probability 3/4, so the edge (u 4 , v) can never be present all by itself. (If it were, then the set {u 1 , u 2 , u 3 , u 4 } together would have higher probability of reaching v than the set {u 1 , u 2 , u 3 }.) Thus, we have that q {i} = 0 for all i. The same argument shows that q {i, j} = 0 for all i, j.",
               "Thus, the only non-empty edge sets with non-zero probabilities can be those of size three or four. If node u 1 is the only active node, then v will become active with probability 1/2, so the edge (u 1 , v) is present with probability 1/2. Hence, q {1,2,3} + q {1,2,4} + q {1,3,4} + q {1,2,3,4} = 1 2 , while q {1,2,3} + q {1,2,4} + q {1,3,4} + q {2,3,4} + q {1,2,3,",
               "Therefore, q {2,3,4} = 1/4, and a similar argument for nodes u 2 , u 3 , u gives that q S = 1/4 for each set S of cardinality 3. But then, the total probability mass on edge sets is at least 5/4, as there are four such sets S, and the empty set also has probability 1/4. This is a contradiction, so there is no such distribution over graphs.",
               "The example above raises the interesting question of which instances of the General Threshold or Cascade model are in fact instances of the Triggering model, for a suitably defined distribution over graphs. In other words, for which models does the Triggering Set technique imply submodularity? Subsequent to the publication of the conference versions of the present work, a complete characterization of such models was provided by : an instance of the general threshold model has an equivalent Triggering model formulation if and only if each node's activation function f v has discrete derivatives of alternating signs. More specifically, the derivative of a set function g with respect to an element v is defined as g v (S) = g(S{v}) ? g(S); higher derivatives are defined as derivatives of THEORY OF COMPUTING, Volume 11 (4), 2015, pp. 105-147 derivatives. The precise necessary and sufficient condition for the Triggering model is then that all functions f v be non-negative and decreasing, all even derivatives be non-positive, and all odd derivatives be non-negative."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 21
     },
     {
          "head": {
               "n": "5.2",
               "text": "Proof of submodularity",
               "type": "modelling"
          },
          "paragraphs": [
               "We now establish the following theorem:",
               "Theorem 5.1. If all influence functions p u (v, S) are non-increasing in S, then() is monotone and submodular.",
               "Proof. The monotonicity is an immediate consequence of Lemma 5.2 below, applied with V = V and p v (u, S) = p v (u, S) for all S, v, u. So we focus on submodularity for the remainder of the proof. We have to show that, whenever A ? A , we have(A{x}) ?(A)(A{x}) ?(A ), for any node x /A . The basic idea of the proof is to characterize(A{x}) ?(A) in terms of a residual process which targets only the node x, and has appropriately modified success probabilities (similarly for(A{x}) ?(A )). Let ?(A) denote the set of nodes active at the time of quiescence (a random variable). To show that the residual processes indeed have the same distributions over final active sets ?({x}) as the original processes, we use Lemma 3.2.",
               "Given a node set B, we define the residual process on the set V \\ B: the success probabilities are",
               "and the only node targeted is x, targeted at time 1. Let ? B (x) denote the set of nodes active at the time of quiescence of the residual process; notice that this is a random variable. We claim that, conditioned on the event that [?(A) = B], the variable ? B (x) has the same distribution as the variable ?(A{x}) \\ ?(A).",
               "In order to prove this fact, we focus on the threshold interpretation of the process, and assign node x a waiting time ofx = n + 1. By Lemma 3.2, this view does not change the distribution of ?(A{x}) \\ ?(A). Then, x is the only node at time n + 1 which has not exhausted its activation attempts; by the conditioning, the other active nodes (which have exhausted their activation attempts) are those from B. This implies that only nodes from V \\ B will make activation attempts after time n + 1. By using the same order of activation attempts, and the same coin flips for each pair u, vV \\ B, a simple inductive proof on the time t shows that the set S of nodes is active in the residual process at time t if and only if the set SB is active in the original process at time n + t. In particular, this shows that the two random variables have the same distributions.",
               "Having shown this equivalence, we want to compare the expected sizes of ? B (x) and ? B (x), when B ? B . We write",
               "as well as",
               "First off, notice that the node set V \\ B of the former process is a superset of V \\ B . Furthermore, for all nodes u, v and node sets S, the decreasing cascade condition implies that",
               "THEORY OF COMPUTING, Volume 11 (4), 2015, pp. 105-147",
               "Lemma 5.2 below proves the intuitively obvious fact that the combination of a larger ground set of nodes and larger success probabilities results in a larger set of activated nodes, i. e.,",
               "Finally, we can rewrite the expected number of active nodes as",
               "The inequality followed by applying Inequality (5.1) under the sum. In both of the steps surrounding the inequality, we used that Prob [?(A) = B, ?(A ) = B ] = 0 whenever B ? B , by the monotonicity of the cascade process. This completes the proof of submodularity. 6",
               "Lemma 5.2. Let V ? V , and assume that p v (u, S)p v (u, S) for all nodes u, vV and all sets S. If A ? A are the targeted sets for cascade processes on V and V , then the expected size of the active set at the end of the process on V is no smaller than the corresponding expected size for the process on V .",
               "Proof. This claim is most easily seen in the threshold view of the process. Equation (3.1) shows that the activation functions f v , f v corresponding to the success probabilities p v (u, S) and p v (u, S) satisfy f v (S)f v (S), for all nodes v and sets S. Then, for any fixed thresholdsv , a simple inductive proof on time steps t shows that the set of active nodes in the former process (with functions f v ) is always a subset of the set of active notes in the latter one (with functions f v ). Since the inequality thus holds for every point of the probability space, it holds in expectation.",
               "We remark here again that subsequent to the original publication of our work, a stronger version of Theorem 5.1 was established by , who showed that in the general Threshold model, whenever the activation functions f v () are monotone and submodular, so is(). As we showed in Section 3.1 that the Decreasing Cascade model is equivalent to a (special case of the) Threshold model with submodular activation functions, the theorem of implies Theorem 5.1."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 22
     },
     {
          "head": {
               "n": "6",
               "text": "Non-progressive processes",
               "type": "modelling"
          },
          "paragraphs": [
               "We have thus far been concerned with the progressive case, in which nodes only go from inactivity to activity, but not vice versa. When considering the non-progressive case, in which nodes can switch in both directions, the first question is how to exactly define the process and the objective function, since the \"number of active nodes\" is not a well-defined quantity any more. In this section, we consider what is likely the most natural choice of the process and objective function, and show that with these choices, the non-progressive case can in fact be reduced to the progressive case.",
               "Because the local threshold functions f v are monotone, the Threshold model is inherently progressive, as is the Cascade model. Perhaps the simplest way of defining a non-progressive version of the process is to have each node choose a new threshold(t) v in each time step t. 7 Thus, a previously active node can become inactive as a result of choosing a larger threshold. Formally, at each step t, each node v chooses a new value(t) v independently and uniformly at random from the interval . Node v will be active in",
               "v , where S t?1 is the set of neighbors of v that are active in step t ? 1. When nodes can switch from active to inactive, care must be taken in defining the objective function. A natural choice is to define(A) as the sum, over all nodes v, of the number of time steps during which v is active. This definition is motivated by considering an active node as a potential customer in that time step, such as an instructor adopting a textbook during a particular semester. This definition is naturally generalized by assigning different weights to nodes in different time steps, such as exponential time-discounting common in economics.",
               "Compared to the progressive model, it may now also be beneficial to choose times for particular activations. More formally, we assume that there is a time horizonfor which the process will be run. We define an intervention as the activation of a particular node v at a particular time t. (Notice that v itself may quickly de-activate, but we hope to create a large \"ripple effect.\") Simple examples show that to maximize influence, one should not necessarily perform all k interventions at time 0; for example, G may not even have k nodes. Thus, from the perspective of influence maximization, we now ask the following question: Suppose that we have a non-progressive model that is going to run forsteps, and during this process, we are allowed to make up to k interventions. Which k interventions should we perform? The influence maximization problem in the non-progressive Threshold model is to find the k interventions with maximum influence.",
               "We can show that the non-progressive influence maximization problem reduces to the progressive case in a different graph. Given a graph G = (V, E) and a time limit, we build a layered graph Gon|V | nodes: there is a copy v t for each node v in G and each time step t. Let",
               "be the t th layer of G. For each node v t , we define its influence function to be",
               "this function simply applies v's original influence function to the set of nodes that are active in step t ? 1. 8 At the other extreme would be for each node to have the same threshold throughout. This model would be identical to the Linear Threshold model as we have defined it previously. A more realistic model would posit a strong but not perfect correlation between thresholds for different time steps; we leave the exploration of such models to future work.",
               "8 If different time steps carry different weights, then we simply assign each node v t the corresponding weight; see the discussion in Section 2.3."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 23
     },
     {
          "head": {
               "text": "THEORY OF COMPUTING, Volume 11 (4), 2015, pp. 105-147",
               "type": "modelling"
          },
          "paragraphs": [
               "Theorem 6.1. The non-progressive influence maximization problem on G over a time horizonis equivalent to the progressive influence maximization problem on the layered graph G. Node v is active at time t in the non-progressive process if and only if v t is activated in the progressive process.",
               "Proof. The proof simply couples the thresholds of nodes v t with the threshold chosen by node v in step t in the non-progressive model. Then, the theorem follows by a straightforward induction proof on t.",
               "Thus, models where we have approximation algorithms for the progressive case carry over. Theorem 6.1 also implies approximation results for certain non-progressive models used by Asavathiratham et al. to model cascading failures in power grids .",
               "Note that the non-progressive model discussed here differs from the model of Domingos and Richardson in two ways. We are concerned with the sum over all time steps tof the (possibly weighted) expected number of active nodes at time t, for a given a time limit, while study the limit of this process: the expected number of nodes active at time t as t goes to infinity. Further, we consider interventions for a particular node v, at a particular time t, while the interventions considered by permanently affect the activation probability function of the targeted nodes."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 24
     },
     {
          "head": {
               "n": "7",
               "text": "General marketing strategies",
               "type": "modelling"
          },
          "paragraphs": [
               "In the formulation of the problem, we have so far assumed that for one unit of budget, we can deterministically target any node v for activation. This is clearly a highly simplified view. In a more realistic scenario, we may have a number m of different marketing actions M i available, each of which may affect some subset of nodes by increasing their probabilities of becoming active, without necessarily making them active deterministically. The more we spend on any one action, the stronger its effect will be; however, different nodes may respond to marketing actions in different ways.",
               "In a general model, we choose investments x i into marketing actions M i , such that the total investments do not exceed the budget. A marketing strategy is then an m-dimensional vector x x x of investments. The probability that node v will become active is determined by the strategy, and denoted by h v (x x x). We assume that this function is non-decreasing and satisfies the following \"diminishing returns\" property for all x x xy y y and a a a0 0 0 (where we write x x xy y y or a a a0 0 0 to denote that the inequalities hold in all coordinates):",
               "Intuitively, inequality (7.1) states that any marketing action is more effective when the targeted individual is less \"marketing-saturated\" at that point; it captures concavity of the activation functions in non-negative directions.",
               "We are trying to maximize the expected size of the final active set. As a function of the marketing strategy x x x, each node v becomes active independently with probability h v (x x x), resulting in a (random) set of initial active nodes A. Given the initial set A, the expected size of the final active set is(A). 9 Recently, a slightly different model of partial influence by a marketer has been proposed by Gnne? and Raghavan (see ) and Demaine et al. . In their model, a marketer also divides a budget among actions, but each action only affects a single individual. Their model is best understood in the threshold model: a node v on whom the marketer spent x v units of budget becomes active whenvx v + f v (S). Thus, the effort of the marketer not only creates a possibility of initial activation, but permanently increases a node's propensity for being activated. The models are technically incomparable.",
               "THEORY OF COMPUTING, Volume 11 (4), 2015, pp. 105-147",
               "The expected revenue of the marketing strategy x x x is therefore",
               "In order to (approximately) maximize g, we assume that we can evaluate the function at any point x x x approximately, and find a direction i with approximately maximal gradient. Specifically, let e e e i denote the unit vector along the i th coordinate axis, andbe some constant. We assume that there exists some1 such that we can find an i with",
               "for each j. We divide each unit of the total budget k into equal parts of size. Starting with an all-0 investment 0 0 0, we perform an approximate gradient ascent, by repeatedly (a total of k/times) addingunits of budget to the investment in the action M i that approximately maximizes the gradient. Formally, the algorithm is defined as Algorithm 2. Let i t be a direction maximizing g(x x x (t) +e e e i t ) ? g(x x x (t) )."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 25
     },
     {
          "head": {
               "text": "4:",
               "type": "modelling"
          },
          "paragraphs": [
               "Set x x x (t+1) = x x x (t) +e e e i t ."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 26
     },
     {
          "head": {
               "text": "5: end for",
               "type": "modelling"
          },
          "paragraphs": [
               "The proof that this algorithm gives a good approximation consists of two steps. First, we show that whenever a function g is non-negative, non-decreasing, and satisfies the \"diminishing returns\" condition (7.1), the hill-climbing algorithm gives a constant-factor approximation. Then, we show that the specific function g we are trying to optimize is indeed non-negative, non-decreasing, and satisfies Condition (7.1).",
               "Theorem 7.1. Let g be a non-negative and monotone non-decreasing function satisfying the \"diminishing returns\" condition (7.1). Assume that the hill climbing algorithm is run with a total budget of k, divided into pieces of sizeeach, and a direction i t with-approximately largest gradient 10 is chosen in each step t.",
               "When the hill-climbing algorithm finishes with strategy x x x, it guarantees that",
               "wherx x x denotes the optimal solution subject toi ? x ik.",
               "10 A direction i has-approximately largest gradient from a point x x x if",
               "for all j.",
               "THEORY OF COMPUTING, Volume 11 (4), 2015, pp. 105-147",
               "Proof. The proof is quite similar to, and builds on, the analysis used by Nemhauser et al. . The investments of the optimal solution?xsolution? solution?x x x may not be multiples of, so we round them up to the nearest multiple of, resulting in a vector y y y = (y 1 , . . . , y m )? x x x. The increase in each coordinate is at most, soi y ik + m.",
               "Consider iteration t of the algorithm. Because g is non-decreasing, we know that x x x (t)0 0 0, so x x x (t) + y y yy y y, and therefore (again by monotonicity of g) g(x x x (t) + y y y)g(y y y)g(? x x x). In turn, we now derive an upper bound on g(x x x (t) + y y y) in terms of our solution x x x (t) .",
               "Because g has diminishing returns, we know that",
               "Because y i is a multiple of, we obtain, again from the diminishing returns property of g, that",
               "By the choice of the direction i t ,",
               ".",
               ", and rewriting x x x (t) as a telescoping series now shows that",
               "j<t Multiplying both sides of the t th inequality by",
               "and summing them all up yields that the term ? t appears with coefficient",
               "in the t th inequality, and with coefficient 1 ?k +m k?1 ? j in the j th inequality for j > t, so it appears a total of times. On the left-hand side of the inequality obtained by summing up, the coefficient for g(? x x x) is",
               "and similarly for the coefficient of g(0 0 0). Hence, the inequality obtained by summing is",
               "Dividing both sides by (k +m)/(), and bounding that",
               "by the non-negativity of g. This completes the proof.",
               "With Theorem 7.1 in hand, it remains to show that g is non-negative, monotone, and satisfies Condition 7.1. The first two are clear, so we only prove the third.",
               "First, we determine an expression for the difference g(x x x + a a a) ? g(x x x) when a a a0 0 0, and then show that this difference is non-increasing as a function of x x x. Using Lemma 7.2 below, and changing order of summations, we can write (for an arbitrary, but fixed, ordering of vertices) ",
               "Let us consider each of the sums separately. All products are non-negative, as are all of the differences of the form h u (y y y + a a a) ? h u (y y y) (and similar ones), by monotonicity and the diminishing returns property of the h u (). That leaves the terms",
               "which are non-positive by submodularity of(). Hence, g does indeed have the diminishing returns property.",
               "Lemma 7.2. If a 1 , . . . , a n and b 1 , . . . , b n are any numbers, then ",
               "THEORY OF COMPUTING, Volume 11 (4), 2015, pp. 105-147",
               "Proof. The proof is by induction on n. For n = 1, the claim obviously holds. For n > 1, we can write",
               "completing the proof."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 27
     },
     {
          "head": {
               "n": "8",
               "text": "Experiments",
               "type": "experiment"
          },
          "paragraphs": [
               "In addition to obtaining worst-case guarantees on the performance of the greedy approximation algorithm, we are interested in understanding its behavior in practice, and comparing its performance to other heuristics for identifying influential individuals. We find that the greedy algorithm achieves significant performance gains over several widely used structural measures of influence in social networks ."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 28
     },
     {
          "head": {
               "n": "8.1",
               "text": "The network data",
               "type": "experiment"
          },
          "paragraphs": [
               "For evaluation, it is desirable to use a network data set that exhibits many of the structural features of large-scale social networks. At the same time, we do not address the issue of inferring actual influence parameters from network observations (see, e. g., , or for a string of more recent work on this question). Thus, for our testbed, we employ a collaboration graph obtained from co-authorships in physics publications, with simple settings of the influence parameters. It has been argued extensively that co-authorship networks capture many of the key features of social networks more generally . The co-authorship data set was compiled from the complete list of papers in the high-energy physics theory section of the e-print arXiv (www.arxiv.org), as of winter of 2002. The collaboration graph contains a node for each researcher who has at least one paper with coauthor(s) in the arXiv database. For each paper with two or more authors, we inserted an edge for each pair of authors. (Single-author papers were ignored.) Notice that this results in parallel edges when two researchers have co-authored multiple papers-we kept these parallel edges as they can be interpreted to indicate stronger social ties between the researchers involved. The resulting graph has 10748 nodes, and edges between about 53000 pairs of nodes.",
               "While processing the data, we corrected many common types of mistakes automatically or manually. In order to deal with aliasing problems at least partially, we abbreviated first names, and unified spellings for foreign characters. We believe that the resulting graph is a good approximation to the actual collaboration graph. (The sheer volume of data prohibits a complete manual cleaning pass.)"
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 29
     },
     {
          "head": {
               "n": "8.2",
               "text": "The influence models",
               "type": "modelling"
          },
          "paragraphs": [
               "We compared the algorithms in three different models of influence. In the Linear Threshold model, we treated the multiplicity of edges as weights. If nodes u, v have c u,v parallel edges between them and degrees d u , d v , then the edge (u, v) has weight c u,v /d v , and the edge (v, u) has weight c u,v /d u .",
               "In the Independent Cascade model, we assigned a uniform probability of p to each edge of the graph, choosing p to be 1% and 10% in separate trials. If nodes u and v have c u,v parallel edges, then we assume that for each of those c u,v edges, u has a chance of p to activate v, i. e., u has a total probability of 1 ? (1 ? p) c u,v of activating v once it becomes active.",
               "The Independent Cascade model with uniform probabilities p on the edges has the property that high-degree nodes not only have a chance to influence many other nodes, but also to be influenced by them. Whether or not this is a desirable interpretation of the influence data is an application-specific issue. Motivated by this issue, we chose to also consider an alternative interpretation, where edges into high-degree nodes are assigned smaller probabilities. We study a special case of the Independent Cascade model that we term \"weighted cascade,\" in which each edge from node u to v is assigned probability 1/d v of activating v. The weighted Cascade model resembles the Linear Threshold model in that the expected number of neighbors who would succeed in activating a node v is 1 in both models."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 30
     },
     {
          "head": {
               "n": "8.3",
               "text": "The algorithms and implementation",
               "type": "modelling"
          },
          "paragraphs": [
               "We compare our greedy algorithm with heuristics based on nodes' degrees and centrality within the network, as well as the crude baseline of choosing random nodes to target. The degree and centrality-based heuristics are commonly used in the sociology literature as estimates of a node's influence .",
               "The high-degree heuristic chooses nodes v in order of decreasing degrees d v . Considering high-degree nodes as influential has long been a standard approach for social and other networks , and is known in the sociology literature as \"degree centrality.\" \"Distance centrality\" is another commonly used influence measure in sociology, building on the assumption that a node with short paths to other nodes in a network will have a higher chance of influencing them. Hence, we select nodes in order of increasing average distance to other nodes in the network. As the arXiv collaboration graph is not connected, we assigned a distance of n-the number of nodes in the graph-for any pair of unconnected nodes. This value is significantly larger than any actual distance, and thus can be considered to play the role of an infinite distance. In particular, nodes in the largest connected component will have smallest average distance.",
               "Finally, we consider, as a baseline, the result of choosing nodes uniformly at random. Notice that because the optimization problem is NP-hard, and the collaboration graph is prohibitively large, we cannot compute the optimum value to verify the actual quality of approximations.",
               "Both in choosing the nodes to target with the greedy algorithm, and in evaluating the performance of the algorithms, we need to compute the value(A). As discussed in Section 4, computing(A) exactly is #P-complete; however, very good estimates can be obtained by simulating the random process. More specifically, we simulate the process 10000 times for each targeted set, re-choosing thresholds or edge outcomes pseudo-randomly from the interval [0, 1] every time. Previous runs indicate that the quality of approximation after 10000 iterations is comparable to that after 300000 or more iterations.",
               "In all of the experiments, we vary k from 1 to 30. This is in part for computational reasons, and in 8.4",
               "The results shows the performance of the algorithms in the Linear Threshold model. The greedy algorithm outperforms the high-degree node heuristic by about 18%, and the central node heuristic by over 40%.",
               "(As expected, choosing random nodes is not a good idea.) This shows that significantly better marketing results can be obtained by explicitly considering the dynamics of information in a network, rather than relying solely on structural properties of the graph. When investigating the reason why the high-degree and centrality heuristics do not perform as well, one sees that they ignore such network effects. In particular, neither of the heuristics incorporates the fact that many of the most central (or highest-degree) nodes may be clustered, so that targeting all of them is unnecessary. In fact, the uneven nature of these curves suggests that the network influence of many nodes is not accurately reflected by their degree or centrality. shows the results for the weighted Cascade model. Notice the striking similarity to the Linear Threshold model. The scale is slightly different (all values are about 25% smaller), but the behavior is qualitatively the same, even with respect to the exact nodes whose network influence is not reflected accurately by their degree or centrality. The reason is that in expectation, each node is influenced by the same number of other nodes in both models (see Section 4), and the degrees are relatively concentrated around their expectation of 1. The graph for the Independent Cascade model with probability 1%, given in , seems very similar to the previous two at first glance. Notice, however, the very different scale: on average, each targeted node only activates three additional nodes. Hence, the network effects in the Independent Cascade model with very small probabilities are much weaker than in the other models. Several nodes have degrees well exceeding 100, so the probabilities on their incoming edges are even smaller than 1% in the weighted Cascade model. This suggests that the network effects observed for the Linear Threshold and weighted Cascade models rely heavily on low-degree nodes as multipliers, even though targeting high-degree nodes is a reasonable heuristic. Also notice that in the Independent Cascade model, the heuristic of choosing random nodes performs significantly better than in the previous two models.",
               "The improvement in performance of the \"random nodes\" heuristic is even more pronounced for the Independent Cascade model with probabilities equal to 10%, depicted in . In that model, the \"random nodes\" heuristic starts to outperform both the high-degree and the central nodes heuristics when more than 12 nodes are targeted. It is initially surprising that random targeting for this model should lead to more activations than centrality-based targeting, but in fact there is a natural underlying reason that we explore now.",
               "The first targeted node, if chosen somewhat judiciously, will activate a large fraction of the network, in our case almost 25%. However, any additional nodes will only reach a small additional fraction of the network. In particular, other central or high-degree nodes are very likely to be activated by the initially chosen one, and thus have hardly any marginal gain. This explains the shapes of the curves for the high-degree and distance centrality heuristics, which leap up to about 2415 activated nodes, but make virtually no progress afterwards. The greedy algorithm, on the other hand, takes the effect of the first chosen node into account, and targets nodes with smaller marginal gain afterwards. Hence, its active set keeps growing, although at a much smaller slope than in other models.",
               "The random heuristic does not do as well initially as the other heuristics, but with sufficiently many attempts, it eventually hits some highly influential nodes and becomes competitive with the centralitybased node choices. Because it does not focus exclusively on central nodes, it eventually targets nodes with additional marginal gain, and surpasses the two centrality-based heuristics.",
               "In summary, our experiments show that on a data set with many of the features of a real social network, a greedy algorithm motivated by theoretical insights significantly outperforms several standard heuristics. This is not to say that better algorithms do not exist: for example, a local search step after the greedy algorithm would likely improve the results further. Similarly, there may well be other, more efficient, heuristics with similar or better performance in practice-see the discussion in Section 1.2. Our main goal here was primarily to establish that theoretical guarantees and practical performance are not mutually exclusive."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 31
     },
     {
          "head": {
               "n": "9",
               "text": "Conclusions",
               "type": "conclusion"
          },
          "paragraphs": [
               "Peer influence and word-of-mouth effects play an important role in the dissemination of ideas and innovations and the adoption of new products. When designing campaigns to promote the adoption of ideas or products, it is therefore important to take these network effects into account. In the present work, we studied this optimization problem formally, under several widely used models of influence and cascading behavior. We showed that under these models, a simple greedy algorithm gives a 1 ? 1/e THEORY OF COMPUTING, Volume 11 (4), 2015, pp. 105-147 approximation for the influence maximization objective, and that these results extend to a more general setting in which a marketing budget is to be divided among different available marketing actions, each of which may affect subsets of nodes. Our theoretical analysis is complemented by experiments performed on a collaboration network extracted from the arXiv site. Many important directions remain for future work. While Mossel and Roch generalize the submodularity results of our work to more general Threshold models, we do not as of yet have a complete understanding of the full range of models for which approximate influence maximization is tractable. Even more immediately, while the factor 1 ? 1/e in the approximation guarantee is matched by an approximation hardness result for the Independent Cascade model, no approximation hardness whatsoever is known for the Linear Threshold model. Establishing an improved approximation guarantee, or any kind of approximation hardness, would therefore be of high interest.",
               "Several natural modeling extensions have been studied in the literature recently, including competition between multiple cascades, negative opinions, and negative tie strengths. An important property shared by these extensions is that they need to introduce a timing component, wherein the decision of a node for a particular state depends on which neighbor influences the node first. This stands in contrast with the strong timing independence for our basic models, captured by Lemma 3.2. Defining a consistent model for negative influence or competition which is independent of timing components would be of interest; a promising direction could be a return to graphical models in the vein of those studied by Domingos and Richardson .",
               "In order to apply the models and algorithms of the present (or subsequent) work in the real world, it is necessary to estimate the models' parameters (such as influence probabilities in the Cascade model, or THEORY OF COMPUTING, Volume 11 (4), 2015, pp. 105-147 edge weights in the Threshold model). A string of recent papers, e. g., , begin to address the inference problem. The general approach is to use multiple instances of cascades, usually with information on the times at which nodes became active, and to perform a Maximum Likelihood or similar estimation. Thus, these inference algorithms assume that the model parameters are invariant across all observations. This is a strong assumption: in practice, the influence of one node on another will depend on the context of the product or innovation that is being recommended. Inferring parameters under weaker assumptions, such as a latent low-dimensional space on products which would explain the parameters, is a promising direction. At an even more fundamental level, a thorough validation of the present (and other) models of social influence is clearly necessary."
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 32
     },
     {
          "head": {
               "text": "ABOUT THE AUTHORS",
               "type": "conclusion"
          },
          "paragraphs": [
               "DAVID KEMPE received his Ph. D. from Cornell University in 2003, and has been on the faculty in the Computer Science Department at USC since the Fall of 2004, where he is currently an Associate Professor.",
               "His primary research interests are in computer science theory and the design and analysis of algorithms, with a particular emphasis on social networks, algorithms for feature selection, and game-theoretic and pricing questions. He is a recipient of the NSF CAREER award, the VSoE Junior Research Award, the ONR Young Investigator Award, a Sloan Fellowship, and an Okawa Fellowship, in addition to several USC mentoring awards.",
               "JON KLEINBERG is the Tisch University Professor in the Departments of Computer Science and Information Science at Cornell University. His research focuses on algorithmic issues at the interface of networks and information, with an emphasis on the social and information networks that underpin the Web and other on-line media.",
               "He is a member of the National Academy of Sciences and the National Academy of Engineering, and is the recipient of awards including a MacArthur Fellowship, the Nevanlinna Prize, the Harvey Prize, the ACM SIGKDD Innovation Award, and the ACM-Infosys Foundation Award in the Computing Sciences.",
               "?VA TARDOS is a Jacob Gould Schurman Professor of Computer Science at Cornell University. Her research interest is algorithms and algorithmic game theory, the subarea of theoretical computer science theory of designing systems and algorithms for selfish users. Her research focuses on algorithms and games on networks.",
               "She has been elected to the National Academy of Engineering, the National Academy of Sciences, is an external member of the Hungarian Academy of Sciences, and is the recipient of a number of fellowships and awards including the Packard Fellowship, the G?del Prize, Dantzig Prize, Fulkerson Prize, and the IEEE Technical Achievement Award. She was editor editor-in-chief of the SIAM Journal of is currently an editor of several other journals including the Journal of the ACM and Combinatorica.",
               "THEORY OF COMPUTING, Volume 11 (4), 2015, pp. 105-147"
          ],
          "paper_id": "22937470-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 33
     }
]