[
     {
          "head": {
               "n": "0",
               "text": "abstract",
               "type": "abstract"
          },
          "paragraphs": [
               "Officially, AI was born in 1956. Since then, very impressive progress has been made in many areas-but not in the realm of human level machine intelligence."
          ],
          "paper_id": "2223c2b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 0,
          "fromPaper": "Toward Human Level Machine Intelligence - Is It Achievable? The Need for a Paradigm Shift"
     },
     {
          "head": {
               "n": "1.",
               "text": "Introduction",
               "type": "introduction"
          },
          "paragraphs": [
               "A chievement of human level machine intelligence (HLMI) has profound implications for modern society-a society which is becoming increasingly infocentric in its quest for efficiency, convenience and enhancement of quality of life. Achievement of human level machine intelligence has long been one of the basic objectives of AI. In the fifties of last century, the question \"Can machines think?\" was an object of many spirited discussions and debates . Exaggerated expectations were the norm, with few exceptions. In an article \"Thinking machines-a new field in electrical engineering,\" published in January l950, I began with a sample of headlines of articles which appeared in the popular press in the late forties. One of them read \"Electric brain capable of translating foreign languages is being built.\" Today, more than half a century later, we have translation software, but nothing that approaches the level of human translation. In l948, on the occasion of inauguration of IBM's Mark l relay computer, Howard Aiken, Director of Harvard's Computation Laboratory, said \"There is no problem in applied mathematics that this computer cannot solve.\" Today, there is no dearth of problems which cannot be solved by any supercomputer. Exaggerated expectations should be forgiven. As Jules Verne wrote at the turn of last century, \"Scientific progress is driven by exaggerated expectations.\"",
               "Where do we stand today? What can we expect in the future? Officially, AI was born in l956. Today, half a century later, there is much that AI can be proud of-but not in the realm of human level machine intelligence. A telling benchmark is summarization. We have software that can passably summarize a class of documents but nothing that can summarize miscellaneous articles, much less books. We have humanoid robots but nothing that can compare in agility with that of a four year old child. We can automate driving a car in very light city traffic but there is nothing on the horizon that could automate driving in Cairo. Far too often, we have to struggle with a dumb automated customer service system which we are forced to use. Such experiences make us keenly aware that human level machine intelligence is an objective rather than reality. The Turing Test lies far beyond. What should be noted, however, is that authoritative views within the AI community tend to be substantially more optimistic. Representative views can be found in ( .",
               "In an article \"A new direction in AI-toward a computational theory of perceptions,\" AI Magazine, 200l, I argued that, in large measure, the lack of significant progress in many realms of human level machine intelligence is attributable to AI's failure to develop a machinery for dealing with perceptions. Underlying human level machine intelligence are two remarkable human capabilities. First, the capability to perform a wide variety of physical and mental tasks, such as driving a car in heavy city traffic, without any measurements and any computations. And second, the capability to reason, converse and make rational decisions in an environment of imprecision, uncertainty, incompleteness of information, partiality of truth and partiality of possibility. A principal objective of human level intelligence is mechanization of these remarkable human capabilities.",
               "What is widely unrecognized within the AI community is that mechanization of these capabilities is beyond the reach of methods based on classical, Aristotelian, bivalent logic and bivalent-logic-based probability theory. In short, if the question is: Can human level machine intelligence be achieved through the use of methods based on bivalent logic and bivalent-logic-based probability theory, then in my view the answer is: No. If the question is: Can human level machine intelligence be achieved sometime in the future, then my answer is: Possibly, but the challenge will be hard to meet. Extensions of existing techniques will not be sufficient. Basically, what is needed is a paradigm shift. More specifically, what is needed is an addition to the armamentarium of AI of two methodologies: (a) a nontraditional methodology of computing with words (CW) or, more generally, NL-Computation; and (b) a countertraditional methodology which involves a progression from computing with numbers to computing with words. The centerpiece of these methodologies is the concept of precisiation of meaning-a concept drawn from fuzzy logic.",
               "What is fuzzy logic? What does it have to offer? There are many misconceptions about fuzzy logic. The following prcis of fuzzy logic is intended to correct the misconceptions.",
               "Fuzzy logic is not fuzzy. Basically, fuzzy logic is a precise logic of imprecision and approximate reasoning. In fact, fuzzy logic is much more than a logical system. It has many facets. The principal facets are logical, fuzzy-set-theoretic, epistemic and relational . Most of the applications of fuzzy logic involve the concept of a linguistic variable and the machinery of fuzzy if-then rules. The formalism of linguistic variables and fuzzy if-then rules is associated with the relational facet. The cornerstones of fuzzy logic are graduation, granulation, precisiation and the concept of a generalized constraint ( ). Graduation should be understood as an association of a concept with grades or degrees.",
               "In fuzzy logic, everything is or is allowed to be a matter of degree or, equivalently, fuzzy. Furthermore, in fuzzy logic everything is or is allowed to be granulated, with a granule being a clump of attribute values drawn together by indistinguishability, equivalence, proximity or functionality. Graduated granulation or, equivalently, fuzzy granulation is inspired by what humans employ to deal with complexity, imprecision and uncertainty. Graduated granulation underlies the concept of a linguistic variable. When Age, for example, is treated as a linguistic variable, its granular values may be young, middleaged and old. The granular values of Age are labels of fuzzy sets. Informally, a fuzzy set is a class with unsharp boundary. A fuzzy set is defined by its membership function. A trapezoidal membership which defines middle-age is shown in .",
               "A concept which plays a pivotal role in fuzzy logic is that of a generalized constraint, represented as X isr R, where X is the constrained variable, R is the constraining relation and r is an indexical variable which defines the modality of the constraint, that is, its semantics. The principal generalized constraints are possibilistic, probabilistic and veristic. The fundamental thesis of fuzzy logic is that information may be represented as a generalized constraint. A consequence of the fundamental thesis is that the meaning of a proposition, p, may likewise be represented as a generalized constraint. The concept of a generalized constraint serves as a basis for representation of and computation with propositions drawn from a natural language. This is the province of NL-Computation/Computing with Words-computation with information described in natural language.",
               "NL-Computation opens the door to achievement of human level machine intelligence. The validity of this assertion rests on two basic facts. First, much of human knowledge, and especially world knowledge, is described in natural language. And second, a natural language is basically a system for describing perceptions. What this implies is that NL-Computation serves two major functions: (a) providing a conceptual framework and techniques for precisiation of natural language in the context of human level machine intelligence; and (b) providing a capability to compute with natural language descriptions of perceptions. These capabilities play essential roles in progression toward human level machine intelligence.",
               "Human level machine intelligence has many components. The principal components are shown in . Basically, achievement of human level machine intelligence requires a mechanization of the components of HLMI. Among the principal components of HLMI the component which stands out in importance involves mechanization of natural language understanding. A prerequisite to mechanization of natural ? Imprecision of Meaning = Elasticity of Meaning    language understanding is precisiation of meaning. Humans can understand unprecisiated natural language but machines cannot ( ). What has been widely unrecognized is that in the final analysis, progress toward achievement of human level machine intelligence requires a resol ution of a critical problem-the problem of precisiation of meaning. The two cornerstones of fuzzy logic-precisiation of meaning and the concept of a generalized constraint-are of direct relevance to human level machine intelligence. The primary purpose of this paper is to bring these concepts to the attention of the Computational Intelligence community. The exposition which follows is based on ) and . Additional details may be found in these papers."
          ],
          "paper_id": "2223c2b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 1,
          "fromPaper": "Toward Human Level Machine Intelligence - Is It Achievable? The Need for a Paradigm Shift"
     },
     {
          "head": {
               "text": "Precisiation Language p: Object of Precisiation",
               "type": "introduction"
          },
          "paragraphs": [
               "Semantic imprecision of natural languages is a very basic characteristic-a characteristic which is rooted in imprecision of perceptions. Basically, a natural language is a system for describing perceptions. Perceptions are imprecise. Imprecision of perceptions entails semantic imprecision of natural languages.",
               "The concept of precisiation has few precursors in the literature of logic, probability theory and philosophy of languages ). The reason is that the conceptual structure of bivalent logic-on which the literature is based-is much too limited to allow a full development of the concept of precisiation. In HLMI what is used for this purpose is the conceptual structure of fuzzy logic.",
               "Precisiation and precision have many facets. More specifically, it is expedient to consider what may be labeled-precisiation, withbeing an indexical variable whose values identify various modalities of precisiation. In particular, it is important to differentiate between precision in value (-precision) and precision in meaning (m-precision). For example, proposition X = 5 is-precise and m-precise, but proposition 2X6, is-imprecise and m-precise. Similarly, proposition \"X is a normally distributed random variable with mean 5 and variance 2,\" is-imprecise and mprecise. Some of the basic concepts relating to precisiation are defined in .",
               "A further differentiation applies to m-precisiation. Thus, mh-precisiation is human-oriented meaning precisiation, while it mm-precisiation is machine-oriented or, equivalently, mathematically-based meaning precisiation ( ). A dictionary definition may be viewed as a form of mh-precisiation, while a mathematical definition of a concept, e.g., stability, is mm-precisiation whose result is mm-precisiand of stability.",
               "So far as imprecisiation is concerned, it may be forced or deliberate. Imprecisiation is forced when a precise value of a variable is not known. Imprecisiation is deliberate when a precise value is not needed and precision carries a cost. Familiar examples of deliberate imprecisiation are data compression and summarization. For convenience, the precisiand and imprecisiand of p are denoted as p * and *"
          ],
          "paper_id": "2223c2b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 2,
          "fromPaper": "Toward Human Level Machine Intelligence - Is It Achievable? The Need for a Paradigm Shift"
     },
     {
          "head": {
               "n": "2.",
               "text": "The Concept of Precisiation",
               "type": "introduction"
          },
          "paragraphs": [
               "In one form or another, precisiation of meaning has always played an important role in science. Mathematics is a quintessential example of what may be called a meaning precisiation language. Precisiation of meaning has direct relevance to mechanization of natural language understanding. For this reason, precisiation of meaning is an issue that is certain to grow in visibility and importance as we move further into the age of machine intelligence and automated reasoning.",
               "p, respectively. Note: Deliberate imprecisiation plays an important role in many applications of fuzzy logic especially in the realm of consumer products where cost is an important consideration. In such applications, what is employed is what is referred to as \"The fuzzy logic gambit\" ). In the fuzzy logic gambit deliberate v-imprecisiation is followed by mm-precisiation.",
               "A more general illustration of mm-precisiation relates to representation of a function as a collection of fuzzy if-then rules-a mode of representation which is widely used in practical applications of fuzzy logic . More specifically, let f be a function from reals to reals which is represented as .",
               "where small, medium and large are labels of fuzzy sets. In this representation, the collection in question may be viewed as mhprecisiand of f . When the collection is interpreted as a fuzzy graph ( ) representation of f assumes the form.",
               "which is a disjunction of Cartesian products of small, medium and large. This representation is mm-precisiand of f .",
               "In general, a precisiend may have many precisiands. As an illustration, consider the proposition \"X is approximately a,\" or \"X is * a\" for short, where a is a real number. How can \"X is * a\" be precisiated? The simplest precisiand of \"X is * a\" is \"X = a,\" [Figures 10(a) and 10(b)]. This mode of precisiation is referred to as sprecisiation, with s standing for singular. This is a mode of precisiation which is widely used in science and especially in probability theory. In the latter case, most real-world probabilities are not known exactly but in practice are frequently computed with as if they are exact numbers. For example, if the probability of an event is stated to be 0.7, then it should be understood that 0.7 is actually * 0.7, that is, approximately 0.7. A standard practice is to treat * 0.7 as 0.7000, that is, as an exact number.",
               "Next in simplicity is representation of * a is",
               "In dealing with meaning, it is necessary to differentiate between this intension or, equivalently, the intensional meaning, i-meaning, of p, and the extension, or, equivalently, the extensional, e-meaning of p. The concepts of extension and intension are drawn from logic and, more particularly, from modal logic and possible world semantics ( ). Basically, e -meaning is attribute-free and i-meaning is attribute-based. As a simple illustration, if A is a finite set in a universe of discourse, U , then the e-meaning of A, that is, its extension is the list of elements of A, {u 1 ,, u n }, u i being the name of ith element of A, with no attributes associated with the u i . Let a(u i ) be an attribute-vector associated with each u i . Then the intension of A is a recognition algorithm which, given a(u i ), recognizes whether u i is or is not an element of A. If A is a fuzzy set with membership functionA then the e-meaning and i-meaning of A may be expressed compactly as",
               "whereA (u)/u means thatA (u) is the grade of membership of u i in A; and an interval centering on a. This mode of precisiation is referred to c g-precisiation, with c g standing for crisp-granular. Next is f g-precisiation of * a, with the precisiand being a fuzzy interval centering on a. Next is p-precisiation of * a, with the precisiand being a probability distribution centering on a. And so on.",
               "An analogy is helpful in understanding the relationship between a precisiend and its precisiands. More specifically, a mm-precisiand, p * , may be viewed as a model of precisiend, p, in the same sense as a differential equation may be viewed as a model of a physical system.",
               "In the context of modeling, an important characteristic of a model is its \"closeness of fit.\" In the context of NLComputation, an analogous concept is that of cointension. The concept is discussed in the following. Precisiation is a prerequisite to computation with information described in natural language. To be useful, precisiation of a precisiend, p, should result in a precisiand, p * , whose meaning, in some specified sense, should be close to that of p. Basically, cointension of p * and p is the degree to which the meaning of p  ",
               "with the understanding that in the i-meaning of A the membership function,A is defined on the attribute space. It should be noted that when A is defined through exemplification, it is said to be defined ostensively. Thus, o-meaning of A consists of exemplars of A. An ostensive definition may be viewed as a special case of extensional definition. A neural network may be viewed as a system which derives i-meaning from o-meaning.",
               "Clearly, i-meaning is more informative than e-meaning. For this reason, cointension is defined in terms of intensions rather than extensions of precisiend and precisiand. Thus, meaning will be understood to be i-meaning, unless stated to the contrary. However, when the precisiend is a concept which plays the role of definiendum and we know its extension but not its intension, cointension has to involve the extension of the definiendum (precisiend) and the intension of the definiens .",
               "As an illustration, let p be the concept of bear market. A dictionary definition of p-which may be viewed as a mh-precisiand of preads \"A prolonged period in which investment prices fall, accompanied by widespread pessimism.\" A widely accepted quantitative definition of bear market is: We classify a bear market as a 30 percent decline after 50 days, or a 13 percent decline after 145 days. (Shuster) This definition may be viewed as a mm-precisiand of bear market. Clearly, the quantitative definition, p * , is not a good fit to the perception of the meaning of bear market which is the basis for the dictionary definition. In this sense, the quantitative definition of bear market is not cointensive.",
               "Intensions are more informative than extensions in the sense that more can be inferred from propositions whose meaning is expressed intensionally rather than extensionally. The assertion will be precisiated at a later point. For the present, a simple example will suffice.",
               "Consider the proposition p: Most Swedes are tall. Let U be a population of n Swedes, U = (u 1 , . . . , u n ), u 1 = name of ith Swede.",
               "A precisiand of p may be represented as",
               "where most is a fuzzy quantifier which is defined as a fuzzy subset of the unit interval (Zadeh 1983). Lettall (u i ), i = 1, . . . , n) be the grade of membership of u i in the fuzzy set of tall Swedes. Then the e-meaning of tall Swedes may be expressed in symbolic form as"
          ],
          "paper_id": "2223c2b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 3,
          "fromPaper": "Toward Human Level Machine Intelligence - Is It Achievable? The Need for a Paradigm Shift"
     },
     {
          "head": {
               "text": "Human-Machine Communication",
               "type": "introduction"
          },
          "paragraphs": [
               "Accordingly, the i-precisiand of p may be expressed as 1 n (tall (u 1 )) ++tall (u n ) is most.",
               "? mm-precisiation is desirable but not mandatory.",
               "? mm-precisiation is mandatory.",
               "Similarly, the i-precisiand of p may be represented as 1",
               "? Humans can understand unprecisiated natural language. Machines cannot.",
               "As will be seen later, given the e-precisiend of p we can compute the answer to the query: How many Swedes are not tall?",
               "The answer is 1-most. However, we cannot compute the Scientific Progress",
               "? mh-Precisiation mm-Precisiation answer to the query: How many Swedes are short? The same applies to the query: What is the average height of Swedes? As will be shown later, the answers to these queries can be computed given the i-precisiand of p. The concept of cointensive precisiation has important implications for the way in which scientific concepts are defined. The standard practice is to define a concept within the conceptual structure of bivalent logic, leading to a bivalent definition under which the universe of discourse is partitioned into two classes: objects which fit the concept and those which do not, with no shades of gray allowed. Such definition is valid when the concept that is defined, the definiendum, is crisp, that is, bivalent. The problem is that in reality most scientific concepts are fuzzy, that is, are a matter of degree. Familiar examples are the concepts of causality, relevance, stability, independence and bear market. In general, when the definiendum (precisiend) is a fuzzy concept, the definiens (precisiand) is not cointensive, which is the case with the bivalent definition of bear market. More generally, bivalent definitions of fuzzy concepts are vulnerable to the Sorites (heap) paradox . between humans, HHC, precisiation of meaning is desirable but not mandatory. In communication between a human and machine, HMC, precisiation is mandatory because a machine cannot understand unprecisiated natural language. In the case of HMC, an important issue relates to whether the precisiator is the sender (human, s-precisiation) or the recipient (machine, rprecisiation) . In most applications of fuzzy logic, the precisiator is the sender (human), an example is the Honda fuzzy logic transmission ( . In the case of s-precisiation, context-dependence is not a problem. As a consequence, precisiation is a much simpler function than it is in the case of rprecisiation. It should be noted that mechanization of natural language understanding involves for the most part r-precisiation.",
               "In HHC, mm-precisiation is a major application area for generalized-constraint-based semantics . Generalized-constraint-based semantics provides a basis for reformulation of bivalent-logic-based definitions of scientific concepts, associating Richter-like scales with concepts which are traditionally defined as bivalent concepts but in reality are fuzzy concepts. Examples: recession, civil war, arthritis, randomness, causality."
          ],
          "paper_id": "2223c2b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 4,
          "fromPaper": "Toward Human Level Machine Intelligence - Is It Achievable? The Need for a Paradigm Shift"
     },
     {
          "head": {
               "n": "5.",
               "text": "The Concept of a Generalized Constraint",
               "type": "introduction"
          },
          "paragraphs": [
               "In fuzzy logic, a proposition, p, is viewed as an answer to a question, q, of the form \"What is the value of X ?\" Thus p is a carrier of information about X . In this perspective, the meaning of p, M (p), is the information which p carries about X . An important consequence of the fundamental thesis of fuzzy logic is what is referred to as the meaning postulate. In symbolic form, the postulate is expressed as M (p) = GC(X (p)), where GC(X (p)) is a generalized constraint on the variable which is constrained by p. In plain words, the meaning postulate assents that the meaning of a proposition may be represented as a generalized constraint. It is this postulate that makes the concept of a generalized constraint a cornerstone of fuzzy logic. By providing a mechanism for precisiating the meaning of a proposition, the concept of a generalized constraint opens the door to a wide-ranging enlargement of the role of natural languages in scientific theories. What is referred to as PNL, Precisiated Natural Language, serves this purpose ).",
               "A point which should be noted is that the question to which p is an answer is not uniquely determined by p; hence X (p) is not uniquely defined by p. Generally, however, among the possible questions there is one which is most likely. For example, if p is \"Monika is young,\" then the most likely question is \"How old is Monika?\" In this example, X is Age(Monika).",
               "The concept of precisiation has a direct bearing on communication in the context of human level machine intelligence ( ? Sender: (a) I will be pleased to do so (s-precisiation) (b) sorry, it is your problem (r-precisiation) C is the set of values which X is allowed to take. A typical constraint is hard (inelastic) in the sense that if u is a value of X then u satisfies the constraint if and only if uC . The problem with hard constraints is that most real-world constraints are not hard, meaning that most real-world constraints have some degree of elasticity. For example, the constraints \"check-out time is 1 pm,\" and \"speed limit is 100 kmh,\" are, in reality, not hard. How can such constraints be defined? The concept of a generalized constraint is motivated by questions of this kind .",
               "Real-world constraints may assume a variety of forms. They may be simple in appearance and yet have a complex structure. Reflecting this reality, a generalized constraint, GC (X ), is defined as an expression of the form.",
               "? X is an n-ary variable, X = (X 1 , . . . , X n ) ? X is a proposition, e.g., X = Leslie is tall ? X is a function ? X is a function of another variable, X = f (Y ) ? X is conditioned on another variable, X /Y ? X has a structure, e.g., X = Location(Residence(Carol)) ? X is a group variable. In this case, there is a group, G ;",
               "with each member of the group, Name i , i = 1, . . . , n, associated with an attribute-value, A i . A i may be vectorvalued. Symbolically",
               "where X is the constrained variable; R is a constraining relation which, in general, is non-bivalent; and r is an indexing variable which identifies the modality of the constraint, that is, its semantics. The constrained variable, X , may assume a variety of forms. In particular,",
               "A generalized constraint is associated with a test-score function, ts(u), which associates with each object, u, to which the constraint is applicable, the degree to which u satisfies the constraint. Usually, ts(u) is a point in the unit interval. However, if necessary, the test-score may be a vector, an element of a semiring , an element of a lattice or, more generally, an element of a partially ordered set, or a bimodal distribution-a constraint which will be described later. The test-score function defines the semantics of the constraint with which it is associated.",
               "The constraining relation, R, is, or is allowed to be, nonbivalent (fuzzy). The principal modalities of generalized constraints are summarized in the following. X is small.",
               "In this case, the fuzzy set labeled small is the possibility distribution of X (Zadeh 1978; . Ifsmall is the membership function of small, then the semantics of \"X is small\" is defined by",
               "where u is a generic value of X . with R playing the role of the probability distribution of X . For example.",
               "means that X is a normally distributed random variable with mean m and variance2 . If X is a random variable which takes values in a finite set {u 1 , . . . , u n } with respective probabilities p 1 , . . . , p n , then X may be expressed symbolically as",
               "where?1 R is inverse of the membership function of R, and t is a fuzzy truth value which is a subset of , .",
               "with the semantics X isu R.",
               "What is important to note is that in fuzzy logic a probabilistic constraint is viewed as an instance of a generalized constraint.",
               "When X is a generalized constraint, the expression",
               "The usuality constraint presupposes that X is a random variable, and that probability of the event {X isu R} is usually, where usually plays the role of a fuzzy probability which is a fuzzy number . For example.",
               "X isu small X isp R means that \"usually X is small\" or, equivalently, is interpreted as a probability qualification of X , with R being Prob{X is small} is usually. the probability of X , . For example.",
               "(X is small) isp likely, where small is a fuzzy subset of the real line, means that probability of the fuzzy event {X is small} is likely. More specifically, if X takes values in the interval and g is the probability density function of X , then the probability of the fuzzy event \"X is small\" may be expressed as In this expression, small may be interpreted as the usual value of X . The concept of a usual value has the potential of playing a significant role in decision analysis, since it is more informative than the concept of an expected value.",
               "This expression for test-score function defines the semantics of probability qualification of a possibilistic constraint.",
               "X is a fuzzy-set-valued random variable and R is a fuzzy random set (f) Fuzzy-graph ( r = f q)",
               "In",
               "X is a function, f , and R is a fuzzy graph ( which constrains f . A fuzzy graph is a disjunction of Cartesian granules expressed as where R plays the role of a verity (truth) distribution of X . In particular, if X takes values in a finite set {u 1 , . . . , u n } with respective verity (truth) values t 1 , . . . , t n , then X may be expressed as",
               "where the A i and B i , i = 1,, n, are fuzzy subsets of the real line, andis the Cartesian product. A fuzzy graph is frequently described as a collection of fuzzy if-then rules ( .",
               "meaning that Ve r(X = u i ) = t i , i = 1, . . . , n For example, if Robert is half German, quarter French and quarter Italian, then",
               "The concept of a fuzzy-graph constraint plays an important role in applications of fuzzy logic ( .",
               "When X is a generalized constraint, the expression X isv R"
          ],
          "paper_id": "2223c2b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 5,
          "fromPaper": "Toward Human Level Machine Intelligence - Is It Achievable? The Need for a Paradigm Shift"
     },
     {
          "head": {
               "n": "7.",
               "text": "The Concept of Bimodal Constraint/Distribution",
               "type": "introduction"
          },
          "paragraphs": [
               "In the bimodal constraint, is interpreted as verity (truth) qualification of X . For example, X isbm R, (X is small) isv very.true, R is a bimodal distribution of the form should be interpreted as \"It is very true that X is small.\" The semantics of truth qualification is defined by )",
               "with the understanding that Prob(X is A i ) is P i . , that is, P i is a granular value of Prob (X is A i ), i = 1, . . . , n. (See next section for definition of granular value).",
               "To clarify the meaning of a bimodal distribution it is expedient to start with an example. I am considering buying Ford stock. I ask my stockbroker, \"What is your perception of the near-term prospects for Ford stock?\" He tells me, \"A moderate decline is very likely; a steep decline is unlikely; and a moderate gain is not likely.\" My question is: What is the probability of a large gain?",
               "Information provided by my stock broker may be represented as a collection of ordered pairs: ? Price: (unlikely, steep.decline), (very.likely, moderate. decline), (not.likely, moderate.gain)) In this collection, the second element of an ordered pair is a fuzzy event or, generally, a possibility distribution, and the first element is a fuzzy probability. The expression for Price is an example of a bimodal distribution.",
               "The importance of the concept of a bimodal distribution derives from the fact that in the context of human-centric systems, most probability distributions are bimodal. Bimodal distributions can assume a variety of forms. The principal types are Type 1, Type 2 and Type 3 . Type 1, 2 and 3 bimodal distributions have a common framework but differ in important detail ). A bimodal distribution may be viewed as an important generalization of standard probability distribution. For this reason, bimodal distributions of Type 1, 2, 3 are discussed in greater detail in the following. ? Type 1 (default):",
               "X is a random variable taking values in U A 1 , . . . ,A n , A are events (fuzzy sets) p i = Prob(X is A i ), i = 1, . . . , n i p i is unconstrained P i = granular value of P i BD: bimodal distribution: ((P 1 , A 1 ),, (P n , A n )) or, equivalently,",
               "Problem: What is the granular probability, P, of A? In general, this probability is fuzzy-set-valued. ? Type 2 (fuzzy random set): X is a fuzzy-set-valued random variable with values A 1 ,, A n (fuzzy sets) P i = Prob(X = A i ), i = 1, . . . , n P i : granular value of P i BD:",
               "X isrs (P 1 \\A 1 ++ P n \\A n ) i p i = 1 Problem: What is the granular probability, P, of A? P is not definable. What are definable are (a) the expected value of the conditional possibility of A given BD, and (b) the expected value of the conditional necessity of A given BD ? Type 3 (Dempster-Shafer) :",
               "X is a random variable taking values X 1 , . . . , X n with probabilities p 1 , . . . , p n X i is a random variable taking values in",
               "Because probability distributions of the X i in the A i are not specified, p is interval-valued. What is important to note is that the concepts of upper and lower probabilities break down when the A i are fuzzy sets (Zadeh 1979a). Note: In applying Dempster-Shafer theory it is important to check on whether the data fit Type 3 model. In many cases, the correct model is Type 1 rather than Type 3.",
               "The importance of bimodal distributions derives from the fact that in many realistic settings a bimodal distribution is the best approximation to our state of knowledge. An example is assessment of degree of relevance, since relevance is generally not well defined. If I am asked to assess the degree of relevance of a book on knowledge representation to summarization, my state of knowledge about the book may not be sufficient to justify an answer such as 0.7. A better approximation to my state of knowledge may be \"likely to be high.\" Such an answer is an instance of a bimodal distribution. A special case of primary constraints is what may be called standard constraints: bivalent possibilistic, probabilistic and bivalent veristic. Standard constraints form the basis for the conceptual framework of bivalent logic and probability theory.",
               "A generalized constraint is composite if it can be generated from other generalized constraints through conjunction, and/or projection and/or constraint propagation and/or qualification and/or possibly other operations. For example, a random-set constraint may be viewed as a conjunction of a probabilistic constraint and either a possibilistic or veristic constraint. The DempsterShafer theory of evidence is, in effect, a theory of possibilistic random-set constraints. The derivation graph of a composite constraint defines how it can be derived from primary constraints.",
               "The three primary constraints-possibilistic, probabilistic and veristic-are closely related to a concept which has a position of centrality in human cognition-the concept of partiality. In the sense used here, partial means: a matter of degree or, more or less equivalently, fuzzy. In this sense, almost all human concepts are partial (fuzzy). Familiar examples of fuzzy concepts are: knowledge, understanding, friendship, love, beauty, intelligence, belief, causality, relevance, honesty, mountain and, most important, truth, likelihood and possibility. Is a specified concept, C , fuzzy? A simple test is: If C can be hedged, then it is fuzzy. For example, in the case of relevance, we can say: very relevant, quite relevant, slightly relevant, etc. Consequently, relevance is a fuzzy concept.",
               "The three primary constraints may be likened to the three primary colors: red, blue and green. In terms of this analogy, existing theories of uncertainty may be viewed as theories of different mixtures of primary constraints. For example, the Dempster-Shafer theory of evidence is a theory of a mixture of probabilistic and possibilistic constraints. The Generalized Theory of Uncertainty (GTU)  A very simple example of a semantic rule is:",
               "where u and v are generic values of X , Y ; andA andB are the membership functions of A and B, respectively. In principle, GCL is an infinite set. However, in most applications only a small subset of GCL is likely to be needed.",
               "A key idea which underlies NL-Computation is embodied in the meaning postulate-a postulate which asserts that the meaning of a proposition, p, drawn from a natural language is representable as a generalized constraint. In the context of GCL, the meaning postulate asserts that p may be precisiated through translation into GCL. Transparency of translation may be enhanced through annotation. Simple example of annotation, Monika is young ?X /Age (Monika) is R/young."
          ],
          "paper_id": "2223c2b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 6,
          "fromPaper": "Toward Human Level Machine Intelligence - Is It Achievable? The Need for a Paradigm Shift"
     },
     {
          "head": {
               "n": "9.",
               "text": "The Generalized Constraint Language and Standard Constraint Language",
               "type": "introduction"
          },
          "paragraphs": [
               "A concept which has a position of centrality in GTU is that of Generalized Constraint Language (GCL). Informally, GCL is the set of all generalized constraints together with the rules governing syntax, semantics and generation. Simple examples of elements of GCL are:",
               "whereis conjunction.",
               "In fuzzy logic, the set of all standard constraints together with the rules governing syntax, semantics and generation constitute the Standard Constraint Language (SCL). SCL is a subset of GCL.",
               "In previous sections, we have employed the concept of a granular value in an informal fashion, without formulating a definition. The concept of a generalized constraint makes it possible to define a granular value more precisely. Let X be a variable taking values in a universe of discourse U, U = {u}. If a is an element of U , and it is known that the value of X is a, then a is referred to as a singular value of X . If there is some uncertainty about the value of X , the available information induces a restriction on the possible values of X which may be represented as a generalized constraint GC(X ), X isr R. Thus a generalized constraint defines a granule which is referred to as a granular value of X , G r(X ) ). For example, if is known to lie in the interval [a h], then is a granular value of X . Similarly, if X isp N (m,2 ), then N (m,2 ) is a granular value of X . What is important to note is that defining a granular value in terms of a generalized constraint makes a granular value mm-precise. It is this characteristic of granular values that underlies the concept of a linguistic variable . Symbolically, representing a granular value as a generalized constraint may be expressed as Gr(X ) = GC(X ). It should be noted that, in general, perception-based information is granular .",
               "The importance of the concept of a granular value derives from the fact that it plays a central role in computation with information described in natural language. More specifically, when a proposition expressed in a natural language is represented as a system of generalized constraints, it is, in effect, a system of granular values. Thus, computation with information described in natural language ultimately reduces to computation with granular values. Such computation is the province of Granular Computing . R. Kurzweil, Singularity is Near. Viking Press, New There are many reasons why achievement of human level machine intelligence is a challenge that is hard to meet. One of the principal reasons is the need for mechanization of two remarkable human capabilities. First, the capability to converse, communicate, reason and make rational decisions in an environment of imprecision, uncertainty, incompleteness of information, partiality of truth and partiality of possibility. And second, the capability to perform a wide variety of physical and mental tasks-such as driving a car in heavy city traffic-without any measurements and any computations. What is well understood is that a prerequisite to mechanization of these capabilities is mechanization of natural language understanding. But what is widely unrecognized is that mechanization of natural language understanding is beyond the reach of methods based on bivalent logic and bivalent-logic-based probability theory. In addition, what is widely unrecognized is that mechanization of natural language understanding is contingent on precisiation of meaning."
          ],
          "paper_id": "2223c2b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 7,
          "fromPaper": "Toward Human Level Machine Intelligence - Is It Achievable? The Need for a Paradigm Shift"
     },
     {
          "head": {
               "n": "10.",
               "text": "Concluding Remarks",
               "type": "introduction"
          },
          "paragraphs": [
               "Humans can understand unprecisiated natural language but machines cannot. Natural languages are intrinsically imprecise. Basically, a natural language is a system for describing perceptions. Perceptions are intrinsically imprecise. Imprecision of natural languages is rooted in imprecision of perceptions.",
               "The principal thesis of this paper is that to address the problem of precisiation of meaning it is necessary to employ the machinery of fuzzy logic. In addition, the machinery of fuzzy logic is needed for mechanization of human reasoning. In this perspective, fuzzy logic is of direct relevance to achievement of human level machine intelligence. The cornerstones of fuzzy logic are the concepts of graduation, granulation, precisiation and generalized constraint."
          ],
          "paper_id": "2223c2b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 8,
          "fromPaper": "Toward Human Level Machine Intelligence - Is It Achievable? The Need for a Paradigm Shift"
     }
]