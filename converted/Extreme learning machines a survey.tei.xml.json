[
     {
          "head": {
               "n": "0",
               "text": "abstract",
               "type": "abstract"
          },
          "paragraphs": [
               "Computational intelligence techniques have been used in wide applications. Out of numerous computational intelligence techniques, neural networks and support vector machines (SVMs) have been playing the dominant roles. However, it is known that both neural networks and SVMs face some challenging issues such as: (1) slow learning speed, (2) trivial human intervene, and/or (3) poor computational scalability. Extreme learning machine (ELM) as emergent technology which overcomes some challenges faced by other techniques has recently attracted the attention from more and more researchers. ELM works for generalized single-hidden layer feedfor-ward networks (SLFNs). The essence of ELM is that the hidden layer of SLFNs need not be tuned. Compared with those traditional computational intelligence techniques, ELM provides better generalization performance at a much faster learning speed and with least human intervene. This paper gives a survey on ELM and its variants, especially on (1) batch learning mode of ELM, (2) fully complex ELM, (3) online sequential ELM, (4) incremental ELM, and (5) ensemble of ELM."
          ],
          "paper_id": "2209d210-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 0,
          "fromPaper": "Extreme learning machines: a survey"
     },
     {
          "head": {
               "n": "1",
               "text": "Introduction",
               "type": "introduction"
          },
          "paragraphs": [
               "There exist many types of neural networks, however, feedforward neural networks may be one of the most popular neural networks. A feedforward neural network consists of one input layer receiving the stimuli from external environments, one or multi-hidden layers, and one output layer sending the network output to external environments. Three main approaches are usually used in training feedforward networks:",
               "1. Gradient-descent based (e.g. backpropagation (BP) method for multi-layer feedforward neural networks). Additive type of hidden nodes are most often used in such networks. For additive hidden node with the activation function g?x? : R ! R (e.g. sigmoid:",
               "g?x? ? 1=?1 ? exp??x??), the output function of the ith node in the lth hidden layer is given by ",
               "i and x ?l? : Gradient-descent based learning algorithms usually run much slower than expected. 2. Standard optimization method based (e.g. support vector machines, SVMs , for a specific type of SLFNs, the so-called support vector network). Rosenblatt investigated perceptrons (multi-layer feedforward neural networks) half a century ago. Rosenblatt suggested a learning mechanism where only the weights of the connections from the last hidden layer to the output layer were adjusted. After all the rest weights fixed the input data are actually transformed into a feature space Z of the last hidden layer (cf. ). In this feature space a linear decision function is constructed:",
               "where b i is the output weight between the output node and the ith neuron in the last hidden layer of a perceptron, and z i ?x? is the output of the ith neuron in the last hidden layer of the perceptron. In order to find an alternative solution of z i ?x?; in 1995 Cortes and Vapnik proposed the SVM which maps the data from the input space to some high dimensional feature space Z through some nonlinear mapping chosen a priori. Optimization methods are used to find the separating hyperplane which maximizes the separating margins of two different classes in the feature space. 3. Least-square based (e.g. radial basis function (RBF) network learning ). For RBF hidden node with activation function g?x? : R ! R (e.g. Gaussian:",
               "g?x? ? exp??x 2 ?; G?a i ; b i ; x? is given by where a i and b i are the center and impact factor of the ith RBF hidden node. R ? indicates the set of all positive real values. The RBF network is a special case of SLFNs with RBF nodes in its hidden layer (cf. ). Each RBF node has its own centroid and impact factor, and its output is given by a radially symmetric function of the distance between the input and the center. In Lowe's RBF network implementation , the centers a i of RBF hidden nodes can be randomly selected from the training data or from the region of training data instead of tuning, and all the impact factors b i of RBF hidden nodes are usually set with the same value (p. 173 of ). After RBF hidden nodes parameters ?a i ; b i ? fixed, the output weight vector b i linking the ith RBF hidden node to the output layer becomes the only unknown parameter which can be resolved by least-square method. Single-hidden layer feedforward network",
               "Extreme learning machines (ELMs) were originally developed for the SLFNs and then extended to the ''generalized'' SLFNs. Such generalized SLFNs need not be neuron alike . The essence of ELM is that: different from the common understanding of learning, the hidden layer of SLFNs need not be tuned. One of the typical implementation of ELMs is to apply random computational nodes in the hidden layer, which may be independent of the training data. Different from traditional learning algorithms for neural networks ELM not only tends to reach the smallest training error but also the smallest norm of output weights. According to the neural network theory , for feedforward neural networks reaching smaller training error the smaller the norm of weights is, the better generalization performance the networks tend to have. Since in ELM the hidden layer need not be tuned and the hidden layer parameters can be fixed, the output weights can then be resolved using the lease-square method."
          ],
          "paper_id": "2209d210-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 1,
          "fromPaper": "Extreme learning machines: a survey"
     },
     {
          "head": {
               "n": "2",
               "text": "Learning theories of ELMs",
               "type": "introduction"
          },
          "paragraphs": [
               "The interpolation capability and universal approximation capability of ELMs have been investigated in Huang et al. .The output function of SLFNs with L hidden nodes can be represented by",
               "independent of the training samples . One of the typical implementation of ELMs is that the hidden node parameters ?a i ; b i ? of ELM can be randomly generated. The learning capability of extreme learning machines have been studied in two aspects: interpolation capability and universal approximation capability .",
               "where g i denotes the output function G?a i ; b i ; x? of the ith hidden node. For additive nodes with activation function g, g i is defined as 2.1 Interpolation theorem",
               "For N arbitrary distinct samples",
               "SLFNs with L hidden nodes are mathematically modeled as and for RBF nodes with activation function g, g i is defined as",
               "In the past two decades, the interpolation and universal approximation capabilities of SLFNs have been investigated thoroughly. It was proved that N arbitrary distinct samples can be learned precisely by SLFNs with N threshold hidden nodes. Further study gave a more complete answer on the interpolation capability of SLFNs and proved that an SLFN with at most N hidden nodes and with any arbitrary bounded nonlinear activation function which has a limit at one infinity can learn any N arbitrary distinct samples with zero error. Such activation functions include the threshold, ramp and sigmoid functions as well as the radial basis, ''cosine squasher'' and many nonregular functions. Many researchers have rigorously proved in theory that given activation function g(x) satisfying certain mild conditions there exists a sequence of network functions {f L } approximating to any given continuous target function f with any expected learning error [ 0: In all these conventional neural network theories, all the parameters in any f L of the network sequence (e.g. the hidden layer parameters ?a i ; b i ? and the output weights b i ) are required freely adjustable. According to these conventional neural network theories, hidden layer parameters ?a i ; b i ? need to be tuned properly and appropriate values of network parameters (e.g. ?a i ; b i ? and b i ) need to be found for any given target function f. To minimize the effort spent on adjusting hidden layer parameters ?a i ; b i ? has been tried in the past two decades. Instead of adjusting all the parameters of hidden layers in all f L of the network sequence, some researchers suggested incremental methods for SLFNs which adjust the parameters of newly added hidden nodes and then fix them after tuning. The parameters of the existing hidden nodes will remain fixed and never be updated in the further learning procedure. Hidden layer parameters in those conventional learning models need to be adjusted at least once based on the training samples. In contrast, all the parameters of the hidden layer in the ELMs need not be tuned and can be That SLFNs can approximate these N samples with zero error means that",
               "The above N equations can be written compactly as:",
               "where h?x N ? ",
               ". ."
          ],
          "paper_id": "2209d210-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 2,
          "fromPaper": "Extreme learning machines: a survey"
     },
     {
          "head": {
               "n": "6",
               "text": "4 7 5",
               "type": "introduction"
          },
          "paragraphs": [
               "and T ? . . . H is called the hidden layer output matrix of the SLFN ; the ith column of H is the ith hidden node output with respect to inputs x 1 ; x 2 ; . . .; x N : h?x? ? G?a 1 ; b 1 ; x?; . . .; g?a L ; b L ; x? is called the hidden layer feature mapping. The ith row of H is the hidden layer feature mapping with respect to the ith input x i : h?x i ?: It has been proved that from the interpolation capability point of view, if the activation function g is infinitely differentiable in any interval the hidden layer parameters can be randomly generated.",
               "Theorem 2.1Given any small positive value [ 0; activation function g : R ! R which is infinitely differentiable in any interval, and N arbitrary distinct samples",
               "there exists L B N such that for any",
               "randomly generated from any intervals of R d ? R; according to any continuous probability distribution, with probability one,",
               "X From the interpolation point of view the maximum number of hidden nodes required is not larger than the number of training samples. In fact, if L = N, the training errors can be zero. From interpolation point of view, wide type of activation functions can be used in ELM, which include the sigmoid functions, the radial basis, sine, cosine, exponential, and many other non-regular functions . It may be too strict to request that activation functions of hidden nodes are infinitely differentiable. For example, it may not include some important activation functions such as threshold function: g?x? ? 1 x ! 0 ? 0 x\\0 : Threshold networks are very popular in real applications, especially in digital hardware implementation. However, as threshold function is not differentiable, researchers did not manage to find any efficient direct learning algorithms for threshold networks in the past two decades . Interestingly, from the universal approximation point of view, the above mentioned interpolation theorem can be extended to almost any type of nonlinear piecewise continuous function including the threshold function, and thus an efficient direct learning algorithm (e.g. ELM) can be applied to those cases which cannot be handled by other learning techniques in the past decades.",
               "Different from the randomness mentioned in other learning methods , all the hidden node parameters ?a i ; b i ? in ELMs can be independent of the training samples and can be randomly generated before the training samples observed. (Refer to for the details of the differences between ELM and Igelnik and Pao and Lowe et al. ).",
               "Definition 2.3 The function sequence fg i ? G?a i ; b i ; x?g is said randomly generated if the corresponding parameters",
               "based on a continuous sampling distribution probability.",
               "Lemma 2.1 (Proposition 1 of ) Given g : R ! R; spanfg?a ? x ? b? : ?a; b? 2 R d ? Rg is dense in L p for every p 2 ?1; 1?; if and only if g is not a polynomial (almost everywhere). and",
               "is dense in L p for every p 2 ?1; 1?:"
          ],
          "paper_id": "2209d210-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 3,
          "fromPaper": "Extreme learning machines: a survey"
     },
     {
          "head": {
               "n": "2.2",
               "text": "Universal approximation theorem",
               "type": "introduction"
          },
          "paragraphs": [
               "Huang et al. proved in theory that SLFNs with randomly generated additive or RBF nodes can universally approximate any continuous target functions over any compact subset",
               "; the inner product hu; vi is defined by",
               "The norm in L 2 (X) space is denoted as k ? k; and the closeness between the network function f L and the target function f is measured by the L 2 (X) distance: Lemmas 2.1 and 2.2 show that feedforward neural networks with additive or RBF hidden nodes can approximate any target continuous function provided that the hidden node parameters ?a i ; b i ? are tuned properly and appropriate values are given. Lemmas 2.1 and 2.2 only show the universal approximation capability of feedforward neural networks with additive or RBF hidden nodes, however, how to find the suitable hidden node parameters ?a i ; b i ? remains open, and many tuning based learning algorithms have been suggested in the past. Huang et al. proved that given any bounded nonconstant piecewise continuous activation function g : R ! R for additive nodes or integrable piecewise continuous activation function g : R ! R (and R R g?x? dx 6 ? 0) for RBF nodes, the hidden layer of such SLFN need not be tuned, in fact, all the hidden nodes can be randomly generated. SLFNs with randomly generated hidden nodes can universally approximate any target functions. Let e L : f -f L denote the residual error function for the current network f L with L hidden nodes where f 2 L 2 ?X? is the target function. The output layer may have more than one nodes, m [ 1, that is, the function f is a multi-output function: f ? ?f ?1? ; . . .; f ?m? ? T : The corre-"
          ],
          "paper_id": "2209d210-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 4,
          "fromPaper": "Extreme learning machines: a survey"
     },
     {
          "head": {
               "n": "3",
               "text": "ELM",
               "type": "introduction"
          },
          "paragraphs": [
               "The essence of ELM is that: 1. The hidden layer of ELM need not be iteratively tuned . 2. According to feedforward neural network theory , both the training error kHb ? Tk and the norm of weights kbk need to be minimized . 3. The hidden layer feature mapping need to satisfy the universal approximation condition (Theorems 2.3 and 2.4) .",
               "According to Theorems 2.1 and 2.4 the hidden nodes can be randomly generated, the only unknown parameters in SLFNs are the output weights vectors b i between the hidden layer and the output layer, which can simply be resolved by ordinary least-square directly.",
               "Theorem 2.3 can be further extended from additive or RBF hidden nodes cases to ''generalized'' SLFNs . Given a type of piecewise computational hidden nodes (possibly not neural alike nodes), if SLFNs can work as universal approximators with adjustable hidden parameters, from a function approximation point of view the hidden node parameters of such ''generalized'' SLFNs can actually be randomly generated according to any continuous sampling distribution. In theory, the parameters of these SLFNs can be analytically determined by ELM instead of being tuned. Tuning is actually not required in such generalized SLFNs which include sigmoid networks, RBF networks, trigonometric networks, threshold networks, fully complex neural networks, high-order networks, ridge polynomial networks, etc.",
               "Hidden node parameters ?a i ; b i ? remain fixed after randomly generated. To train an SLFN is simply equivalent to finding a least-squares solution ^ b of the linear system Hb ? T :  where the output parameters are determined by ordinary least square can work as universal approximators if only the activation function g is nonconstant piecewise and spanfG?a; b; x? : ?a;",
               "If the number L of hidden nodes is equal to the number N of distinct training samples, L = N, according to Theorem 2.1 matrix H is square and invertible when hidden node parameters ?a i ; b i ? are randomly chosen, and thus SLFNs can approximate these training samples with zero error.However, in most cases the number of hidden nodes is much less than the number of distinct training samples, L ( N; H is a nonsquare matrix and there may not exist a i ;",
               "The smallest norm least-squares solution of the above linear system is:",
               "where H y is the Moore-Penrose generalized inverse of matrix H . Thus, ELM can be summarized as follows:",
               "Algorithm ",
               "ELM algorithm can work with wide type of activation function. Many popular learning algorithms do not deal with threshold networks directly. Instead some analog networks are used to approximate threshold networks such that gradient-descent method can finally be used . However, ELM can be used to train threshold networks directly . Different methods can be used to calculate Moore-Penrose generalized inverse of a matrix: orthogonal projection method, orthogonalization method, iterative method, and singular value decomposition (SVD) . ?1",
               "3.2 Random hidden layer feature mapping based ELM The orthogonal projection method can be efficiently used in ELM : ",
               "and the corresponding output function of ELM is: ?1",
               "In these implementations, the condition on the number of hidden nodes can be mild, it does not closely depend on the number of training samples N. It works for both the cases L \\ N or L C N. This is different from the interpolation theorem which requires L B N (Theorem 2.1), but consistent to the universal approximation theorem (Theorem 2.4). shows a classification boundary obtained by ELM for a binary-class case. Formula   further extended this study to generalized SLFNs with different type of hidden nodes (feature mappings) as well as kernels and showed that the simple unified algorithm of ELM can be obtained for regression, binary and multi-label classification cases which, however, have to be handled separately by SVMs and its variants .",
               "Or we can have 3.3 Kernel based ELM ",
               "and the corresponding output function of ELM is:",
               "Huang et al. also studied the kernel based ELM. If the hidden layer feature mapping h(x) is unknown to users, one can define a kernel matrix for ELM as follows:",
               "Huang et al. shows that the solutions and  ",
               "Then the output function of ELM can be written compactly as: ?1",
               "? . . .",
               "Real-valued neural network models such as feedforward neural networks, RBF networks and recurrent neural networks. 2. Complex-valued neural networks: this approach has attracted considerable attention in channel equalization applications in the past 15 years . Split-complex activation (basis) functions consisting of two realvalued activation functions, one processing the real part and the other processing the imaginary part, have been traditionally employed in these complex-valued neural networks.",
               "In this specific kernel implementation of ELM, the hidden layer feature mapping h(x) need not be known to users, instead its corresponding kernel K(u, v) (e.g. K?u; v? ? exp??cku ? vk 2 ?) is given to users. The number of hidden nodes L (the dimensionality of the hidden layer feature space) need not be specified either. Thus, the ELM algorithm can be rewritten for the kernel case as follows:",
               "Calculate the output function:",
               "Instead of using split-complex activation function, extreme learning machine can use fully complex activation function directly. Li et al proved the universal approximation capability of extreme learning machine with fully complex activation function: ",
               "It can be seen that kernel based ELM algorithm can be implemented in a single learning step. Frnay and Verleysen [50, 51] studied the kernel implementation of ELM if h?x? is known to users. If the hidden layer feature mapping h(x) is known to users, Frnay and Verleysen defined the ELM kernel as",
               "When the network architecture is fixed (with fixed L), from Theorem 4.1 we have",
               "A parameter-insensitive kernel with analytic form can then be obtained for SVM for regression, which significantly reduces the computational complexity. We conjecture that Frnay and Verleysen's ELM kernel can work for SVM and its variants as well as in (25). All the above mentioned can be applied in regression, binary and multilabel classification applications directly. ELMs can be applied to complex space as well.",
               "Theorem 4.2 Given any complex continuous discriminatory or any complex bounded nonlinear piecewise continuous function r : C ! C; for any continuous target function f :",
               "? g randomly generated based on any continuous sampling distribution probability, lim L!1 kf ? f L k ? 0 holds with probability one if the output weights b i are determined by ordinary least square to minimize f ?z? ? P L i?1 b i g i ?z? :",
               "Thus, the ELM algorithms introduced in Sect. 3 can be linearly extended to the complex domain. Compared to others equalizers, ELM can obtain much lower symbol error rate (SER) and provide parsimonious structures for applications in the complex domain ."
          ],
          "paper_id": "2209d210-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 5,
          "fromPaper": "Extreme learning machines: a survey"
     },
     {
          "head": {
               "n": "5",
               "text": "Online sequential ELM (OS-ELM)",
               "type": "introduction"
          },
          "paragraphs": [
               "In high speed digital communication systems, equalizers are very often used at receivers to recover the original symbols from the received signals . Two conventional approaches are usually used for solving equalization problems.",
               "ELM algorithms introduced in Sect. 3 learn training samples only after all training samples are ready. In many industrial applications training data may come one by one or chunk by chunk. In these cases, on-line sequential learning algorithms are preferred over batch learning algorithms as sequential learning algorithms do not require retraining whenever a new data is received. Sequential learning is difficult to be implemented for feedforward neural networks with additive (e.g. ) or RBF hidden nodes . Most of the conventional online sequential learning algorithms have several parameters for users to specify and it is very time-consuming to tune those parameters. OS-ELM is a simple and efficient online sequential learning algorithm that can handle both additive and RBF nodes in a unified framework. OS-ELM can learn the training data not only one-by-one but also chunk by chunk (with fixed or varying length) and discard the data for which the training has already been done. The training observations are sequentially presented to the learning algorithm (one-by-one or chunk-by-chunk with varying or fixed chunk length). A single or a chunk of training observations is discarded and may not be used any more as soon as the learning procedure for that particular observation(s) is completed. According to Sect. 3, one of the solutions of the output weight vector b is:",
               "where N k?1 denotes the number of observations in the (k ? 1)th chunk. (b) Calculate the partial hidden layer output matrix H k?1 for the (k ? 1)th chunk of data @ k?1 : ",
               "Sequential implementation of the least-squares solution of Eq. 29 results in the OS-ELM which uses the recursive least squares algorithm . OS-ELM Algorithm: step 1 Initialization Phase: Initialize the learning using a small chunk of initial training data @ 0 ? f?x i ; t i ?g Seen from the above OS-ELM algorithm, OS-ELM and ELM can achieve the same learning performance (training error and generalization accuracy) when rank(H 0 ) = L. In addition, if N 0 = N, OS-ELM also becomes the batch ELM. In OS-ELM, the chunk size of incoming training data need not be constant. When the training data is received one-by-one instead of chunk-by-chunk, N k?1 : 1, formula (32) has the following simple format (ShermanMorrison formula ): ",
               "OS-ELM is efficient in time-series prediction which is required in many real-world problems. The chaotic Mackey-Glass differential delay equation is one of the classical benchmark time series problems in literature:",
               "7 5",
               "(c) Estimate the initial output weight b ?0? ?",
               "and",
               "for a = 0.2, b = 0.1, and s = 17. Integrating the equation over the time interval ?t; t ? Dt? by the trapezoidal rule yields: ",
               "vations:",
               "The time series is generated under the condition x(t -s) = 0.3 for 0 B t B s and predicted with t = 50 available. These conventional methods may also face local minima issues. Although many other incremental learning algorithms have been proposed in literature , unlike I-ELM the universal approximation capability of these previous learning algorithms has not been proved. Different from other conventional incremental learning algorithms which may have several parameters for us to specify, I-ELM has no parameters for users to specify except the maximum network architecture and the expected accuracy. Experimental results show that I-ELM outperforms other learning algorithms (including support vector regression (SVR) , stochastic gradient-descent BP , and incremental RBF networks , RANEKF , MRAN , GAP-RBF , GGAP-RBF ) in terms of generalization performance and learning speed. I-ELMs can be implemented in different ways:",
               "1.4",
               "1.  Time-series prediction: the approximated curve obtained by OS-ELM. OS-ELM is trained by the training observations is from t = 1 to t = 4, 000, and the predicted period is from t = 4,001 to t = 4,500 sample steps ahead using the four past samples: s n-t , s n-t-6 , s n-t-12 , and s n-t-18 . Hence, the nth input-output instance is:",
               "1. Basic I-ELM Every time only one hidden node is randomly generated and added to the existing network . 2. Enhanced I-ELM Every time k hidden nodes are randomly generated. However, among the k randomly generated hidden nodes only the most appropriate hidden node will be added to the existing network .",
               "x n ? ?s n?t ; s n?t?6 ; s n?t?12 ; s n?t?18 ? T y n ? s n shows the approximated curve of OS-ELM in this time-series prediction. In this simulation, Dt ? 1; and the training observations is from t = 1 to t = 4,000 and the testing observations from t = 4,001 to t = 4,500. The number of hidden nodes of OS-ELM is L = 120.",
               "Compared to the original I-ELM , this enhanced implementation will produce a more compact network architecture and the learning can be completed in a faster convergence rate and learning speed. I-ELM is a specific case of EI-ELM when k = 1. The universal approximation capability of ELMs was proved using incremental learning method where the hidden nodes are added one by one . The proof itself is indeed a practical incremental constructive method, which actually shows an efficient way to construct an incremental feedforward network (referred to as I-ELMs). Different from other incremental learning algorithms which may only work with some type of hidden nodes (e.g. resource allocation network and its variants work only for RBF networks), I-ELM can work well with a wide type of activation functions no matter whether they are sigmoidal or nonsigmoidal, continuous or noncontinuous, and differentiable or non-differentiable. The traditional gradient-descent based learning algorithms cannot be applied to networks with non-differential activation functions such as threshold networks since the required derivatives are not",
               "where",
               "According to formula (36), the weight b L (j) between the Lth newly added node and the jth output node should be",
               "In real applications, only the training samples are available, the target function f(x) is unknown and the exact functional form of e L-1 (j)* is not available, thus, formula (36) cannot be calculated explicitly. Instead, formula (36) can be estimated based on the training samples:",
               "where e (j) (p) is the corresponding residual error of the jth output node before the Lth new hidden neuron is added.",
               "T is the activation vector of the newly added node for all the N training samples and E ?j? ? ?e ?j? ?1?; . . .; e ?j? ?N?? T is the residual vector the jth output node with respect to all the N training samples before this new hidden node added. Let E ? ?E ?1? ; . . .; E ?m? ?:",
               "the ith trial of hidden node for all the N training samples and b (i) (j) is the corresponding output weight between the ith trial of hidden node and the jth output node. E (i) (j) of formula (39) is the residual error vector of the jth output node if the ith trial of hidden node is added. E (j) in the right hand of formula (39) represents the earlier residual error vector corresponding to the jth output node before the new node added.",
               "EI-ELM Algorithm: Given a training set @ ? f?x i ; t i ?jx i 2 R d ; t i 2 R; i ? 1; . . .; Ng; hidden node output function G , maximum number L max of hidden nodes, maximum number k of trials of assigning random hidden nodes at each step, and expected learning accuracy ; ",
               "Before learning, there is no node in the network and the initial residual error is set as the expected target vector T of the training data set as shown in step 1. Learning will stop when the number L of hidden nodes has exceeded the predefined maximum number L max or the residual error E is small enough (kEk\\). step 2b randomly generates k new hidden nodes and step 2c will choose and add the most appropriate hidden node of the k randomly generated hidden nodes. ^ h ?i? in formula is the activation vector of",
               "The idea of neural network ensemble was proposed by Hansen and Salamon . Their work showed that a single network's performance can be expected to improve using an ensemble of neural networks with a plurality consensus scheme. This technique has been spread widely after that. The most prevailing approaches for training neural networks comprised the ensemble are Bagging and Boosting . An integration of several ELMs was proposed by Sun et al to predict the future sales amount. Several ELM networks were connected in parallel and the average of the ELMs' outputs was used as the final predicted sales amount. The resulting ensemble has better generalization performance. Heeswijk et al. investigated the adaptive ensemble models of ELM on the application of one-step ahead prediction in (non-)stationary time series. It was verified that the method did work on stationary time series and the capability of the method on non-stationary time series was tested. The empirical studies showed that the adaptive ensemble model achieved an acceptable testing error with good adaptivity. Heeswijk et al.",
               "also studied ELM ensemble for large scale regression applications. Furthermore, network ensembles are potentially important methods to perform sequential learning . Network ensemble consists of a few of single networks that may have different adaptabilities to the new data. Some of the networks in the ensemble may adapt faster and better to the new data than others, which could make the ensemble overcome the problem of networks that could not adapt well to the new data. proposed an integrated network structure, which is called ensemble of online sequential ELM (EOS-ELM). EOS-ELM comprised several OS-ELM networks. The average value of outputs of each OS-ELM in the ensemble was used as the final measurement of network performance.",
               "The simulation results proved that EOS-ELM is more stable than original OS-ELM in each trial of simulation for most problems."
          ],
          "paper_id": "2209d210-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 6,
          "fromPaper": "Extreme learning machines: a survey"
     },
     {
          "head": {
               "n": "8",
               "text": "Pruning ELM",
               "type": "introduction"
          },
          "paragraphs": [
               "Rong et al. presented a pruned ELM (referred to as P-ELM) as a systematic and automated method for ELM classifier network design. It starts with a large network and then eliminates the hidden nodes that have low relevance to the class labels by using statistical criteria, namely, the Chisquared (v 2 ) and information gain (IG) measures. P-ELM mainly focuses on pattern classification applications. Another pruning algorithm called optimally-pruned ELM (referred to as OP-ELM) was proposed by Miche et al. . The OP-ELM methodology has three steps: (1) build the SLFN using the original ELM algorithm; (2) rank the hidden nodes by applying multi-response sparse regression algorithm (MRSR) ; and (3) select the hidden nodes through leave-one-out (LOO) validation. OP-ELM is applicable for both regression and classification applications. squares (OLS). OLS selects a suitable set of variables to form the subset model from a large set of candidates. At each step, the net decrease in the residual error is maximized. The key advantage of the algorithm is that it can explicitly identify the net contribution of the newly added node without solving the whole least-squares problem, which significantly reduces the computational complexity. However, OLS cannot guarantee an optimal solution because it is greedy and on the basis of a local optimization . By modifying the classic forward selection algorithm, a constructive hidden nodes selection method for ELM (CS-ELM) was proposed,which is less greedy and without any matrix decompositions. At each step of CS-ELM, the hidden node with an output that has the highest correlation with the current residual is selected."
          ],
          "paper_id": "2209d210-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 7,
          "fromPaper": "Extreme learning machines: a survey"
     },
     {
          "head": {
               "n": "9.3",
               "text": "Two-stage ELM for regression",
               "type": "introduction"
          },
          "paragraphs": [
               "9 Constructive model selection of ELM"
          ],
          "paper_id": "2209d210-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 8,
          "fromPaper": "Extreme learning machines: a survey"
     },
     {
          "head": {
               "n": "9.1",
               "text": "Error minimized ELM",
               "type": "introduction"
          },
          "paragraphs": [
               "Error minimized ELM (EM-ELM) is an error minimization based method in which the number of hidden nodes can grow one-by-one or group-by-group until optimal. The approach can significantly reduce the computational complexity and its convergence was proved as well. In EM-ELM, the hidden nodes are randomly generated and added to the network sequentially. Further study of EM-ELM shows that some newly added hidden nodes may be more efficient in reducing the residual error as compared to other hidden nodes. Hence, an enhancement of EM-ELM (referred to as EEM-ELM) was proposed by applying random search method. In the enhancement of EM-ELM, the hidden node is added to the network one-byone. At each incremental learning step, k hidden nodes are randomly generated and the hidden node that leads to highest residual error reduction will be added to the network, and then the output weights are updated incrementally in the same way of original EM-ELM.",
               "It is found that the parsimonious network structure is probably missed by some greedy selection methods due to the fact that the hidden nodes added earlier may become insignificant when other hidden nodes are added to the network. In FCA , the researchers solved this problem by adding a fine tuning phase after the forward selection, which reviewed the hidden nodes selected in forward selection phase and replaced the selected hidden nodes with candidate nodes that achieve more contribution. Inspired by the above mentioned CS-ELM and the FCA algorithm, a two-stage algorithm was proposed and it is called TS-ELM . The first stage attempts to select hidden nodes by forward recursive algorithm and the selection is terminated by the final prediction error (FPE) criterion; while the second stage is a backward refinement phase that removes the insignificant hidden nodes by applying LOO method."
          ],
          "paper_id": "2209d210-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 9,
          "fromPaper": "Extreme learning machines: a survey"
     },
     {
          "head": {
               "n": "9.2",
               "text": "Stepwise forward selection based constructive ELM for regression",
               "type": "introduction"
          },
          "paragraphs": [
               "Instead of using a simple selection method that randomly generates a group of hidden nodes in each step of the training process (i.e. like in EEM-ELM), one could randomly generate a large number of hidden nodes as the candidate reservoir and then pick the hidden node one-by-one via a stepwise forward selection method. The fast construction algorithm (FCA) proposed in is a constructive hidden node selection method for ELM based on orthogonal least SVM has become one of the most popular classifiers. SVM has been extensively applied in wide type of applications. As explained in Cortes and Vapnik , SVM can be seen as a specific type of SLFNs, the socalled support vector networks. A multi-layer feedforward network (cf. ) can be considered to transform the input data into a feature space Z of the last hidden layer . In order to find a solution of z i (x) where z i (x) is the activation function of the ith node of the last hidden layer, Cortes and Vapnik proposed the support vector machine which maps the data from the input space to some high dimensional feature space Z through some nonlinear mapping /?x? : x i ! /?x i ?: Standard optimization methods are used to find the separating hyperplane which maximizes the separating margins of two different classes in the feature space:",
               "sign?b ? h?x??: In ELM, to minimize the norm of the output weights kbk is actually to maximize the distance of the separating margins of the two different classes in the ELM feature space: 2=kbk; which is similar to SVM's target. From the standard optimization theory point of view, the objective (44) of ELM in minimizing both the training errors and the output weights can be written as:",
               "where k is a user specified parameter and provides a tradeoff between the distance of the separating margin and the training error. Vectors x i for which t i ?b ? /?x i ? ? b? ? 1 is termed support vectors. The hyperplane w ? /?x? ? b ? 0 separates the training data with a maximal margin in the feature space. It maximizes the distance 2=kbk between two different classes in the feature space Z. To train such a SVM is equivalent to solving the following dual optimization problem:",
               "Minimize:",
               "Subject to:",
               "which is very similar to SVM's optimization problem (40) with two main differences:",
               "subject to:",
               "where each Lagrange multiplier a i corresponds to a training example (x i , t i ). Kernel functions K?u; v? ? /?u? ? /?v? are usually used in the implementation of SVM learning algorithm:",
               "1. Different from the conventional SVM, the randomness can be adopted in the ELM mapping h(x), that is, all the parameters of h(x) are chosen randomly. 2. The bias b is not required in the ELM's optimization constrains since in theory ELM with h(x) has universal approximation capability and the separating hyperplane in the ELM feature space tends to pass through the origin. In SVM, the feature mapping /(x) is unknown and it is not required to satisfy universal approximation condition. However, in ELM, the feature mapping h(x) is required to satisfy universal approximation conditions (Theorems 2.3 and 2.4). Based on the Karush-Kuhn-Tucker (KKT) conditions , the equivalent dual optimization problem can be obtained:",
               "The SVM kernel function K(u, v) needs to satisfy Mercer's condition . The decision function of SVM is: Further study showed that SVM's optimization constrains can be milder if ELM kernel is used, and the optimal solution can be obtained more efficiently. ELM is to minimize the training error as well as the norm of the output weights :",
               "As the separating hyperplane tends to pass through the origin in the ELM feature space, the above dual ELM optimization problem does not have the condition P i=1 N t i a i = 0, Vi, which is, however, required in the conventional dual SVM optimization problem (41). With the ELM kernel  Experimental results have shown that the generalization performance of ELM is less sensitive to the user specified parameters especially the number of hidden nodes. Thus, compared to SVM, users can use ELM easily and effectively by avoiding tedious and time-consuming parameter tuning.",
               "As a learning technique, ELM has demonstrated good potentials to resolving regression and classification problems. Recently, ELM techniques have received considerable attention in computational intelligence and machine learning communities, in both theoretic study and applications . Fundamentals of ELM techniques are composed of twofold: universal approximation capability with random hidden layer, and various learning techniques with easy and fast implementations. The following issues on ELM remain open and may be worth investigating in the future.",
               "1. As observed in experimental studies , the performance of ELM is stable in a wide range of number of hidden nodes. Compared to the BP learning algorithm, the performance of ELM is not very sensitive to the number of hidden nodes. However, how to prove it in theory remains open. 2. One of the typical implementations of ELM is to use random nodes in the hidden layer and the hidden layer of SLFNs need not be tuned. It is interesting to see that the generalization performance of ELM turns out to be very stable. How to estimate the oscillation bound of the generalization performance of ELM remains open too. 3. It seems that ELM performs better than other conventional learning algorithms in applications with higher noise. How to prove it in theory is not clear. 4. Experimental results show that compared to backpropagation algorithm, SVM and least-square SVM (LS-SVM) ELM usually achieve similar or better generalization in regression and classification applications. How to prove it in theory is still an open problem. 5. ELM provides a batch learning kernel solution which is much simpler than other kernel learning algorithms such as LS-SVM . It is known that it is not straightforward to have an efficient online sequential implementation of SVM and LS-SVM. However, due to the simplicity of ELM, it may be easier to implement the online sequential variant of the kernel based ELM (25)."
          ],
          "paper_id": "2209d210-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 10,
          "fromPaper": "Extreme learning machines: a survey"
     }
]