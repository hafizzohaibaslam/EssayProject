[
     {
          "head": {
               "n": "0",
               "text": "abstract",
               "type": "abstract"
          },
          "paragraphs": [
               "Cluster analysis of gene-wide expression data from DNA microarray hybridization studies has proved to be a useful tool for identifying biologically relevant groupings of genes and constructing gene regulatory networks. The motivation for considering mutual information is its capacity to measure a general dependence among gene random variables. We propose a novel clustering strategy based on minimizing mutual information among gene clusters. Simulated annealing is employed to solve the optimization problem. Bootstrap techniques are employed to get more accurate estimates of mutual information when the data sample size is small. Moreover, we propose to combine the mutual information criterion and traditional distance criteria such as the Euclidean distance and the fuzzy membership metric in designing the clustering algorithm. The performances of the new clustering methods are compared with those of some existing methods, using both synthesized data and experimental data. It is seen that the clustering algorithm based on a combined metric of mutual information and fuzzy membership achieves the best performance. The supplemental material is available at www.gspsnap.tamu.edu/gspweb/zxb/glioma_zxb."
          ],
          "paper_id": "222e22f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 0,
          "fromPaper": "Gene Clustering Based on Clusterwide Mutual Information"
     },
     {
          "head": {
               "n": "1.",
               "text": "INTRODUCTION",
               "type": "introduction"
          },
          "paragraphs": [
               "T o understand the nature of cellular functions, it is necessary to study the behavior of genes in a holistic ( ) rather than in an individual manner because the expressions and activities of genes are not isolated or independent of each other. Due to the large number of genes and the high complexity of biological networks, clustering is a useful exploratory technique for the analysis of gene expression data. Clustering has been used in a number of studies to obtain a global, unsupervised perspective on the similarity of expression pro les . A wide range of clustering algorithms has been proposed to analyze gene expression data, including hierarchical clustering , self-organizing maps ), K-means ), graph-theoretic approaches , support vector machines ( ) and fuzzy C-means ). Successes in application have been reported for many clustering approaches ( ) but so far no single method has emerged as the method of choice in the gene expression analysis community.",
               "In this paper, we develop a new gene clustering strategy based on minimizing the mutual information among clusters. Pertinent to our approach is a study based on computing the mutual information for all pairs of genes and then choosing a threshold of the mutual information to create clusters of genes encompassing those with mutual information higher than the threshold ). These have been similar treatments ). These works are based on pairwise mutual information (PMI) and thus essentially only explore the marginal distributions of the multi-dimensional data. Our clustering strategy is based on minimizing the mutual information of the variables among clusters, and hence it fully explores the underlying joint probability distribution of the data. Bootstrap techniques are employed to obtain more accurate estimates of the mutual information for the typically small sets of data samples. Data are assumed to be truncated ( .",
               "Mutual-information-based clustering minimizes the statistical correlation among clusters, whereas the traditional K-means and fuzzy C-means algorithms minimize the total variance within different clusters. It is natural to consider combining these two paradigms to obtain more effective clustering techniques. To this end, we propose two clustering algorithms based on a combined mutual information and Euclidean distance criterion and a combined mutual information and fuzzy membership criterion. The performances of the new clustering methods are compared with that of some existing methods, using both synthesized data and experimental data.",
               "The rest of this paper is organized as follows. In Section 2, we formulate the new clustering strategy based on mutual information minimization. We also discuss the bootstrap procedure for estimating the mutual information from a small dataset, as well as the simulated annealing procedure for solving the corresponding optimization problem. In Section 3, we propose two additional clustering methods based on combining the mutual information metric and the conventional Euclidean distance or the fuzzy membership metric. In Section 4, we provide performance comparisons of the above clustering methods using synthesized data. In Section 5, we present clustering results on gene microarray measurement data. Section 6 contains the conclusions."
          ],
          "paper_id": "222e22f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 1,
          "fromPaper": "Gene Clustering Based on Clusterwide Mutual Information"
     },
     {
          "head": {
               "n": "2.",
               "text": "GENE CLUSTERING VIA MUTUAL INFORMATION MINIMIZATION",
               "type": "introduction"
          },
          "paragraphs": [
               "In this section, we propose a new gene clustering method based on mutual information minimization. We introduce the concepts of mutual information and normalized mutual information, discuss bootstrap procedures for estimating the mutual information from a small dataset, introduce clustering based on pairwise mutual information, formulate the new clustering strategy based on mutual information minimization, and present a simulated annealing algorithm for solving the corresponding optimization."
          ],
          "paper_id": "222e22f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 2,
          "fromPaper": "Gene Clustering Based on Clusterwide Mutual Information"
     },
     {
          "head": {
               "n": "2.1.",
               "text": "Mutual information",
               "type": "introduction"
          },
          "paragraphs": [
               "The motivation for considering mutual information is its capability to measure a general dependence among random variables. Shannon's information theory provides a suitable formalism for quantifying such a concept. The entropy of a gene expression pattern is a measure of the uncertainty information content in that pattern. Given a random vector X and its probability distribution P .X D x i /; i D 1; ? ? ? ; N x , where N x is the number of possible values X can take, the entropy is de ned as"
          ],
          "paper_id": "222e22f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 3,
          "fromPaper": "Gene Clustering Based on Clusterwide Mutual Information"
     },
     {
          "paragraphs": [
               "Higher entropy for gene variables means that their expression levels are more randomly distributed. The joint entropy of X and Y is a measure of the uncertainty information between X and Y , and is dee ned by",
               "where N y is the number of possible values Y can take. When certain variables are known and others are not, the remaining uncertainty is measured by the conditional entropy",
               "The mutual information between X and Y is a measure of information about X (or Y ) contained in Y (or X) and is given by",
               "It is known that mutual information is always nonnegative, i.e., I .XI Y / ? 0 ( . Note that the mutual information de ned in (4) is not normalized; and I .XI Y / can be quite small even if X and Y are highly correlated since H .X/ and H .Y / may be small. Therefore, we normalize I .XI Y / by the maximal entropy of each of the contributing sequences, giving a high value for highly correlated sequences, independent of the individual entropy :",
               "Unlike the Euclidean distance, this measure also recognizes negatively and nonlinearly correlated data sets as proximal ).",
               "The probabilities in (6) can be estimated by the corresponding histograms, i.e.,",
               "where M is the total number of samples, and #.x i / denotes the number of occurrences of x i ."
          ],
          "paper_id": "222e22f0-97d4-11e8-9580-1f0eb29018a9",
          "head": {
               "type": "introduction"
          },
          "paragraphNo": 4,
          "fromPaper": "Gene Clustering Based on Clusterwide Mutual Information"
     },
     {
          "head": {
               "n": "2.2.",
               "text": "Mutual information estimation based on bootstrap",
               "type": "introduction"
          },
          "paragraphs": [
               "In practice, the sample size M is typically small, compared with the total number of possible values N x and N y . In order to get a more accurate estimate of the mutual information, we resort to the bootstrap technique ( .",
               "Let z D [z 1 ; z 2 ; ? ? ? ; z N ] denote the vector of N gene variables, where z i 2 f?1; 0; 1g. Denote Z D [z.1/; z.2/; ? ? ? ; z.M/] as M realizations (i.e., samples) of z. At each iteration of the bootstrap procedure, M random draws are performed on Z, to form a \"resample\"",
               ", and the mutual information is computed based on the resample. The basic bootstrap method for estimating the mutual information is summarized as follows. ",
               "I is the mean of the mutual information values in the interval",
               "We set ? D 0:05 and P D 1,000 in our simulations. According to , Algorithm 1 can be substantially improved because the interval calculated is an interval with coverage less than the nominal values . Next we give a more sophisticated bootstrap algorithm that would lead to a more accurate estimate of mutual information )."
          ],
          "paper_id": "222e22f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 5,
          "fromPaper": "Gene Clustering Based on Clusterwide Mutual Information"
     },
     {
          "head": {
               "n": "2.3.",
               "text": "Clustering based on pairwise mutual information",
               "type": "introduction"
          },
          "paragraphs": [
               "Here we describe a threshold clustering algorithm based on pairwise mutual information (PMI). In , a threshold clustering algorithm is developed using jackknife correlation. We apply this idea to the pairwise mutual information-based cluster analysis. The PMI works as follows: a candidate cluster is formed by starting with the rst gene and grouping the gene that has smallest mutual-information-based distance with the target gene. The distance is dee ned as",
               "Each iteration adds the gene that has a minimal distance to the target gene to the cluster. The process continues until no gene can be added without surpassing the distance threshold. A second candidate cluster is formed by starting with the second gene and repeating the same procedure. Note that all genes are made available to the second gene, that is, the genes from the rst candidate cluster are not removed from consideration. The process continues for all genes. The largest candidate cluster is selected and retained.",
               "The genes in the largest candidate cluster are removed from the whole gene set, and the entire procedure is repeated on the smaller gene set. For the predee ned cluster number case, K in this study, when the number of clusters reaches K, add all the remaining genes to the last cluster. In this algorithm, the threshold is chosen as the mean of the distances of all gene pairs, or chosen empirically )."
          ],
          "paper_id": "222e22f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 6,
          "fromPaper": "Gene Clustering Based on Clusterwide Mutual Information"
     },
     {
          "head": {
               "n": "2.4.",
               "text": "Problem formulation",
               "type": "modelling"
          },
          "paragraphs": [
               "In this paper, we x the number of clusters. Suppose we are to partition the set of gene variables into",
               "The cost function is de ned as the sum of pairwise mutual information between any two subsets,",
               "i6 Dj where s denotes a particular partition scheme. The simulated annealing algorithm is employed to nd an optimal partition scheme such that the cost function attains the minimum, i.e.,",
               "where S denotes the set of all possible K-partition schemes."
          ],
          "paper_id": "222e22f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 7,
          "fromPaper": "Gene Clustering Based on Clusterwide Mutual Information"
     },
     {
          "head": {
               "n": "3.",
               "text": "COMBINED MUTUAL-INFORMATION AND DISTANCE-BASED CLUSTERING ALGORITHMS",
               "type": "modelling"
          },
          "paragraphs": [
               "The Euclidean distance measure can capture only positive correlations between temporal gene expression patterns, whereas mutual information can capture any correlative behavior (positive, negative, and nonlinear) between expression time series ). When the sample size is large, the mutual information can be estimated accurately, and then the mutual-information minimization clustering exhibits optimality. However, in practice, the data sample size is small and mutual information estimation is problematic. In order to enhance the clustering performance under such a condition, we propose to combine the mutual information criterion and the traditional distance criterion in designing the clustering algorithm. "
          ],
          "paper_id": "222e22f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 8,
          "fromPaper": "Gene Clustering Based on Clusterwide Mutual Information"
     },
     {
          "head": {
               "n": "3.1.",
               "text": "Clustering based on combined mutual information and Euclidean distance metrics",
               "type": "experiment"
          },
          "paragraphs": [
               "The K-means algorithm for clustering is as follows. Given a partition s (i.e., given the values f1 i;k g), we calculate the centroid of each cluster c k . We then reassign each y i to its nearest centroid to get a new partition s. This procedure is repeated until there is no more change in the partition. The mutual-information-based clustering technique minimizes the statistical correlation between different clusters while the traditional K-means algorithm minimizes the total variance within different clusters. Here we propose to combine the two different objectives. The new objective function is given by",
               "where 0?1 is a weight factor to adjust the relative importance of the two criteria; f .s/ is dee ned in (13); and 1=M and 2=.K 2 ? K/ are normalization constants. The procedure for solving this optimization problem is summarized as follows."
          ],
          "paper_id": "222e22f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 9,
          "fromPaper": "Gene Clustering Based on Clusterwide Mutual Information"
     },
     {
          "head": {
               "text": "Algorithm 3 (Clustering based on combined mutual information and Euclidean distance).",
               "type": "experiment"
          },
          "paragraphs": [
               "The algorithm is the same as the simulated annealing algorithm introduced in Section 2 with the objective function replaced by (17) and c k being the mean of the samples in the k-th cluster at each iteration."
          ],
          "paper_id": "222e22f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 10,
          "fromPaper": "Gene Clustering Based on Clusterwide Mutual Information"
     },
     {
          "head": {
               "n": "3.2.",
               "text": "Clustering based on combined mutual information and fuzzy membership metrics",
               "type": "experiment"
          },
          "paragraphs": [
               "The fuzzy C-means method is a variation of the K-means method in which each gene y i ; i D 1; 2; ? ? ? ; N has a degree of membership u i;k .0u i;k1/ of belonging to each cluster k such that P N iD1 u i;k D 1; 1kK. Randomly set the initial membership matrix U D .u i;k / N ?K ; .u i;k 2 [0; 1]/, and calculate the centroid of each cluster c k as",
               "Then the objective function associated with a particular partition s is",
               "The fuzzy C-means algorithm for clustering is as follows. After determining the initial u i;k , c k and g.s/, repeat , , and  ",
               "from s .0/ ; //Assume that gene i was originally (?) // assigned to cluster r. We then random pick a cluster label j .j 6 D i/, // and assign gene i to cluster j , and exchange the values of u i;r and u i;j . "
          ],
          "paper_id": "222e22f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 11,
          "fromPaper": "Gene Clustering Based on Clusterwide Mutual Information"
     },
     {
          "head": {
               "n": "4.",
               "text": "CLUSTERING PERFORMANCE ON SIMULATED DATA",
               "type": "experiment"
          },
          "paragraphs": [
               "In this section, we test the performance of several clustering algorithms using simulated data. The algorithms under consideration include ? K-means algorithm; ? Fuzzy C-means algorithm; ? Clustering algorithm based on mutual information (MI) minimization discussed in Section 2; ? Clustering algorithm based on the combined metric of mutual information and Euclidean distance (MIK) discussed in Section 3.1; ? Clustering algorithm based on the combined metric of mutual information and fuzzy membership distance (MIF) discussed in Section 3.2; ? Hierarchical clustering algorithm: single linkage clustering algorithm; ? Biclustering algorithm (a node-deletion algorithm proposed by Chen and Church ). ? Threshold clustering algorithm based on pairwise mutual information (PMI) explained in Section 2.",
               "Numerous cluster measures based on the sample points have been proposed ). Many of these are based on spatial separation, an exception being the gure of merit (FOM), which is based on the consistency of clusters when leaving a point out ). Since our interest is solely with algorithm accuracy, in this paper we measure performance by the percentage of points placed into correct clusters ). Performance analysis on synthetic data is critical because only in this way do we have ground truth (true clusters) from which to measure performance deviation."
          ],
          "paper_id": "222e22f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 12,
          "fromPaper": "Gene Clustering Based on Clusterwide Mutual Information"
     },
     {
          "head": {
               "text": "Example 1",
               "type": "experiment"
          },
          "paragraphs": [
               "In this example, we assume there are four binary random variables x 1 ; x 2 ; x 3 ; x 4 such that their joint distribution satiss es",
               "where p.?/ follows a Bernoulli distribution. Hence, the two clusters are .x 1 ; x 2 / and .x 3 ; x 4 /. The probabilities used in the Bernoulli distribution are 1=2 n for each state, where n is the number of variables. In the rst simulation, we vary the sample size M and perform the cluster analysis using the above eight algorithms. The results are the average of 100 simulations. The value of?isof?of?is set as 0.5 empirically in this paper. Since the mutual information estimation will become imprecise with an increasing number of gene variables, ? should become small to decrease the effect from the imprecise mutual information. and show the clustering results using different sample sizes. It is seen that the MI method always outperforms the fuzzy C-means, the K-means, the linkage, the biclustering, and the pairwise MI methods. The combined mutual-information and fuzzy membership-based clustering algorithm has the best clustering accuracy. The MIK method has similar performance as the MI method. The MI-based clustering methods become more accurate as the sample size increases, whereas the fuzzy C-means, the K-means, the linkage, and the biclustering algorithm are insensitive to the sample size. Note that the biclustering algorithm is the algorithm 1 (single node deletion) proposed by , where the parameter(maximum acceptable mean squared residue score) is set as 0:1. Next, we x the sample size to M D 30 and compare the seven clustering algorithms (not including the biclustering algorithm since it is for two-cluster clustering) under different numbers of variables and clusters as in . The data are generated according to",
               "where X 1 ; X 2 ; ? ? ? ; X K are the K clusters. The MI method outperforms the fuzzy C-means, the K-means, the linkage, and the pairwise MI methods. The MIK method has a similar performance as the MI method. Again, the combined MIF clustering algorithm has the best performance. Note that algorithm performances degrade substantially as the number of genes increases. This is to be expected and can be signii cantly recti ed by replicating experiments ). The degree of degradation depends on the distributions governing the data according to . Performance holds up better for fuzzy C-means if the distributions are separated and their variances small and holds up better for mutual-information clustering if the mutual information within the individual distributions is high in comparison to the mutual information between individual distributions. "
          ],
          "paper_id": "222e22f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 13,
          "fromPaper": "Gene Clustering Based on Clusterwide Mutual Information"
     },
     {
          "head": {
               "text": "Example 2",
               "type": "experiment"
          },
          "paragraphs": [
               "In this example, associated with each gene x i , we have M observations x i 4 D [x i .1/; x i .2/; ? ? ? ; x i .M/] T , which is the quantized version of a continuous random vector z i 4 D [z i .1/; z i .2/; ? ? ? ; z i .M/] T . The vector z i is generated in the following way: if x i belongs to the k-th cluster, then",
               "where",
               "is called a template ) and ",
               "where ? 2 k < 1. In the simulations, we set 2 3 2 3 6 6 6 6 6 6 6 4 ",
               "First, the clustering performance under different numbers of genes and clusters is given in , for ? 2 D 0:9 and M D 30. In the rst case, where there are N D 4 genes and K D 2 clusters, the performance of the MI method is much better than the fuzzy C-means, the linkage, and the K-means methods, and the MIF method has the best performance. With an increased number of variables and clusters, the MI method is slightly better than the fuzzy C-means, but the MIF method still has the best performance. The K-means, the linkage, and the PMI methods have the worst performances.",
               "In and , the clustering performance under different values of ? 2 is shown with N D 4; K D 2, and M D 30. When ? 2 D 0, the fuzzy C-means method has the best performance and the MI method is worse than the fuzzy C-means, the linkage, the biclustering, the MIK, and the MIF methods. With an increased ? 2 , the performance of the MI method becomes better, and the fuzzy C-means method becomes worse. When ? 2 ? 0:3, the MI method outperforms the fuzzy C-means. It is interesting to note that the combined MIF clustering algorithm has the best performance except for case ? 2 D 0. When ? 2 D 0, the data are completely uncorrelated, and therefore the mutual information criterion is not effective in clustering them.",
               "Next, the clustering performance under different numbers of genes and clusters is given in , for ? 2 D 0:1 and M D 30. In the rst case, where there are N D 4 genes and K D 2 clusters, the performance  of the MI method is much better than the fuzzy C-means and K-means methods, and the MIF method has the best performance. With increased numbers of variables and clusters, the MI method is slightly better than the fuzzy C-means, but the MIF method still has the best performance. The K-means and the PMI methods have the worst performance."
          ],
          "paper_id": "222e22f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 14,
          "fromPaper": "Gene Clustering Based on Clusterwide Mutual Information"
     },
     {
          "head": {
               "n": "5.",
               "text": "EXPERIMENTAL ANALYSIS",
               "type": "experiment"
          },
          "paragraphs": [
               "We have applied the clustering algorithms to binarized expression data for 597 genes derived from 26 human glioma surgical tissue samples ). The original expression data is adjusted by combining genes possessing the same binarized expression pro les ). The adjusted set has 526 genes. Both the original and reduced sets are available at gspsnap.tamu.edu/gspweb/zxb/glioma_ zxb/glioma_web.htm, along with the clusters determined by the algorithms. Owing to the size of the gene set and the computational requirements of the algorithms (in particular, simulated annealing), parallel implementations have been developed and run on the NIH Beowulf Cluster.",
               "The effect of combining the fuzzy and MI clustering criteria can be seen in , which shows (a) the binary pro les for the adjusted gene set (red D C1, green D 0), (b) the fuzzy C-means clusters, and (c) the MIF clusters. Essentially, two small clusters were broken out from fuzzy C-means clusters to become new clusters in the MIF clustering. The twelve-gene and ve-gene clusters are listed in , respectively. While these new clusters were not separated by the fuzzy C-means criterion, their internal mutual information was suf ciently high relative to their mutual information with the original clusters that the combined algorithm separated them out. While the number of genes changed between the fuzzy C-means "
          ],
          "paper_id": "222e22f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 15,
          "fromPaper": "Gene Clustering Based on Clusterwide Mutual Information"
     },
     {
          "paragraphs": [
               "Tight junction protein 1 (TJP1); zonula occludens (ZO1) 76",
               "Retinoblastoma-associat ed protein 1 (RB1) 159",
               "Gene 159 interferon regulatory factor 1 (IRF1) 189",
               "Tumor necrosis factor receptor 1-associated death domain protein 224",
               "Caspase 4 (CASP4); CASP5; ICH2 cysteine protease 235 DNA polymerase alpha catalytic subunit (POLA) 270",
               "Basic transcription factor 2 44-kDa subunit Transcriptional repressor NF-X1 290",
               "Transcription factor relB; I-rel 299 45-kDa nuclear factor (NF45) 343",
               "Interferon alpha/beta/omega receptor subunit 1 406",
               "Corticotropin-releasing factor receptor 1 (CRFR; CRF1) and (CRHR1) 90-kDa TATA (TAF3C); transcription factor TFIIIB 90-kDa subunit (TFIIIB90) 325",
               "Transcription factor HTF4; TCF12; E-box-binding protein HEB and MIF algorithms is small, the error decrease is not insigni cant. The MIF error from the objective function of Equation goes from 2.013 for the fuzzy C-means clustering to 1.084 for the MIF clustering.",
               "The clustering results of the other methods are available at gspsnap.tamu.edu/gspweb/zxb/glioma_zxb (user: gspweb; passwd: gsplab). The clustering results using MIK are similar to the results using the fuzzy C-means clustering method. Some genes in different clusters are listed in the above web site. The MIK error from the objective function of Equation goes from 2.013 for the fuzzy C-means clustering to 0.908 for the MIK clustering. Compared with the MIF and MIK algorithms, the MI and PMI methods give quite different clustering results. This is to be expected since they depend only on mutual information, not a weighted combination of mutual information and Euclidean distance factors."
          ],
          "paper_id": "222e22f0-97d4-11e8-9580-1f0eb29018a9",
          "head": {
               "type": "experiment"
          },
          "paragraphNo": 16,
          "fromPaper": "Gene Clustering Based on Clusterwide Mutual Information"
     },
     {
          "head": {
               "n": "6.",
               "text": "CONCLUSIONS",
               "type": "conclusion"
          },
          "paragraphs": [
               "In this study, we have proposed a novel clustering strategy based on minimizing mutual information among gene clusters. Simulated annealing was employed to solve the optimization problem. Bootstrap techniques were employed to get more accurate estimation of mutual information when the data sample size is small. Moreover, we proposed to combine the mutual information criterion and traditional distance criteria, such as the Euclidean distance and the fuzzy membership metric, in designing the clustering algorithm. The performance of the new clustering methods has been compared with that of some existing methods, using both synthesized data and experimental data. The clustering algorithm based on a combined metric of mutual information and fuzzy membership has achieved the best performance.",
               "Note that the clustering algorithm (named \"cluster ensembles\") of combining multiple partitions ( ) is quite different from our methods. There, given a dataset, assume there are different partitions (say, obtained by some clustering algorithms); then the cluster ensembles method is to nd a partition of the dataset that is an optimal combination of the different partitions. The mutual information dee ned in cluster ensembles is based on different partitions, and it just depends on the numbers of elements in the clusters of the partitions."
          ],
          "paper_id": "222e22f0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 17,
          "fromPaper": "Gene Clustering Based on Clusterwide Mutual Information"
     }
]