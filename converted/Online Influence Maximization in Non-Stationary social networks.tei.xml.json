[
     {
          "head": {
               "n": "0",
               "text": "abstract",
               "type": "abstract"
          },
          "paragraphs": [
               "Social networks have been popular platforms for information propagation. An important use case is viral marketing: given a promotion budget, an advertiser can choose some influential users as the seed set and provide them free or discounted sample products; in this way, the advertiser hopes to increase the popularity of the product in the users' friend circles by the world-of-mouth effect, and thus maximizes the number of users that information of the production can reach. There has been a body of literature studying the influence maximization problem. Nevertheless, the existing studies mostly investigate the problem on a one-off basis, assuming fixed known influence probabilities among users, or the knowledge of the exact social network topology. In practice, the social network topology and the influence probabilities are typically unknown to the advertiser, which can be varying over time, i.e., in cases of newly established, strengthened or weakened social ties. In this paper, we focus on a dynamic non-stationary social network and design a randomized algorithm, RSB, based on multi-armed bandit optimization, to maximize influence propagation over time. The algorithm produces a sequence of online decisions and calibrates its explore-exploit strategy utilizing outcomes of previous decisions. It is rigorously proven to achieve an upper-bounded regret in reward and applicable to large-scale social networks. Practical effectiveness of the algorithm is evaluated using both synthetic and real-world datasets, which demonstrates that our algorithm outperforms previous stationary methods under non-stationary conditions. its neighbors at each time stamp independently of the history thus far, and a node only attempts to activate a neighbor once. In the linear threshold model [1], a node will be activated only when the sum of influence probabilities from its neighbors exceeds a threshold. The influence probability in the above models, namely the probability for node u to activate its neighbor v after u has been activated, is often decided empirically in studies designing influence maximization algorithms, e.g., according to inverse of the indegree of v. Based on these information propagation models, existing studies mostly tackle the influence maximization problem on a one-off basis, assuming that both the social network topology and influence probabilities are fixed and available as input. Kempe et al. [1] prove that the influence maximization problem is NP hard but can be approximated to within a factor of (1 ? 1"
          ],
          "paper_id": "23601340-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 0,
          "fromPaper": "Online Influence Maximization in Non-Stationary Social Networks"
     },
     {
          "head": {
               "text": "A. Influence Maximization with Bandit Optimization",
               "type": "abstract"
          },
          "paragraphs": [
               "Recently, multi-armed bandit optimization has been applied to solve influence maximization problem with incomplete information of the social network. In particular, combinatorial bandits are highly correlated to the influence maximization problem, where the decision-making agent needs to select multiple arms in each time stage. Chen et al. define",
               "The simplest idea to tackle non-stationary rewards is to decrease the weights of earlier feedback in next-step decision making . The problem it may lead to is that without sufficient feedback information, it is hard to achieve a good accuracy of reward estimation. Some algorithm designs assume abrupt changes of the distributions occurring at arbitrary intervals , and allow the agent to query a set of arms not picked before and obtain outcomes as if these arms were played. This assumption is reasonable in a stock market, i.e., people can acquire information of stocks they have never purchased by following bearish or bullish trends, but not for influence propagation, where there is no channel to obtain outcomes of untried arms. Besbes et al. assume that the total variation of the rewards is given and design a randomized algorithm based on Exp3, which assigns exponential weights to arms for exploration and exploitation in adversary bandit . Only one arm is selected in each time stage, while we focus on the case of combinatorial bandits. It is non-trivial to extend the algorithm to combinatorial scenarios.",
               "Gai et al.",
               "study non-stationary bandit optimization under the assumption that the state of a selected arm evolves as an irreducible finite-state Markov process with unknown transition matrix, while the distributions of other arms stay unchanged (rested arms). Their work is applicable to many graph theory problems, e.g., channel allocation in cognitive radio networks. However, assuming rested arms is not realistic in influence propagation in social networks. The same authors further investigate restless bandits with Markov rewards , where the states of an arm evolve dynamically over time no matter whether it has been played. The algorithm utilizes regenerative property of a Markov chain and achieves a regret near logarithmic on the total number of time stages. Both studies rely on an initialization stage, in which each arm is tried for at least once. This is impractical for influence propagation (e.g., market campaign) in a large-scale network, as the cost of trying all nodes is unaffordable. use Kalman filter to update estimation of the reward distribution, and evaluate their results by simulation without theoretical analysis. Kalman filter is only applicable to linear dynamic system and the states inherently form a Markov chain. It is not realistic to make the Markov chain assumption in influence propagation, since human behavior does not simply depend on one's latest status.",
               "number of activated users after we add a into S. Let f t (S) be an influence spread function in time stage t, indicating the total number of activated nodes in t based on seed set S. The value of f t (S) is a random variable. The expectation E[f t (S)] is non-negative, monotone and submodular, as proven in . The submodularity of the spread function is useful such that we can utilize the benchmark based on greedy optimal value. The expected reward of selecting an arm a|S in t is hence E[f t (S{a})] ? E[f t (S)]. Note that the expectation E[] is taken over both randomized rewards and randomized policies, where a policy refers to the agent's strategy for seed selection, which is random given the random nature of our algorithm."
          ],
          "paper_id": "23601340-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 1,
          "fromPaper": "Online Influence Maximization in Non-Stationary Social Networks"
     },
     {
          "paragraphs": [
               "In each time stage t, starting from an empty set S t = ?, we obtain a seed set of size K by adding nodes to S t one by one in some order. Let S t = (a We model the social network as an influence graph G = (N , E). N = {1, 2, . . . , N } is the set of users (nodes), where N is the total number of nodes. E is the set of social connections among the nodes. An unknown influence probability p t is selected as the k th seed in t given previous choices in S",
               "] denote the expected marginal gain of choosing a k t as the k th t n,m is associated with each edge (n, m)E, which is time varying following an unknown, non-stationary distribution: after user n is activated (e.g., obtained information of a product), he may activate his neighbor m (e.g., share information of the product) with different probabilities at different time stages t. In this way, each edge (n, m) is associated with a non-stationary Bernoulli distribution: in t, user n may activate his neighbor m with probability p seed in t. The expected total reward in time stage t is ? r t (",
               "(1:k?1) t t n,m , or not with probability 1 ? p t n,m . We do not assume any cascade model of the information propagation system (e.g., independent cascade model or linear threshold model), and our algorithm works with various cascade models as long as the information spread brought by an activate node can be modeled as a random variable.",
               "Let T be the total number of time stages that the system spans. In each time stage, a set of K seeds are selected as information sources (e.g., the seed users that an advertiser directly promotes the product to, whose number is decided by the promotion budget), from which the information spreads to other nodes in the network. The seed set is repeatedly selected over different time stages. For example, a company may carry out a promotion campaign for a series of time stages, e.g., a number of consecutive days. After the promotion in each time stage via a potentially different set of seeds, the company collects statistics on the number of purchases of their promoted product(s) and utilizes this feedback to update its seed selection strategies in later time stages. The goal is to maximize the expected overall influence spread in the whole time span 1, 2, . . . , T , i.e., the expected total number of activated nodes. Let M be the collection of all subsets of N . In our bandit optimization framework, we define a|S, meaning node a under a given set SM, as an arm. The expected reward of selecting an arm a|S is the expected marginal gain by adding a into the existing seed set S, i.e., the expected additional",
               "In this model, maximizing the expected total number of activated nodes in 1, . . . , T is equivalent to maximizing the expected overall reward in the entire span,",
               "It is further equivalent to minimizing the regret, the gap between the expected overall reward that the agent can obtain by running our online algorithm and the offline optimal expected overall reward computed using full knowledge of the system. In our algorithm design, we aim to minimize the weak regret, i.e., the gap between the expected overall reward achieved by our algorithm and the offline expected overall reward achieved by using the same best seed set S * in all time stages, namely S *arg max",
               ", computed based on full knowledge of the entire system. Such a weak regret is the difference between the expected overall reward obtained by our algorithm and that achieved by the best single action, i.e., sticking with one seed set in all time stages. Weak regret is commonly used in the literature on analysing non-stationary bandit algorithms, and the key ingredient is to form accurate estimates on the average condition for each arm , so as to find the arm performing best in a long term. In particular, we analyze a greedy weak regret, with detailed definition given in Definition 2 in Sec. V, that compares the expected overall reward produced by our algorithm with the lower bound of an approximate offline overall reward achieved by a single best seed set derived by a greedy approach. Greedy weak regret is a concept narrowed down from weak regret, when the best single action is decided by a greedy algorithm. We apply this notion so as to compare with the lower bound of the greedy optimal value. ",
               ", ?nN , SM. N # of nodes N the set of nodes M the collection of subsets of N T the total number of time stages C input parameter to Alg. 1 K the size of seed setinput parameter to Alg. 1",
               "In Alg. 1, the K seeds are selected sequentially (line 3). The weights w associated with the nodes should be equal at the beginning of each time stage, and adjusted based on updated v, each time after the seed set has been updated (lines 4-6).",
               "The computation of w",
               "the influence spread of seed set S in t r k t (a|S",
               "reward of choosing a as k th seed based on S",
               "(1:k?1) t in t ? r k t (a|S",
               "expected reward of choosing a as k th seed based on S",
               "greedy weak regret in the whole system span Reg k (t) position weak regret for the k th seed in t a k aims to balance exploitation and exploration: the first term is calculated based on past reward information (exploitation) and the second constant term is assigned for each arm no matter how many times it has been tried (exploration). Next, the probability for adding an additional node into the already selected set of seeds is decided by normalizing its weight over the weights of all the remaining nodes not in the existing seed set (lines 7-9). An arm is randomly selected according to the probability distribution and a reward a|S "
          ],
          "paper_id": "23601340-97d4-11e8-9580-1f0eb29018a9",
          "head": {
               "type": "abstract"
          },
          "paragraphNo": 2,
          "fromPaper": "Online Influence Maximization in Non-Stationary Social Networks"
     },
     {
          "head": {
               "text": "14:",
               "type": "abstract"
          },
          "paragraphs": [
               "for all nN \\{a}, set?rset? set?r",
               "for each arm n|S",
               "end for"
          ],
          "paper_id": "23601340-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 3,
          "fromPaper": "Online Influence Maximization in Non-Stationary Social Networks"
     },
     {
          "head": {
               "text": "18:",
               "type": "abstract"
          },
          "paragraphs": [
               "end for 19: end for",
               ", ?SM, which denotes the expected overall influence spread over the whole system span. F (S) is a submodular function since it is the summation of submodular functions E[f t (S)], ?t = 1, . . . , T . Then we can design a greedy approach to compute a S that approximately maximizes the expected overall reward T t=1 ? r t (S) = T t=1 E[f t (S)] based on full knowledge of the system: after deciding S",
               "(1:k?1) , we select a local optimal node as the k th seed, that maximizes the expected marginal influence spread, i.e., node u such that uarg max",
               "We next analyze an upper bound of the greedy weak regret achieved by Alg. 1. Let OP T denote the offline maximal value of the expected overall reward T t=1 ? r t (S) = T t=1 E[f t (S)] over all SM, computed based on complete knowledge of the influence probability distributions and the social graph topologies in 1, . . . , T . Let S * be the offline optimal seed set, i.e., the single best seed set that maximizes T t=1 ? r t (S).",
               "{v})?F (S (1:k?1) )}. We can easily prove that the approximate offline solution computed this way achieves an approximation ratio of 1 ? 1 e , i.e., the expected overall reward it achieves is at least (1 ? 1"
          ],
          "paper_id": "23601340-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 4,
          "fromPaper": "Online Influence Maximization in Non-Stationary Social Networks"
     },
     {
          "head": {
               "text": "A. Reduction from Greedy Weak Regret to Position Weak Regret",
               "type": "abstract"
          },
          "paragraphs": [
               "We define a position optimal reward OP T k as the sum of the expected marginal gains achieved by using the best k th seed in all time stages. The best k th seed maximizes T t=1 ? r e )OP T , following Theorem 3.5 in , based on submodularity of the spread function and local optimality when selecting each seed. The reason that we compute this approximate offline solution using the greedy approach (which runs in polynomial time) is that computing S * has been shown to be an NP hard problem .",
               "Using the approximate offline overall reward computed as above, we define a greedy weak regret as follows, which we use to evaluate the performance of our algorithm RSB. ) based on full knowledge of the system, given the first k ? 1 seeds in S",
               "(1:k?1) t in each t derived using RSB. The idea is to reduce the original problem of finding the best solution of the full set to a parallel bandit setting, finding the best k th element under the condition determined by our algorithm. Let?Let?Let? k denote this optimal k th seed, i.e., ? a karg max Definition 2. The greedy weak regret is defined as the gap between the lower bound of the approximate offline overall reward derived by the greedy approach and the expected overall reward produced by RSB in Alg. 1, i.e.,",
               ".",
               ". Such a best k th seed",
               "The following theorem shows that the overall position weak regret provides an upper bound of the greedy weak regret. The proof can be found in Appendix B."
          ],
          "paper_id": "23601340-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 5,
          "fromPaper": "Online Influence Maximization in Non-Stationary Social Networks"
     },
     {
          "head": {
               "text": "A. Data Sets",
               "type": "experiment"
          },
          "paragraphs": [
               "Theorem 3. The greedy weak regret is upper bounded by the sum of position weak regrets over all positions k = 1, 2, . . . , K, i.e.,",
               "Based on Theorem 3, we seek to bound the position weak regret for each k, in order to derive an upper bound of Reg G (T )."
          ],
          "paper_id": "23601340-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 6,
          "fromPaper": "Online Influence Maximization in Non-Stationary Social Networks"
     },
     {
          "head": {
               "text": "B. Bounding Greedy Weak Regret",
               "type": "experiment"
          },
          "paragraphs": [
               "According to Definition 1, the position weak regret for the k th seed is",
               "Let D be the upper bound of the realization of reward, i.e., r k t (n|S)D, ?nN , SM. The following theorem states an upper bound of the position weak regret for each k. In particular, ifis set to a special value, it can minimize the regret bound. The proof can be found in Appendix C."
          ],
          "paper_id": "23601340-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 7,
          "fromPaper": "Online Influence Maximization in Non-Stationary Social Networks"
     },
     {
          "head": {
               "text": "1) Tencent Weibo Traces: We produce a dynamic social network based on Tencent Weibo",
               "type": "experiment"
          },
          "paragraphs": [
               "2 traces containing the following links among 4257 users for 7 consecutive days during November 2011. Each directed following link (n, m) indicates that user n follows user m . The links among the users vary from one day to the next, giving a dynamic social graph. To prolong the trace duration, we further repeat the variation of the social graph on 7-day cycles to form a 100-day duration (T ), which we believe reasonable since human behavior may well follow a weakly periodicity.",
               "2) Synthetic Data: As Weibo traces only provide the dynamic behavior of a specific social network, we also generate a synthetic dynamic social network by combining the model in with the Erd? os-Rnyi model and preferential attachment: we generate an initial graph with 5000 nodes and connect each pair of node with probability 0.005 (a directed link); then in each time stage, we select 1000 edges uniformly and change their heads to other nodes randomly picked with probabilities proportional to their indegrees. In this way, we generate a sequence of social graphs for T = 100 time stages. Preferential attachment is a representative mechanism to model the topology of a social network, that the more connected a node is, the more likely it is to receive new links.",
               ") be the"
          ],
          "paper_id": "23601340-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 8,
          "fromPaper": "Online Influence Maximization in Non-Stationary Social Networks"
     },
     {
          "head": {
               "text": "B. Time-varying Influence Probabilities",
               "type": "experiment"
          },
          "paragraphs": [
               "We employ the following three models to generate nonuniform and time-varying influence probabilities in a social graph. ",
               "If we set= min{1,",
               "The greedy weak regret achieved by Alg. 1 is upper bounded as follows:",
               "m is the indegree of node m at t. The probabilities are varying due to the changes of links in a dynamic social graph.",
               "? The Trivalency (TR) model : in each time stage, the influence probability of an edge in the social graph is assigned a value among {0.1, 0.01, 0.001} uniformly randomly, corresponding to three types of social tiesstrong, medium and weak. The assigned probability on an edge may change from one time stage to the next.",
               "? A Fluctuating Reward (FR) model. We design this model such that influence probabilities evolve over time in a similar fashion as a sinusoidal wave (also similar to that used in ): the influence probability of each edge starts from a random value drawn uniformly within [0, 0.1]; then in each time stage, it increases or decreases at a constant rate 0.3",
               "T until reaching the largest value 0.1 or the smallest value 0.",
               "i.e., the upper bound of the greedy weak regret of Alg. 1 is",
               "It shows that our greedy weak regret is sublinear with both N and T . The proof can be found in Appendix D."
          ],
          "paper_id": "23601340-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 9,
          "fromPaper": "Online Influence Maximization in Non-Stationary Social Networks"
     },
     {
          "head": {
               "text": "C. Schemes for Comparison",
               "type": "experiment"
          },
          "paragraphs": [
               "We compare RSB with a random algorithm and OG-UCB proposed in . With the random algorithm, the agent always selects a seed uniformly randomly among all candidate nodes. OG-UCB is designed for stationary scenarios, which associates a confidence bound with each arm and chooses the arm with the highest upper confidence bound greedily.",
               "We note that although a number of bandit algorithms have been proposed for influence maximization (as discussed in Sec. II-A), most are not directly comparable since they run with the complete knowledge of a social network. We compare with OG-UCB since it is the only existing bandit algorithm without requiring knowledge of the social graph topology. In addition, the bandit algorithms designed for non-stationary systems in Sec. II-B either deal with 1 arm or assume Markov rewards, and hence cannot be readily extended for comparison.",
               "In computing greedy weak regret, we also compute the approximate offline optimal overall reward by the greedy offline algorithm discussed before Definition 2 in Sec. V."
          ],
          "paper_id": "23601340-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 10,
          "fromPaper": "Online Influence Maximization in Non-Stationary Social Networks"
     },
     {
          "head": {
               "text": "D. Evaluation Results",
               "type": "experiment"
          },
          "paragraphs": [
               "To show greedy weak regret values in a unified range in our figures, we plot the ratio between greedy weak regret and the approximate offline optimal overall reward, i.e., approx. offline opt. overall reward?overall reward by RSB approx. offline opt. overall reward . Especially, a data point at a specific T represents the above ratio computed using overall rewards in . We set K = 5,= 0.2 (default), D = 120 and C = 1 in our experiments. show the results obtained using synthetic data or Tencent Weibo traces under different time-varying models of influence probabilities. We observe that the regret ratios (and hence information spread) achieved by RSB and the random algorithm are usually similar at the early stages of the system, when RSB has not cumulated much feedback. RSB gets better than the other algorithms (lower regret and hence better spread) after more time stages, validating that RSB can improve with more feedback received from the real system. Besides, OG-UCB performs the worst especially with the ongoing of time, showing that it is only suitable for fixed influence probability distributions and does not work well in cases of time-varying influence probabilities. The increase of cumulative regret by RSB with the increase of time stages, if any, is always slower than that of the other algorithms.",
               "In , we compare the regret ratios of RSB achieved under different values of input parameter, using Tencent Weibo traces under the FR model. From line 5 of Alg. 1, we can see= 0 represents pure exploitation and= 1 indicates pure exploration. Although Theorem 4 requires> 0 for the bound to be meaningful, we can still test the extreme case that= 0 when running the algorithm in practice. RSB performs worst in these extreme cases.? = 0.18 is computed following the formula in Theorem 4 which minimizes the theoretical upper bound. We observe that? achieves nearlowest regrets in actual execution of our algorithm under practical settings as well.",
               "In , we evaluate the impact of different graph sizes N , by extracting subgraphs of different sizes using Tencent Weibo traces. We observe that the regret is larger in larger networks, but it always improve when the system runs for longer period of time.   Proof: We prove the following inequality for each position k by induction.",
               "The base case k = 0 is trivial. In the induction, let .",
               "Then we have The inequality (5) uses the fact that e x1 + x + (e ? 2)x Then we can get for x1 and the inequality (6) is derived by the facts (3) and (4). Using the inequality 1 + xe x and taking logarithms, we have )."
          ],
          "paper_id": "23601340-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 11,
          "fromPaper": "Online Influence Maximization in Non-Stationary Social Networks"
     },
     {
          "head": {
               "text": "n=1",
               "type": "experiment"
          },
          "paragraphs": [
               "Since over the whole period, the expected marginal gain of any node nN is no larger than that of the best seed, which is R . Then summing over t, we can get all reward obtained by the agent over period 1, . . . , T for the position k as follows.",
               "max . Since node n j is chosen arbitrarily, we can choose it as the best seed?seed?seed? k under the conditional set S",
               "(1:k?1) t , thus we have T t=1 ? r k t (? a k |S",
               "(1:k?1) t ) = R k max . Combined with these two results, we have",
               "Note that the expectation above is under any conditional set S",
               "(1:k?1) t For any node n jN whatever the agent selects it, we have , ?t = 1, 2, . . . , T , which is related to previous choices for position 1, . . . , k ? 1. This is consistent with Definition 1, that S is decided by Alg. 1. The expectation in R",
               "max is also reduced to randomizing on reward only. Since Then we take the expectation on policy's actions as well as random rewards. Noting that given the choice a 1 , a 2 , . . . , a t?1 before, for any node n jN \\S Note that if= 1, we haveN C ln N(1 + (e ? 2)"
          ],
          "paper_id": "23601340-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 12,
          "fromPaper": "Online Influence Maximization in Non-Stationary Social Networks"
     }
]