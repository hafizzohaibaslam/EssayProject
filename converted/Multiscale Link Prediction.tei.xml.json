[
     {
          "head": {
               "n": "0",
               "text": "abstract",
               "type": "abstract"
          },
          "paragraphs": [
               "1. INTRODUCTION"
          ],
          "paper_id": "234167b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 0,
          "fromPaper": "Multi-Scale Link Prediction"
     },
     {
          "head": {
               "n": "2.",
               "text": "RELATED WORK",
               "type": "relatedwork"
          },
          "paragraphs": [
               "Link prediction refers to the problem of inferring new interactions among members in a network. The first systematic treatment of the problem appeared in , where a variety of proximity measures, such as Common Neighbors Assume we are given a graph G = (V, E), where V = {1,, n} is the set of vertices representing the users in a social network and E = {eij|i, jV} is the set of weighted edges quantifying the connection between user i and user j. Let A = [aij] be the corresponding nn adjacency matrix of G such that aij = eij, if there is an edge between i and j and 0 otherwise. For simplicity, we assume G is an undirected graph, i.e., A is symmetric.",
               "As shown in , proximity measures can be computed from A. Many of these measures can be represented as a matrix function f (A), where the (i, j)-th element represents the value of a proximity measure between user i and user j . One popular measure is the number of common neighbors, which can be captured by fcn(A) = A 2 , describing a very localized view of interactions between vertices by considering only paths of length 2. A more extensive measure is the popular Katz measure . Such path-based proximity measures often achieve better accuracy at the cost of higher computational complexity. The Katz measure is defined as follows where the diagonal blocks Aii, i = 1, . . . , c, correspond to the local adjacency matrix of each cluster i. For every cluster, the best rank-r approximation is computed as AiiUiiU T i , wherei is a diagonal matrix with the r largest eigenvalues of Aii, and Ui is an orthonormal matrix with the corresponding eigenvectors. Finally, CLRA aligns the low-rank approximation of each cluster together to obtain the clustered low-rank approximation of the entire adjacency matrix A. Mathematically,",
               "where I is the identity matrix and1/A2 is a damping parameter. As we can see, the Katz measure takes O(n 3 ) time, which is computationally infeasible for large-scale networks with millions of nodes.",
               "Here, dimensionality reduction methods, such as the singular value decomposition (SVD), play an important role. These methods are particularly useful, since it suffices to have a reasonably good estimation of a given proximity measure for most applications. Furthermore, low-rank approximation of the adjacency matrix serves as a useful conceptual and computational tool for the graph.",
               "Assume that we are given a rank-r approximation of the nn matrix A as follows",
               "AijUi, for i, j = 1, . . . , c, which is the optimal S in the least squares sense. Note that the blockdiagonal matrix U = diag(U1, . . . , Uc) is also orthonormal and Sii =i are diagonal. It is shown in that CLRA achieves accurate approximations while being efficient in both computational speed and memory usage. However, the drawback of CLRA is that it only uses one clustering structure, whereas it has been shown that many large social networks lack such structure . In this paper, we overcome this limitation by taking a multi-scale approach.",
               "Based on the estimated proximity measures, we can perform link prediction on social networks. Link prediction deals with networks that evolve over time Specifically, given a snapshot of a network at time t1, the task is to predict links that would form at a future time step t2. A high proximity score between two users captures the high correlation between them and thus a high chance to form a new link in the future.",
               "where U is an nr orthonormal matrix (i.e. U T U = Ir is an identity matrix), and S is an rr matrix. Using this low-rank approximatio? A, the CN measure can be approximated as fcn(A)U S 2 U T . Similarly, the Katz measure is approximated by",
               "In general, f (A)U f (S)U T , which requires less computational resources as the matrix function is only evaluated on the much smaller S matrix.",
               "However, computing the low-rank approximation of a massive graph via SVD or other popular dimensionality reduction methods can still be a computational bottleneck. Recently, the technique of clustered low rank approximation (CLRA) was proposed in as a scalable and accurate low-rank approximation method. The basic idea of CLRA is to preserve important structural information by clustering the graph G into c disjoint clusters. Then it computes a low-rank approximation of each cluster, which is finally extended to approximate the entire graph.",
               "Assume that the graph has been clustered into c clusters and the vertices are ordered as follows In this section we present our multi-scale link prediction (MSLP) framework for social networks. Our method mainly consists of three phases: hierarchical clustering, subspace approximation and multi-scale prediction. Specifically, we first construct a hierarchy tree with a fast top-down hierarchical clustering approach. Then, a multi-scale low-rank approximation to the original graph is computed when traversing the hierarchy in a bottom-up fashion. An important technical contribution of our paper is a fast tree-structured approximation algorithm that enables us to compute the subspace of a parent cluster quickly by using subspaces of its child clusters. This allows us to compute each level's lowrank approximation efficiently. Finally, we combine proximity measures, which are computed using the multi-scale low-rank approximation of the graph, and make our final predictions."
          ],
          "paper_id": "234167b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 1,
          "fromPaper": "Multi-Scale Link Prediction"
     },
     {
          "head": {
               "n": "4.1",
               "text": "Hierarchical Clustering",
               "type": "relatedwork"
          },
          "paragraphs": [
               "The first step of our method is to hierarchically cluster or partition a given graph. The purpose of this is to efficiently generate a multi-scale approximation of the graph using the constructed hierarchical structure. This, in turn, makes predictions more accurate and robust as we combine predictions at each level of the hierarchy in the final step.",
               "Generally, there are two main approaches for hierarchical clustering: agglomerative (or bottom-up) approach and divisive (or top-down) approach. The agglomerative approach initially treats each vertex as one cluster and continually merges pairs of clusters as it moves up the hierarchy. The : Percentage of within-cluster edges using Graclus. Numbers in brackets represent random clustering. It can be seen that Graclus is quite effective in finding good clustering structure. (these networks contain about 2 million nodes -details are given in ",
               "level 2",
               "Figure 1: Hierarchical structure example.",
               "divisive approach takes the opposite direction, that is, all vertices are placed in a single cluster and recursively partitioned into smaller clusters. Due to the large scale of the problem and the availability of efficient clustering software, such as Graclus , we employ the divisive approach in our work. Given a graph G = (V, E), our goal is to construct a level hierarchy, so as to generate a multi-scale view of the graph. We form c nodes (clusters) at the first level of the hierarchy by clustering V into c disjoint sets V scheme is also very fast. Clustering the three networks of into 5 levels with 2 clusters at each level can be completed in just 5 minutes on a 8-core 3.40GHz machine. In the next section, we show how to use the hierarchy structure to efficiently construct a multi-scale approximation of largescale graphs."
          ],
          "paper_id": "234167b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 2,
          "fromPaper": "Multi-Scale Link Prediction"
     },
     {
          "head": {
               "n": "4.2",
               "text": "Subspace Approximation",
               "type": "relatedwork"
          },
          "paragraphs": [
               "(1)",
               "(1)",
               "c , where the superscript denotes the level of the hierarchy. Then, we proceed to the second level of the hierarchy tree by further clustering each node in the first level V After constructing the hierarchy for a given graph, we can compute low-rank approximations of A at each level of the hierarchy to obtain a multi-scale approximation. Specifically, we employ CLRA to obtain the approximation. gives an example of a simple three level hierarchy to better illustrate our method. By applying CLRA on each of the 3 levels in the example, we have 3 clustered low-rank approximations of A as follows",
               "? Level 0:",
               ". . , c, and generate c child nodes from each of them as V",
               "As a consequence, we will have c 2 nodes on the second level of the hierarchy. This process repeats until the desired number of levels is reached. Many classic clustering methods can be used as a base clustering method. In this paper, we use the Graclus algorithm to cluster each node because of its ability to scale up to very large graphs. However, our algorithm can utilize any other graph clustering method.",
               "The hierarchy of A at level p, after sorting the vertices, can be written as",
               "? Level 1:",
               "? Level 2:",
               "where the columns of",
               "wherc = c p is the number of nodes in level p and each are the set of orthonormal basis vectors forming the subspace for cluster i at level p and",
               "c, is an mimi matrix that can be viewed as a local adjacency matrix of cluster i at level p. The off-diagonal mimj blocks, A (p) ij , where i = j, contains the set of edges between clusters i and j.",
               "As it is desirable to capture most of the links within clusters, we compare with random clustering in terms of the percentage of within-cluster links on three large-scale social networks in . For each level of the hierarchy tree, the within-cluster links are those that connect two vertices in the same cluster. As shown in , the percentage of within-cluster edges of random clustering is much smaller than the hierarchical clustering scheme used in this paper, and the gap becomes much larger when going down the hierarchy. Even at the deepest level, the clustering scheme we use can still capture more than half of the edges compared with less than 10% in the LiveJournal and MySpace graphs when using random clustering.",
               "As a final remark, we note that the hierarchical clustering j , 1i, j2 p . Level 0 can be viewed as a special case of CLRA, where the entire graph is treated as a single cluster, which yields a global view of the entire matrix A. Lower levels in the hierarchy will preserve more local information within each cluster. Thus, each level of approximation concentrates on different levels of granularity, resulting in a multi-scale approximation of A.",
               "An important issue here is how to compute each level's approximation of A efficiently. A straightforward solution would be use standard dimensionality reduction methods, "
          ],
          "paper_id": "234167b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 3,
          "fromPaper": "Multi-Scale Link Prediction"
     },
     {
          "head": {
               "text": "0.96",
               "type": "relatedwork"
          },
          "paragraphs": [
               "Algorithm 1: Tree-structured approximation of dominant subspace of parent cluster from child clusters Input: nn adjacency matrix of parent cluster A = A (P ) , child cluster's subspaces U , . . . , U 0.94"
          ],
          "paper_id": "234167b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 4,
          "fromPaper": "Multi-Scale Link Prediction"
     },
     {
          "head": {
               "text": "0.92",
               "type": "relatedwork"
          },
          "paragraphs": [
               ", target rank r. Output: dominant subspace for parent cluster A (P ) , i.e. U (P ) . ) and parent cluster's subspace U (P ) tree computed using Algorithm 1.",
               "such as SVD. This can be computed efficiently for clusters at the deepest level of the hierarchy tree, since the size of each cluster is relatively small. However, the computational cost becomes prohibitive as the size of the cluster increases, which is the case for upper levels in the hierarchy tree. We propose a more scalable and effective method to address this issue. For clarity and brevity, we focus on a local view of the hierarchy as shown in , where A (P ) is a parent ",
               "11 and A 22 are its two child clusters. A key observation we make is that the subspaces between any two adjacent levels in the hierarchy tree should be close to each other. That is, U (P ) should be close to",
               "). This is because many of the links that are in A (P ) should be captured by its child clusters A"
          ],
          "paper_id": "234167b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 5,
          "fromPaper": "Multi-Scale Link Prediction"
     },
     {
          "head": {
               "text": "(C) 11",
               "type": "relatedwork"
          },
          "paragraphs": [
               "and A",
               "22 . Consequently, if we are given diag(U",
               "), one should be able to compute U (P ) faster than computing it from scratch. Thus, we propose an algorithm that uses the child cluster's subspace to compute the parent cluster's subspace.",
               "Our proposed method, tree-structured approximation of subspace, is listed in Algorithm 1. The main idea is to construct a matrix Y = A (P ) ? that covers as much of the range space of A (P ) as possible. This can be done efficiently using jj into c clusters. // e.g. Graclus end end /* Subspace approximation */ // approximation for deepest level. Compute U () , S () using CLRA. // approximation for intermediate levels.",
               "Compute U (i) using Algorithm 1.",
               "). Note that both A (P ) and ? are sparse. Then, an orthonormal matrix Q is computed from Y as a basis for the range of Y (e.g. using the QRdecomposition). Finally, Q is further used to compute U (P ) (P ) via the eigen-decomposition of the matrix B = Q T AQ. This last step is also fast since B is a small crcr matrix, where r is the rank of the approximation of each child node.",
               "The subspace approximation scheme in Algorithm 1 is more efficient than the truncated eigen-decomposition (EIG), since the latter needs to be computed from scratch and is time consuming when dealing with large-scale matrices. We note that Y = (AA T )A? can be used for higher accuracy, though we did not find any significant improvement in the results. shows principal angles between the parent cluster's subspace U levels are close to each other. We also show principal angles between U eig and the parent cluster's subspace U tree computed using Algorithm 1. We see that these subspaces are even closer to each other, showing that our algorithm can accurately approximate the parent cluster's subspace. ) for the Flickr dataset. The cosine of principal angles are close to 1, supporting our observation that the subspaces of two adjacent As mentioned in Section 2, many proximity measures for link prediction f (A) are expensive to compute on large-scale networks because of their high complexity. One solution is to approximate A by a low-rank approximatio? A and then compute approximated proximity measures with f ( ? A) to make predictions. This stems from the idea that most of the action in A can be captured by a few latent factors, which can be extracted with low-rank approximations of A."
          ],
          "paper_id": "234167b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 6,
          "fromPaper": "Multi-Scale Link Prediction"
     },
     {
          "head": {
               "n": "4.3",
               "text": "Multi-Scale Link Prediction",
               "type": "relatedwork"
          },
          "paragraphs": [
               "It has been shown that CLRA provides an accurate and scalable low-rank approximation, and can be used for effi- Memory usage: For a rank-r approximation, EIG needs to store r eigenvectors and eigenvalues which takes O(nr + r) memory. Compared with EIG, CLRA is memory efficient as it only takes O(nr + c 2 r 2 ) memory for a larger rank-c r approximation . MSLP basically has the same memory usage as CLRA. While MSLP achieves a multi-scale approximation, it is not necessary to store the subspaces for all levels simultaneously. We can reuse the memory allocated for the child cluster's subspace to store the parent cluster's subspace using Algorithm 1."
          ],
          "paper_id": "234167b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 7,
          "fromPaper": "Multi-Scale Link Prediction"
     },
     {
          "head": {
               "n": "5.",
               "text": "EXPERIMENTAL EVALUATION",
               "type": "experiment"
          },
          "paragraphs": [
               "cient proximity estimation . However, CLRA just uses one clustering structure making it sensitive to a particular clustering and biased against links that appear between clusters. Our proposed method alleviates such problem with a multi-scale approach.",
               "The main idea is that, under a hierarchical clustering, all links will eventually belong to at least one cluster. That is, even if we miss a between-cluster link at a certain level, it still has a good chance of getting corrected by upper levels as it will eventually become a within-cluster link. Moreover, links that lie within clusters at multiple levels, such as from the deepest level, get emphasized multiple times. Those links will have the propensity of being included in the final prediction, which aligns with the intuition that links are more likely to form within tight clusters.",
               "Once the multi-scale low-rank approximation of A is obtained, we now perform multi-scale link prediction. From each low-rank approximation of the hierarchy, ? A (i) , i = 0, 1, . . . , the approximated proximity measure can be computed with f ( ? A (i) ). This gives a total of + 1 proximity measures for each link, which are combined to make final predictions. Formally, our multi-scale predictions are given by In this section we present experimental results that evaluate both accuracy and scalability of our method, MultiScale Link Prediction (MSLP), for link prediction. First we present a detailed analysis of our method using the Karate club network as a case study. This will give a better understanding of our algorithm and illustrate where it succeeds. Next we provide results under different parameter settings on a large social network. Lastly, we compare MSLP to other popular methods on massive real-world social networks with millions of users and demonstrate its superior performance."
          ],
          "paper_id": "234167b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 8,
          "fromPaper": "Multi-Scale Link Prediction"
     },
     {
          "head": {
               "n": "5.1",
               "text": "Case study: Karate club",
               "type": "experiment"
          },
          "paragraphs": [
               "where wi's are the weights for different levels and g() is the predictor, e.g. top-k scoring links. For simplicity, we use the same weight for all levels in this work, i.e. wi = 1/( + 1). The entire flow of our proposed method, Multi-Scale Link Prediction (MSLP), is listed in Algorithm 2. Next, we analyze the computation time and memory usage of MSLP.",
               "Computation time: As mentioned earlier, the hierarchical clustering is fast and linear in the number of edges in the network and can be finished in a few hundred seconds on networks with 2 million nodes. Computing the approximated proximity scores as a final step for a given user is simply a matrix multiplication of low-rank matrices and time complexity is O(nr 2 ). In general, we set the number of clusters c and the rank in each cluster r to be fairly small. Among the three phases of MSLP, the subspace approximation phase is the dominant part of the computation time.",
               "In , we compare the CPU time for subspace approximation by Algorithm 1 and EIG on three large-scale social networks with about 2 million users. We can see in that for each intermediate level from 4 to 0, the subspace approximation in MSLP is up to 10 times faster than that of EIG, demonstrating the effectiveness of Algorithm 1. Furthermore, since we operate on each cluster independently, MSLP can be easily parallelized to gain greater speedups.",
               "We first start our performance analysis on a well-known small social network, Zachary's Karate club network . The Karate club network represents a friendship network among 34 members of the club with 78 links. The clustering structure of the Karate club network is a standard example for testing clustering algorithms. We adopt the clustering results from , where the clustering is found via modularity optimization. shows the hierarchy of the Karate club network. The first level has 2 clusters (circle and triangle) with 68 within-cluster links and the second level has 4 clusters (red, yellow, green and blue) with 50 within-cluster links.",
               "As the Karate club network is a small network, we apply the leave-one-out method to compare different methods. We first remove a single link from the network, treat the held out edge as 0 in A, and perform link prediction on the resulting network. For each leave-one-out experiment, we compute the rank of the removed link based on its proximity measure. If the rank of the removed link appears in the top-k list, we count it as a hit. The number of top-k hits is the number of hits out of all leave-one-out experiments.",
               "We compare MSLP to four other methods: RandCluster, common neighbors (CN), Katz and CLRA. In RandCluster, we randomly partition the graph into 4 clusters and compute the Katz measure using CLRA with these clusters. shows the number of top-k hits for each method. Clearly, our method significantly outperforms other methods by achieving a much higher number of hits. This implies that MSLP makes more accurate predictions by considering the hierarchical structure of the network. RandCluster performs the worst, while CLRA has comparable performance with Katz indicating that the network's property can be captured by a few latent factors.",
               "For a better illustration of the advantage of our method, we annotate with the results of top-3 hits. The solid blue links correspond to hits made by MSLP and the dashed red links are hits made by both CLRA and MSLP, i.e. the set of links successfully predicted by CLRA is a subset of that of MSLP. We can see that all hits made by CLRA are within-cluster links (green cluster), showing that CLRA favors within-cluster links. In contrast, MSLP can predict not only more within-cluster links, but also links between clusters (red and yellow). The ability to correctly predict both within and between-cluster links is one of the main advantages of our multi-scale approach."
          ],
          "paper_id": "234167b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 9,
          "fromPaper": "Multi-Scale Link Prediction"
     },
     {
          "head": {
               "n": "5.2.1",
               "text": "Evaluation methodology",
               "type": "experiment"
          },
          "paragraphs": [
               "We evaluate the accuracy of different methods by computing the true positive rate (TPR) and the false positive rate (FPR), defined by TPR = # of correctly predicted links # of actual links , In this section we present the results of link prediction on large real-world datasets. We start by examining how the parameters of MSLP affect performance. Particularly, we investigate how different hierarchical clustering structures impact the performance of MSLP. For this, we use a large real-world network: Epinions, which is an online social network from Epinions.com with 32,223 users and 684,026 links ."
          ],
          "paper_id": "234167b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 10,
          "fromPaper": "Multi-Scale Link Prediction"
     },
     {
          "head": {
               "n": "5.2",
               "text": "Experiments on Large Datasets",
               "type": "experiment"
          },
          "paragraphs": [
               "Next we use three real-world massive online social networks with millions of nodes: Flickr, LiveJournal and MySpace , and compare MSLP to other methods. These datasets have timestamps associated with them and we summarize each snapshot in . The adjacency matrix at the first timestamp, At 1 , is used to compute proximity measures, and the adjacency matrix at the next timestamp, At 2 , is used for testing and evaluation. Since these networks are very large, we randomly select 5,000 users and evaluate on these users. Performance measures are averaged over 30 iterations of such sampling.",
               "As pointed out in , most of all newly formed links in social networks close a path of length two and form a triangle, i.e., appear in a user's 2-hop neighborhood. All three datasets show that this is the case for at least 90% of test links in the second timestamp. For similar reasons as in , we focus on predicting links to users that are within its 2-hop neighborhood.",
               "for all links in a sampled test set. Our evaluation is based on receiver operating characteristic (ROC) curve and its area under the ROC curve (AUC) that present achievable TPR with respect to FPR. Predicting links with proximity measures involves some thresholding on the measures to produce top-k predictions. The ROC curves captures the full spectrum of prediction performance by varying the decision threshold.",
               "However, in a practical sense, a user is recommended only a small number of top-k predictions and the hope is that most of them are correct. Thus, we focus on the region of low FPR by plotting FPR along the x-axis in log-scale, since it reflects the quality of these top-k links. In the same spirit, we also use the Precision at Top-k, i.e., the number of correct predictions out of top-k recommendations, as our evaluation metric. Other methods for comparison: We have carefully chosen a variety of proximity measures to compare with: Preferential Attachment (PA), Adamic-Adar score (AA), Random Walk with Restarts (RWR), common neighbors (CN) and Katz . The actual values of Katz quickly becomes difficult to compute as scale increases due to its high computational cost. Therefore, we employ the Lanczos method for its speed and good approximation of the real Katz values. We also consider a supervised machine learning method (LR) . For the latter, we extracted five network-based features: paths of lengths 3, 4 and 5, CN, and AA. Using these features, a logistic regression model is trained over a sampled set of positive and negative links from 10,000 users as in . Next we evaluate on different parameter settings by varying the three main parameters of MSLP: number of levels in the hierarchy , number of clusters at each node in the : Varying hierarchical clustering structure by changing (a) the number of clusters per node at each level and (b) the number of levels on Epinions dataset. Results show that MSLP is not only more robust than CLRA to different clustering structures, but also outperforms other methods in most cases. Percentage is the percentage of within-cluster edges (Numbers in brackets represent percentage of within-cluster edges of random clustering)."
          ],
          "paper_id": "234167b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 11,
          "fromPaper": "Multi-Scale Link Prediction"
     },
     {
          "head": {
               "text": "LiveJournal",
               "type": "experiment"
          },
          "paragraphs": [
               "(a) Changing the number of clusters per node at each level. hierarchy c, and rank r. We fix = 3, c = 2, and r = 20 while changing one parameter at a time and measure AUC and precision at top-20. The Epinions network does not have time information, thus we randomly sample a number of links and treat them as test links in At 2 . The sampling is performed such that about 90% of test links appear in a user's 2-hop neighborhood.",
               "We compare the performance of our method to two other low-rank approximation methods: eigen-decomposition (EIG), clustered low rank approximation at the deepest level in the hierarchy tree (CLRA). Rank: shows performance of the three low-rank approximation methods with different ranks. MSLP consistently performs better in terms of both AUC and Precision at Top-20 than the other two methods. The accuracy of CLRA deteriorates as the rank increases. This implies that the low-rank approximation of each cluster starts to accumulate noise at larger ranks. It is shown that low-rank approximation methods tend to perform best at an intermediate rank , which is also the case here with r = 20. methods in all cases with the only exception of (a) at c = 5. The results clearly show that MSLP is robust to different hierarchical structures. Furthermore, to see that MSLP is robust to cluster structures, we randomly perturb clusters at the deepest level of the hierarchy by moving vertices from their original cluster to another random cluster. shows the result of moving 0%, 10% and 20% of vertices. Even with 10% of vertices shuffled, MSLP still outperforms CLRA with no shuffling. It is clear that, while CLRA's performance decreases rapidly, MSLP still performs well in low FPR regions."
          ],
          "paper_id": "234167b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 12,
          "fromPaper": "Multi-Scale Link Prediction"
     },
     {
          "head": {
               "n": "5.2.3",
               "text": "Results on Large-scale social networks",
               "type": "experiment"
          },
          "paragraphs": [
               "Hierarchical clustering structure: Next we experiment with various hierarchical clustering structures. shows how the performance changes as the hierarchical clustering structure changes. For a complete comparison, results of other methods are also given in While the accuracy of CLRA degrades as the percentage decreases, MSLP is still able to perform better than other In this section, we present results on real-world networks with millions of nodes presented in . We construct a hierarchical structure with = 5 and c = 2 for all three networks, and use r = 100 for EIG, CLRA and MSLP. We set= 0.0005 for the Katz measure, which yields the best results. give AUC and precision at top-100 results for the various methods, respectively. MSLP-Katz gives a significant improvement over the Katz measure and outperforms all other methods. Specifically, it gains a relative improvement of up to 4% in AUC and 15% in precision over the next best performing method. We emphasize the superior   formance in terms of precision compared to MSLP in the MySpace dataset, where only 56% of the edges are within clusters, whereas MSLP achieves the best result. Overall, the superior performance of MSLP illustrates the effectiveness of our multi-scale approach.",
               "We note that the majority of time is taken by computing CLRA at the deepest level and thereafter low-rank approximations of upper levels can be obtained efficiently due to Algorithm 1. However, CLRA can be easily parallelized as computing the subspace of each cluster is independent to other clusters. Thus, MSLP can achieve much more speedup by using a parallel implementation and serve as a highly scalable method for link prediction."
          ],
          "paper_id": "234167b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 13,
          "fromPaper": "Multi-Scale Link Prediction"
     },
     {
          "head": {
               "n": "6.",
               "text": "CONCLUSIONS",
               "type": "conclusion"
          },
          "paragraphs": [
               "performance in terms of precision at top-100 of MSLP-Katz as shown in . This is a very appealing aspect of MSLP as it reflects the quality of top recommendations. In contrast, MSLP-CN remains comparable to CN, but performs better than EIG and CLRA. Surprisingly, the supervised method LR does not perform well, which is consistent with results found in . Note that we only use networkbased features and no additional features for training. However, engineering for more features is a difficult task and constructing good features itself can be computationally expensive. gives ROC curves focused on the low FPR region for the three large-scale networks. We note that only one representative method from methods that have similar performance is plotted for the sake of clarity. We observe that MSLP-Katz performs the best in all three datasets with significant improvements over Katz. For a given TPR, MSLP reduces FPR by 10% on average and at most 20% compared to others in all datasets. For completeness, we present the full range of the ROC curve for LiveJournal in . Note that much of the performance boost comes from the left side of the curve, which corresponds to the area of interest. That is, MSLP achieves good prediction quality for the highest predicted scores.",
               "While dimensionality reduction methods, such as EIG and CLRA, tend to perform well in all three datasets, they are limited to a single low-rank representation of the network. Furthermore, CLRA has the largest drop in relative perIn this paper, we have presented a general framework for multi-scale link prediction by combining predictions from multiple scales using hierarchical clustering. A novel treestructured approximation method is proposed to achieve fast and scalable multi-scale approximations. Extensive experimental results on large real-world datasets have been presented to demonstrate the effectiveness of our method. This significantly widens the accessibility of state-of-the-art proximity measures for large-scale applications.",
               "For future work, we plan to investigate methods to learn the weights for various levels of the hierarchy, since some levels may have better predictions and deserve larger weights in the final prediction. In this work, we use a balanced hierarchical structure mainly for its simplicity in combining predictions. However, a more realistic setting would be to use an unbalanced hierarchical clustering structure. The issue here is how to combine predictions from different levels as some links may not receive predictions at certain levels. We also plan to develop a parallelized version of MSLP as each level of the hierarchy can be easily parallelized."
          ],
          "paper_id": "234167b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 14,
          "fromPaper": "Multi-Scale Link Prediction"
     },
     {
          "head": {
               "n": "7.",
               "text": "ACKNOWLEDGMENTS",
               "type": "acknowledgement"
          },
          "paragraphs": [
               "This research was supported by NSF grants CCF-1117055 and CCF-0916309.  "
          ],
          "paper_id": "234167b0-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 15,
          "fromPaper": "Multi-Scale Link Prediction"
     }
]