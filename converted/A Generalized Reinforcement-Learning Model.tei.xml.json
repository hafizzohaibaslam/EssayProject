[
     {
          "head": {
               "n": "0",
               "text": "abstract",
               "type": "abstract"
          },
          "paragraphs": [
               "Reinforcement learning is the process by which an autonomous agent uses its experience interacting with an environment to improve its behavior. The Markov decision process (mdp) model is a popular way of formalizing the reinforcement-learning problem, but it is by no means the only way. In this paper, we show how many of the important theoretical results concerning reinforcement learning in mdps extend to a generalized mdp model that includes mdps, two-player games and mdps under a worst-case optimality criterion as special cases. The basis of this extension is a stochastic-approximation theorem that reduces asynchronous convergence to synchronous convergence."
          ],
          "paper_id": "2115ae10-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 0,
          "fromPaper": "A Generalized Reinforcement-Learning Model: Convergence and Applications A Generalized Reinforcement-Learning Model: Convergence and Applicationa A Generalized Reinforcement-Learning Model: Convergence and Applications"
     },
     {
          "head": {
               "n": "1",
               "text": "INTRODUCTION",
               "type": "introduction"
          },
          "paragraphs": [
               "Reinforcement learning is the process by which an agent improves its behavior in an environment via experience. A reinforcement-learning scenario is deened by the experience presented to the agent at each step, and the criterion for evaluating the agent's behavior.",
               "One particularly well-studied reinforcement-learning scenario is that of a single agent maximizing expected discounted total reward in a nite-state environment; experiences are of the form hx; a; y; ri, where x is a state, a is an action, y is a resulting state and r is the scalar immediate reward to the agent. A discount parameter 0 < 1 controls the degree to which future rewards are signiicant compared to immediate rewards.",
               "The theory of Markov decision processes can be used as a theoretical foundation for important results concerning this reinforcement-learning scenario 1]. A ((nite) Markov decision process (mdp) 18] is deened by the tuple hS; A; P; Ri, where S represents a nite set of states, A a nite set of actions, P a transition function, and R a reward function.",
               "The optimal behavior for an agent in an mdp depends on the optimality criterion; for the innnite-horizon discounted criterion, the optimal behavior can be found by identifying the optimal value function, deened recursively by V (x) = max a R(x; a) + X y P(x; a; y)V",
               "! ;",
               "for all states x 2 S, where R(x; a) is the immediate reward for taking action a from state x, 0 < 1 is a discount factor, and P(x; a; y) is the probability that state y is reached from state x when action a 2 A is chosen. These simultaneous equations, known as the Bellman equations, can be solved using a variety of techniques ranging from successive approximation to linear programming In the absence of complete information regarding the transition and reward functions, reinforcement-learning methods can be used to nd optimal value functions. Both modelfree (direct) methods, such as Q-learning and model-based (indirect) methods, such as prioritized sweeping and DYNA have been explored and many have been shown to converge to optimal value functions under the proper conditions .",
               "As we mentioned before, not all reinforcement-learning scenarios of interest can be modeled as mdps. A great deal of reinforcement-learning research has been directed to the problem of solving two-player games , for example, and the reinforcementlearning algorithms for solving mdps and their convergence proofs do not apply directly to games. In one form of two-player game, experiences are of the form hx; a; y; ri, where states x and y contain additional information concerning which player (maximizer or minimizer) gets to choose the action in that state, and the optimality criterion is minimax optimality.",
               "There are deep similarities between mdps and games; for example, it is possible to deene a set of Bellman equations for the optimal minimax value of a two-player zero-sum game, V (x) = ( max a2A R(x; a) + P y P(x; a; y)V (y) ; if maximizer moves in x min a2A (R(x; a) + P x P(x; a; y)V (y)); if minimizer moves in x, where R(x; a) is the reward to the maximizing player. When 0 < 1, these equations have a unique solution and can be solved by successive-approximation methods In addition, we show in this paper that the natural extension of several reinforcement-learning algorithms for mdps converge to optimal value functions in two-player games.",
               "In this paper, we introduce a generalized Markov decision process model with applications to reinforcement learning, and list some of the important results concerning the model. Generalized mdps provide a foundation for the use of reinforcement learning in mdps and games, as well in risk-sensitive reinforcement learning exploration-sensitive reinforcement learning 11], reinforcement learning in simultaneous-action games 13], and other models. Our main theorem addresses the convergence of asynchronous stochastic processes and shows how this problem can be reduced to determining the convergence of a corresponding synchronous one; it can be used to prove the convergence of model-free and model-based reinforcementlearning algorithms under a variety of diierent reinforcement-learning scenarios.",
               "In Section 2, we present generalized mdps, and motivate them using two detailed examples. In Section 3, we describe a stochastic-approximation theorem, and in Section 4 we show several applications of the theorem that prove the convergence of learning processes in generalized mdps."
          ],
          "paper_id": "2115ae10-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 1,
          "fromPaper": "A Generalized Reinforcement-Learning Model: Convergence and Applications A Generalized Reinforcement-Learning Model: Convergence and Applicationa A Generalized Reinforcement-Learning Model: Convergence and Applications"
     },
     {
          "head": {
               "n": "2",
               "text": "THE GENERALIZED MODEL",
               "type": "modelling"
          },
          "paragraphs": [
               "In this section, we introduce our generalized mdp model. We begin by summarizing some of the more signiicant results regarding the standard mdp model and some important results for two-player games."
          ],
          "paper_id": "2115ae10-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 2,
          "fromPaper": "A Generalized Reinforcement-Learning Model: Convergence and Applications A Generalized Reinforcement-Learning Model: Convergence and Applicationa A Generalized Reinforcement-Learning Model: Convergence and Applications"
     },
     {
          "head": {
               "n": "2.1",
               "text": "MARKOV DECISION PROCESSES",
               "type": "modelling"
          },
          "paragraphs": [
               "To provide a point of departure for our generalization of Markov decision processes, we begin by describing some results concerning the use of reinforcement learning in the mdp scenario described earlier. These results are well established; proofs of the unattributed claims can be found in Puterman's mdp book 18].",
               "The ultimate target of learning is an optimal policy. A policy is some function that tells the agent which actions should be chosen under which circumstances. A policy is optimal under the expected discounted total reward criterion if, with respect to the space of all possible policies, maximizes the expected discounted total reward from all states.",
               "Maximizing over the space of all possible policies is practically infeasible. However, mdps have an important property that makes it unnecessary to consider such a broad space of possibilities. We say a policy is stationary and deterministic if it maps directly from states to actions, ignoring everything else, and we write (x) as the action chosen by when the current state is x. In expected discounted total reward mdp environments, there is always a stationary deterministic policy that is optimal; we will therefore use the word \\policy\" to mean stationary deterministic policy, unless otherwise stated.",
               "The value function for a policy , V , maps states to their expected discounted total reward under policy . It can be deened by the simultaneous equations V (x) = R(x; a) + X y P(x; a; y)V (y):",
               "It is also possible to condition the immedate rewards on the state y as well; this is somewhat more general, but complicates the presentation. The optimal value function V is the value function of an optimal policy; it is unique for 0 < "
          ],
          "paper_id": "2115ae10-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 3,
          "fromPaper": "A Generalized Reinforcement-Learning Model: Convergence and Applications A Generalized Reinforcement-Learning Model: Convergence and Applicationa A Generalized Reinforcement-Learning Model: Convergence and Applications"
     },
     {
          "head": {
               "n": "2.2",
               "text": "ALTERNATING MARKOV GAMES",
               "type": "modelling"
          },
          "paragraphs": [
               "In alternating Markov games, two players take turns issuing actions to try to maximize their own expected discounted total reward. The model is deened by the tuple hS 1 ; S 2 ; A; B; P; Ri, where S 1 is the set of states in which player 1 issues actions from the set A, S 2 is the set of states in which player 2 issues actions from the set B, P is the transition function, and R is the reward function for player 1. In the zero-sum games we consider, the rewards to player 2 (the minimizer) are simply the additive inverse of the rewards for player 1 (the maximizer).",
               "Markov decision processes are a special case of alternating Markov games in which S 2 = ;;",
               "Condon 5] proves this and the other unattributed results in this section. A common optimality criterion for alternating Markov games is discounted minimax optimality. Under this criterion, the maximizer should choose actions so as to maximize its reward in the event that the minimizer chooses the best possible counter-policy. An equivalent deenition is for the minimizer to choose actions to minimize its reward against the maximizer with the best possible counter-policy. A pair of policies is said to be in equilibrium if neither player has any incentive to change policies if the other player's policy remains The value function for a pair of equilibrium policies is the optimal value function for the game; it is unique when 0 < 1, and can be found by successive approximation. For both players, there is always a deterministic stationary optimal policy. Any myopic policy with respect to the optimal value function is optimal, and any pair of optimal policies is in equilibrium.",
               "Dynamic-programming operators, Bellman equations, and reinforcement-learning algorithms can be deened for alternating Markov games by starting with the deenitions used in mdps and changing the maximum operators to either maximums or minimums conditioned on the state. We show below that the resulting algorithms share their convergence properties with the analogous algorithms for mdps."
          ],
          "paper_id": "2115ae10-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 4,
          "fromPaper": "A Generalized Reinforcement-Learning Model: Convergence and Applications A Generalized Reinforcement-Learning Model: Convergence and Applicationa A Generalized Reinforcement-Learning Model: Convergence and Applications"
     },
     {
          "head": {
               "n": "2.3",
               "text": "GENERALIZED MDPS",
               "type": "modelling"
          },
          "paragraphs": [
               "In alternating Markov games and mdps, optimal behavior can be speciied by the Bellman equations; any myopic policy with respect to the optimal value function is optimal. In this section, we generalize the Bellman equations to deene optimal behavior for a broad class of reinforcement-learning models. The objective criterion used in these models is additive in that the the value of a policy is some measure of the total reward received.",
               "The generalized Bellman equations can be written",
               "(1)",
               "Here, \\ N \" and \\ L \" represent operators that summarize values over actions as a function of the state x and next states as a function of the state-action pair (x; a), respectively. For Markov decision processes, N a f(x; a) = max a f(x; a) and L y g(x; a; y) = P y P(x; a; y)g(x; a; y). For alternating Markov games, L y g(x; a; y) = P y P(x; a; y)g(x; a; y) and N a f(x; a) = max a f(x; a) or min a f(x; a) depending whether x is in S 1 or S 2 . A large variety of other models can be represented in this framework; several examples are discussed in Section 4.",
               "From a reinforcement-learning perspective, the value functions deened by the generalized mdp model can be interpreted as the total value of the rewards received by an agent selecting actions in a stochastic environment. The agent begins in state x, takes action a, and ends up in state y.  The next section describes a general theorem that can be used to prove the convergence of several reinforcement-learning algorithms for these and other models."
          ],
          "paper_id": "2115ae10-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 5,
          "fromPaper": "A Generalized Reinforcement-Learning Model: Convergence and Applications A Generalized Reinforcement-Learning Model: Convergence and Applicationa A Generalized Reinforcement-Learning Model: Convergence and Applications"
     },
     {
          "head": {
               "n": "3",
               "text": "CONVERGENCE THEOREM",
               "type": "modelling"
          },
          "paragraphs": [
               "The process of nding an optimal value function can be viewed in the following general way. At any moment in time, there is a set of values representing the current approximation of the optimal value function. On each iteration, we apply some dynamic-programming operator, perhaps modiied by experience, to the current approximation to generate a new approximation. Over time, we would like the approximation to tend toward the optimal value function.",
               "In this process, there are two types of approximation going on simultaneously. The rst is an approximation of the dynamic-programming operator for the underlying model, and the second is the use of the approximate dynamic-programming operator to the optimal value function. This section presents a theorem that gives a set of conditions under which this type of simultaneous stochastic approximation converges to an optimal value function.",
               "First, we need to deene the general stochastic process. Let the set X be the states of the model, and the set B (X) of bounded, real-valued functions over X be the set of value functions. Let T : B (X) ! B (X) be an arbitrary contraction mapping and V be the point of T.",
               "If we had direct access to the contraction mapping T, we could use it to successively approximate V . In most reinforcement-learning scenarios, T is not available and we must use our experience to construct approximations of T. Consider a sequence of random operators T t : B (X) ! (B(X) ! B (X)) and deene U t+1 = T t U t ]V where V and U 0 2 B (X) are arbitrary value functions. We say T t approximates T at V with probability 1 uniformly over X, if U t converges to TV uniformly over X 1 . The basic idea is that T t is a randomized version of T in some sense; it uses U t as \\memory\" to help it approximate TV .",
               "The following theorem shows that, under the proper conditions, we can use the sequence T t to estimate the point V of T.",
               "Theorem 1 Let T be an arbitrary mapping with xed point V , and let T t approximate T at V with probability 1 uniformly over X. Let V 0 be an arbitrary value function, and deene V t+1 = T t V t ]V t . If there exist functions 0 F t (x) 1 and 0 G t (x) 1 satisfying the conditions below with probability one, then V t converges to V with probability 1 uniformly over X:",
               "1. for all U 1 , and U 2 2 B (X) and all x 2 X, j((",
               "2. for all U and V 2 B (X), and all x 2 X, j((",
               "3. for all k > 0, n t=k G t (x) converges to zero uniformly in x as n increases; and, 4. there exists 0 < 1 such that for all x 2 X and large enough t, F t (x) (1 ? G t (x)):",
               "Note that from the conditions of the theorem, it follows that T is a contraction operator at V with index of contraction . The theorem is proven in an extended version of this paper We next describe some of the intuition behind the statement of the theorem and its conditions.",
               "The iterative approximation of V is performed by computing V t+1 = t V t ]V t , where T t approximates T with the help of the \\memory\" present in V t . Because of Conditions 1 and 2, G t (x) is the extent to which the estimated value function depends on its present value and F t (x) 1 ? G t (x) is the extent to which the estimated value function is based on \\new\" information (this reasoning becomes clearer in the context of the applications in Section 4).",
               "In some applications, such as Q-learning, the contribution of new information needs to decay over time to insure that the process converges. In this case, G t (x) needs to converge to one. Condition 3 allows G t (x) to converge to 1 as long as the convergence is slow enough to incorporate suucient information for the process to converge.",
               "Condition 4 links the values of G t (x) and F t (x) through some quantity < 1. If it were somehow possible to update the values synchronously over the entire state space, the process would converge to V even when = 1. In the more interesting asynchronous case, when = 1, the long-term behavior of V t is not immediately clear; it may even be that V t converges to something other than V . The requirement that < 1 insures that the use of outdated information in the asynchronous updates does not cause a problem in convergence. One of the most noteworthy aspects of this theorem is that it shows how to reduce the problem of approximating V to the problem of approximating T at a particular point V (in particular, it is enough if T can be approximated at V ); in many cases, the latter is much easier to achieve and also to prove. For example, the theorem makes the convergence of Q-learning a consequence of the classical Robbins-Monro theorem"
          ],
          "paper_id": "2115ae10-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 6,
          "fromPaper": "A Generalized Reinforcement-Learning Model: Convergence and Applications A Generalized Reinforcement-Learning Model: Convergence and Applicationa A Generalized Reinforcement-Learning Model: Convergence and Applications"
     },
     {
          "head": {
               "n": "4",
               "text": "APPLICATIONS",
               "type": "modelling"
          },
          "paragraphs": [
               "This section makes use of Theorem 1 to prove the convergence of various reinforcementlearning algorithms. If y t is randomly selected according to the probability distribution deened by P(x t ; a t ; ), N is a non-expansion, and both the expected value and the variance of N a Q(y t ; a) exist given the way y t is sampled, r t has variance and expected value given x t and a t equal to R(x t ; a t ), the learning rates are decayed so that P t (x t = x; a t = a) t (x; a) = 1 and P t (x t = x; a t = a) t (x; a) 2 < 1 with probability 1 uniformly over X A 2 , then a standard result from the theory of stochastic approximation 20] states that T t approximates T with probability 1 uniformly over X A. That is, this method of using a decayed, exponentially weighted average correctly computes the average one-step reward. Let G t (x; a) = ( 1 ? t (x; a); if x = x t and a = a t ;"
          ],
          "paper_id": "2115ae10-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 7,
          "fromPaper": "A Generalized Reinforcement-Learning Model: Convergence and Applications A Generalized Reinforcement-Learning Model: Convergence and Applicationa A Generalized Reinforcement-Learning Model: Convergence and Applications"
     },
     {
          "head": {
               "n": "4.1",
               "text": "GENERALIZED Q-LEARNING FOR EXPECTED VALUE MODELS",
               "type": "modelling"
          },
          "paragraphs": [
               "0; otherwise, and F t (x; a) = ( t (x; a); if x = x t and a = a t ;",
               "0; otherwise. These functions satisfy the conditions of Theorem 1 (Condition 3 is implied by the restrictions placed on the sequence of learning rates t ).",
               "Theorem 1 therefore implies that this generalized Q-learning algorithm converges to the optimal Q function with probability 1 uniformly over X A. The convergence of Q-learning for discounted mdps and alternating Markov games follows trivially from this. Extensions of this result for undiscounted \\all-policies-proper\" mdps a soft state aggregation learning rule and a \\spreading\" learning rule 19] are given in an extended version of this paper 28]."
          ],
          "paper_id": "2115ae10-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 8,
          "fromPaper": "A Generalized Reinforcement-Learning Model: Convergence and Applications A Generalized Reinforcement-Learning Model: Convergence and Applicationa A Generalized Reinforcement-Learning Model: Convergence and Applications"
     },
     {
          "head": {
               "n": "4.2",
               "text": "Q-LEARNING FOR MARKOV GAMES",
               "type": "modelling"
          },
          "paragraphs": [
               "Markov games are a generalization of mdps and alternating Markov games in which both players simultaneously choose actions at each step. The basic model was developed by Shapley and is deened by the tuple hS; A; B; P; Ri and discount factor . As in alternating Markov games, the optimality criterion is one of discounted minimax optimality, but because the players move simultaneously, the Bellman equations take on a more complex form:",
               "In these equations, R(x; a; b) is the immediate reward for the maximizer for taking action a in state x at the same time the minimizer takes action b, P(x; a; b; y) is the probability that state y is reached from state x when the maximizer takes action a and the minimizer takes action b, and represents the set of discrete probability distributions over the set A. The sets S, A, and B are Once again, optimal policies are policies that are in equilibrium, and there is always a pair of optimal policies that are stationary. Unlike mdps and alternating Markov games, the optimal policies are sometimes stochastic; there are Markov games in which no deterministic policy is optimal. The stochastic nature of optimal policies explains the need for the optimization over probability distributions in the Bellman equations, and stems from the fact that players must avoid being \\second guessed\" during action selection. An equivalent set of equations can be written with a stochastic choice for the minimizer, and also with the roles of the maximizer and minimizer reversed.",
               "The Q-learning update rule for Markov games given step t experience hx t ; a t ; b t ; y t ; r t i has the form Q t+1 (x t ; a t ; b t ) := (1 ? t (x t ; a t ; b t ))Q t (x t ; a t ; b t ) + t (x t ; a t ; b t ) The results of the previous section prove that this rule converges to the optimal Q function under the proper conditions."
          ],
          "paper_id": "2115ae10-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 9,
          "fromPaper": "A Generalized Reinforcement-Learning Model: Convergence and Applications A Generalized Reinforcement-Learning Model: Convergence and Applicationa A Generalized Reinforcement-Learning Model: Convergence and Applications"
     },
     {
          "head": {
               "n": "4.3",
               "text": "RISK-SENSITIVE MODELS",
               "type": "modelling"
          },
          "paragraphs": [
               "Heger described an optimality criterion for mdps in which only the worst possible value of the next state makes a contribution to the value of a state. An optimal policy under this criterion is one that avoids states for which a bad outcome is possible, even if it is not probable; for this reason, the criterion has a risk-averse quality to it. The generalized Bellman equations for this criterion are ! :",
               "The argument in Section 4.5 shows that model-based reinforcement learning can be used to nd optimal policies in risk-sensitive models, as long as N does not depend on R or P, and P is estimated in a way that preserves its zero vs. non-zero nature in the limit. For the model in which N a f(x; a) = max a f(x; a), Heger deened a Q-learning-like algorithm that converges to optimal policies without estimating R and P online. In essence, the learning algorithm uses an update rule analogous to the rule in Q-learning with the additional requirement that the initial Q function be set optimistically; that is, Q 0 (x; a) must be larger than Q (x; a) for all x and a. Like Q-learning, this learning algorithm is a generalization of Korf's LRTA* algorithm for stochastic environments.",
               "Using Theorem 1 it is possible to prove the convergence of a generalization of Heger's algorithm to models where N a f(x; a) = f(x; a (f; x)) for some function a (); that is, as long as the summary value of f(x; a) is equal to f(x; a ) for some a . The proof is based on estimating the Q-learning algorithm from above by an appropriate process where the Q function is updated only if the received experience tuple is an extremity according to the optimality equation; details are given in the extended paper 28]."
          ],
          "paper_id": "2115ae10-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 10,
          "fromPaper": "A Generalized Reinforcement-Learning Model: Convergence and Applications A Generalized Reinforcement-Learning Model: Convergence and Applicationa A Generalized Reinforcement-Learning Model: Convergence and Applications"
     },
     {
          "head": {
               "n": "4.4",
               "text": "EXPLORATION-SENSITIVE MODELS",
               "type": "modelling"
          },
          "paragraphs": [
               "John 11] considered the implications of insisting that reinforcement-learning agents keep exploring forever; he found that better learning performance can be achieved if the Qlearning rule is changed to incorporate the condition of persistent exploration. In John's formulation, the agent is forced to adopt a policy from a restricted set; in one example, the agent must choose a stochastic stationary policy that selects actions at random 5% of the time.",
               "This approach requires that the deenition of optimality be changed to reeect the restric- which corresponds to a generalized mdp model with L y g(x; a; y) = P y P(x; a; y)g(x; a; y) and N a f(x; a) = sup 2P 0 P a (x; a)f(x; a). ) is a probability distribution for any given state x, N is a non-expansion and, thus, the convergence of the associated Qlearning algorithm follows from the arguments in Section 4.1. As a result, John's learning rule gives the optimal policy under the revised optimality criterion."
          ],
          "paper_id": "2115ae10-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 11,
          "fromPaper": "A Generalized Reinforcement-Learning Model: Convergence and Applications A Generalized Reinforcement-Learning Model: Convergence and Applicationa A Generalized Reinforcement-Learning Model: Convergence and Applications"
     },
     {
          "head": {
               "n": "4.5",
               "text": "MODEL-BASED METHODS",
               "type": "modelling"
          },
          "paragraphs": [
               "The deening assumption in reinforcement learning is that the reward and transition functions, R and P, are not known in advance. Although Q-learning shows that optimal value functions can be estimated without ever explicitly learning R and P, learning R and P makes more eecient use of experience at the expense of additional storage and computation 15].",
               "The parameters of R and P can be learned from experience by keeping statistics for each state-action pair on the expected reward and the proportion of transitions to each next state. In model-based reinforcement learning, R and P are estimated on-line, and the value function is updated according to the approximate dynamic-programming operator derived from these estimates. Theorem 1 implies the convergence of a wide variety of model-based reinforcement-learning methods. The dynamic-programming operator deening the optimal value for generalized mdps is given in Equation 2. Here we assume that L may depend on P and/or R, but N may not. It is possible to extend the following argument to allow N to depend on P and R as well. In model-based reinforcement learning, R and P are estimated by the quantities R t and P t , and L t is an estimate of the L operator deened using R t and P t . As long as every state-action pair is visited innnitely often, there are a number of simple methods for computing R t and P t that converge to R and P. A bit more care is needed to insure that L t converges to L , however. For example, in expected-reward models, L y g(x; a; y) = P y P(x; a; y)g(x; a; y) and the convergence of P t to P guarantees the convergence of L t to L . On the other hand, in a risk-sensitive model, L y g(x; a; y) = min y:P(x;a;y)>0 g(x; a; y) and it is necessary to approximate P in a way that insures that the set of y such that P t (x; a; y) > 0 converges to the set of y such that P(x; a; y) > 0. This can be accomplished easily, for example, by setting P t (x; a; y) = 0 if no transition from x to y under a has been observed.",
               "Assuming P and R can be estimated in a way that results in the convergence of L t to L , the approximate dynamic-programming operator T t deened by",
               "; if x 2 t U(x); otherwise, converges to T with probability 1 uniformly. Here, the set t S represents the set of states whose values are updated on step t; one popular choice is to set t = fx t g. 1; otherwise, and F t (x) = ( if x 2 t ; 0; otherwise, satisfy the conditions of Theorem 1 as long as each x is in innnitely many t sets (Condition 3) and the discount factor is less than 1 (Condition 4).",
               "As a consequence of this argument and Theorem 1, model-based methods can be used to nd optimal policies in mdps, alternating Markov games, Markov games, risk-sensitive mdps, and exploration-sensitive mdps. Also, if R t = R and P t = P for all t, this result implies that real-time dynamic programming converges to the optimal value function"
          ],
          "paper_id": "2115ae10-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 12,
          "fromPaper": "A Generalized Reinforcement-Learning Model: Convergence and Applications A Generalized Reinforcement-Learning Model: Convergence and Applicationa A Generalized Reinforcement-Learning Model: Convergence and Applications"
     },
     {
          "head": {
               "n": "5",
               "text": "CONCLUSIONS",
               "type": "conclusion"
          },
          "paragraphs": [
               "In this paper, we presented a generalized model of Markov decision processes, and proved the convergence of several reinforcement-learning algorithms in the generalized model.",
               "Other Results We have derived a collection of results for the generalized mdp model that demonstrate its general applicability: the Bellman equations can be solved by value iteration; a myopic policy with respect to an approximately optimal value function gives an approximately optimal policy 9]; when N has a particular \\maximization\" property, policy iteration converges to the optimal value function; and, for models with nite state and action spaces, both value iteration and policy iteration identify optimal policies in pseudopolynomial time.",
               "Related Work The work presented here is closely related to several previous research eeorts. Szepesvv ari 27] described a related generalized reinforcement-learning model, and presented conditions under which there is an optimal (stationary) policy that is myopic with respect to the optimal value function. Jaakkola, Jordan, and Singh 10] and Tsitsiklis 31] developed the connection between stochastic-approximation theory and reinforcement learning in mdps. Our work is similar in spirit to that of Jaakkola, et al. We believe the form of Theorem 1 makes it particularly convenient for proving the convergence of reinforcement-learning algorithms; our theorem reduces the proof of the convergence of an asynchronous process to a simpler proof of convergence of a corresponding synchronized one. This idea enables us to prove the convergence of asynchronous stochastic processes whose underlying synchronous process is not of the Robbins-Monro type (e.g., risk-sensitive mdps, model-based algorithms, etc.).",
               "Future Work There are many areas of interest in the theory of reinforcement learning that we would like to address in future work. The results in this paper primarily concern reinforcement-learning in contractive models ( < 1 or all-policies-proper), and there are important non-contractive reinforcement-learning scenarios, for example, reinforcement learning under an average-reward criterion 14]. It would be interesting to develop a TD() algorithm for generalized mdps; this has already been done for mdps Theorem 1 is not restricted to nite state spaces, and it might be valuable to prove the convergence of a reinforcement-learning algorithm for a innnite state-space model. Conclusion By identifying common elements among several reinforcement-learning scenarios, we created a new class of models that generalizes existing models in an interesting way. In the generalized framework, we replicated the established convergence proofs for reinforcement learning in Markov decision processes, and proved new results concerning the convergence of reinforcement-learning algorithms in game environments, under a risksensitive assumption, and under an exploration-sensitive assumption. At the heart of our results is a new stochastic-approximation theorem that is easy to apply to new situations."
          ],
          "paper_id": "2115ae10-97d4-11e8-9580-1f0eb29018a9",
          "paragraphNo": 13,
          "fromPaper": "A Generalized Reinforcement-Learning Model: Convergence and Applications A Generalized Reinforcement-Learning Model: Convergence and Applicationa A Generalized Reinforcement-Learning Model: Convergence and Applications"
     }
]